{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from transformer_lens import HookedTransformer\n",
    "from jaxtyping import Float, Int, Bool\n",
    "from torch import Tensor\n",
    "from tqdm.auto import tqdm\n",
    "import plotly.io as pio\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px \n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from IPython.display import display, HTML\n",
    "from datasets import load_dataset\n",
    "from collections import Counter\n",
    "import pickle\n",
    "import os\n",
    "import plotly.graph_objects as go\n",
    "from scipy.stats import norm, variation, skew, kurtosis\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import json\n",
    "from tqdm import trange\n",
    "\n",
    "import haystack_utils\n",
    "from haystack_utils import get_mlp_activations\n",
    "from hook_utils import get_ablate_neuron_hook, save_activation\n",
    "from pythia_160m_utils import get_neuron_accuracy, ablation_effect\n",
    "import plotting_utils\n",
    "from plotting_utils import plot_neuron_acts, color_binned_histogram\n",
    "\n",
    "pio.renderers.default = \"notebook_connected+notebook\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.autograd.set_grad_enabled(False)\n",
    "torch.set_grad_enabled(False)\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model roneneldan/TinyStories-1M into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "model = HookedTransformer.from_pretrained(\"roneneldan/TinyStories-1M\",\n",
    "    center_unembed=True,\n",
    "    center_writing_weights=True,\n",
    "    fold_ln=True,\n",
    "    device=\"cuda\")\n",
    "\n",
    "with open('data/TinyStories-train.txt', 'r') as f:\n",
    "    full_data = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14815490\n"
     ]
    }
   ],
   "source": [
    "print(len(full_data))\n",
    "prompts = [data[:400] for data in full_data[:200]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7357\n"
     ]
    }
   ],
   "source": [
    "tokens_set = set()\n",
    "for prompt in full_data[:20000]:\n",
    "    tokens = model.to_tokens(prompt).flatten().tolist()\n",
    "    tokens_set.update(tokens)\n",
    "\n",
    "\n",
    "print(len(tokens_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "\n",
    "N = 10_000\n",
    "\n",
    "# Resize the tokenizer vocabulary to the top 10,000 tokens\n",
    "# restricted_vocab = sorted(model.tokenizer.get_vocab().items(), key=operator.itemgetter(1))[:N]\n",
    "# model.tokenizer = {k: v for k, v in restricted_vocab}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "def get_neuron_token_variance(\n",
    "        model: HookedTransformer, data: list[str],\n",
    "        disable_tqdm=True, hook_pre=False\n",
    ") -> pd.DataFrame:\n",
    "    '''\n",
    "    Get the variance of the activations of a neuron for each token in the dataset.\n",
    "    This looks like a tensor of tokens and a tensor of activations, and building:\n",
    "      a tensor of token x activation_sum (10000 x d_mlp)\n",
    "      a tensor of token_count\n",
    "      repeat layer by layer (or do it all at once if we have memory)\n",
    "    '''\n",
    "    token_neuron_acts = torch.zeros(model.cfg.n_layers, model.cfg.d_vocab, model.cfg.d_mlp).cuda()\n",
    "    token_neuron_acts_squared = torch.zeros(model.cfg.n_layers, model.cfg.d_vocab, model.cfg.d_mlp).cuda()\n",
    "\n",
    "    tokens = torch.empty(0, dtype=int).cuda()\n",
    "    for item in data:\n",
    "        tokens = torch.cat([tokens, model.to_tokens(item)[0]], dim=0)\n",
    "    tokens = tokens.flatten()\n",
    "    token_counts = torch.bincount(tokens, minlength=model.cfg.d_vocab).repeat(model.cfg.d_mlp, 1).reshape(-1, model.cfg.d_mlp)\n",
    "\n",
    "    for layer in trange(model.cfg.n_layers):\n",
    "        acts = get_mlp_activations(data, layer, model, mean=False, disable_tqdm=disable_tqdm, hook_pre=hook_pre, \n",
    "                                   context_crop_start=0, context_crop_end=400)\n",
    "        print(\"acts nan\", acts.isnan().sum())\n",
    "        for i in range(tokens.shape[0]):\n",
    "            token_neuron_acts[layer, tokens[i]] += acts[i]\n",
    "            token_neuron_acts_squared[layer, tokens[i]] += acts[i] ** 2\n",
    "\n",
    "        mean_acts = token_neuron_acts[layer] / token_counts + 1e-8\n",
    "        print(\"mean acts nan\", mean_acts.isnan().sum())\n",
    "        mean_acts_squared = token_neuron_acts_squared[layer] / token_counts + 1e-8\n",
    "        var_acts = mean_acts_squared - mean_acts ** 2\n",
    "        token_neuron_acts[layer] = var_acts\n",
    "\n",
    "    return token_neuron_acts\n",
    "\n",
    "token_neuron_acts = get_neuron_token_variance(model, full_data[:4000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in token_neuron_acts.keys():\n",
    "    with open(f'data/neuron_token_vars_{layer}', 'wb') as f:\n",
    "        pickle.dump(token_neuron_acts[layer], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7357\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([7357, 256])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_token_indices = list(tokens_set)\n",
    "print(len(valid_token_indices))\n",
    "token_neuron_acts[0][valid_token_indices, :].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5.4765e-05, -4.8202e-02,         nan,         nan,         nan,\n",
       "                nan, -1.1680e+01,         nan,         nan,         nan,\n",
       "                nan,  7.7938e-02, -1.8480e-02,  1.0072e-03,         nan,\n",
       "                nan,         nan,         nan,         nan,         nan,\n",
       "                nan,         nan,         nan,         nan,         nan,\n",
       "        -9.1745e-01, -8.1557e+01,         nan,         nan,         nan,\n",
       "        -3.7388e-01,         nan, -3.1596e+01, -8.5876e+01, -3.8638e+01,\n",
       "                nan, -3.8681e+01,         nan, -1.4826e+03, -4.1907e+03,\n",
       "        -6.4918e-02, -1.4258e+01, -6.9301e+02, -1.4280e+01, -2.1543e-02,\n",
       "        -4.6847e+03, -2.2924e+00, -3.9207e+02,         nan, -9.0109e+01,\n",
       "        -7.6054e+01, -2.5891e+02,         nan,         nan, -1.6895e+03,\n",
       "                nan, -1.8139e+02, -3.8759e+02,         nan,         nan,\n",
       "                nan,         nan,         nan,         nan,  1.2338e-03,\n",
       "        -1.2670e+01, -1.0005e+02, -3.6061e+03, -5.7064e+01,         nan,\n",
       "        -7.9731e+01, -3.0256e+02, -3.5717e+00,         nan,         nan,\n",
       "                nan, -2.4020e+01, -3.3758e+03, -7.3525e+01,         nan,\n",
       "                nan, -1.1339e+04, -1.6170e+00, -3.8446e+00,         nan,\n",
       "                nan,         nan,         nan, -6.6096e-01, -6.0159e+03,\n",
       "                nan,         nan,         nan,         nan,         nan,\n",
       "                nan,         nan,         nan,         nan,         nan,\n",
       "                nan,         nan,         nan,         nan,         nan,\n",
       "                nan,         nan,         nan,         nan,         nan,\n",
       "                nan,         nan,         nan,         nan,         nan,\n",
       "                nan,         nan,         nan,         nan,         nan,\n",
       "                nan,         nan,         nan,         nan,         nan,\n",
       "                nan,         nan,         nan,         nan,         nan,\n",
       "                nan,         nan,         nan,         nan,         nan,\n",
       "                nan,         nan,         nan,         nan,         nan,\n",
       "                nan,         nan,         nan,         nan,         nan,\n",
       "                nan,         nan,         nan,         nan,         nan,\n",
       "                nan,         nan,         nan,         nan,         nan,\n",
       "                nan,         nan,         nan,         nan,         nan,\n",
       "                nan,         nan,         nan,         nan,         nan,\n",
       "                nan,         nan,         nan,         nan,         nan,\n",
       "                nan,         nan,         nan,         nan,         nan,\n",
       "                nan,         nan,         nan,         nan,         nan,\n",
       "                nan,         nan,         nan,         nan,         nan,\n",
       "                nan,         nan,         nan,         nan,         nan,\n",
       "                nan,         nan,         nan,         nan,         nan,\n",
       "                nan,         nan,         nan,  8.0860e-03,         nan,\n",
       "                nan,         nan,         nan,         nan,         nan,\n",
       "                nan,         nan,         nan,         nan,         nan,\n",
       "                nan,         nan,         nan,         nan,         nan,\n",
       "                nan,         nan,         nan,         nan,         nan,\n",
       "         4.7388e-04,         nan,         nan,         nan,         nan,\n",
       "                nan,         nan,         nan,         nan,         nan,\n",
       "                nan,         nan,         nan,         nan,         nan,\n",
       "                nan,         nan,         nan,         nan,         nan,\n",
       "                nan,         nan,         nan,         nan,         nan,\n",
       "                nan,         nan, -5.3355e-02,         nan,         nan,\n",
       "        -1.8545e-01, -4.3392e-02,         nan,         nan,         nan,\n",
       "                nan], device='cuda:0')"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_neuron_acts[0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensors = [torch.tensor([1, 2]), torch.tensor([3, 4, 5]), torch.tensor([6])]\n",
    "lengths = [len(tensor) for tensor in tensors]\n",
    "result = torch.empty(sum(lengths), dtype=torch.int64)\n",
    "\n",
    "start = 0\n",
    "for tensor in tensors:\n",
    "    end = start + len(tensor)\n",
    "    result[start:end] = tensor\n",
    "    start = end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# large_acts_df = get_neuron_moments(model, prompts, [])\n",
    "# layer_neuron_tuple = large_acts_df.sort_values('skew', ascending=False).iloc[:1][['layer', 'neuron']].values.tolist()[0]\n",
    "# layer, neuron = layer_neuron_tuple\n",
    "\n",
    "# hook_name = f'blocks.{layer}.mlp.hook_post'\n",
    "# with model.hooks([(hook_name, save_activation)]):\n",
    "#     model(prompts[-1])\n",
    "# acts = model.hook_dict[hook_name].ctx['activation']\n",
    "# haystack_utils.clean_print_strings_as_html(model.to_str_tokens(prompts[-1]), acts[0, :, neuron], max_value=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
