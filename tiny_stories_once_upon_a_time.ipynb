{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformer_lens import HookedTransformer\n",
    "from jaxtyping import Float\n",
    "from torch import Tensor\n",
    "import plotly.io as pio\n",
    "import numpy as np\n",
    "from tqdm import trange\n",
    "\n",
    "pio.renderers.default = \"notebook_connected+notebook\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.autograd.set_grad_enabled(False)\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "import haystack_utils\n",
    "import hook_utils\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/TinyStories-train.txt: Loaded 14815490 examples with 0 to 4928 characters each.\n"
     ]
    }
   ],
   "source": [
    "data = haystack_utils.load_txt_data('data/TinyStories-train.txt')[:500]\n",
    "filtered_prompts = [prompt for prompt in data if not prompt.startswith(\"Once\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model roneneldan/TinyStories-1M into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "model = HookedTransformer.from_pretrained(\"roneneldan/TinyStories-1M\",\n",
    "    center_unembed=True,\n",
    "    center_writing_weights=True,\n",
    "    fold_ln=True,\n",
    "    device=\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Once', 'One', 'Yesterday', 'At', 'There', 'Today', 'On', 'Em', 'When', 'In']\n",
      "[' upon', 'bie', 'ancer', 'itting', 'll', 'aj', 'packs', 'upon', 'uttering', ' haven']\n",
      "[' a', ' an', ' the', ' some', ' two', ' something', ' another', ' one', ' very', ' many']\n",
      "[' time', ' day', ' week', ' evening', ' morning', ' afternoon', ' night', ' Sunday', ' way', ' year']\n"
     ]
    }
   ],
   "source": [
    "# Similar words stored near in the embed\n",
    "tokens = model.to_tokens(\"Once upon a time\", prepend_bos=False)\n",
    "logits = model.W_E[tokens].squeeze(0) @ model.W_U\n",
    "\n",
    "for i in range(4):\n",
    "    values, indices = torch.topk(logits[i], 10)\n",
    "    print(model.to_str_tokens(indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1094, device='cuda:0') tensor(0.2133, device='cuda:0')\n",
      "['cow', 'ns', 'ridge', 'wo', ' seventeen', 'eight', 'umbers', 'please', 'ucks', 'icians', 'oxy', 'bread', 'Seven', 'uns', 'bang', 'uggets', 'oster', 'wy', 'Ta', 'arella', 'irteen', ' reins', 'vals', ' fries', 'heddar', 'nine', 'aff', 'zee', 'rots', 'six', 'ban', 'bet', 'Six', 'Four', 'umbs', 'arians', 'Twenty', 'gary', 'cards', 'urger', 'workers', 'Sal', 'apple', 'rito', 'ulent', 'haw', 'cream', 'ction', 'leaf', 'load', 'agna', 'shirts', 'ksh', 'icy', 'keys', 'Apple', 'ants', 'trained', 'orted', 'ender', 'gob', 'chini', 'ocol', ' ounces', 'talk', 'rob', 'Dragon', 'chip', 'amer', 'plets', 'fed', 'iri', 'pill', 'mons', 'int', ' cents', 'reath', 'cker', 'pper', 'war', 'rol', 'Ah', 'osaurus', 'Ten', 'dollar', 'girls', 'seven', 'mur', 'affles', 'Offic', 'ws', 'ounced', 'ucker', 'iw', 'eating', 'ates', 'Ar', 'Chuck', 'Eight', 'mus']\n"
     ]
    }
   ],
   "source": [
    "# Associations\n",
    "tokens = model.to_tokens(\"cow\", prepend_bos=False)\n",
    "logits = (model.W_E[tokens].squeeze(0) @ model.W_U).squeeze(0)\n",
    "mean_logit = logits.median() # mean is lower\n",
    "\n",
    "associated_tokens = []\n",
    "for string in \"grass brown field moo meat milk udders animal\".split(' '):\n",
    "    token = model.to_tokens(string, prepend_bos=False)\n",
    "    associated_tokens.append(token[0, 0])\n",
    "associated_tokens = torch.stack(associated_tokens)\n",
    "mean_associated_logit = logits[associated_tokens].median() # mean is similar\n",
    "\n",
    "print(mean_logit, mean_associated_logit)\n",
    "\n",
    "values, indices = torch.topk(logits, 100)\n",
    "print(model.to_str_tokens(indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.0134, device='cuda:0')\n",
      "tensor([ 0.8695, -0.3610,  1.9775], device='cuda:0')\n",
      "[' a', ' an', ' the', ' some', ' two', ' something', ' another', ' one', ' very', ' many', ' big', ' her', ' so', ' his', ' all', ' in', ' lots', ' to', ' more', ' three', ' four', ' it', ' their', ' on', ' out', ' there', ' small', ' too', ' that', ' and', ' not', '.', ' no', ' good', ' with', ' right', ' really', ' both', ' nice', ' different', ' only', ' five', ',', '\\n', ' like', ' hard', ' A', ' bright', ' just', ' when', ' much', ' little', ' long', ' cool', ' brave', ' someone', ' fast', ' strong', ' Lily', ' funny', ' this', ' look', ' warm', ' made', ' fun', ' wide', ' new', ' as', ' silly', ' red', ' hot', ' each', ' close', ' open', ' pretty', ' Mom', ' what', ' Tom', ' other', ' its', ' even', ' far', ' tall', ' The', ' shiny', ' dark', ' loud', ' white', ' deep', ' soft', ' fair', ' black', ' j', ' outside', ' kind', ' your', 'A', ' for', ' help', ' happy']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tokens = model.to_tokens(\"Once upon a time\", prepend_bos=False).squeeze(0)\n",
    "logits = (model.W_E[tokens] @ model.W_U)\n",
    "median_logit = logits.median()\n",
    "print(median_logit)\n",
    "\n",
    "next_logits = []\n",
    "for i in range(tokens.shape[0] - 1):\n",
    "    next_logits.append(logits[i, tokens[i + 1]])\n",
    "next_logits = torch.stack(next_logits)\n",
    "print(next_logits)\n",
    "\n",
    "# 'a' token seems to boost some sensible completions\n",
    "values, indices = torch.topk(logits[-2], 100)\n",
    "print(model.to_str_tokens(indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'prompts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Generate heaps of token sequences and average their cache, then find the difference with the OUAT neurons\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m# Ablate components until OUAT fails\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m# n-l AND?\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m _, cache \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mrun_with_cache(model\u001b[39m.\u001b[39mto_tokens(prompts)[:, :\u001b[39m40\u001b[39m])\n\u001b[1;32m      7\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrand_hook\u001b[39m(value, hook):\n\u001b[1;32m      8\u001b[0m     cache_val \u001b[39m=\u001b[39m cache[hook\u001b[39m.\u001b[39mname][:value\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m), :value\u001b[39m.\u001b[39msize(\u001b[39m1\u001b[39m), :value\u001b[39m.\u001b[39msize(\u001b[39m2\u001b[39m)]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'prompts' is not defined"
     ]
    }
   ],
   "source": [
    "# Generate heaps of token sequences and average their cache, then find the difference with the OUAT neurons\n",
    "# Ablate components until OUAT fails\n",
    "# n-l AND?\n",
    "\n",
    "_, cache = model.run_with_cache(model.to_tokens(prompts)[:, :40])\n",
    "\n",
    "def rand_hook(value, hook):\n",
    "    cache_val = cache[hook.name][:value.size(0), :value.size(1), :value.size(2)]\n",
    "    mean_val = cache_val.mean(dim=0)\n",
    "    broadcasted_val = mean_val.unsqueeze(0).expand_as(value)\n",
    "    value = broadcasted_val\n",
    "    return value\n",
    "\n",
    "prompt = \"Once\"\n",
    "print(model.generate(prompt, 20, temperature=0, use_past_kv_cache=False))\n",
    "for layer in range(model.cfg.n_layers):\n",
    "    with model.hooks([(f'blocks.{layer}.hook_attn_out', rand_hook)]):\n",
    "        print(f\"Attn {layer}\", model.generate(prompt, 20, temperature=0, use_past_kv_cache=False))\n",
    "    with model.hooks([(f'blocks.{layer}.hook_mlp_out', rand_hook)]):\n",
    "        print(f\"MLP {layer}\", model.generate(prompt, 20, temperature=0, use_past_kv_cache=False))\n",
    "\n",
    "# Necessary components:\n",
    "# MLP0, MLP1, MLP2, MLP3, MLP4, MLP5, MLP7 (every MLP but 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ablate by cosine sim - degrades around 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure out which neurons directly write to each vocab\n",
    "# And see if we can ablate everything else\n",
    "\n",
    "tokens = model.to_tokens(\"Once upon a time\", prepend_bos=False)  # [1, 4]\n",
    "token_dirs = model.tokens_to_residual_directions(tokens)[0]  # [4, 64]\n",
    "token_dirs_reshaped = token_dirs.unsqueeze(1).unsqueeze(1)  # [4, 1, 1, 64]\n",
    "W_out_reshaped = model.W_out.unsqueeze(0)  # [1, 8, 256, 64]\n",
    "\n",
    "cosine_sim = torch.nn.CosineSimilarity(dim=-1, eps=1e-6)\n",
    "result = cosine_sim(token_dirs_reshaped, W_out_reshaped)  # [4, 8, 256]\n",
    "\n",
    "_, cache = model.run_with_cache(tokens)\n",
    "acts = [cache[f'blocks.{layer}.mlp.hook_post'][0] for layer in range(model.cfg.n_layers)] # [[batch pos]]*n_layers\n",
    "acts = torch.stack(acts, dim=1) # \n",
    "\n",
    "\n",
    "layer_neuron_tuples = []\n",
    "for token_index in range(result.size(0)):\n",
    "    values, indices = torch.topk(result[token_index].view(-1), 20, dim=-1)\n",
    "    layer_indices, neuron_indices = np.unravel_index(indices.cpu().numpy(), (8, 256))\n",
    "    layer_neuron_tuples.extend(zip(layer_indices.tolist(), neuron_indices.tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6df5f72e6c14347b2c3e120afac58fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once in a time, a little girl named Amy was\n"
     ]
    }
   ],
   "source": [
    "layer_neuron_dict = haystack_utils.get_neurons_by_layer(layer_neuron_tuples)\n",
    "\n",
    "sorted_tuples = []\n",
    "sorted_acts = []\n",
    "\n",
    "for layer in layer_neuron_dict.keys():\n",
    "    neurons = layer_neuron_dict[layer]\n",
    "    mean_acts = haystack_utils.get_mlp_activations(filtered_prompts, layer, model, context_crop_start=2, hook_pre=False, neurons=neurons, disable_tqdm=True)\n",
    "    sorted_tuples.extend([(layer, neuron) for neuron in neurons])\n",
    "    sorted_acts.extend(mean_acts)\n",
    "    assert len(sorted_tuples) == len(sorted_acts)\n",
    "\n",
    "hooks = hook_utils.get_ablate_context_neurons_hooks(sorted_tuples, sorted_acts)\n",
    "with model.hooks(hooks):\n",
    "    print(model.generate(\"Once\", 10, temperature=0, use_past_kv_cache=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ablate by DLA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batched_dot_product(x, y):\n",
    "    return torch.vmap(torch.dot)(x, y)\n",
    "    \n",
    "\n",
    "def neuron_DLA(prompt: str, model: HookedTransformer, pos=np.s_[-1:]) -> tuple[Float[Tensor, \"component\"], list[str]]:\n",
    "    '''Gets full resid decomposition including all neurons'''\n",
    "    tokens = model.to_tokens(prompt)\n",
    "    answers = tokens[:, 1:]\n",
    "    tokens = tokens[:, :-1]\n",
    "    \n",
    "    _, cache = model.run_with_cache(tokens)\n",
    "    attrs, labels = cache.get_full_resid_decomposition(-1, expand_neurons=True, apply_ln=True, return_labels=True, pos_slice=pos)\n",
    "    answer_residual_directions = model.tokens_to_residual_directions(answers)\n",
    "    if answer_residual_directions.ndim == 1:\n",
    "        answer_residual_directions = answer_residual_directions.unsqueeze(0)  # [1 d_model]\n",
    "    elif answer_residual_directions.ndim == 3:\n",
    "        answer_residual_directions = answer_residual_directions[0]  # [pos d_model]\n",
    "    answer_residual_directions = answer_residual_directions[pos]  # [pos d_model]\n",
    "    neuron_indices = [i for i in range(len(labels)) if 'N' in labels[i]]\n",
    "    neuron_labels = [labels[i] for i in neuron_indices]\n",
    "    neuron_attrs = attrs[neuron_indices, :].squeeze(1)\n",
    "    results = []\n",
    "    for i in range(neuron_attrs.shape[1]):\n",
    "        results.append(batched_dot_product(neuron_attrs[:, i], answer_residual_directions[[i]].repeat(neuron_attrs.shape[0], 1)))\n",
    "    return torch.stack(results), neuron_labels\n",
    "\n",
    "def get_neuron_mean_acts(dla_layer_neuron_tuples: list[tuple[int, int]]) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    layer_neuron_dict = haystack_utils.get_neurons_by_layer(dla_layer_neuron_tuples)\n",
    "\n",
    "    sorted_dla_layer_neuron_tuples = []\n",
    "    sorted_acts = []\n",
    "\n",
    "    for layer in layer_neuron_dict.keys():\n",
    "        neurons = layer_neuron_dict[layer]\n",
    "        mean_acts = haystack_utils.get_mlp_activations(filtered_prompts[:200], layer, model, context_crop_start=0, hook_pre=False, neurons=neurons, disable_tqdm=True)\n",
    "        sorted_dla_layer_neuron_tuples.extend([(layer, neuron) for neuron in neurons])\n",
    "        sorted_acts.extend(mean_acts)\n",
    "        assert len(sorted_dla_layer_neuron_tuples) == len(sorted_acts)\n",
    "\n",
    "    return sorted_dla_layer_neuron_tuples, sorted_acts\n",
    "\n",
    "def get_unspecified_neurons(model: HookedTransformer, neurons: list[tuple[int, int]]):\n",
    "    layer_dict = haystack_utils.get_neurons_by_layer(neurons)\n",
    "    unspecified = []\n",
    "    for layer in range(model.cfg.n_layers):\n",
    "        for neuron in range(model.cfg.d_mlp):\n",
    "            if not neuron in layer_dict[layer]:\n",
    "                unspecified.append((layer, neuron))\n",
    "    return unspecified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    }
   ],
   "source": [
    "# High level viz\n",
    "\n",
    "# x, y = haystack_utils.DLA([\"Once upon\"], model)\n",
    "# haystack_utils.line(x.cpu().squeeze(0))\n",
    "\n",
    "attrs, labels = neuron_DLA(\"Once upon a time\", model, pos=np.s_[-4:])\n",
    "# haystack_utils.line(attrs[0].cpu().numpy(), xlabel=\"Correct logit\", ylabel=\"\", title=\"DLA per neuron in layer\")\n",
    "\n",
    "# print(attrs.sum())\n",
    "# px.histogram(attrs.flatten().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(7, 132), (2, 125), (7, 219), (0, 153), (0, 6), (7, 32), (0, 234), (1, 39), (2, 75), (7, 59), (7, 127), (0, 60)]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b4eca547b6046398ad4a0547134e9ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a mood, there was a little girl named\n"
     ]
    }
   ],
   "source": [
    "# Ablate the top 3 DLA neurons for each index and check that it messes things up\n",
    "\n",
    "# Get top neurons\n",
    "dla_layer_neuron_tuples = []\n",
    "for token_index in range(4):\n",
    "    values, indices = torch.topk(attrs[token_index], 3, dim=-1)\n",
    "    layer_indices, neuron_indices = np.unravel_index(indices.cpu().numpy(), (model.cfg.n_layers, model.cfg.d_mlp))\n",
    "    dla_layer_neuron_tuples.extend(zip(layer_indices.tolist(), neuron_indices.tolist()))\n",
    "\n",
    "print(dla_layer_neuron_tuples)\n",
    "\n",
    "sorted_dla_tuples, sorted_acts = get_neuron_mean_acts(dla_layer_neuron_tuples)\n",
    "hooks = hook_utils.get_ablate_context_neurons_hooks(sorted_dla_tuples, sorted_acts)\n",
    "with model.hooks(hooks):\n",
    "    print(model.generate(\"Once\", 10, temperature=0, use_past_kv_cache=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ablate for each token individually \n",
    "\n",
    "from transformer_lens import utils\n",
    "strings = [\"Once\", \" upon\", \" a\", \" time\"]\n",
    "\n",
    "# Minimal top DLA neurons\n",
    "for token_index, n_ablations in [(0, 10), (1, 20), (2, 6)]:\n",
    "    values, indices = torch.topk(attrs[token_index], n_ablations, dim=-1)\n",
    "    layer_indices, neuron_indices = np.unravel_index(indices.cpu().numpy(), (model.cfg.n_layers, model.cfg.d_mlp))\n",
    "    token_dla_layer_neuron_tuples = zip(layer_indices.tolist(), neuron_indices.tolist())\n",
    "    sorted_dla_tuples, sorted_acts = get_neuron_mean_acts(token_dla_layer_neuron_tuples)\n",
    "    hooks = hook_utils.get_ablate_context_neurons_hooks(sorted_dla_tuples, sorted_acts)\n",
    "    with model.hooks(hooks):\n",
    "        test_prompt = \"\".join(strings[:token_index + 1])\n",
    "        # print(utils.test_prompt(test_prompt, strings[token_index + 1] or 'butterfly', model))\n",
    "        # print(model.generate(\"Once\", 10, temperature=0, use_past_kv_cache=False, verbose=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ablating the disjoint set and ablating to a different prompt - both cursed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "302391b955e341839e382abd2a8945c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OnceOnceOnceOnceOnceOnceOnceOnceOnceOnceOnce\n"
     ]
    }
   ],
   "source": [
    "# # Try ablating the disjoint set\n",
    "# unspecified_neuron_tuples = get_unspecified_neurons(model, dla_layer_neuron_tuples)\n",
    "# layer_neuron_dict = haystack_utils.get_neurons_by_layer(unspecified_neuron_tuples)\n",
    "\n",
    "# sorted_dla_layer_neuron_tuples = []\n",
    "# sorted_acts = []\n",
    "\n",
    "# for layer in layer_neuron_dict.keys():\n",
    "#     neurons = layer_neuron_dict[layer]\n",
    "#     mean_acts = haystack_utils.get_mlp_activations(filtered_prompts, layer, model, context_crop_start=2, hook_pre=False, neurons=neurons, disable_tqdm=True)\n",
    "#     sorted_dla_layer_neuron_tuples.extend([(layer, neuron) for neuron in neurons])\n",
    "#     sorted_acts.extend(mean_acts)\n",
    "#     assert len(sorted_dla_layer_neuron_tuples) == len(sorted_acts)\n",
    "\n",
    "# hooks = hook_utils.get_ablate_context_neurons_hooks(sorted_dla_layer_neuron_tuples, sorted_acts)\n",
    "# with model.hooks(hooks):\n",
    "#     print(model.generate(\"Once\", 10, temperature=0, use_past_kv_cache=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d8620d32e294e72a32c5072c1c1d5fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time\n"
     ]
    }
   ],
   "source": [
    "# def get_resample_neurons_hooks(neurons: list[tuple[int, int]], resampled_cache):\n",
    "#     layer_neurons = haystack_utils.get_neurons_by_layer(neurons)\n",
    "#     hooks = []\n",
    "#     for layer, neurons in layer_neurons.items():\n",
    "#         def resample_neurons_hook(value, hook):\n",
    "#             resample_cache_slice = np.s_[:value.shape[0], :value.shape[1], neurons]\n",
    "#             value[:, :, neurons] = resampled_cache[hook.name][resample_cache_slice]\n",
    "#             return value\n",
    "#         hooks.append((f'blocks.{layer}.mlp.hook_post', resample_neurons_hook))\n",
    "#     return hooks\n",
    "\n",
    "# # Try again with resampled activations\n",
    "# _, resample_cache = model.run_with_cache([\"Once upon a time\"])\n",
    "# hooks = get_resample_neurons_hooks(sorted_dla_layer_neuron_tuples, resample_cache)\n",
    "# with model.hooks(hooks):\n",
    "#     print(model.generate(\"Once\", 3, temperature=0, use_past_kv_cache=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Causal study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [01:03<00:00,  7.93s/it]\n"
     ]
    }
   ],
   "source": [
    "def get_neuron_loss_increases(prompt: str, positionwise=False):\n",
    "    original_loss = model([prompt], return_type='loss', loss_per_token=positionwise)\n",
    "    \n",
    "    losses = []\n",
    "    for layer in trange(model.cfg.n_layers):\n",
    "        mean_acts = haystack_utils.get_mlp_activations(data[:200], layer, model, disable_tqdm=True, context_crop_start=0)\n",
    "        for neuron in range(model.cfg.d_mlp):\n",
    "            hook = hook_utils.get_ablate_neuron_hook(layer, neuron, mean_acts[neuron])\n",
    "            with model.hooks([hook]):\n",
    "                ablated_loss = model([prompt], return_type='loss', loss_per_token=positionwise)\n",
    "                losses.append((ablated_loss - original_loss)[0])\n",
    "    return losses\n",
    "\n",
    "losses = get_neuron_loss_increases(\"Once upon a time\", positionwise=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hook_inputs_for_token_index(loss_increases_by_neuron, model=model, filtered_prompts=filtered_prompts):\n",
    "    values, indices = torch.topk(loss_increases_by_neuron, 40)\n",
    "\n",
    "    layer_indices, neuron_indices = np.unravel_index(indices.cpu().numpy(), (model.cfg.n_layers, model.cfg.d_mlp))\n",
    "    causal_layer_neuron_tuples = list(zip(layer_indices.tolist(), neuron_indices.tolist()))\n",
    "    layer_neuron_dict = haystack_utils.get_neurons_by_layer(causal_layer_neuron_tuples)\n",
    "\n",
    "    sorted_dla_layer_neuron_tuples = []\n",
    "    sorted_acts = []\n",
    "\n",
    "    for layer in layer_neuron_dict.keys():\n",
    "        neurons = layer_neuron_dict[layer]\n",
    "        mean_acts = haystack_utils.get_mlp_activations(filtered_prompts[:200], layer, model, context_crop_start=0, neurons=neurons, disable_tqdm=True)\n",
    "        sorted_dla_layer_neuron_tuples.extend([(layer, neuron) for neuron in neurons])\n",
    "        sorted_acts.extend(mean_acts)\n",
    "        assert len(sorted_dla_layer_neuron_tuples) == len(sorted_acts)\n",
    "\n",
    "    return sorted_dla_layer_neuron_tuples, sorted_acts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(4, 175), (4, 98), (4, 152), (4, 122), (4, 219), (4, 80), (4, 49), (4, 129), (4, 61), (3, 60), (3, 118), (3, 75), (3, 211), (3, 87), (3, 146), (3, 158), (6, 236), (6, 126), (6, 223), (6, 19)] [tensor(-0.0236, device='cuda:0'), tensor(-0.0060, device='cuda:0'), tensor(-0.0116, device='cuda:0'), tensor(-0.0268, device='cuda:0'), tensor(-0.0180, device='cuda:0'), tensor(-0.0217, device='cuda:0'), tensor(-0.0229, device='cuda:0'), tensor(-0.0370, device='cuda:0'), tensor(-0.0272, device='cuda:0'), tensor(-0.0124, device='cuda:0'), tensor(-0.0243, device='cuda:0'), tensor(-0.0105, device='cuda:0'), tensor(-0.0090, device='cuda:0'), tensor(-0.0152, device='cuda:0'), tensor(-0.0273, device='cuda:0'), tensor(0.0013, device='cuda:0'), tensor(-0.0016, device='cuda:0'), tensor(-0.0352, device='cuda:0'), tensor(0.0206, device='cuda:0'), tensor(0.0329, device='cuda:0')]\n"
     ]
    }
   ],
   "source": [
    "# Get \"upon\" neurons\n",
    "num_neurons = 20\n",
    "\n",
    "token_index = 1\n",
    "loss_increases = torch.tensor([loss[token_index] for loss in losses])\n",
    "sorted_ablation_tuples, sorted_acts = get_hook_inputs_for_token_index(loss_increases)\n",
    "print(sorted_ablation_tuples[:num_neurons], sorted_acts[:num_neurons])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a717efb226a4ef087dde46e815d57c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once, the little little little little little little little little\n"
     ]
    }
   ],
   "source": [
    "hooks = hook_utils.get_ablate_context_neurons_hooks(sorted_ablation_tuples[:num_neurons], sorted_acts[:num_neurons])\n",
    "with model.hooks(hooks):\n",
    "    print(model.generate(\"Once\", 10, temperature=0, use_past_kv_cache=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DLA:\n",
      "[(7, 132), (2, 125), (7, 219), (0, 155), (3, 90), (0, 166), (7, 86), (0, 135), (7, 55), (6, 136)]\n",
      "tensor([4.9803, 2.3184, 1.6035, 1.5476, 1.3802, 1.3253, 1.2454, 1.0499, 1.0420,\n",
      "        1.0340], device='cuda:0')\n",
      "Ablation:\n",
      "[(7, 132), (3, 22), (6, 179), (4, 178), (6, 181), (5, 11), (4, 36), (7, 219), (4, 38), (4, 239)]\n",
      "tensor([ 4.9803,  0.5572, -0.7496, -0.4230, -0.5537,  0.5289,  0.1620,  1.6035,\n",
      "         0.0813, -0.3386], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "def compare_dla_and_ablation(token_index, attrs, losses, num_neurons=20):\n",
    "    print(\"DLA:\")\n",
    "    values, indices = torch.topk(attrs[token_index], num_neurons, dim=-1)\n",
    "    layer_indices, neuron_indices = np.unravel_index(indices.cpu().numpy(), (model.cfg.n_layers, model.cfg.d_mlp))\n",
    "    dla_layer_neuron_tuples = list(zip(layer_indices.tolist(), neuron_indices.tolist()))\n",
    "    indices_1d_dla = [np.ravel_multi_index(index_2d, (model.cfg.n_layers, model.cfg.d_mlp)) for index_2d in dla_layer_neuron_tuples[:num_neurons]]\n",
    "    \n",
    "    print(dla_layer_neuron_tuples[:num_neurons])\n",
    "    print(attrs[token_index][indices_1d_dla])\n",
    "\n",
    "    print(\"Ablation:\")\n",
    "    loss_increases_by_neuron = torch.tensor([loss[token_index] for loss in losses])\n",
    "    values, indices = torch.topk(loss_increases_by_neuron, num_neurons)\n",
    "    indices = indices.numpy()\n",
    "    layer_indices, neuron_indices = np.unravel_index(indices[:num_neurons], (model.cfg.n_layers, model.cfg.d_mlp))\n",
    "    \n",
    "    print(list(zip(layer_indices.tolist(), neuron_indices.tolist())))\n",
    "    print(attrs[token_index][indices.tolist()[:num_neurons]])\n",
    "\n",
    "compare_dla_and_ablation(token_index=0, attrs=attrs, losses=losses, num_neurons=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a63d72fc102494aaac19f0557a2cf23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once for a little little bunny named Benny who loved to\n"
     ]
    }
   ],
   "source": [
    "important_neurons = [(4, 175), (4, 98)]\n",
    "hooks = hook_utils.get_ablate_context_neurons_hooks(sorted_ablation_tuples[:2], sorted_acts[:2])\n",
    "with model.hooks(hooks):\n",
    "    print(model.generate(\"Once\", 10, temperature=0, use_past_kv_cache=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
