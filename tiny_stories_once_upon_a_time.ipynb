{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "from tqdm.auto import tqdm\n",
    "from transformer_lens import HookedTransformer\n",
    "from jaxtyping import Float, Int, Bool\n",
    "from torch import Tensor\n",
    "from tqdm.auto import tqdm\n",
    "import plotly.io as pio\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px \n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from IPython.display import display, HTML\n",
    "from datasets import load_dataset\n",
    "from collections import Counter\n",
    "import pickle\n",
    "import os\n",
    "import plotly.graph_objects as go\n",
    "from scipy.stats import norm\n",
    "from einops import einsum\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "pio.renderers.default = \"notebook_connected+notebook\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.autograd.set_grad_enabled(False)\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "import haystack_utils\n",
    "from haystack_utils import get_mlp_activations\n",
    "from hook_utils import get_ablate_neuron_hook, save_activation\n",
    "from pythia_160m_utils import get_neuron_accuracy, ablation_effect\n",
    "import plotting_utils\n",
    "from plotting_utils import plot_neuron_acts, color_binned_histogram\n",
    "import hook_utils\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/tiny_stories_chatgpt.json', 'r') as f:\n",
    "    prompts = json.load(f)\n",
    "\n",
    "filtered_prompts = [prompt for prompt in prompts if not prompt.startswith(\"Once\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model roneneldan/TinyStories-1M into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "model = HookedTransformer.from_pretrained(\"roneneldan/TinyStories-1M\",\n",
    "    center_unembed=True,\n",
    "    center_writing_weights=True,\n",
    "    fold_ln=True,\n",
    "    device=\"cuda\")\n",
    "\n",
    "# large_acts_df = plotting_utils.get_neuron_moments(model, prompts,\n",
    "#                                                   [[i, j] for i in range(8) for j in range(256)], hook_pre=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ac28344c1cc43d19b9986165aa44803",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, there was a big bear who\n"
     ]
    }
   ],
   "source": [
    "print(model.generate('Once'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate heaps of token sequences and average their cache, then find the difference with the OUAT neurons\n",
    "# Ablate components until OUAT fails\n",
    "# n-l AND?\n",
    "\n",
    "_, cache = model.run_with_cache(model.to_tokens(prompts)[:, :40])\n",
    "\n",
    "def rand_hook(value, hook):\n",
    "    cache_val = cache[hook.name][:value.size(0), :value.size(1), :value.size(2)]\n",
    "    mean_val = cache_val.mean(dim=0)\n",
    "    broadcasted_val = mean_val.unsqueeze(0).expand_as(value)\n",
    "    value = broadcasted_val\n",
    "    return value\n",
    "\n",
    "prompt = \"Once\"\n",
    "print(model.generate(prompt, 20, temperature=0))\n",
    "for layer in range(model.cfg.n_layers):\n",
    "    with model.hooks([(f'blocks.{layer}.hook_attn_out', rand_hook)]):\n",
    "        print(f\"Attn {layer}\", model.generate(prompt, 20, temperature=0))\n",
    "    with model.hooks([(f'blocks.{layer}.hook_mlp_out', rand_hook)]):\n",
    "        print(f\"MLP {layer}\", model.generate(prompt, 20, temperature=0))\n",
    "\n",
    "# Necessary components:\n",
    "# MLP0, MLP1, MLP2, MLP3, MLP4, MLP5, MLP7 (every MLP but 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ablate by cosine sim - degrades around 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure out which neurons directly write to each vocab\n",
    "# And see if we can ablate everything else\n",
    "\n",
    "tokens = model.to_tokens(\"Once upon a time\", prepend_bos=False)  # [1, 4]\n",
    "token_dirs = model.tokens_to_residual_directions(tokens)[0]  # [4, 64]\n",
    "token_dirs_reshaped = token_dirs.unsqueeze(1).unsqueeze(1)  # [4, 1, 1, 64]\n",
    "W_out_reshaped = model.W_out.unsqueeze(0)  # [1, 8, 256, 64]\n",
    "\n",
    "cosine_sim = torch.nn.CosineSimilarity(dim=-1, eps=1e-6)\n",
    "result = cosine_sim(token_dirs_reshaped, W_out_reshaped)  # [4, 8, 256]\n",
    "\n",
    "_, cache = model(tokens)\n",
    "acts = [cache[f'blocks.{layer}.mlp.hook_post'][0] for layer in range(model.cfg.n_layers)] # [[batch pos]]*n_layers\n",
    "acts = torch.stack(acts, dim=1) # \n",
    "\n",
    "\n",
    "layer_neuron_tuples = []\n",
    "for token_index in range(result.size(0)):\n",
    "    values, indices = torch.topk(result[token_index].view(-1), 20, dim=-1)\n",
    "    layer_indices, neuron_indices = np.unravel_index(indices.cpu().numpy(), (8, 256))\n",
    "    layer_neuron_tuples.extend(zip(layer_indices.tolist(), neuron_indices.tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfe68b77dff24c43bc388aa7d4b3deb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once in a time, a little girl named Amy was\n"
     ]
    }
   ],
   "source": [
    "layer_neuron_dict = haystack_utils.get_neurons_by_layer(layer_neuron_tuples)\n",
    "\n",
    "sorted_layer_neuron_tuples = []\n",
    "sorted_acts = []\n",
    "\n",
    "for layer in layer_neuron_dict.keys():\n",
    "    neurons = layer_neuron_dict[layer]\n",
    "    mean_acts = haystack_utils.get_mlp_activations(filtered_prompts, layer, model, context_crop_start=2, hook_pre=False, neurons=neurons, disable_tqdm=True)\n",
    "    sorted_layer_neuron_tuples.extend([(layer, neuron) for neuron in neurons])\n",
    "    sorted_acts.extend(mean_acts)\n",
    "    assert len(sorted_layer_neuron_tuples) == len(sorted_acts)\n",
    "\n",
    "hooks = hook_utils.get_ablate_context_neurons_hooks(sorted_layer_neuron_tuples, sorted_acts)\n",
    "with model.hooks(hooks):\n",
    "    print(model.generate(\"Once\", 10, temperature=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DLA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batched_dot_product(x, y):\n",
    "    return torch.vmap(torch.dot)(x, y)\n",
    "\n",
    "def mlp_DLA(prompt: str, model: HookedTransformer) -> tuple[Float[Tensor, \"component\"], list[str]]:\n",
    "    tokens = model.to_tokens(prompt)\n",
    "    answers = tokens[:, 1:]\n",
    "    tokens = tokens[:, :-1]\n",
    "    answer_residual_directions = model.tokens_to_residual_directions(answers)  # [pos d_model]\n",
    "    _, cache = model.run_with_cache(tokens)\n",
    "\n",
    "    # find how much each neuron writes to the correct logit for a single pos\n",
    "    logit_attrs = torch.zeros(4, model.cfg.n_layers, model.cfg.d_mlp)\n",
    "    for layer in range(model.cfg.n_layers):\n",
    "        mlp_post = cache[f'blocks.{layer}.mlp.hook_post'][0]\n",
    "        for neuron in range(model.cfg.d_mlp):\n",
    "            mlp_post[:, neuron]\n",
    "            model.W_out[layer][neuron]\n",
    "            out_vectors = mlp_post[:, neuron].view(-1, 1) * model.W_out[layer][neuron].view(1, -1)\n",
    "            result = batched_dot_product(out_vectors, answer_residual_directions.squeeze(0))\n",
    "            logit_attrs[:, layer, neuron] = result\n",
    "    return logit_attrs\n",
    "\n",
    "def get_unspecified_neurons(model: HookedTransformer, neurons: list[tuple[int, int]]):\n",
    "    layer_dict = haystack_utils.get_neurons_by_layer(neurons)\n",
    "    unspecified = []\n",
    "    for layer in range(model.cfg.n_layers):\n",
    "        for neuron in range(model.cfg.d_mlp):\n",
    "            if not neuron in layer_dict[layer]:\n",
    "                unspecified.append((layer, neuron))\n",
    "    return unspecified\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_attrs = mlp_DLA(\"Once upon a time\", model)\n",
    "# haystack_utils.line(layer_dict[0][0].cpu().numpy(), xlabel=\"Correct logit\", ylabel=\"\", title=\"DLA per neuron in layer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(7, 132), (2, 125), (7, 219), (0, 155), (3, 90)]\n",
      "[(0, 78), (0, 22), (2, 236), (2, 125), (5, 164)]\n"
     ]
    }
   ],
   "source": [
    "dla_layer_neuron_tuples = []\n",
    "for token_index in range(4):\n",
    "    values, indices = torch.topk(logit_attrs[token_index].view(-1), 5, dim=-1)\n",
    "    layer_indices, neuron_indices = np.unravel_index(indices.cpu().numpy(), (8, 256))\n",
    "    dla_layer_neuron_tuples.extend(zip(layer_indices.tolist(), neuron_indices.tolist()))\n",
    "\n",
    "print(dla_layer_neuron_tuples[:5])\n",
    "print(layer_neuron_tuples[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f341929cfdf84eee952c7fd3380bb285",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once Emily, named Sam,? She was a little\n"
     ]
    }
   ],
   "source": [
    "layer_neuron_dict = haystack_utils.get_neurons_by_layer(dla_layer_neuron_tuples)\n",
    "\n",
    "sorted_layer_neuron_tuples = []\n",
    "sorted_acts = []\n",
    "\n",
    "for layer in layer_neuron_dict.keys():\n",
    "    neurons = layer_neuron_dict[layer]\n",
    "    mean_acts = haystack_utils.get_mlp_activations(filtered_prompts, layer, model, context_crop_start=2, hook_pre=False, neurons=neurons, disable_tqdm=True)\n",
    "    sorted_layer_neuron_tuples.extend([(layer, neuron) for neuron in neurons])\n",
    "    sorted_acts.extend(mean_acts)\n",
    "    assert len(sorted_layer_neuron_tuples) == len(sorted_acts)\n",
    "\n",
    "hooks = hook_utils.get_ablate_context_neurons_hooks(sorted_layer_neuron_tuples, sorted_acts)\n",
    "with model.hooks(hooks):\n",
    "    print(model.generate(\"Once\", 10, temperature=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a39231ceff034f4ea11ee42d3e60c11b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OnceOnceOnceOnceOnceOnceOnceOnceOnceOnceOnce\n"
     ]
    }
   ],
   "source": [
    "unspecified_neuron_tuples = get_unspecified_neurons(model, dla_layer_neuron_tuples)\n",
    "layer_neuron_dict = haystack_utils.get_neurons_by_layer(unspecified_neuron_tuples)\n",
    "\n",
    "sorted_layer_neuron_tuples = []\n",
    "sorted_acts = []\n",
    "\n",
    "for layer in layer_neuron_dict.keys():\n",
    "    neurons = layer_neuron_dict[layer]\n",
    "    mean_acts = haystack_utils.get_mlp_activations(filtered_prompts, layer, model, context_crop_start=2, hook_pre=False, neurons=neurons, disable_tqdm=True)\n",
    "    sorted_layer_neuron_tuples.extend([(layer, neuron) for neuron in neurons])\n",
    "    sorted_acts.extend(mean_acts)\n",
    "    assert len(sorted_layer_neuron_tuples) == len(sorted_acts)\n",
    "\n",
    "hooks = hook_utils.get_ablate_context_neurons_hooks(sorted_layer_neuron_tuples, sorted_acts)\n",
    "with model.hooks(hooks):\n",
    "    print(model.generate(\"Once\", 10, temperature=0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
