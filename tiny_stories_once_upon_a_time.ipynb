{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "from tqdm.auto import tqdm\n",
    "from transformer_lens import HookedTransformer\n",
    "from jaxtyping import Float, Int, Bool\n",
    "from torch import Tensor\n",
    "from tqdm.auto import tqdm\n",
    "import plotly.io as pio\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px \n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from IPython.display import display, HTML\n",
    "from datasets import load_dataset\n",
    "from collections import Counter\n",
    "import pickle\n",
    "import os\n",
    "import plotly.graph_objects as go\n",
    "from scipy.stats import norm\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "pio.renderers.default = \"notebook_connected+notebook\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.autograd.set_grad_enabled(False)\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "import haystack_utils\n",
    "from haystack_utils import get_mlp_activations\n",
    "from hook_utils import get_ablate_neuron_hook, save_activation\n",
    "from pythia_160m_utils import get_neuron_accuracy, ablation_effect\n",
    "import plotting_utils\n",
    "from plotting_utils import plot_neuron_acts, color_binned_histogram\n",
    "\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/tiny_stories_chatgpt.json', 'r') as f:\n",
    "    prompts = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model roneneldan/TinyStories-1M into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "model = HookedTransformer.from_pretrained(\"roneneldan/TinyStories-1M\",\n",
    "    center_unembed=True,\n",
    "    center_writing_weights=True,\n",
    "    fold_ln=True,\n",
    "    device=\"cuda\")\n",
    "\n",
    "# large_acts_df = plotting_utils.get_neuron_moments(model, prompts,\n",
    "#                                                   [[i, j] for i in range(8) for j in range(256)], hook_pre=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ac28344c1cc43d19b9986165aa44803",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, there was a big bear who\n"
     ]
    }
   ],
   "source": [
    "print(model.generate('Once'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate heaps of token sequences and average their cache, then find the difference with the OUAT neurons\n",
    "# Ablate components until OUAT fails\n",
    "# n-l AND?\n",
    "\n",
    "_, cache = model.run_with_cache(model.to_tokens(prompts)[:, :40])\n",
    "\n",
    "def rand_hook(value, hook):\n",
    "    cache_val = cache[hook.name][:value.size(0), :value.size(1), :value.size(2)]\n",
    "    mean_val = cache_val.mean(dim=0)\n",
    "    broadcasted_val = mean_val.unsqueeze(0).expand_as(value)\n",
    "    value = broadcasted_val\n",
    "    return value\n",
    "\n",
    "prompt = \"Once\"\n",
    "print(model.generate(prompt, 20, temperature=0))\n",
    "for layer in range(model.cfg.n_layers):\n",
    "    with model.hooks([(f'blocks.{layer}.hook_attn_out', rand_hook)]):\n",
    "        print(f\"Attn {layer}\", model.generate(prompt, 20, temperature=0))\n",
    "    with model.hooks([(f'blocks.{layer}.hook_mlp_out', rand_hook)]):\n",
    "        print(f\"MLP {layer}\", model.generate(prompt, 20, temperature=0))\n",
    "\n",
    "# Necessary components:\n",
    "# MLP0, MLP1, MLP2, MLP3, MLP4, MLP5, MLP7 (every MLP but 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure out which neurons directly write to each vocab\n",
    "# And see if we can ablate everything else\n",
    "\n",
    "tokens = model.to_tokens(\"Once upon a time\", prepend_bos=False)  # [1, 4]\n",
    "token_dirs = model.tokens_to_residual_directions(tokens)[0]  # [4, 64]\n",
    "token_dirs_reshaped = token_dirs.unsqueeze(1).unsqueeze(1)  # [4, 1, 1, 64]\n",
    "W_out_reshaped = model.W_out.unsqueeze(0)  # [1, 8, 256, 64]\n",
    "\n",
    "cosine_sim = torch.nn.CosineSimilarity(dim=-1, eps=1e-6)\n",
    "result = cosine_sim(token_dirs_reshaped, W_out_reshaped)  # [4, 8, 256]\n",
    "\n",
    "layer_neuron_tuples = []\n",
    "for token_index in range(result.size(0)):\n",
    "    values, indices = torch.topk(result[token_index].view(-1), 30, dim=-1)\n",
    "    layer_indices, neuron_indices = np.unravel_index(indices.cpu().numpy(), (8, 256))\n",
    "    layer_neuron_tuples.extend(zip(layer_indices.tolist(), neuron_indices.tolist()))\n",
    "\n",
    "layer_neuron_dict = haystack_utils.get_neurons_by_layer(layer_neuron_tuples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_prompts = [prompt for prompt in prompts if not prompt.startswith(\"Once\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_layer_neuron_tuples = []\n",
    "sorted_acts = []\n",
    "\n",
    "for layer in layer_neuron_dict.keys():\n",
    "    neurons = layer_neuron_dict[layer]\n",
    "    mean_acts = haystack_utils.get_mlp_activations(filtered_prompts, layer, model, context_crop_start=2, hook_pre=False, neurons=neurons, disable_tqdm=True)\n",
    "    sorted_layer_neuron_tuples.extend([(layer, neuron) for neuron in neurons])\n",
    "    sorted_acts.extend(mean_acts)\n",
    "    assert len(sorted_layer_neuron_tuples) == len(sorted_acts)\n",
    "\n",
    "import hook_utils\n",
    "hooks = hook_utils.get_ablate_context_neurons_hooks(sorted_layer_neuron_tuples, sorted_acts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b3ac1327af8410cae0c0064dd25bf4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, there\n",
      "Once there, there\n"
     ]
    }
   ],
   "source": [
    "with model.hooks(hooks):\n",
    "    print(model.generate(\"Once\", 10, temperature=0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
