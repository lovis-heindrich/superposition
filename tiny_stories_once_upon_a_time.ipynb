{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformer_lens import HookedTransformer\n",
    "from jaxtyping import Float\n",
    "from torch import Tensor\n",
    "import plotly.io as pio\n",
    "import numpy as np\n",
    "from tqdm import trange\n",
    "\n",
    "pio.renderers.default = \"notebook_connected+notebook\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.autograd.set_grad_enabled(False)\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "import haystack_utils\n",
    "import hook_utils\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_large/TinyStories-train.txt: Loaded 935512 examples with 0 to 4287 characters each.\n"
     ]
    }
   ],
   "source": [
    "data = haystack_utils.load_txt_data('data_large/TinyStories-train.txt')[:500]\n",
    "filtered_prompts = [prompt for prompt in data if not prompt.startswith(\"Once\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model roneneldan/TinyStories-1M into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "model = HookedTransformer.from_pretrained(\"roneneldan/TinyStories-1M\",\n",
    "    center_unembed=True,\n",
    "    center_writing_weights=True,\n",
    "    fold_ln=True,\n",
    "    device=\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Once', 'One', 'Yesterday', 'At', 'There', 'Today', 'On', 'Em', 'When', 'In']\n",
      "[' upon', 'bie', 'ancer', 'itting', 'll', 'aj', 'packs', 'upon', 'uttering', ' haven']\n",
      "[' a', ' an', ' the', ' some', ' two', ' something', ' another', ' one', ' very', ' many']\n",
      "[' time', ' day', ' week', ' evening', ' morning', ' afternoon', ' night', ' Sunday', ' way', ' year']\n"
     ]
    }
   ],
   "source": [
    "# Similar words stored near in the embed\n",
    "tokens = model.to_tokens(\"Once upon a time\", prepend_bos=False)\n",
    "logits = model.W_E[tokens].squeeze(0) @ model.W_U\n",
    "\n",
    "for i in range(4):\n",
    "    values, indices = torch.topk(logits[i], 10)\n",
    "    print(model.to_str_tokens(indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1094, device='cuda:0') tensor(0.2133, device='cuda:0')\n",
      "['cow', 'ns', 'ridge', 'wo', ' seventeen', 'eight', 'umbers', 'please', 'ucks', 'icians', 'oxy', 'bread', 'Seven', 'uns', 'bang', 'uggets', 'oster', 'wy', 'Ta', 'arella', 'irteen', ' reins', 'vals', ' fries', 'heddar', 'nine', 'aff', 'zee', 'rots', 'six', 'ban', 'bet', 'Six', 'Four', 'umbs', 'arians', 'Twenty', 'gary', 'cards', 'urger', 'workers', 'Sal', 'apple', 'rito', 'ulent', 'haw', 'cream', 'ction', 'leaf', 'load', 'agna', 'shirts', 'ksh', 'icy', 'keys', 'Apple', 'ants', 'trained', 'orted', 'ender', 'gob', 'chini', 'ocol', ' ounces', 'talk', 'rob', 'Dragon', 'chip', 'amer', 'plets', 'fed', 'iri', 'pill', 'mons', 'int', ' cents', 'reath', 'cker', 'pper', 'war', 'rol', 'Ah', 'osaurus', 'Ten', 'dollar', 'girls', 'seven', 'mur', 'affles', 'Offic', 'ws', 'ounced', 'ucker', 'iw', 'eating', 'ates', 'Ar', 'Chuck', 'Eight', 'mus']\n"
     ]
    }
   ],
   "source": [
    "# Associations\n",
    "tokens = model.to_tokens(\"cow\", prepend_bos=False)\n",
    "logits = (model.W_E[tokens].squeeze(0) @ model.W_U).squeeze(0)\n",
    "mean_logit = logits.median() # mean is lower\n",
    "\n",
    "associated_tokens = []\n",
    "for string in \"grass brown field moo meat milk udders animal\".split(' '):\n",
    "    token = model.to_tokens(string, prepend_bos=False)\n",
    "    associated_tokens.append(token[0, 0])\n",
    "associated_tokens = torch.stack(associated_tokens)\n",
    "mean_associated_logit = logits[associated_tokens].median() # mean is similar\n",
    "\n",
    "print(mean_logit, mean_associated_logit)\n",
    "\n",
    "values, indices = torch.topk(logits, 100)\n",
    "print(model.to_str_tokens(indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.0134, device='cuda:0')\n",
      "tensor([ 0.8695, -0.3610,  1.9775], device='cuda:0')\n",
      "[' a', ' an', ' the', ' some', ' two', ' something', ' another', ' one', ' very', ' many', ' big', ' her', ' so', ' his', ' all', ' in', ' lots', ' to', ' more', ' three', ' four', ' it', ' their', ' on', ' out', ' there', ' small', ' too', ' that', ' and', ' not', '.', ' no', ' good', ' with', ' right', ' really', ' both', ' nice', ' different', ' only', ' five', ',', '\\n', ' like', ' hard', ' A', ' bright', ' just', ' when', ' much', ' little', ' long', ' cool', ' brave', ' someone', ' fast', ' strong', ' Lily', ' funny', ' this', ' look', ' warm', ' made', ' fun', ' wide', ' new', ' as', ' silly', ' red', ' hot', ' each', ' close', ' open', ' pretty', ' Mom', ' what', ' Tom', ' other', ' its', ' even', ' far', ' tall', ' The', ' shiny', ' dark', ' loud', ' white', ' deep', ' soft', ' fair', ' black', ' j', ' outside', ' kind', ' your', 'A', ' for', ' help', ' happy']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tokens = model.to_tokens(\"Once upon a time\", prepend_bos=False).squeeze(0)\n",
    "logits = (model.W_E[tokens] @ model.W_U)\n",
    "median_logit = logits.median()\n",
    "print(median_logit)\n",
    "\n",
    "next_logits = []\n",
    "for i in range(tokens.shape[0] - 1):\n",
    "    next_logits.append(logits[i, tokens[i + 1]])\n",
    "next_logits = torch.stack(next_logits)\n",
    "print(next_logits)\n",
    "\n",
    "# 'a' token seems to boost some sensible completions\n",
    "values, indices = torch.topk(logits[-2], 100)\n",
    "print(model.to_str_tokens(indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'prompts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Generate heaps of token sequences and average their cache, then find the difference with the OUAT neurons\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m# Ablate components until OUAT fails\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m# n-l AND?\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m _, cache \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mrun_with_cache(model\u001b[39m.\u001b[39mto_tokens(prompts)[:, :\u001b[39m40\u001b[39m])\n\u001b[1;32m      7\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrand_hook\u001b[39m(value, hook):\n\u001b[1;32m      8\u001b[0m     cache_val \u001b[39m=\u001b[39m cache[hook\u001b[39m.\u001b[39mname][:value\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m), :value\u001b[39m.\u001b[39msize(\u001b[39m1\u001b[39m), :value\u001b[39m.\u001b[39msize(\u001b[39m2\u001b[39m)]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'prompts' is not defined"
     ]
    }
   ],
   "source": [
    "# Generate heaps of token sequences and average their cache, then find the difference with the OUAT neurons\n",
    "# Ablate components until OUAT fails\n",
    "# n-l AND?\n",
    "\n",
    "_, cache = model.run_with_cache(model.to_tokens(prompts)[:, :40])\n",
    "\n",
    "def rand_hook(value, hook):\n",
    "    cache_val = cache[hook.name][:value.size(0), :value.size(1), :value.size(2)]\n",
    "    mean_val = cache_val.mean(dim=0)\n",
    "    broadcasted_val = mean_val.unsqueeze(0).expand_as(value)\n",
    "    value = broadcasted_val\n",
    "    return value\n",
    "\n",
    "prompt = \"Once\"\n",
    "print(model.generate(prompt, 20, temperature=0, use_past_kv_cache=False))\n",
    "for layer in range(model.cfg.n_layers):\n",
    "    with model.hooks([(f'blocks.{layer}.hook_attn_out', rand_hook)]):\n",
    "        print(f\"Attn {layer}\", model.generate(prompt, 20, temperature=0, use_past_kv_cache=False))\n",
    "    with model.hooks([(f'blocks.{layer}.hook_mlp_out', rand_hook)]):\n",
    "        print(f\"MLP {layer}\", model.generate(prompt, 20, temperature=0, use_past_kv_cache=False))\n",
    "\n",
    "# Necessary components:\n",
    "# MLP0, MLP1, MLP2, MLP3, MLP4, MLP5, MLP7 (every MLP but 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ablate by cosine sim - degrades around 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure out which neurons directly write to each vocab\n",
    "# And see if we can ablate everything else\n",
    "\n",
    "tokens = model.to_tokens(\"Once upon a time\", prepend_bos=False)  # [1, 4]\n",
    "token_dirs = model.tokens_to_residual_directions(tokens)[0]  # [4, 64]\n",
    "token_dirs_reshaped = token_dirs.unsqueeze(1).unsqueeze(1)  # [4, 1, 1, 64]\n",
    "W_out_reshaped = model.W_out.unsqueeze(0)  # [1, 8, 256, 64]\n",
    "\n",
    "cosine_sim = torch.nn.CosineSimilarity(dim=-1, eps=1e-6)\n",
    "result = cosine_sim(token_dirs_reshaped, W_out_reshaped)  # [4, 8, 256]\n",
    "\n",
    "_, cache = model.run_with_cache(tokens)\n",
    "acts = [cache[f'blocks.{layer}.mlp.hook_post'][0] for layer in range(model.cfg.n_layers)] # [[batch pos]]*n_layers\n",
    "acts = torch.stack(acts, dim=1) # \n",
    "\n",
    "\n",
    "layer_neuron_tuples = []\n",
    "for token_index in range(result.size(0)):\n",
    "    values, indices = torch.topk(result[token_index].view(-1), 20, dim=-1)\n",
    "    layer_indices, neuron_indices = np.unravel_index(indices.cpu().numpy(), (8, 256))\n",
    "    layer_neuron_tuples.extend(zip(layer_indices.tolist(), neuron_indices.tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6df5f72e6c14347b2c3e120afac58fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once in a time, a little girl named Amy was\n"
     ]
    }
   ],
   "source": [
    "layer_neuron_dict = haystack_utils.get_neurons_by_layer(layer_neuron_tuples)\n",
    "\n",
    "sorted_tuples = []\n",
    "sorted_acts = []\n",
    "\n",
    "for layer in layer_neuron_dict.keys():\n",
    "    neurons = layer_neuron_dict[layer]\n",
    "    mean_acts = haystack_utils.get_mlp_activations(filtered_prompts, layer, model, context_crop_start=2, hook_pre=False, neurons=neurons, disable_tqdm=True)\n",
    "    sorted_tuples.extend([(layer, neuron) for neuron in neurons])\n",
    "    sorted_acts.extend(mean_acts)\n",
    "    assert len(sorted_tuples) == len(sorted_acts)\n",
    "\n",
    "hooks = hook_utils.get_ablate_context_neurons_hooks(sorted_tuples, sorted_acts)\n",
    "with model.hooks(hooks):\n",
    "    print(model.generate(\"Once\", 10, temperature=0, use_past_kv_cache=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batched_dot_product(x, y):\n",
    "    return torch.vmap(torch.dot)(x, y)\n",
    "    \n",
    "def neuron_DLA(prompt: str, model: HookedTransformer, pos=np.s_[-1:]) -> tuple[Float[Tensor, \"component\"], list[str]]:\n",
    "    '''Gets full resid decomposition including all neurons'''\n",
    "    tokens = model.to_tokens(prompt)\n",
    "    answers = tokens[:, 1:]\n",
    "    tokens = tokens[:, :-1]\n",
    "    \n",
    "    _, cache = model.run_with_cache(tokens)\n",
    "    attrs, labels = cache.get_full_resid_decomposition(-1, expand_neurons=True, apply_ln=True, return_labels=True, pos_slice=pos)\n",
    "    \n",
    "    # I think it removes the batch dimension if it's one\n",
    "    answer_residual_directions = model.tokens_to_residual_directions(answers)\n",
    "    if answer_residual_directions.ndim == 1:\n",
    "        answer_residual_directions = answer_residual_directions.unsqueeze(0)  # [1 d_model]\n",
    "    elif answer_residual_directions.ndim == 3:\n",
    "        answer_residual_directions = answer_residual_directions[0]  # [pos d_model]\n",
    "    answer_residual_directions = answer_residual_directions[pos]  # [pos d_model]\n",
    "\n",
    "    neuron_indices = [i for i in range(len(labels)) if 'N' in labels[i]]\n",
    "    neuron_labels = [labels[i] for i in neuron_indices]\n",
    "    neuron_attrs = attrs[neuron_indices, :].squeeze(1)\n",
    "    results = []\n",
    "    for i in range(neuron_attrs.shape[1]):\n",
    "        results.append(batched_dot_product(neuron_attrs[:, i], answer_residual_directions[[i]].repeat(neuron_attrs.shape[0], 1)))\n",
    "    return torch.stack(results), neuron_labels\n",
    "\n",
    "def get_neuron_mean_acts(neurons: list[tuple[int, int]], model: HookedTransformer = model) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    layer_neuron_dict = haystack_utils.get_neurons_by_layer(neurons)\n",
    "    sorted_layer_neuron_tuples = []\n",
    "    sorted_acts = []\n",
    "\n",
    "    for layer in layer_neuron_dict.keys():\n",
    "        neurons = layer_neuron_dict[layer]\n",
    "        mean_acts = haystack_utils.get_mlp_activations(filtered_prompts[:200], layer, model, context_crop_start=0, hook_pre=False, neurons=neurons, disable_tqdm=True)\n",
    "        sorted_layer_neuron_tuples.extend([(layer, neuron) for neuron in neurons])\n",
    "        sorted_acts.extend(mean_acts)\n",
    "        assert len(sorted_layer_neuron_tuples) == len(sorted_acts)\n",
    "\n",
    "    return sorted_layer_neuron_tuples, sorted_acts\n",
    "\n",
    "def get_unspecified_neurons(model: HookedTransformer, neurons: list[tuple[int, int]]):\n",
    "    layer_dict = haystack_utils.get_neurons_by_layer(neurons)\n",
    "    unspecified = []\n",
    "    for layer in range(model.cfg.n_layers):\n",
    "        for neuron in range(model.cfg.d_mlp):\n",
    "            if not neuron in layer_dict[layer]:\n",
    "                unspecified.append((layer, neuron))\n",
    "    return unspecified\n",
    "\n",
    "def get_neuron_loss_increases(prompt: str, positionwise: bool=False, model: HookedTransformer=model) -> torch.Tensor:\n",
    "    n_tokens = model.to_tokens(prompt).shape[1] - 1\n",
    "    original_loss = model([prompt], return_type='loss', loss_per_token=positionwise)\n",
    "    \n",
    "    losses = []\n",
    "    for layer in trange(model.cfg.n_layers):\n",
    "        mean_acts = haystack_utils.get_mlp_activations(data[:200], layer, model, disable_tqdm=True, context_crop_start=0)\n",
    "        for neuron in range(model.cfg.d_mlp):\n",
    "            hook = hook_utils.get_ablate_neuron_hook(layer, neuron, mean_acts[neuron])\n",
    "            with model.hooks([hook]):\n",
    "                ablated_loss = model([prompt], return_type='loss', loss_per_token=positionwise)\n",
    "                losses.append((ablated_loss - original_loss)[0])\n",
    "    return torch.stack(losses).reshape(n_tokens, model.cfg.n_layers * model.cfg.d_mlp)\n",
    "\n",
    "def compare_dla_and_ablation(token_index: int, dla_attrs_by_neuron: torch.Tensor, ablation_losses_by_neuron: torch.Tensor, num_neurons=20, model: HookedTransformer=model):\n",
    "    print(\"DLA:\")\n",
    "    values, indices = torch.topk(dla_attrs_by_neuron[token_index], num_neurons, dim=-1)\n",
    "    layer_indices, neuron_indices = np.unravel_index(indices.cpu().numpy(), (model.cfg.n_layers, model.cfg.d_mlp))\n",
    "    dla_layer_neuron_tuples = list(zip(layer_indices.tolist(), neuron_indices.tolist()))\n",
    "    indices_1d_dla = [np.ravel_multi_index(index_2d, (model.cfg.n_layers, model.cfg.d_mlp)) for index_2d in dla_layer_neuron_tuples[:num_neurons]]\n",
    "    \n",
    "    print(dla_layer_neuron_tuples[:num_neurons])\n",
    "    print(dla_attrs_by_neuron[token_index][indices_1d_dla])\n",
    "\n",
    "    print(\"Ablation:\")\n",
    "    loss_increases_by_neuron = ablation_losses_by_neuron[:, token_index]\n",
    "    values, indices = torch.topk(loss_increases_by_neuron, num_neurons)\n",
    "    layer_indices, neuron_indices = np.unravel_index(indices.cpu().numpy()[:num_neurons], (model.cfg.n_layers, model.cfg.d_mlp))\n",
    "    \n",
    "    print(list(zip(layer_indices.tolist(), neuron_indices.tolist())))\n",
    "    print(dla_attrs_by_neuron[token_index][indices.tolist()[:num_neurons]])\n",
    "\n",
    "def get_hook_inputs_for_token_index(loss_increases_by_neuron, model: HookedTransformer=model, filtered_prompts=filtered_prompts, k=40):\n",
    "    values, indices = torch.topk(loss_increases_by_neuron, k)\n",
    "\n",
    "    layer_indices, neuron_indices = np.unravel_index(indices.cpu().numpy(), (model.cfg.n_layers, model.cfg.d_mlp))\n",
    "    causal_layer_neuron_tuples = list(zip(layer_indices.tolist(), neuron_indices.tolist()))\n",
    "    layer_neuron_dict = haystack_utils.get_neurons_by_layer(causal_layer_neuron_tuples)\n",
    "\n",
    "    sorted_dla_layer_neuron_tuples = []\n",
    "    sorted_acts = []\n",
    "\n",
    "    for layer in layer_neuron_dict.keys():\n",
    "        neurons = layer_neuron_dict[layer]\n",
    "        mean_acts = haystack_utils.get_mlp_activations(filtered_prompts[:200], layer, model, context_crop_start=0, neurons=neurons, disable_tqdm=True)\n",
    "        sorted_dla_layer_neuron_tuples.extend([(layer, neuron) for neuron in neurons])\n",
    "        sorted_acts.extend(mean_acts)\n",
    "        assert len(sorted_dla_layer_neuron_tuples) == len(sorted_acts)\n",
    "\n",
    "    return sorted_dla_layer_neuron_tuples, sorted_acts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ablate by DLA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    }
   ],
   "source": [
    "# High level viz\n",
    "\n",
    "# x, y = haystack_utils.DLA([\"Once upon\"], model)\n",
    "# haystack_utils.line(x.cpu().squeeze(0))\n",
    "\n",
    "attrs, labels = neuron_DLA(\"Once upon a time\", model, pos=np.s_[-4:])\n",
    "# haystack_utils.line(attrs[0].cpu().numpy(), xlabel=\"Correct logit\", ylabel=\"\", title=\"DLA per neuron in layer\")\n",
    "\n",
    "# print(attrs.sum())\n",
    "# px.histogram(attrs.flatten().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 197), (1, 1221), (0, 2869), (0, 1322), (0, 1701), (0, 2995), (0, 2995), (1, 1221), (1, 944), (1, 191), (3, 2447), (1, 75)]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a74797ed8b4d483d828a8557b5e38442",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, there was a little girl named\n"
     ]
    }
   ],
   "source": [
    "# Ablate the top 3 DLA neurons for each index and check that it messes things up\n",
    "\n",
    "# Get top neurons\n",
    "dla_layer_neuron_tuples = []\n",
    "for token_index in range(4):\n",
    "    values, indices = torch.topk(attrs[token_index], 3, dim=-1)\n",
    "    layer_indices, neuron_indices = np.unravel_index(indices.cpu().numpy(), (model.cfg.n_layers, model.cfg.d_mlp))\n",
    "    dla_layer_neuron_tuples.extend(zip(layer_indices.tolist(), neuron_indices.tolist()))\n",
    "\n",
    "print(dla_layer_neuron_tuples)\n",
    "\n",
    "sorted_dla_tuples, sorted_acts = get_neuron_mean_acts(dla_layer_neuron_tuples)\n",
    "hooks = hook_utils.get_ablate_context_neurons_hooks(sorted_dla_tuples, sorted_acts)\n",
    "with model.hooks(hooks):\n",
    "    print(model.generate(\"Once\", 10, temperature=0, use_past_kv_cache=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ablate for each token individually \n",
    "from transformer_lens import utils\n",
    "strings = [\"Once\", \" upon\", \" a\", \" time\"]\n",
    "\n",
    "# Minimal top DLA neurons\n",
    "for token_index, n_ablations in [(0, 10), (1, 20), (2, 6)]:\n",
    "    values, indices = torch.topk(attrs[token_index], n_ablations, dim=-1)\n",
    "    layer_indices, neuron_indices = np.unravel_index(indices.cpu().numpy(), (model.cfg.n_layers, model.cfg.d_mlp))\n",
    "    token_dla_layer_neuron_tuples = zip(layer_indices.tolist(), neuron_indices.tolist())\n",
    "    sorted_dla_tuples, sorted_acts = get_neuron_mean_acts(token_dla_layer_neuron_tuples)\n",
    "    hooks = hook_utils.get_ablate_context_neurons_hooks(sorted_dla_tuples, sorted_acts)\n",
    "    with model.hooks(hooks):\n",
    "        test_prompt = \"\".join(strings[:token_index + 1])\n",
    "        # print(utils.test_prompt(test_prompt, strings[token_index + 1] or 'butterfly', model))\n",
    "        # print(model.generate(\"Once\", 10, temperature=0, use_past_kv_cache=False, verbose=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ablating the disjoint set and ablating to a different prompt - both cursed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "302391b955e341839e382abd2a8945c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OnceOnceOnceOnceOnceOnceOnceOnceOnceOnceOnce\n"
     ]
    }
   ],
   "source": [
    "# # Try ablating the disjoint set\n",
    "# unspecified_neuron_tuples = get_unspecified_neurons(model, dla_layer_neuron_tuples)\n",
    "# layer_neuron_dict = haystack_utils.get_neurons_by_layer(unspecified_neuron_tuples)\n",
    "\n",
    "# sorted_dla_layer_neuron_tuples = []\n",
    "# sorted_acts = []\n",
    "\n",
    "# for layer in layer_neuron_dict.keys():\n",
    "#     neurons = layer_neuron_dict[layer]\n",
    "#     mean_acts = haystack_utils.get_mlp_activations(filtered_prompts, layer, model, context_crop_start=2, hook_pre=False, neurons=neurons, disable_tqdm=True)\n",
    "#     sorted_dla_layer_neuron_tuples.extend([(layer, neuron) for neuron in neurons])\n",
    "#     sorted_acts.extend(mean_acts)\n",
    "#     assert len(sorted_dla_layer_neuron_tuples) == len(sorted_acts)\n",
    "\n",
    "# hooks = hook_utils.get_ablate_context_neurons_hooks(sorted_dla_layer_neuron_tuples, sorted_acts)\n",
    "# with model.hooks(hooks):\n",
    "#     print(model.generate(\"Once\", 10, temperature=0, use_past_kv_cache=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d8620d32e294e72a32c5072c1c1d5fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time\n"
     ]
    }
   ],
   "source": [
    "# def get_resample_neurons_hooks(neurons: list[tuple[int, int]], resampled_cache):\n",
    "#     layer_neurons = haystack_utils.get_neurons_by_layer(neurons)\n",
    "#     hooks = []\n",
    "#     for layer, neurons in layer_neurons.items():\n",
    "#         def resample_neurons_hook(value, hook):\n",
    "#             resample_cache_slice = np.s_[:value.shape[0], :value.shape[1], neurons]\n",
    "#             value[:, :, neurons] = resampled_cache[hook.name][resample_cache_slice]\n",
    "#             return value\n",
    "#         hooks.append((f'blocks.{layer}.mlp.hook_post', resample_neurons_hook))\n",
    "#     return hooks\n",
    "\n",
    "# # Try again with resampled activations\n",
    "# _, resample_cache = model.run_with_cache([\"Once upon a time\"])\n",
    "# hooks = get_resample_neurons_hooks(sorted_dla_layer_neuron_tuples, resample_cache)\n",
    "# with model.hooks(hooks):\n",
    "#     print(model.generate(\"Once\", 3, temperature=0, use_past_kv_cache=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Causal study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [01:30<00:00, 22.70s/it]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "compare_dla_and_ablation() got an unexpected keyword argument 'attrs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[132], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m losses \u001b[39m=\u001b[39m get_neuron_loss_increases(\u001b[39m\"\u001b[39m\u001b[39mOnce upon a time\u001b[39m\u001b[39m\"\u001b[39m, positionwise\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m      3\u001b[0m num_neurons \u001b[39m=\u001b[39m \u001b[39m20\u001b[39m\n\u001b[0;32m----> 4\u001b[0m compare_dla_and_ablation(token_index\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m, attrs\u001b[39m=\u001b[39;49mattrs, losses\u001b[39m=\u001b[39;49mlosses, num_neurons\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m)\n",
      "\u001b[0;31mTypeError\u001b[0m: compare_dla_and_ablation() got an unexpected keyword argument 'attrs'"
     ]
    }
   ],
   "source": [
    "attrs, labels = neuron_DLA(\"Once upon a time\", model, pos=np.s_[-4:])\n",
    "losses = get_neuron_loss_increases(\"Once upon a time\", positionwise=True)\n",
    "num_neurons = 20\n",
    "compare_dla_and_ablation(token_index=0, attrs=attrs, losses=losses, num_neurons=10)\n",
    "\n",
    "# Get \"upon\" neurons\n",
    "# token_index = 1\n",
    "# loss_increases = losses[token_index]\n",
    "# sorted_ablation_tuples, sorted_acts = get_hook_inputs_for_token_index(loss_increases)\n",
    "# print(sorted_ablation_tuples[:num_neurons], sorted_acts[:num_neurons])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "453eb893d3494492a582f8dfa5b055bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, there was a little girl named\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4561bfd9ec644da2b2e42d570c1b23cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, there was a little girl named\n"
     ]
    }
   ],
   "source": [
    "# Get \"upon\" neurons\n",
    "token_index = 1\n",
    "sorted_ablation_tuples, sorted_acts = get_hook_inputs_for_token_index(losses[token_index])\n",
    "hooks = hook_utils.get_ablate_context_neurons_hooks(sorted_ablation_tuples[:num_neurons], sorted_acts[:num_neurons])\n",
    "with model.hooks(hooks):\n",
    "    print(model.generate(\"Once\", 10, temperature=0, use_past_kv_cache=False))\n",
    "\n",
    "hooks = hook_utils.get_ablate_context_neurons_hooks(sorted_ablation_tuples[:2], sorted_acts[:2])\n",
    "with model.hooks(hooks):\n",
    "    print(model.generate(\"Once\", 10, temperature=0, use_past_kv_cache=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model roneneldan/TinyStories-33M into HookedTransformer\n",
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [01:27<00:00, 21.77s/it]\n"
     ]
    }
   ],
   "source": [
    "model = HookedTransformer.from_pretrained(\"roneneldan/TinyStories-33M\",\n",
    "    center_unembed=True,\n",
    "    center_writing_weights=True,\n",
    "    fold_ln=True,\n",
    "    device=\"cuda\")\n",
    "\n",
    "attrs, labels = neuron_DLA(\"Once upon a time\", model, pos=np.s_[-4:])\n",
    "losses = get_neuron_loss_increases(\"Once upon a time\", positionwise=True, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 12288]) torch.Size([12288, 4])\n",
      "DLA:\n",
      "[(0, 197), (1, 1221), (0, 2869), (3, 2448), (0, 1322), (0, 983), (0, 746), (0, 2170), (0, 2995), (0, 108)]\n",
      "tensor([1.4333, 0.5989, 0.5042, 0.4689, 0.4635, 0.4530, 0.4415, 0.4353, 0.3970,\n",
      "        0.3825], device='cuda:0')\n",
      "Ablation:\n",
      "[(0, 197), (0, 983), (0, 2037), (1, 2614), (1, 1221), (0, 742), (3, 939), (0, 367), (0, 1794), (0, 1322)]\n",
      "tensor([ 1.4333,  0.4530, -0.2397,  0.1655,  0.5989, -0.0087,  0.0752,  0.2479,\n",
      "         0.3282,  0.4635], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(attrs.shape, losses.shape)\n",
    "num_neurons = 10\n",
    "compare_dla_and_ablation(token_index=0, attrs=attrs, losses=losses, num_neurons=num_neurons, model=model)\n",
    "\n",
    "# Get \"upon\" neurons\n",
    "# token_index = 1\n",
    "# loss_increases = losses[token_index]\n",
    "# sorted_ablation_tuples, sorted_acts = get_hook_inputs_for_token_index(loss_increases, model=model)\n",
    "# print(sorted_ablation_tuples[:num_neurons], sorted_acts[:num_neurons])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get \"upon\" neurons\n",
    "token_index = 3\n",
    "loss_increases = losses[token_index]\n",
    "sorted_ablation_tuples, sorted_acts = get_hook_inputs_for_token_index(loss_increases, model, k=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n",
      "[(0, 2171), (0, 1304), (0, 2869), (0, 1371), (0, 563), (0, 2556), (0, 1613), (0, 321), (0, 797), (0, 372), (0, 367), (0, 2992), (0, 637), (0, 2663), (0, 954), (0, 1284), (0, 916), (0, 1332), (0, 2505), (0, 1701), (0, 2932), (0, 286), (0, 1085), (0, 2317), (0, 1714), (0, 823), (0, 1322), (0, 2814), (0, 493), (0, 1780), (0, 1404), (0, 951), (0, 459), (0, 2128), (0, 2092), (0, 3010), (0, 2961), (0, 983), (0, 269), (0, 2834), (0, 1165), (0, 3024), (0, 1975), (0, 1533), (0, 361), (0, 1563), (0, 2964), (0, 1825), (0, 1033), (0, 2210), (0, 1909), (0, 828), (0, 526), (0, 2616), (0, 1463), (0, 2913), (0, 2930), (0, 2759), (0, 663), (0, 2698), (0, 1089), (0, 1116), (0, 1733), (0, 1633), (0, 739), (0, 965), (0, 1527), (0, 2516), (0, 794), (0, 1084), (0, 2594), (0, 925), (0, 3068), (0, 1879), (0, 2694), (0, 533), (0, 477), (0, 1949), (0, 1428), (0, 1034), (0, 187), (0, 2349), (0, 1699), (0, 549), (0, 100), (0, 2891), (0, 979), (0, 1938), (0, 1736), (0, 955), (0, 811), (0, 2533), (0, 1487), (0, 1118), (0, 2695), (0, 1783), (0, 91), (0, 1006), (0, 1662), (0, 2898), (0, 108), (0, 2919), (0, 902), (0, 2995), (0, 890), (0, 14), (0, 650), (0, 1950), (0, 720), (0, 2574)]\n",
      "[tensor(-0.0389, device='cuda:0'), tensor(-0.0275, device='cuda:0'), tensor(0.1754, device='cuda:0'), tensor(-0.0556, device='cuda:0'), tensor(-0.0177, device='cuda:0'), tensor(-0.0790, device='cuda:0'), tensor(-0.0872, device='cuda:0'), tensor(-0.0540, device='cuda:0'), tensor(0.1647, device='cuda:0'), tensor(0.0161, device='cuda:0'), tensor(-0.0028, device='cuda:0'), tensor(-0.0697, device='cuda:0'), tensor(-0.0247, device='cuda:0'), tensor(-0.0213, device='cuda:0'), tensor(-0.0657, device='cuda:0'), tensor(0.1047, device='cuda:0'), tensor(-0.0484, device='cuda:0'), tensor(0.0422, device='cuda:0'), tensor(-0.0260, device='cuda:0'), tensor(-0.0302, device='cuda:0'), tensor(-0.0505, device='cuda:0'), tensor(-0.0379, device='cuda:0'), tensor(-0.0324, device='cuda:0'), tensor(0.0264, device='cuda:0'), tensor(-0.0580, device='cuda:0'), tensor(-0.0669, device='cuda:0'), tensor(2.2310, device='cuda:0'), tensor(0.0294, device='cuda:0'), tensor(-0.0219, device='cuda:0'), tensor(0.0362, device='cuda:0'), tensor(-0.0468, device='cuda:0'), tensor(-0.0312, device='cuda:0'), tensor(-0.0442, device='cuda:0'), tensor(-0.0341, device='cuda:0'), tensor(-0.0272, device='cuda:0'), tensor(0.0229, device='cuda:0'), tensor(-0.0703, device='cuda:0'), tensor(1.5116, device='cuda:0'), tensor(-0.0017, device='cuda:0'), tensor(0.0155, device='cuda:0'), tensor(-0.0186, device='cuda:0'), tensor(0.0403, device='cuda:0'), tensor(-0.0262, device='cuda:0'), tensor(-0.0515, device='cuda:0'), tensor(-0.0185, device='cuda:0'), tensor(-0.0435, device='cuda:0'), tensor(0.0550, device='cuda:0'), tensor(-0.0585, device='cuda:0'), tensor(-0.0265, device='cuda:0'), tensor(-0.0302, device='cuda:0'), tensor(-0.0560, device='cuda:0'), tensor(-0.0194, device='cuda:0'), tensor(-0.0139, device='cuda:0'), tensor(-0.0555, device='cuda:0'), tensor(-0.0183, device='cuda:0'), tensor(-0.0791, device='cuda:0'), tensor(-0.0053, device='cuda:0'), tensor(-0.0091, device='cuda:0'), tensor(-0.0383, device='cuda:0'), tensor(-0.0239, device='cuda:0'), tensor(-0.0521, device='cuda:0'), tensor(-0.0642, device='cuda:0'), tensor(-0.0556, device='cuda:0'), tensor(0.2679, device='cuda:0'), tensor(0.0156, device='cuda:0'), tensor(0.5999, device='cuda:0'), tensor(-0.0599, device='cuda:0'), tensor(-0.0381, device='cuda:0'), tensor(-0.0578, device='cuda:0'), tensor(-0.0798, device='cuda:0'), tensor(-0.0403, device='cuda:0'), tensor(0.0476, device='cuda:0'), tensor(-0.0312, device='cuda:0'), tensor(-0.0500, device='cuda:0'), tensor(0.0067, device='cuda:0'), tensor(-0.0241, device='cuda:0'), tensor(-0.0338, device='cuda:0'), tensor(0.0034, device='cuda:0'), tensor(-0.0342, device='cuda:0'), tensor(-0.0777, device='cuda:0'), tensor(-0.0633, device='cuda:0'), tensor(-0.0162, device='cuda:0'), tensor(-0.0568, device='cuda:0'), tensor(-0.0174, device='cuda:0'), tensor(0.0058, device='cuda:0'), tensor(0.0193, device='cuda:0'), tensor(-0.0479, device='cuda:0'), tensor(-0.0575, device='cuda:0'), tensor(-0.0367, device='cuda:0'), tensor(-0.0405, device='cuda:0'), tensor(-0.0516, device='cuda:0'), tensor(-0.0437, device='cuda:0'), tensor(0.0121, device='cuda:0'), tensor(-0.0595, device='cuda:0'), tensor(-0.0098, device='cuda:0'), tensor(-0.0449, device='cuda:0'), tensor(0.0108, device='cuda:0'), tensor(-0.0461, device='cuda:0'), tensor(-0.0311, device='cuda:0'), tensor(-0.0545, device='cuda:0'), tensor(0.0360, device='cuda:0'), tensor(-0.0717, device='cuda:0'), tensor(-0.0151, device='cuda:0'), tensor(3.2731, device='cuda:0'), tensor(0.0195, device='cuda:0'), tensor(0.4053, device='cuda:0'), tensor(-0.0191, device='cuda:0'), tensor(0.0407, device='cuda:0'), tensor(-0.0310, device='cuda:0'), tensor(-0.0474, device='cuda:0')]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a3f1d4da12f4bedaf0d1087b5db0569",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a little girl named Lily. She loved to\n"
     ]
    }
   ],
   "source": [
    "num_neurons=110\n",
    "print(len(sorted_ablation_tuples))\n",
    "print(sorted_ablation_tuples[:num_neurons])\n",
    "print(sorted_acts[:num_neurons])\n",
    "\n",
    "hooks = hook_utils.get_ablate_context_neurons_hooks(sorted_ablation_tuples[:num_neurons], sorted_acts[:num_neurons])\n",
    "with model.hooks(hooks):\n",
    "    print(model.generate(\"Once\", 10, temperature=0, use_past_kv_cache=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 191), (3, 2447), (1, 75), (0, 2995), (1, 202), (3, 1489), (2, 2471), (3, 1009), (0, 1304), (2, 952), (0, 197), (3, 2009), (1, 2984), (1, 1089), (3, 262), (0, 983), (3, 342), (3, 658), (1, 2046), (1, 854), (2, 1515), (3, 1052), (3, 1558), (0, 2505), (2, 1368), (2, 2308), (1, 1901), (1, 1876), (1, 846), (3, 2007), (1, 1221), (1, 33), (3, 901), (0, 459), (1, 1381), (0, 1085), (0, 1322), (3, 485), (0, 1116), (1, 110), (0, 2949), (1, 1917), (1, 569), (3, 624), (2, 3053), (3, 1482), (3, 2215), (1, 2374), (0, 563), (0, 1371), (3, 70), (0, 2556), (0, 2918), (3, 155), (0, 1540), (2, 632), (1, 538), (2, 1477), (3, 465), (1, 2100), (3, 1098), (3, 1638), (3, 929), (0, 1783), (1, 2370), (3, 1652), (1, 3046), (1, 469), (0, 916), (2, 1890), (0, 493), (0, 1423), (1, 1817), (3, 1792), (2, 472), (1, 2086), (3, 2787), (0, 483), (1, 2133), (0, 321), (1, 974), (1, 2837), (3, 667), (3, 2095), (3, 2701), (2, 285), (1, 1073), (3, 1475), (0, 2930), (2, 3045), (1, 88), (2, 138), (1, 947), (0, 2171), (1, 725), (2, 1492), (0, 828), (0, 2621), (3, 2800), (0, 2480)]\n"
     ]
    }
   ],
   "source": [
    "dla_layer_neuron_tuples = []\n",
    "\n",
    "values, indices = torch.topk(attrs[-1], 100, dim=-1)\n",
    "layer_indices, neuron_indices = np.unravel_index(indices.cpu().numpy(), (model.cfg.n_layers, model.cfg.d_mlp))\n",
    "dla_layer_neuron_tuples = list(zip(layer_indices.tolist(), neuron_indices.tolist()))\n",
    "\n",
    "print(dla_layer_neuron_tuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_dla_tuples, sorted_acts = get_neuron_mean_acts(dla_layer_neuron_tuples, model)\n",
    "hooks = hook_utils.get_ablate_context_neurons_hooks(sorted_dla_tuples, sorted_acts)\n",
    "\n",
    "    # print(model.generate(\"Once\", 10, temperature=0, use_past_kv_cache=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<|endoftext|>', 'Once', ' upon', ' a']\n",
      "Tokenized answer: [' time']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">        Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">15.12</span><span style=\"font-weight: bold\"> Prob:  </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5.80</span><span style=\"font-weight: bold\">% Token: | time|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m        Logit: \u001b[0m\u001b[1;36m15.12\u001b[0m\u001b[1m Prob:  \u001b[0m\u001b[1;36m5.80\u001b[0m\u001b[1m% Token: | time|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 15.12 Prob:  5.80% Token: | time|\n",
      "Top 1th token. Logit: 14.99 Prob:  5.10% Token: | dog|\n",
      "Top 2th token. Logit: 14.77 Prob:  4.08% Token: | cat|\n",
      "Top 3th token. Logit: 14.71 Prob:  3.84% Token: | mighty|\n",
      "Top 4th token. Logit: 14.69 Prob:  3.75% Token: | tim|\n",
      "Top 5th token. Logit: 14.27 Prob:  2.46% Token: | little|\n",
      "Top 6th token. Logit: 14.26 Prob:  2.44% Token: | hot|\n",
      "Top 7th token. Logit: 14.19 Prob:  2.28% Token: | story|\n",
      "Top 8th token. Logit: 14.17 Prob:  2.24% Token: | car|\n",
      "Top 9th token. Logit: 14.09 Prob:  2.07% Token: | small|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' time'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' time'\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with model.hooks(hooks):\n",
    "    utils.test_prompt(\"Once upon a\", \"time\", model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
