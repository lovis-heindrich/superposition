{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from transformer_lens import HookedTransformer\n",
    "from tqdm.auto import tqdm\n",
    "import plotly.io as pio\n",
    "import numpy as np\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import wandb\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "import torch.nn.init as init\n",
    "import pickle\n",
    "import os\n",
    "from pathlib import Path\n",
    "from jaxtyping import Int, Float\n",
    "from torch import Tensor\n",
    "import einops\n",
    "import json\n",
    "from collections import Counter\n",
    "from datasets import load_dataset\n",
    "import requests\n",
    "import pandas as pd\n",
    "from ipywidgets import interact, IntSlider\n",
    "import logging\n",
    "from process_tiny_stories_data import load_tinystories_validation_prompts, load_tinystories_tokens\n",
    "\n",
    "pio.renderers.default = \"notebook_connected\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "torch.autograd.set_grad_enabled(False)\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "logging.basicConfig(format='(%(levelname)s) %(asctime)s: %(message)s', level=logging.INFO, datefmt='%I:%M:%S')\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')  # Add the parent directory to the system path\n",
    "import utils.haystack_utils as haystack_utils\n",
    "from sparse_coding.train_autoencoder import AutoEncoder\n",
    "from utils.autoencoder_utils import custom_forward, AutoEncoderConfig, evaluate_autoencoder_reconstruction, get_encoder_feature_frequencies\n",
    "import utils.haystack_utils as haystack_utils\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb650bdd25154070978a7026aa3388f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/1.08k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3719e985be4d440795ac76bd92d9564e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/323M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bab9ff6f721f4b7886fcbde9c35f8b3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/722 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dadb212c7474ecda294f9488869cfcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac6d3e77d86643cabea6a0639d57ba28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00b79fdbc6ce4e85ada08f169da8c877",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eea2359b01ef43a1996d1f274059fd62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/438 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model tiny-stories-2L-33M into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "model = HookedTransformer.from_pretrained(\n",
    "        \"tiny-stories-2L-33M\",\n",
    "        center_unembed=True,\n",
    "        center_writing_weights=True,\n",
    "        fold_ln=True,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "def load_encoder(save_name, model_name):\n",
    "    with open(f\"{model_name}/{save_name}.json\", \"r\") as f:\n",
    "        cfg = json.load(f)\n",
    "\n",
    "    cfg = AutoEncoderConfig(\n",
    "        cfg[\"layer\"], cfg[\"act\"], cfg[\"expansion_factor\"], cfg[\"l1_coeff\"]\n",
    "    )\n",
    "\n",
    "    if cfg.act_name == \"hook_mlp_out\":\n",
    "        d_in = model.cfg.d_model  # d_mlp\n",
    "    else:\n",
    "        d_in = model.cfg.d_mlp\n",
    "    d_hidden = d_in * cfg.expansion_factor\n",
    "\n",
    "    encoder = AutoEncoder(d_hidden, cfg.l1_coeff, d_in)\n",
    "    encoder.load_state_dict(torch.load(os.path.join(model_name, save_name + \".pt\")))\n",
    "    encoder.to(device)\n",
    "    return encoder, cfg\n",
    "\n",
    "\n",
    "def load_checkpoint(save_name, model_name, checkpoint_save_name):\n",
    "    with open(f\"{model_name}/{save_name}.json\", \"r\") as f:\n",
    "        cfg = json.load(f)\n",
    "\n",
    "    cfg = AutoEncoderConfig(\n",
    "        cfg[\"layer\"], cfg[\"act\"], cfg[\"expansion_factor\"], cfg[\"l1_coeff\"]\n",
    "    )\n",
    "\n",
    "    if cfg.act_name == \"hook_mlp_out\":\n",
    "        d_in = model.cfg.d_model  # d_mlp\n",
    "    else:\n",
    "        d_in = model.cfg.d_mlp\n",
    "    d_hidden = d_in * cfg.expansion_factor\n",
    "\n",
    "    encoder = AutoEncoder(d_hidden, cfg.l1_coeff, d_in)\n",
    "    encoder.load_state_dict(torch.load(os.path.join(model_name, checkpoint_save_name)))\n",
    "    encoder.to(device)\n",
    "    return encoder, cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/luciaquirke/code/superposition/sparse_coding/eval_tiny_stories_checkpoints.ipynb Cell 3\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/luciaquirke/code/superposition/sparse_coding/eval_tiny_stories_checkpoints.ipynb#W2sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m print_model_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mTinyStories 2L 33M Checkpoints\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/luciaquirke/code/superposition/sparse_coding/eval_tiny_stories_checkpoints.ipynb#W2sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m checkpoint_pattern \u001b[39m=\u001b[39m \u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.*_(\u001b[39m\u001b[39m\\\u001b[39m\u001b[39md+)\u001b[39m\u001b[39m\\\u001b[39m\u001b[39m.pt$\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/luciaquirke/code/superposition/sparse_coding/eval_tiny_stories_checkpoints.ipynb#W2sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m final_checkpoint \u001b[39m=\u001b[39m load_encoder(save_name, model_name)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/luciaquirke/code/superposition/sparse_coding/eval_tiny_stories_checkpoints.ipynb#W2sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmean_max_cosine_sim\u001b[39m(first, second):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/luciaquirke/code/superposition/sparse_coding/eval_tiny_stories_checkpoints.ipynb#W2sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     result \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros(\u001b[39mlen\u001b[39m(first))\n",
      "\u001b[1;32m/Users/luciaquirke/code/superposition/sparse_coding/eval_tiny_stories_checkpoints.ipynb Cell 3\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/luciaquirke/code/superposition/sparse_coding/eval_tiny_stories_checkpoints.ipynb#W2sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m d_hidden \u001b[39m=\u001b[39m d_in \u001b[39m*\u001b[39m cfg\u001b[39m.\u001b[39mexpansion_factor\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/luciaquirke/code/superposition/sparse_coding/eval_tiny_stories_checkpoints.ipynb#W2sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m encoder \u001b[39m=\u001b[39m AutoEncoder(d_hidden, cfg\u001b[39m.\u001b[39ml1_coeff, d_in)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/luciaquirke/code/superposition/sparse_coding/eval_tiny_stories_checkpoints.ipynb#W2sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m encoder\u001b[39m.\u001b[39mload_state_dict(torch\u001b[39m.\u001b[39;49mload(os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(model_name, save_name \u001b[39m+\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39m.pt\u001b[39;49m\u001b[39m\"\u001b[39;49m)))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/luciaquirke/code/superposition/sparse_coding/eval_tiny_stories_checkpoints.ipynb#W2sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m encoder\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/luciaquirke/code/superposition/sparse_coding/eval_tiny_stories_checkpoints.ipynb#W2sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39mreturn\u001b[39;00m encoder, cfg\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.0/lib/python3.10/site-packages/torch/serialization.py:809\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[39mexcept\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    808\u001b[0m                 \u001b[39mraise\u001b[39;00m pickle\u001b[39m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(e)) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 809\u001b[0m         \u001b[39mreturn\u001b[39;00m _load(opened_zipfile, map_location, pickle_module, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mpickle_load_args)\n\u001b[1;32m    810\u001b[0m \u001b[39mif\u001b[39;00m weights_only:\n\u001b[1;32m    811\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.0/lib/python3.10/site-packages/torch/serialization.py:1172\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1170\u001b[0m unpickler \u001b[39m=\u001b[39m UnpicklerWrapper(data_file, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1171\u001b[0m unpickler\u001b[39m.\u001b[39mpersistent_load \u001b[39m=\u001b[39m persistent_load\n\u001b[0;32m-> 1172\u001b[0m result \u001b[39m=\u001b[39m unpickler\u001b[39m.\u001b[39;49mload()\n\u001b[1;32m   1174\u001b[0m torch\u001b[39m.\u001b[39m_utils\u001b[39m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[1;32m   1176\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.0/lib/python3.10/site-packages/torch/serialization.py:1142\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   1140\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1141\u001b[0m     nbytes \u001b[39m=\u001b[39m numel \u001b[39m*\u001b[39m torch\u001b[39m.\u001b[39m_utils\u001b[39m.\u001b[39m_element_size(dtype)\n\u001b[0;32m-> 1142\u001b[0m     typed_storage \u001b[39m=\u001b[39m load_tensor(dtype, nbytes, key, _maybe_decode_ascii(location))\n\u001b[1;32m   1144\u001b[0m \u001b[39mreturn\u001b[39;00m typed_storage\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.0/lib/python3.10/site-packages/torch/serialization.py:1116\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[0;34m(dtype, numel, key, location)\u001b[0m\n\u001b[1;32m   1112\u001b[0m storage \u001b[39m=\u001b[39m zip_file\u001b[39m.\u001b[39mget_storage_from_record(name, numel, torch\u001b[39m.\u001b[39mUntypedStorage)\u001b[39m.\u001b[39m_typed_storage()\u001b[39m.\u001b[39m_untyped_storage\n\u001b[1;32m   1113\u001b[0m \u001b[39m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[1;32m   1114\u001b[0m \u001b[39m# stop wrapping with TypedStorage\u001b[39;00m\n\u001b[1;32m   1115\u001b[0m typed_storage \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstorage\u001b[39m.\u001b[39mTypedStorage(\n\u001b[0;32m-> 1116\u001b[0m     wrap_storage\u001b[39m=\u001b[39mrestore_location(storage, location),\n\u001b[1;32m   1117\u001b[0m     dtype\u001b[39m=\u001b[39mdtype,\n\u001b[1;32m   1118\u001b[0m     _internal\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m   1120\u001b[0m \u001b[39mif\u001b[39;00m typed_storage\u001b[39m.\u001b[39m_data_ptr() \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1121\u001b[0m     loaded_storages[key] \u001b[39m=\u001b[39m typed_storage\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.0/lib/python3.10/site-packages/torch/serialization.py:217\u001b[0m, in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdefault_restore_location\u001b[39m(storage, location):\n\u001b[1;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m _, _, fn \u001b[39min\u001b[39;00m _package_registry:\n\u001b[0;32m--> 217\u001b[0m         result \u001b[39m=\u001b[39m fn(storage, location)\n\u001b[1;32m    218\u001b[0m         \u001b[39mif\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    219\u001b[0m             \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.0/lib/python3.10/site-packages/torch/serialization.py:182\u001b[0m, in \u001b[0;36m_cuda_deserialize\u001b[0;34m(obj, location)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_cuda_deserialize\u001b[39m(obj, location):\n\u001b[1;32m    181\u001b[0m     \u001b[39mif\u001b[39;00m location\u001b[39m.\u001b[39mstartswith(\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m--> 182\u001b[0m         device \u001b[39m=\u001b[39m validate_cuda_device(location)\n\u001b[1;32m    183\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(obj, \u001b[39m\"\u001b[39m\u001b[39m_torch_load_uninitialized\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m    184\u001b[0m             \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mdevice(device):\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.0/lib/python3.10/site-packages/torch/serialization.py:166\u001b[0m, in \u001b[0;36mvalidate_cuda_device\u001b[0;34m(location)\u001b[0m\n\u001b[1;32m    163\u001b[0m device \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39m_utils\u001b[39m.\u001b[39m_get_device_index(location, \u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    165\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available():\n\u001b[0;32m--> 166\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mAttempting to deserialize object on a CUDA \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    167\u001b[0m                        \u001b[39m'\u001b[39m\u001b[39mdevice but torch.cuda.is_available() is False. \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    168\u001b[0m                        \u001b[39m'\u001b[39m\u001b[39mIf you are running on a CPU-only machine, \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    169\u001b[0m                        \u001b[39m'\u001b[39m\u001b[39mplease use torch.load with map_location=torch.device(\u001b[39m\u001b[39m\\'\u001b[39;00m\u001b[39mcpu\u001b[39m\u001b[39m\\'\u001b[39;00m\u001b[39m) \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    170\u001b[0m                        \u001b[39m'\u001b[39m\u001b[39mto map your storages to the CPU.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    171\u001b[0m device_count \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mdevice_count()\n\u001b[1;32m    172\u001b[0m \u001b[39mif\u001b[39;00m device \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m device_count:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU."
     ]
    }
   ],
   "source": [
    "import re\n",
    "import torch\n",
    "\n",
    "model_name = \"tiny-stories-2L-33M-checkpoints\"\n",
    "save_name = \"18_morning_sun\"\n",
    "print_model_name = \"TinyStories 2L 33M Checkpoints\"\n",
    "\n",
    "checkpoint_pattern = r\".*_(\\d+)\\.pt$\"\n",
    "\n",
    "final_checkpoint = load_encoder(save_name, model_name)\n",
    "\n",
    "def mean_max_cosine_sim(first, second):\n",
    "    result = torch.zeros(len(first))\n",
    "    for i in range(len(first)):\n",
    "        current_feature = first[i]\n",
    "        max_cosine_sim = torch.max(torch.F.cosine_similarity(current_feature, second))\n",
    "        result[i] = max_cosine_sim\n",
    "    return torch.mean(result)\n",
    "\n",
    "mean_max_cosine_sims = []\n",
    "for checkpoint_name in os.listdir(model_name):\n",
    "    if re.match(checkpoint_pattern, checkpoint_name):\n",
    "        checkpoint = load_checkpoint(save_name, model_name, checkpoint_name)\n",
    "        mean_max_cosine_sims.append(mean_max_cosine_sim(final_checkpoint, checkpoint))\n",
    "\n",
    "\n",
    "# first [features, columns]\n",
    "# cosine_sim = torch.nn.CosineSimilarity(dim=1)\n",
    "# cosine_sim(current_feature, second)\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"Checkpoint\": range(len(os.listdir(model_name))),\n",
    "    \"Mean max cosine sim\": mean_max_cosine_sims\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(df, x=\"Checkpoint\", y=\"Mean max cosine sim\", title=f\"{print_model_name}: Mean Maximum Cosine Similarity\")\n",
    "fig.update_layout(\n",
    "    # xaxis_title=\"L1 coefficient\",\n",
    "    # yaxis_title=\"\",\n",
    "    # legend_title=\"\",\n",
    "    width = 800,\n",
    "    # xaxis={'tickformat':'.1e'}\n",
    ")\n",
    "# fig.update_xaxes(type='linear')\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
