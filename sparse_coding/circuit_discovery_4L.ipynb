{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import pickle\n",
    "import os\n",
    "import sys\n",
    "import requests\n",
    "import logging\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from transformer_lens import HookedTransformer\n",
    "from tqdm.auto import tqdm\n",
    "import plotly.io as pio\n",
    "import numpy as np\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import wandb\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "import torch.nn.init as init\n",
    "from pathlib import Path\n",
    "from jaxtyping import Int, Float\n",
    "from torch import Tensor\n",
    "import einops\n",
    "from collections import Counter\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from ipywidgets import interact, IntSlider\n",
    "from process_tiny_stories_data import load_tinystories_validation_prompts, load_tinystories_tokens\n",
    "from typing import Literal\n",
    "from transformer_lens.utils import test_prompt\n",
    "import pickle\n",
    "from ipywidgets import interact, IntSlider, SelectionSlider\n",
    "from transformer_lens.utils import test_prompt\n",
    "\n",
    "pio.renderers.default = \"notebook_connected\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "torch.autograd.set_grad_enabled(False)\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "logging.basicConfig(format='(%(levelname)s) %(asctime)s: %(message)s', level=logging.INFO, datefmt='%I:%M:%S')\n",
    "sys.path.append('../')  # Add the parent directory to the system path\n",
    "\n",
    "import utils.haystack_utils as haystack_utils\n",
    "from sparse_coding.train_autoencoder import AutoEncoder\n",
    "from utils.autoencoder_utils import custom_forward, AutoEncoderConfig, evaluate_autoencoder_reconstruction, get_encoder_feature_frequencies, load_encoder, generate_with_encoder\n",
    "import utils.haystack_utils as haystack_utils\n",
    "from utils.plotting_utils import line\n",
    "from utils.circuit_discovery_utils import *\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model tiny-stories-33M into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "model_name = \"tiny-stories-33M\"\n",
    "model = HookedTransformer.from_pretrained(\n",
    "    model_name,\n",
    "    center_unembed=True,\n",
    "    center_writing_weights=True,\n",
    "    fold_ln=True,\n",
    "    device=device,\n",
    ")\n",
    "model.set_use_attn_result(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L0 '2_silvery_smoke',\n",
    "# l1 '2_soft_monkey',  \n",
    "# L3 '2_driven_planet'\n",
    "run_names = ['1_skilled_universe', '2_driven_planet']\n",
    "encoders = []\n",
    "for run_name in run_names:\n",
    "    encoder, cfg = load_encoder(run_name, model_name, model)\n",
    "    cfg.run_name = run_name\n",
    "    print(cfg.run_name, cfg.layer, cfg.l1_coeff)\n",
    "    encoders.append((encoder, cfg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(INFO) 09:47:51: Loaded 21990 TinyStories validation prompts\n"
     ]
    }
   ],
   "source": [
    "prompts = load_tinystories_validation_prompts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activations(encoder, cfg, encoder_name, save_path=\"/workspace\"):\n",
    "    path = f\"{save_path}/data/{encoder_name}_activations.pkl\"\n",
    "    if os.path.exists(path):\n",
    "        with open(path, \"rb\") as f:\n",
    "            data = pickle.load(f)\n",
    "            max_activations = data[\"max_activations\"]\n",
    "            max_activation_token_indices = data[\"max_activation_token_indices\"]\n",
    "    else:\n",
    "        max_activations, max_activation_token_indices = get_max_activations(prompts, model, encoder, cfg)\n",
    "        with open(path, \"wb\") as f:\n",
    "            pickle.dump({\"max_activations\": max_activations, \"max_activation_token_indices\": max_activation_token_indices}, f)\n",
    "    return max_activations, max_activation_token_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_activation_data = {}\n",
    "for encoder, cfg in encoders:\n",
    "    run_name = cfg.run_name\n",
    "    max_activations, max_activation_token_indices = get_activations(encoder, cfg, run_name)\n",
    "    max_activation_data[run_name] = {\n",
    "        \"max_activations\": max_activations.cpu(),\n",
    "        \"max_activation_token_indices\": max_activation_token_indices.cpu()\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pairwise cosine circuit discovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_encoder, first_encoder_cfg = encoders[0]\n",
    "second_encoder, second_encoder_cfg = encoders[1]\n",
    "\n",
    "first_encoder_max_activations = max_activation_data[first_encoder_cfg.run_name][\"max_activations\"]\n",
    "first_encoder_max_activation_token_indices = max_activation_data[first_encoder_cfg.run_name][\"max_activation_token_indices\"]\n",
    "second_encoder_max_activations = max_activation_data[second_encoder_cfg.run_name][\"max_activations\"]\n",
    "second_encoder_max_activation_token_indices = max_activation_data[second_encoder_cfg.run_name][\"max_activation_token_indices\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_out = model.W_out[first_encoder_cfg.layer]\n",
    "W_in = model.W_in[second_encoder_cfg.layer]\n",
    "\n",
    "cosine_sims = torch.nn.functional.normalize(first_encoder.W_dec @ W_out, dim=-1) @ torch.nn.functional.normalize(W_in @ second_encoder.W_enc, dim=0)\n",
    "cosine_sims = torch.tril(cosine_sims)\n",
    "\n",
    "def i_to_row_col(i: int, n_cols: int = first_encoder.d_hidden):\n",
    "    row = i // n_cols\n",
    "    col = i % n_cols\n",
    "    return row, col\n",
    "\n",
    "all_sims = cosine_sims.flatten().cpu()\n",
    "top_cosine_similarities, top_cosine_sim_indices = torch.topk(all_sims, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for top_cosine_index in tqdm(top_cosine_sim_indices):\n",
    "    first_encoder_dir, second_encoder_dir = i_to_row_col(top_cosine_index)\n",
    "    top_prompts, top_prompt_token_indices = get_top_activating_examples_for_direction(prompts, first_encoder_dir, first_encoder_max_activations, first_encoder_max_activation_token_indices, k=100)\n",
    "    \n",
    "    original_losses = []\n",
    "    first_encoder_losses = []\n",
    "    second_encoder_losses = []\n",
    "    acts = []\n",
    "    ablated_acts = []\n",
    "    for prompt, pos in zip(top_prompts, top_prompt_token_indices.tolist()):\n",
    "        # Direction losses\n",
    "        original_loss, first_encoder_ablated_loss = evaluate_direction_ablation_single_prompt(prompt, first_encoder, model, first_encoder_dir, first_encoder_cfg, pos=pos)\n",
    "        _, second_encoder_ablated_loss = evaluate_direction_ablation_single_prompt(prompt, second_encoder, model, second_encoder_dir, second_encoder_cfg, pos=pos)\n",
    "        original_losses.append(original_loss)\n",
    "        first_encoder_losses.append(first_encoder_ablated_loss)\n",
    "        second_encoder_losses.append(second_encoder_ablated_loss)\n",
    "\n",
    "        # Second encoder direction activation with and without ablation\n",
    "        act = get_acts(prompt, model, second_encoder, second_encoder_cfg)[pos, second_encoder_dir].item()\n",
    "        encoder_hook_point = f\"blocks.{first_encoder_cfg.layer}.{first_encoder_cfg.act_name}\"\n",
    "        with model.hooks(fwd_hooks=[(encoder_hook_point, get_direction_ablation_hook(first_encoder, first_encoder_dir, pos))]):\n",
    "            ablated_act = get_acts(prompt, model, second_encoder, second_encoder_cfg)[pos, second_encoder_dir].item()\n",
    "        acts.append(act)\n",
    "        ablated_acts.append(ablated_act)\n",
    "\n",
    "    data.append([first_encoder_dir.item(), second_encoder_dir.item(), np.mean(original_losses), np.mean(first_encoder_losses), np.mean(second_encoder_losses), np.mean(acts), np.mean(ablated_acts)])\n",
    "df = pd.DataFrame(data, columns=[\"Encoder 1 direction\", \"Encoder 2 direction\", \"Original loss\", \"Encoder 1 direction ablation loss\", \"Encoder 2 direction ablation loss\", \"Second encoder activation\", \"Second encoder activation after ablation\"])\n",
    "df[\"Cosine similarity\"] = top_cosine_similarities.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt co occurrence analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick interesting looking prompt\n",
    "# Save activations of all directions for that prompt\n",
    "# Save last layer active directions for each earlier direction ablated individually\n",
    "# Compute AND measure for all active directions in last layer based on previous layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"This moral story teaches children that\"\n",
    "second_encoder_acts = get_acts(prompt, model, second_encoder, second_encoder_cfg)[-1]\n",
    "second_encoder_top_acts, second_encoder_top_dirs = torch.topk(second_encoder_acts, 10)\n",
    "second_encoder_direction = second_encoder_top_dirs[0].item()\n",
    "first_encoder_acts = get_acts(prompt, model, first_encoder, first_encoder_cfg)[-1]\n",
    "active_first_encoder_directions = torch.argwhere(first_encoder_acts > 1).flatten().tolist()\n",
    "original_second_encoder_act = second_encoder_top_acts[0].item()\n",
    "#px.histogram(acts.cpu().numpy(), width=700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "hook_point = first_encoder_cfg.encoder_hook_point\n",
    "for first_encoder_direction in active_first_encoder_directions:\n",
    "    ablation_hook = get_direction_ablation_hook(first_encoder, first_encoder_direction, -1)\n",
    "    with model.hooks([(hook_point, ablation_hook)]):\n",
    "        ablated_acts = get_acts(prompt, model, second_encoder, second_encoder_cfg)[-1, second_encoder_direction].item()\n",
    "    data.append([first_encoder_direction, ablated_acts])\n",
    "and_df = pd.DataFrame(data, columns=[\"First encoder direction\", \"Second encoder activation after ablation\"])\n",
    "and_df[\"Activation difference\"] = and_df[\"Second encoder activation after ablation\"] - original_second_encoder_act\n",
    "and_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "hook_point = first_encoder_cfg.encoder_hook_point\n",
    "for prompt_index, prompt in enumerate(prompts[:2]):\n",
    "    second_encoder_acts_all_pos = get_acts(prompt, model, second_encoder, second_encoder_cfg)\n",
    "    first_encoder_acts_all_pos = get_acts(prompt, model, first_encoder, first_encoder_cfg)\n",
    "    num_tokens = second_encoder_acts_all_pos.shape[0]\n",
    "    for position in range(10, num_tokens):\n",
    "        first_encoder_acts = first_encoder_acts_all_pos[position]\n",
    "        second_encoder_acts = second_encoder_acts_all_pos[position]\n",
    "\n",
    "        second_encoder_top_acts, second_encoder_top_dirs = torch.topk(second_encoder_acts, 10)\n",
    "        second_encoder_direction = second_encoder_top_dirs[0].item()\n",
    "        active_first_encoder_directions = torch.argwhere(first_encoder_acts > 1).flatten().tolist()\n",
    "        original_second_encoder_act = second_encoder_top_acts[0].item()\n",
    "\n",
    "        for first_encoder_direction in active_first_encoder_directions:\n",
    "            ablation_hook = get_direction_ablation_hook(first_encoder, first_encoder_direction, position)\n",
    "            with model.hooks([(hook_point, ablation_hook)]):\n",
    "                ablated_acts = get_acts(prompt, model, second_encoder, second_encoder_cfg)[position, second_encoder_direction].item()\n",
    "            data.append([prompt_index, position, first_encoder_direction, second_encoder_direction, ablated_acts, original_second_encoder_act])\n",
    "and_df = pd.DataFrame(data, columns=[\"Prompt\", \"Position\", \"First encoder direction\", \"Second encoder direction\", \"Second encoder activation after ablation\", \"Second encoder activation\"])\n",
    "and_df[\"Activation difference\"] = and_df[\"Second encoder activation after ablation\"] - and_df[\"Second encoder activation\"]\n",
    "and_df = and_df.sort_values(\"Activation difference\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "and_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_examples(prompts: list[str], activations: Float[Tensor, \"n_prompts d_enc\"], direction: int, encoder: AutoEncoder, cfg: AutoEncoderConfig, n=5):\n",
    "    top_idxs = activations[:, direction].argsort(descending=True)[:n].cpu().tolist()\n",
    "    for prompt_index in top_idxs:\n",
    "        prompt = prompts[prompt_index]\n",
    "        prompt_tokens = model.to_str_tokens(model.to_tokens(prompt))\n",
    "        acts = get_acts(prompt, model, encoder, cfg)\n",
    "        direction_act = acts[:, direction].cpu().tolist()\n",
    "        max_direction_act = max(direction_act)\n",
    "        if max_direction_act > 0:\n",
    "            haystack_utils.clean_print_strings_as_html(prompt_tokens, direction_act, max_value=max_direction_act)\n",
    "\n",
    "\n",
    "print_top_examples(prompts, second_encoder_max_activations, second_encoder_direction, second_encoder, second_encoder_cfg, n=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get top token occurrences per direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_direction_token_df(max_activations, prompts, model, encoder, encoder_cfg, percentage_threshold=0.5, save_path=\"/workspace/data/top_token_occurrences\"):\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    file_name = f\"{save_path}/{encoder_cfg.run_name}_direction_token_occurrences.csv\"\n",
    "    if os.path.exists(file_name):\n",
    "        direction_df = pd.read_csv(file_name)\n",
    "    else:\n",
    "\n",
    "        token_wise_activations = eval_direction_tokens_global(max_activations, prompts, model, encoder, encoder_cfg, percentage_threshold=0.5)\n",
    "        total_occurrences = token_wise_activations.sum(1)\n",
    "        max_occurrences = token_wise_activations.max(1)[0]\n",
    "        max_occurring_token = token_wise_activations.argmax(1)\n",
    "        str_tokens = model.to_str_tokens(torch.LongTensor(list(range(model.cfg.d_vocab))))\n",
    "\n",
    "        direction_data = []\n",
    "        for direction in tqdm(range(encoder.d_hidden)):\n",
    "            total_occurrence = total_occurrences[direction].item()\n",
    "            top_occurrence = max_occurrences[direction].item()\n",
    "            top_token = model.to_single_str_token(max_occurring_token[direction].item())\n",
    "            direction_data.append([direction, total_occurrence, top_token, top_occurrence])\n",
    "\n",
    "        direction_df = pd.DataFrame(direction_data, columns=[\"Direction\", \"Total occurrences\", \"Top token\", \"Top token occurrences\"])\n",
    "        direction_df[\"Top token percent\"] = direction_df[\"Top token occurrences\"] / direction_df[\"Total occurrences\"]\n",
    "        direction_df = direction_df.dropna()\n",
    "\n",
    "    print(len(direction_df))\n",
    "    return direction_df\n",
    "\n",
    "direction_df = get_direction_token_df(first_encoder_max_activations, prompts, model, first_encoder, first_encoder_cfg, percentage_threshold=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(direction_df, x=\"Top token percent\", width=700, title=\"Per direction percentage of activations on top token\")\n",
    "fig.update_layout({\n",
    "    \"xaxis_title\": \"Top token activation percentage\",\n",
    "})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_directions = direction_df[(direction_df[\"Top token percent\"] > 0.2) & (direction_df[\"Top token percent\"] < 0.7)][\"Direction\"].tolist()\n",
    "#good_directions = direction_df[(direction_df[\"Top token percent\"] < 0.1)][\"Direction\"].tolist()\n",
    "\n",
    "print(len(good_directions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_examples(prompts: list[str], activations: Float[Tensor, \"n_prompts d_enc\"], direction: int, encoder: AutoEncoder, cfg: AutoEncoderConfig, n=5):\n",
    "    top_idxs = activations[:, direction].argsort(descending=True)[:n].cpu().tolist()\n",
    "    max_direction_act = activations[:, direction].max().item()\n",
    "    for prompt_index in top_idxs:\n",
    "        prompt = prompts[prompt_index]\n",
    "        prompt_tokens = model.to_str_tokens(model.to_tokens(prompt))\n",
    "        acts = get_acts(prompt, model, encoder, cfg)\n",
    "        direction_act = acts[:, direction].cpu().tolist()\n",
    "        if max(direction_act) > 0:\n",
    "            haystack_utils.clean_print_strings_as_html(prompt_tokens, direction_act, max_value=max_direction_act)\n",
    "\n",
    "def print_direction_example(direction, n=10):\n",
    "    print_top_examples(prompts, first_encoder_max_activations, direction, first_encoder, first_encoder_cfg, n)\n",
    "\n",
    "# Max activations\n",
    "_ = interact(print_direction_example, \n",
    "         direction=SelectionSlider(options=good_directions, value=good_directions[0], description='Direction'),\n",
    "         #direction=IntSlider(min=0, max=l0_encoder.d_hidden-1, step=1, value=0),\n",
    "         n=IntSlider(min=1, max=20, step=1, value=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quotation mark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capabilities\n",
    "# Start quotation after \"said\" or \":\" or other obvious tokens (check tokenization)\n",
    "# End quotation after \".\" if started\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prompts = []\n",
    "for prompt in prompts[:20]:\n",
    "    if \"said, \\\"\" in prompt:\n",
    "        index = prompt.index(\"said, \\\"\")\n",
    "        subprompt = prompt[:index+7]\n",
    "        assert subprompt[-1] == \"\\\"\"\n",
    "        test_prompts.append(subprompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model can definitely predict 'said, \"' trigram\n",
    "#test_prompt(test_prompts[1][:-2], \" \\\"\", model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132\n",
      "Spot. Spot saw the shiny car and said, \"Wow, Kitty, your car is so bright and clean!\" Kitty smiled and replied, \"Thank you, Spot. I polish it every day.\"\n"
     ]
    }
   ],
   "source": [
    "test_prompts = []\n",
    "for prompt in prompts[:500]:\n",
    "    if \"said, \\\"\" in prompt:\n",
    "        start_index = prompt.index(\"said, \\\"\") + 7\n",
    "        end_index = prompt.find(\".\\\"\", start_index)\n",
    "        if end_index != -1:\n",
    "            subprompt = prompt[:end_index+2]\n",
    "            test_prompts.append(subprompt)\n",
    "print(len(test_prompts))\n",
    "print(test_prompts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Spot. Spot saw the shiny car and said, \"Wow, Kitty, your car is so bright and clean!\" Kitty smiled and replied, \"Thank you, Spot. I polish it every day.\"'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# '.\"' '?\"' and '!\"' are single tokens\n",
    "test_prompts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<|endoftext|>', 'Spot', '.', ' Spot', ' saw', ' the', ' shiny', ' car', ' and', ' said', ',', ' \"', 'Wow', ',', ' Kitty', ',', ' your', ' car', ' is', ' so', ' bright', ' and', ' clean']\n",
      "Tokenized answer: ['!\"']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">        Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">28.43</span><span style=\"font-weight: bold\"> Prob: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">92.30</span><span style=\"font-weight: bold\">% Token: |!\"|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m        Logit: \u001b[0m\u001b[1;36m28.43\u001b[0m\u001b[1m Prob: \u001b[0m\u001b[1;36m92.30\u001b[0m\u001b[1m% Token: |!\"|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 28.43 Prob: 92.30% Token: |!\"|\n",
      "Top 1th token. Logit: 25.10 Prob:  3.30% Token: |.|\n",
      "Top 2th token. Logit: 24.90 Prob:  2.70% Token: |!|\n",
      "Top 3th token. Logit: 24.29 Prob:  1.47% Token: |.\"|\n",
      "Top 4th token. Logit: 21.03 Prob:  0.06% Token: | now|\n",
      "Top 5th token. Logit: 20.71 Prob:  0.04% Token: | today|\n",
      "Top 6th token. Logit: 20.27 Prob:  0.03% Token: |!\".|\n",
      "Top 7th token. Logit: 19.65 Prob:  0.01% Token: |\".|\n",
      "Top 8th token. Logit: 19.64 Prob:  0.01% Token: | like|\n",
      "Top 9th token. Logit: 19.30 Prob:  0.01% Token: |,|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">'!\"'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'!\"'\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_prompt('Spot. Spot saw the shiny car and said, \"Wow, Kitty, your car is so bright and clean', '!\"', model, prepend_space_to_answer=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DLA(prompts: list[str], model: HookedTransformer, pos=-1) -> tuple[Float[Tensor, \"component\"], list[str]]:\n",
    "    logit_attributions = []\n",
    "    for prompt in tqdm(prompts):\n",
    "        tokens = model.to_tokens(prompt)\n",
    "        answers = tokens[:, 1:]\n",
    "        tokens = tokens[:, :-1]\n",
    "        answer_residual_directions = model.tokens_to_residual_directions(answers)[:, pos]  # [batch pos d_model]\n",
    "        _, cache = model.run_with_cache(tokens)\n",
    "        accumulated_residual, labels = cache.get_full_resid_decomposition(layer=-1, pos_slice=pos, return_labels=True)\n",
    "        scaled_residual_stack = cache.apply_ln_to_stack(accumulated_residual, layer = -1, pos_slice=pos)\n",
    "        logit_attribution = einops.einsum(scaled_residual_stack, answer_residual_directions, \"component batch d_model, batch d_model -> component\") / answers.shape[1]\n",
    "        logit_attributions.append(logit_attribution)\n",
    "    \n",
    "    logit_attributions = torch.stack(logit_attributions).mean(0)\n",
    "    return logit_attributions, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DLA of '.\"', find relevant MLPs if they exist, check encoders of those MLPs for relevant directions\n",
    "dlas, labels = DLA(test_prompts, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#line(dlas.cpu().numpy(), xticks=labels, width=700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
