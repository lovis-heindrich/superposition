{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from transformer_lens import HookedTransformer\n",
    "from tqdm.auto import tqdm\n",
    "import plotly.io as pio\n",
    "import numpy as np\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import wandb\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "import torch.nn.init as init\n",
    "import pickle\n",
    "import os\n",
    "from pathlib import Path\n",
    "from jaxtyping import Int, Float\n",
    "from torch import Tensor\n",
    "import einops\n",
    "import json\n",
    "from collections import Counter\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(format='(%(levelname)s) %(asctime)s: %(message)s', level=logging.INFO, datefmt='%I:%M:%S')\n",
    "pio.renderers.default = \"notebook_connected\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "torch.autograd.set_grad_enabled(False)\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')  # Add the parent directory to the system path\n",
    "import utils.haystack_utils as haystack_utils\n",
    "from sparse_coding.train_autoencoder import AutoEncoder\n",
    "import utils.autoencoder_utils as autils\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model EleutherAI/pythia-70m into HookedTransformer\n",
      "data/german_europarl.json: Loaded 2000 examples with 152 to 2000 characters each.\n",
      "data/english_europarl.json: Loaded 2000 examples with 165 to 2000 characters each.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f38f060a1943474b88a6a93d8be05f68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6704434ec6844cbe9e755d20f330f4a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = HookedTransformer.from_pretrained(\"EleutherAI/pythia-70m\",\n",
    "    center_unembed=True,\n",
    "    center_writing_weights=True,\n",
    "    fold_ln=True,\n",
    "    device=device)\n",
    "\n",
    "german_data = haystack_utils.load_json_data(\"data/german_europarl.json\")\n",
    "english_data = haystack_utils.load_json_data(\"data/english_europarl.json\")\n",
    "\n",
    "english_activations = {}\n",
    "LAYER_TO_ABLATE = 3\n",
    "NEURONS_TO_ABLATE = [669]\n",
    "english_activations[LAYER_TO_ABLATE] = haystack_utils.get_mlp_activations(english_data[:100], LAYER_TO_ABLATE, model, mean=False)\n",
    "MEAN_ACTIVATION_INACTIVE = english_activations[LAYER_TO_ABLATE][:, NEURONS_TO_ABLATE].mean()\n",
    "\n",
    "def deactivate_neurons_hook(value, hook):\n",
    "    value[:, :, NEURONS_TO_ABLATE] = MEAN_ACTIVATION_INACTIVE\n",
    "    return value\n",
    "deactivate_neurons_fwd_hooks=[(f'blocks.{LAYER_TO_ABLATE}.mlp.hook_post', deactivate_neurons_hook)]\n",
    "\n",
    "# Load bigrams\n",
    "with open(\"./data/low_indirect_loss_trigrams.json\", \"r\") as f:\n",
    "    trigrams = json.load(f)\n",
    "\n",
    "all_ignore, valid_tokens = haystack_utils.get_weird_tokens(model, plot_norms=False)\n",
    "common_tokens = haystack_utils.get_common_tokens(german_data[:200], model, all_ignore, k=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AutoEncoderConfig(layer=4, act_name='hook_mlp_out', expansion_factor=8, l1_coeff=0.0008)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_name = \"49_fiery_mountain\"#\"25_gallant_monkey\"\n",
    "model_name = 'pythia-70m'\n",
    "path = Path('pythia-70m')\n",
    "\n",
    "with open(f\"{model_name}/{save_name}.json\", \"r\") as f:\n",
    "    cfg = json.load(f)\n",
    "\n",
    "cfg = autils.AutoEncoderConfig(cfg[\"layer\"], cfg[\"act\"], cfg[\"expansion_factor\"], cfg[\"l1_coeff\"])\n",
    "cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AutoEncoder()"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if cfg.act_name == \"hook_mlp_out\":\n",
    "    d_in = model.cfg.d_model #d_mlp\n",
    "else:\n",
    "    d_in = model.cfg.d_mlp\n",
    "d_hidden = d_in * cfg.expansion_factor\n",
    "\n",
    "encoder = AutoEncoder(d_hidden, cfg.l1_coeff, d_in)\n",
    "encoder.load_state_dict(torch.load(os.path.join(path, save_name + '.pt')))\n",
    "encoder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(INFO) 03:59:51: Top 3 directions with DLA > 0.2: [1762, 1749, 1580]\n",
      "(INFO) 03:59:51: Found 100 prompts with token 'ge'\n",
      "(INFO) 03:59:51: Counter({' Abgeord': 10, ' Angele': 10, 'sgew': 6, ' Ergebn': 3, ' ausgeze': 3, ' angeh': 3, ' angew': 3, ' festgeleg': 3, ' ausgew': 2, ' abgege': 2, 'gegeben': 2, 'räge 37': 2, 'räge.': 2, ' entgegen': 2, ' abgebe': 2, 'räge,': 2, 'läge der': 2, ' gegeben': 2, 'öge,': 1, ' durchgeht': 1, ' ergeben': 1, ' Ausgehend': 1, ' begehen': 1, 'räge unter': 1, 'räge des': 1, 'räge ak': 1, 'isgeben': 1, 'räge ab': 1, 'itergele': 1, ' aufgegr': 1, 'Abgeord': 1, 'ausgehen': 1, ' Berge von': 1, ' ungef': 1, ' ungew': 1, ' Wege zu': 1, 'etzgeber': 1, 'räge in': 1, 'sergebn': 1, ' angepas': 1, 'menge,': 1, ' ausgeü': 1, 'läge d': 1, 'ichgew': 1, ' ungere': 1, 'igegeben': 1, ' \"geist': 1, ' Ungew': 1, 'tgefund': 1, 'ausgeber': 1, ' Angeh': 1, ' Mitgef': 1, 'Entgegen': 1, 'räge gest': 1, 'räge der': 1, ' abgeben': 1, 'ückgef': 1})\n",
      "(INFO) 03:59:51: 'ord' DLA = 0.56\n",
      "(INFO) 03:59:51: Logit 'ord' when reconstructing activations with encoder: 25.44\n",
      "(INFO) 03:59:51: Logit 'ord' when reconstructing activations with encoder zeroing out the direction: 23.65\n",
      "(INFO) 03:59:51: Logprob 'ord' when reconstructing activations with encoder: -2.52\n",
      "(INFO) 03:59:51: Logprob 'ord' when reconstructing activations with encoder zeroing out the direction: -1.75\n",
      "(INFO) 03:59:51: Top boosted: (['halt', 'kl', 'ze', 'for', 'zie', 'gr', 'br', 'bn', 'fall', 'ge', 'hen', 'wo', 'be', 'hend', 'hal'], [2.5333590507507324, 2.2040746212005615, 2.1348958015441895, 2.1245172023773193, 2.0123648643493652, 1.9373162984848022, 1.85601806640625, 1.7851840257644653, 1.7016717195510864, 1.6967730522155762, 1.6546156406402588, 1.5687576532363892, 1.4214191436767578, 1.4116370677947998, 1.3780115842819214])\n",
      "(INFO) 03:59:51: Top deboosted: (['ver', 'k', 'b', 'ord', 'änder', 'h', 'gang', 'z', 'f'], [-1.0598111152648926, -0.8124403357505798, -0.8120492100715637, -0.7756561040878296, -0.6691386103630066, -0.5374684929847717, -0.4423975944519043, -0.26050034165382385, -0.026116246357560158])\n",
      "(INFO) 03:59:52: Mean loss context active: 2.13\n",
      "(INFO) 03:59:52: Mean loss context inactive: 4.88\n",
      "(INFO) 03:59:52: Mean feature activation when context neuron active: 5.23\n",
      "(INFO) 03:59:52: Mean feature activation with context neuron inactive: 4.47\n",
      "(INFO) 03:59:52: Model loss when patching through encoder with context neuron active: 2.52\n",
      "(INFO) 03:59:53: Model loss when patching through encoder with context neuron inactive: 2.73\n",
      "(INFO) 03:59:53: Mean loss when patching second trigram token through encoder: 2.53\n",
      "(INFO) 03:59:53: Mean loss when patching second trigram token through encoder and setting N1762 to activation with context neuron inactive: 2.44\n",
      "(INFO) 03:59:53: Mean loss when patching second trigram token through encoder and setting N1762 to zero: 1.75\n"
     ]
    }
   ],
   "source": [
    "def eval_trigram_direction(trigram, random_prompts, dataset_prompts, encoder_neuron):\n",
    "    correct_token_dla = autils.get_trigram_token_dla(model, encoder, encoder_neuron, trigram, cfg)\n",
    "    last_token_logit_encoded, last_token_logit_zeroed, last_token_logprob_encoded, last_token_logprob_zeroed, boosted_tokens, deboosted_tokens = autils.get_direction_logit_and_logprob_boost(random_prompts, encoder, encoder_neuron, model, trigram, common_tokens, all_ignore, cfg)\n",
    "    #autils.print_direction_activations(german_data[:2], model, encoder, encoder_neuron, cfg)\n",
    "    context_active_loss, context_ablated_loss, feature_activation_context_active, feature_activation_context_inactive = autils.get_context_effect_on_feature_activations(model, random_prompts, encoder, encoder_neuron, deactivate_neurons_fwd_hooks, cfg)\n",
    "    encoder_context_active_loss, encoder_context_inactive_loss = autils.get_encoder_token_reconstruction_losses(random_prompts, model, encoder, deactivate_neurons_fwd_hooks, cfg)\n",
    "    loss_encoder_direction_active, loss_encoder_direction_inactive, loss_encoder_direction_zeroed = autils.get_encoder_feature_reconstruction_losses(random_prompts, encoder, model, encoder_neuron, feature_activation_context_active, feature_activation_context_inactive, cfg)\n",
    "\n",
    "def eval_trigram(trigram: str):\n",
    "    random_trigram_prompts = haystack_utils.generate_random_prompts(trigram, model, common_tokens, n=100, length=20)\n",
    "    trigram_dla = autils.encoder_dla_batched(random_trigram_prompts, model, encoder, cfg)[:, -1].mean(0)\n",
    "    encoder_neurons = autils.get_directions_from_dla(trigram_dla)\n",
    "    dataset_trigram_prompts = autils.get_trigram_dataset_examples(model, trigram, german_data, max_prompts=100)\n",
    "    \n",
    "    eval_trigram_direction(trigram, random_trigram_prompts, dataset_trigram_prompts, encoder_neurons[0])\n",
    "\n",
    "eval_trigram(trigrams[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mats",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
