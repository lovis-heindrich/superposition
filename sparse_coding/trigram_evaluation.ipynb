{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from transformer_lens import HookedTransformer\n",
    "from tqdm.auto import tqdm\n",
    "import plotly.io as pio\n",
    "import numpy as np\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import wandb\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "import torch.nn.init as init\n",
    "import pickle\n",
    "import os\n",
    "from pathlib import Path\n",
    "from jaxtyping import Int, Float\n",
    "from torch import Tensor\n",
    "import einops\n",
    "import json\n",
    "from collections import Counter\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(format='(%(levelname)s) %(asctime)s: %(message)s', level=logging.WARNING, datefmt='%I:%M:%S')\n",
    "pio.renderers.default = \"notebook_connected\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "torch.autograd.set_grad_enabled(False)\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')  # Add the parent directory to the system path\n",
    "import utils.haystack_utils as haystack_utils\n",
    "from sparse_coding.train_autoencoder import AutoEncoder\n",
    "import utils.autoencoder_utils as autils\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model EleutherAI/pythia-70m into HookedTransformer\n",
      "data/german_europarl.json: Loaded 2000 examples with 152 to 2000 characters each.\n",
      "data/english_europarl.json: Loaded 2000 examples with 165 to 2000 characters each.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b4d220bece0490bb13eefa7337973d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40b3587d82094b34b45efe689301c818",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = HookedTransformer.from_pretrained(\n",
    "    \"EleutherAI/pythia-70m\",\n",
    "    center_unembed=True,\n",
    "    center_writing_weights=True,\n",
    "    fold_ln=True,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "german_data = haystack_utils.load_json_data(\"data/german_europarl.json\")\n",
    "english_data = haystack_utils.load_json_data(\"data/english_europarl.json\")\n",
    "\n",
    "english_activations = {}\n",
    "LAYER_TO_ABLATE = 3\n",
    "NEURONS_TO_ABLATE = [669]\n",
    "english_activations[LAYER_TO_ABLATE] = haystack_utils.get_mlp_activations(\n",
    "    english_data[:100], LAYER_TO_ABLATE, model, mean=False\n",
    ")\n",
    "MEAN_ACTIVATION_INACTIVE = english_activations[LAYER_TO_ABLATE][\n",
    "    :, NEURONS_TO_ABLATE\n",
    "].mean()\n",
    "\n",
    "\n",
    "def deactivate_neurons_hook(value, hook):\n",
    "    value[:, :, NEURONS_TO_ABLATE] = MEAN_ACTIVATION_INACTIVE\n",
    "    return value\n",
    "\n",
    "\n",
    "deactivate_neurons_fwd_hooks = [\n",
    "    (f\"blocks.{LAYER_TO_ABLATE}.mlp.hook_post\", deactivate_neurons_hook)\n",
    "]\n",
    "\n",
    "# Load bigrams\n",
    "with open(\"./data/low_indirect_loss_trigrams.json\", \"r\") as f:\n",
    "    trigrams = json.load(f)\n",
    "\n",
    "all_ignore, valid_tokens = haystack_utils.get_weird_tokens(model, plot_norms=False)\n",
    "common_tokens = haystack_utils.get_common_tokens(\n",
    "    german_data[:200], model, all_ignore, k=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AutoEncoderConfig(layer=4, act_name='hook_mlp_out', expansion_factor=8, l1_coeff=0.0008)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_name = \"49_fiery_mountain\"  # \"25_gallant_monkey\"\n",
    "model_name = \"pythia-70m\"\n",
    "path = Path(\"pythia-70m\")\n",
    "\n",
    "with open(f\"{model_name}/{save_name}.json\", \"r\") as f:\n",
    "    cfg = json.load(f)\n",
    "\n",
    "cfg = autils.AutoEncoderConfig(\n",
    "    cfg[\"layer\"], cfg[\"act\"], cfg[\"expansion_factor\"], cfg[\"l1_coeff\"]\n",
    ")\n",
    "cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AutoEncoder()"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if cfg.act_name == \"hook_mlp_out\":\n",
    "    d_in = model.cfg.d_model  # d_mlp\n",
    "else:\n",
    "    d_in = model.cfg.d_mlp\n",
    "d_hidden = d_in * cfg.expansion_factor\n",
    "\n",
    "encoder = AutoEncoder(d_hidden, cfg.l1_coeff, d_in)\n",
    "encoder.load_state_dict(torch.load(os.path.join(path, save_name + \".pt\")))\n",
    "encoder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "607f0ad01c704d3cb88adcf73b28b88d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/228 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def eval_trigram_direction(trigram, random_prompts, encoder_neuron):\n",
    "    correct_token_dla = autils.get_trigram_token_dla(\n",
    "        model, encoder, encoder_neuron, trigram, cfg\n",
    "    )\n",
    "    (\n",
    "        last_token_logit_encoded,\n",
    "        last_token_logit_zeroed,\n",
    "        last_token_logprob_encoded,\n",
    "        last_token_logprob_zeroed,\n",
    "        boosted_tokens,\n",
    "        deboosted_tokens,\n",
    "    ) = autils.get_direction_logit_and_logprob_boost(\n",
    "        random_prompts,\n",
    "        encoder,\n",
    "        encoder_neuron,\n",
    "        model,\n",
    "        trigram,\n",
    "        common_tokens,\n",
    "        all_ignore,\n",
    "        cfg,\n",
    "    )\n",
    "    # autils.print_direction_activations(german_data[:2], model, encoder, encoder_neuron, cfg)\n",
    "    (\n",
    "        context_active_loss,\n",
    "        context_ablated_loss,\n",
    "        feature_activation_context_active,\n",
    "        feature_activation_context_inactive,\n",
    "    ) = autils.get_context_effect_on_feature_activations(\n",
    "        model,\n",
    "        random_prompts,\n",
    "        encoder,\n",
    "        encoder_neuron,\n",
    "        deactivate_neurons_fwd_hooks,\n",
    "        cfg,\n",
    "    )\n",
    "    (\n",
    "        encoder_context_active_loss,\n",
    "        encoder_context_inactive_loss,\n",
    "    ) = autils.get_encoder_token_reconstruction_losses(\n",
    "        random_prompts, model, encoder, deactivate_neurons_fwd_hooks, cfg\n",
    "    )\n",
    "    (\n",
    "        loss_encoder_direction_active,\n",
    "        loss_encoder_direction_inactive,\n",
    "        loss_encoder_direction_zeroed,\n",
    "    ) = autils.get_encoder_feature_reconstruction_losses(\n",
    "        random_prompts,\n",
    "        encoder,\n",
    "        model,\n",
    "        encoder_neuron,\n",
    "        feature_activation_context_active,\n",
    "        feature_activation_context_inactive,\n",
    "        cfg,\n",
    "    )\n",
    "    return {\n",
    "        \"Trigram\": trigram,\n",
    "        \"Direction\": encoder_neuron,\n",
    "        \"Correct token dla\": correct_token_dla,\n",
    "        \"Last token logit encoded\": last_token_logit_encoded,\n",
    "        \"Last token logit zeroed\": last_token_logit_zeroed,\n",
    "        \"Last token logprob encoded\": last_token_logprob_encoded,\n",
    "        \"Last token logprob zeroed\": last_token_logprob_zeroed,\n",
    "        \"Context active loss\": context_active_loss,\n",
    "        \"Context ablated loss\": context_ablated_loss,\n",
    "        \"Encoder context active loss\": encoder_context_active_loss.item(),\n",
    "        \"Encoder context inactive loss\": encoder_context_inactive_loss.item(),\n",
    "        \"Encoder direction active loss\": loss_encoder_direction_active.item(),\n",
    "        \"Encoder direction inactive loss\": loss_encoder_direction_inactive.item(),\n",
    "        \"Direction activation context active\": feature_activation_context_active,\n",
    "        \"Direction activation context inactive\": feature_activation_context_inactive,\n",
    "    }\n",
    "\n",
    "\n",
    "def eval_trigram(trigram: str):\n",
    "    # End in all three trigram tokens\n",
    "    random_trigram_prompts = haystack_utils.generate_random_prompts(\n",
    "        trigram, model, common_tokens, n=100, length=20\n",
    "    )\n",
    "    trigram_dla = autils.encoder_dla_batched(\n",
    "        random_trigram_prompts, model, encoder, cfg\n",
    "    )[:, -1].mean(0)\n",
    "    encoder_neurons = autils.get_directions_from_dla(trigram_dla)\n",
    "    # Don't contain last trigram token\n",
    "    # dataset_trigram_prompts = autils.get_trigram_dataset_examples(model, trigram, german_data, max_prompts=100)\n",
    "    data = []\n",
    "    for encoder_neuron in encoder_neurons:\n",
    "        res = eval_trigram_direction(trigram, random_trigram_prompts, encoder_neuron)\n",
    "        data.append(res)\n",
    "    return data\n",
    "\n",
    "\n",
    "def eval_trigrams(trigrams: list[str]):\n",
    "    data = []\n",
    "    for trigram in tqdm(trigrams):\n",
    "        data += eval_trigram(trigram)\n",
    "    return data\n",
    "\n",
    "\n",
    "data = eval_trigrams(trigrams)\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv(f\"data/trigram_eval_{save_name}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Trigram</th>\n",
       "      <th>Direction</th>\n",
       "      <th>Correct token dla</th>\n",
       "      <th>Last token logit encoded</th>\n",
       "      <th>Last token logit zeroed</th>\n",
       "      <th>Last token logprob encoded</th>\n",
       "      <th>Last token logprob zeroed</th>\n",
       "      <th>Context active loss</th>\n",
       "      <th>Context ablated loss</th>\n",
       "      <th>Encoder context active loss</th>\n",
       "      <th>Encoder context inactive loss</th>\n",
       "      <th>Encoder direction active loss</th>\n",
       "      <th>Encoder direction inactive loss</th>\n",
       "      <th>Direction activation context active</th>\n",
       "      <th>Direction activation context inactive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Abgeord</td>\n",
       "      <td>1762</td>\n",
       "      <td>0.555725</td>\n",
       "      <td>25.235693</td>\n",
       "      <td>23.479805</td>\n",
       "      <td>-2.461441</td>\n",
       "      <td>-1.736089</td>\n",
       "      <td>2.248441</td>\n",
       "      <td>4.811219</td>\n",
       "      <td>2.461417</td>\n",
       "      <td>2.670471</td>\n",
       "      <td>2.460343</td>\n",
       "      <td>2.373877</td>\n",
       "      <td>5.151664</td>\n",
       "      <td>4.392234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Abgeord</td>\n",
       "      <td>1749</td>\n",
       "      <td>0.815784</td>\n",
       "      <td>25.235693</td>\n",
       "      <td>24.718266</td>\n",
       "      <td>-2.461441</td>\n",
       "      <td>-2.601628</td>\n",
       "      <td>2.248441</td>\n",
       "      <td>4.811219</td>\n",
       "      <td>2.461417</td>\n",
       "      <td>2.670471</td>\n",
       "      <td>2.450845</td>\n",
       "      <td>2.506464</td>\n",
       "      <td>0.865864</td>\n",
       "      <td>0.527327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Abgeord</td>\n",
       "      <td>1580</td>\n",
       "      <td>0.341286</td>\n",
       "      <td>25.235693</td>\n",
       "      <td>24.702847</td>\n",
       "      <td>-2.461441</td>\n",
       "      <td>-2.285936</td>\n",
       "      <td>2.248441</td>\n",
       "      <td>4.811219</td>\n",
       "      <td>2.461417</td>\n",
       "      <td>2.670471</td>\n",
       "      <td>2.463660</td>\n",
       "      <td>2.486050</td>\n",
       "      <td>1.243480</td>\n",
       "      <td>1.404255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Anregung</td>\n",
       "      <td>1092</td>\n",
       "      <td>0.741654</td>\n",
       "      <td>24.177307</td>\n",
       "      <td>23.716904</td>\n",
       "      <td>-1.637715</td>\n",
       "      <td>-1.746295</td>\n",
       "      <td>1.815517</td>\n",
       "      <td>6.118330</td>\n",
       "      <td>1.637728</td>\n",
       "      <td>2.021789</td>\n",
       "      <td>1.636093</td>\n",
       "      <td>1.680937</td>\n",
       "      <td>1.026926</td>\n",
       "      <td>0.541619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Anregung</td>\n",
       "      <td>4072</td>\n",
       "      <td>0.524571</td>\n",
       "      <td>24.177307</td>\n",
       "      <td>23.356323</td>\n",
       "      <td>-1.637715</td>\n",
       "      <td>-1.854022</td>\n",
       "      <td>1.815517</td>\n",
       "      <td>6.118330</td>\n",
       "      <td>1.637728</td>\n",
       "      <td>2.021789</td>\n",
       "      <td>1.635981</td>\n",
       "      <td>1.753839</td>\n",
       "      <td>1.287795</td>\n",
       "      <td>0.499931</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Trigram  Direction  Correct token dla  Last token logit encoded  \\\n",
       "0    Abgeord       1762           0.555725                 25.235693   \n",
       "1    Abgeord       1749           0.815784                 25.235693   \n",
       "2    Abgeord       1580           0.341286                 25.235693   \n",
       "3   Anregung       1092           0.741654                 24.177307   \n",
       "4   Anregung       4072           0.524571                 24.177307   \n",
       "\n",
       "   Last token logit zeroed  Last token logprob encoded  \\\n",
       "0                23.479805                   -2.461441   \n",
       "1                24.718266                   -2.461441   \n",
       "2                24.702847                   -2.461441   \n",
       "3                23.716904                   -1.637715   \n",
       "4                23.356323                   -1.637715   \n",
       "\n",
       "   Last token logprob zeroed  Context active loss  Context ablated loss  \\\n",
       "0                  -1.736089             2.248441              4.811219   \n",
       "1                  -2.601628             2.248441              4.811219   \n",
       "2                  -2.285936             2.248441              4.811219   \n",
       "3                  -1.746295             1.815517              6.118330   \n",
       "4                  -1.854022             1.815517              6.118330   \n",
       "\n",
       "   Encoder context active loss  Encoder context inactive loss  \\\n",
       "0                     2.461417                       2.670471   \n",
       "1                     2.461417                       2.670471   \n",
       "2                     2.461417                       2.670471   \n",
       "3                     1.637728                       2.021789   \n",
       "4                     1.637728                       2.021789   \n",
       "\n",
       "   Encoder direction active loss  Encoder direction inactive loss  \\\n",
       "0                       2.460343                         2.373877   \n",
       "1                       2.450845                         2.506464   \n",
       "2                       2.463660                         2.486050   \n",
       "3                       1.636093                         1.680937   \n",
       "4                       1.635981                         1.753839   \n",
       "\n",
       "   Direction activation context active  Direction activation context inactive  \n",
       "0                             5.151664                               4.392234  \n",
       "1                             0.865864                               0.527327  \n",
       "2                             1.243480                               1.404255  \n",
       "3                             1.026926                               0.541619  \n",
       "4                             1.287795                               0.499931  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Low loss increase from using encoder\n",
    "df[\"Encoder loss increase\"] = (\n",
    "    df[\"Encoder direction active loss\"] - df[\"Context active loss\"]\n",
    ").clip(lower=0)\n",
    "# High loss increase when removing direction\n",
    "df[\"Direction ablation loss increase\"] = (\n",
    "    df[\"Encoder direction inactive loss\"] - df[\"Encoder direction active loss\"]\n",
    ").clip(lower=0)\n",
    "# Look for low decoder loss increase but high direction ablation loss increase\n",
    "df[\"Interest\"] = df[\"Direction ablation loss increase\"] - df[\"Encoder loss increase\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "444 385 394 428 444\n"
     ]
    }
   ],
   "source": [
    "boosted_logporb = df[\"Last token logprob encoded\"] > df[\"Last token logprob zeroed\"]\n",
    "boosted_logit = df[\"Last token logit encoded\"] > df[\"Last token logit zeroed\"]\n",
    "pos_dla = df[\"Correct token dla\"] > 0\n",
    "filtered_df = df[boosted_logporb & boosted_logit & pos_dla].copy()\n",
    "print(len(df), len(filtered_df), sum(boosted_logporb), sum(boosted_logit), sum(pos_dla))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Trigram</th>\n",
       "      <th>Direction</th>\n",
       "      <th>Correct token dla</th>\n",
       "      <th>Last token logit encoded</th>\n",
       "      <th>Last token logit zeroed</th>\n",
       "      <th>Last token logprob encoded</th>\n",
       "      <th>Last token logprob zeroed</th>\n",
       "      <th>Context active loss</th>\n",
       "      <th>Context ablated loss</th>\n",
       "      <th>Encoder context active loss</th>\n",
       "      <th>Encoder context inactive loss</th>\n",
       "      <th>Encoder direction active loss</th>\n",
       "      <th>Encoder direction inactive loss</th>\n",
       "      <th>Direction activation context active</th>\n",
       "      <th>Direction activation context inactive</th>\n",
       "      <th>Encoder loss increase</th>\n",
       "      <th>Direction ablation loss increase</th>\n",
       "      <th>Interest</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>innergeme</td>\n",
       "      <td>4072</td>\n",
       "      <td>1.157572</td>\n",
       "      <td>21.290022</td>\n",
       "      <td>16.642542</td>\n",
       "      <td>-3.099989</td>\n",
       "      <td>-6.182594</td>\n",
       "      <td>3.167385</td>\n",
       "      <td>5.402523</td>\n",
       "      <td>3.100044</td>\n",
       "      <td>4.449189</td>\n",
       "      <td>3.085035</td>\n",
       "      <td>3.690939</td>\n",
       "      <td>3.282212</td>\n",
       "      <td>2.547564</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.605904</td>\n",
       "      <td>0.605904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266</th>\n",
       "      <td>Handelsver</td>\n",
       "      <td>584</td>\n",
       "      <td>1.118627</td>\n",
       "      <td>18.120243</td>\n",
       "      <td>15.997128</td>\n",
       "      <td>-2.905767</td>\n",
       "      <td>-4.615692</td>\n",
       "      <td>2.688336</td>\n",
       "      <td>5.865644</td>\n",
       "      <td>2.905791</td>\n",
       "      <td>3.953487</td>\n",
       "      <td>2.866349</td>\n",
       "      <td>3.577502</td>\n",
       "      <td>2.664534</td>\n",
       "      <td>1.429002</td>\n",
       "      <td>0.178014</td>\n",
       "      <td>0.711152</td>\n",
       "      <td>0.533139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>China forder</td>\n",
       "      <td>62</td>\n",
       "      <td>1.345419</td>\n",
       "      <td>20.573227</td>\n",
       "      <td>14.025286</td>\n",
       "      <td>-1.399669</td>\n",
       "      <td>-6.900258</td>\n",
       "      <td>1.867226</td>\n",
       "      <td>4.890429</td>\n",
       "      <td>1.398350</td>\n",
       "      <td>2.367307</td>\n",
       "      <td>1.383643</td>\n",
       "      <td>1.885301</td>\n",
       "      <td>6.459126</td>\n",
       "      <td>5.351360</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.501658</td>\n",
       "      <td>0.501658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>374</th>\n",
       "      <td>isse forder</td>\n",
       "      <td>62</td>\n",
       "      <td>1.345419</td>\n",
       "      <td>21.364992</td>\n",
       "      <td>14.421988</td>\n",
       "      <td>-1.300429</td>\n",
       "      <td>-6.731775</td>\n",
       "      <td>1.781804</td>\n",
       "      <td>4.321985</td>\n",
       "      <td>1.300446</td>\n",
       "      <td>1.988589</td>\n",
       "      <td>1.292664</td>\n",
       "      <td>1.779058</td>\n",
       "      <td>6.286462</td>\n",
       "      <td>5.231203</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.486395</td>\n",
       "      <td>0.486395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>fer forder</td>\n",
       "      <td>62</td>\n",
       "      <td>1.345419</td>\n",
       "      <td>21.535933</td>\n",
       "      <td>14.893001</td>\n",
       "      <td>-1.214864</td>\n",
       "      <td>-6.132004</td>\n",
       "      <td>1.510983</td>\n",
       "      <td>3.983179</td>\n",
       "      <td>1.214860</td>\n",
       "      <td>2.006253</td>\n",
       "      <td>1.185222</td>\n",
       "      <td>1.666187</td>\n",
       "      <td>6.036119</td>\n",
       "      <td>4.929015</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.480965</td>\n",
       "      <td>0.480965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>innergeme</td>\n",
       "      <td>3197</td>\n",
       "      <td>0.832652</td>\n",
       "      <td>21.290022</td>\n",
       "      <td>20.563368</td>\n",
       "      <td>-3.099989</td>\n",
       "      <td>-3.590939</td>\n",
       "      <td>3.167385</td>\n",
       "      <td>5.402523</td>\n",
       "      <td>3.100044</td>\n",
       "      <td>4.449189</td>\n",
       "      <td>3.062783</td>\n",
       "      <td>3.541084</td>\n",
       "      <td>0.529961</td>\n",
       "      <td>0.047972</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.478301</td>\n",
       "      <td>0.478301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382</th>\n",
       "      <td>ktion forder</td>\n",
       "      <td>62</td>\n",
       "      <td>1.345419</td>\n",
       "      <td>21.015951</td>\n",
       "      <td>14.548864</td>\n",
       "      <td>-1.446094</td>\n",
       "      <td>-6.514494</td>\n",
       "      <td>1.728582</td>\n",
       "      <td>4.491117</td>\n",
       "      <td>1.446125</td>\n",
       "      <td>1.951242</td>\n",
       "      <td>1.420306</td>\n",
       "      <td>1.888897</td>\n",
       "      <td>5.752903</td>\n",
       "      <td>4.808158</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.468591</td>\n",
       "      <td>0.468591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>agen forder</td>\n",
       "      <td>62</td>\n",
       "      <td>1.345419</td>\n",
       "      <td>21.917458</td>\n",
       "      <td>15.129575</td>\n",
       "      <td>-1.178817</td>\n",
       "      <td>-5.867035</td>\n",
       "      <td>1.520251</td>\n",
       "      <td>3.618384</td>\n",
       "      <td>1.178815</td>\n",
       "      <td>1.698394</td>\n",
       "      <td>1.166545</td>\n",
       "      <td>1.518345</td>\n",
       "      <td>6.298723</td>\n",
       "      <td>5.314038</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.351800</td>\n",
       "      <td>0.351800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Franco forder</td>\n",
       "      <td>62</td>\n",
       "      <td>1.345419</td>\n",
       "      <td>22.303532</td>\n",
       "      <td>14.885261</td>\n",
       "      <td>-0.786667</td>\n",
       "      <td>-6.230092</td>\n",
       "      <td>1.400424</td>\n",
       "      <td>3.575026</td>\n",
       "      <td>0.786667</td>\n",
       "      <td>1.445746</td>\n",
       "      <td>0.764717</td>\n",
       "      <td>1.108495</td>\n",
       "      <td>6.745555</td>\n",
       "      <td>5.714249</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.343777</td>\n",
       "      <td>0.343777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>357</th>\n",
       "      <td>ige Abge</td>\n",
       "      <td>384</td>\n",
       "      <td>0.599724</td>\n",
       "      <td>22.893623</td>\n",
       "      <td>16.822409</td>\n",
       "      <td>-2.662203</td>\n",
       "      <td>-5.835008</td>\n",
       "      <td>2.695361</td>\n",
       "      <td>5.055069</td>\n",
       "      <td>2.662180</td>\n",
       "      <td>3.176529</td>\n",
       "      <td>2.664087</td>\n",
       "      <td>2.975869</td>\n",
       "      <td>5.693321</td>\n",
       "      <td>4.817931</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.311782</td>\n",
       "      <td>0.311782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424</th>\n",
       "      <td>ung forder</td>\n",
       "      <td>62</td>\n",
       "      <td>1.345419</td>\n",
       "      <td>22.731911</td>\n",
       "      <td>15.661656</td>\n",
       "      <td>-0.879690</td>\n",
       "      <td>-5.423447</td>\n",
       "      <td>0.992219</td>\n",
       "      <td>3.258415</td>\n",
       "      <td>0.879722</td>\n",
       "      <td>1.304010</td>\n",
       "      <td>0.868446</td>\n",
       "      <td>1.161737</td>\n",
       "      <td>6.471165</td>\n",
       "      <td>5.418386</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.293291</td>\n",
       "      <td>0.293291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>436</th>\n",
       "      <td>wei Abge</td>\n",
       "      <td>384</td>\n",
       "      <td>0.599724</td>\n",
       "      <td>23.983540</td>\n",
       "      <td>17.832367</td>\n",
       "      <td>-2.625005</td>\n",
       "      <td>-5.653204</td>\n",
       "      <td>2.646649</td>\n",
       "      <td>5.260794</td>\n",
       "      <td>2.625017</td>\n",
       "      <td>3.147731</td>\n",
       "      <td>2.621516</td>\n",
       "      <td>2.900934</td>\n",
       "      <td>5.622843</td>\n",
       "      <td>4.730890</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.279418</td>\n",
       "      <td>0.279418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>wiegende</td>\n",
       "      <td>393</td>\n",
       "      <td>0.960913</td>\n",
       "      <td>23.917593</td>\n",
       "      <td>23.148394</td>\n",
       "      <td>-1.563304</td>\n",
       "      <td>-1.829517</td>\n",
       "      <td>2.600226</td>\n",
       "      <td>5.077466</td>\n",
       "      <td>1.563264</td>\n",
       "      <td>2.833772</td>\n",
       "      <td>1.568108</td>\n",
       "      <td>1.796915</td>\n",
       "      <td>0.906874</td>\n",
       "      <td>0.098913</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.228806</td>\n",
       "      <td>0.228806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>selbstver</td>\n",
       "      <td>753</td>\n",
       "      <td>0.648003</td>\n",
       "      <td>18.173252</td>\n",
       "      <td>15.647492</td>\n",
       "      <td>-3.141588</td>\n",
       "      <td>-5.513366</td>\n",
       "      <td>2.951469</td>\n",
       "      <td>5.574380</td>\n",
       "      <td>3.141505</td>\n",
       "      <td>3.728266</td>\n",
       "      <td>3.135017</td>\n",
       "      <td>3.539271</td>\n",
       "      <td>7.334629</td>\n",
       "      <td>6.018028</td>\n",
       "      <td>0.183548</td>\n",
       "      <td>0.404254</td>\n",
       "      <td>0.220706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>423</th>\n",
       "      <td>ung bestimm</td>\n",
       "      <td>2550</td>\n",
       "      <td>1.209204</td>\n",
       "      <td>21.953039</td>\n",
       "      <td>15.788952</td>\n",
       "      <td>-2.810383</td>\n",
       "      <td>-5.488988</td>\n",
       "      <td>3.031411</td>\n",
       "      <td>5.282207</td>\n",
       "      <td>2.810365</td>\n",
       "      <td>2.969649</td>\n",
       "      <td>2.799627</td>\n",
       "      <td>3.006994</td>\n",
       "      <td>7.096591</td>\n",
       "      <td>5.901746</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.207367</td>\n",
       "      <td>0.207367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>schafft</td>\n",
       "      <td>2131</td>\n",
       "      <td>1.184455</td>\n",
       "      <td>22.205004</td>\n",
       "      <td>20.676662</td>\n",
       "      <td>-2.083634</td>\n",
       "      <td>-2.672441</td>\n",
       "      <td>2.345168</td>\n",
       "      <td>5.323439</td>\n",
       "      <td>2.083606</td>\n",
       "      <td>3.061061</td>\n",
       "      <td>2.052392</td>\n",
       "      <td>2.229801</td>\n",
       "      <td>1.512561</td>\n",
       "      <td>1.024729</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.177410</td>\n",
       "      <td>0.177410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>bstätig</td>\n",
       "      <td>3139</td>\n",
       "      <td>1.083079</td>\n",
       "      <td>24.361597</td>\n",
       "      <td>20.565302</td>\n",
       "      <td>-2.391039</td>\n",
       "      <td>-4.909102</td>\n",
       "      <td>2.646295</td>\n",
       "      <td>6.061925</td>\n",
       "      <td>2.391018</td>\n",
       "      <td>2.236691</td>\n",
       "      <td>2.392107</td>\n",
       "      <td>2.566377</td>\n",
       "      <td>3.915552</td>\n",
       "      <td>3.592585</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.174270</td>\n",
       "      <td>0.174270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>verhandelt</td>\n",
       "      <td>2570</td>\n",
       "      <td>1.009165</td>\n",
       "      <td>26.756870</td>\n",
       "      <td>26.353092</td>\n",
       "      <td>-1.551715</td>\n",
       "      <td>-1.921748</td>\n",
       "      <td>1.969084</td>\n",
       "      <td>4.524234</td>\n",
       "      <td>1.551808</td>\n",
       "      <td>1.946767</td>\n",
       "      <td>1.513257</td>\n",
       "      <td>1.677322</td>\n",
       "      <td>0.599582</td>\n",
       "      <td>0.344359</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.164065</td>\n",
       "      <td>0.164065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>Welt ihr</td>\n",
       "      <td>3965</td>\n",
       "      <td>1.741047</td>\n",
       "      <td>20.625347</td>\n",
       "      <td>18.003666</td>\n",
       "      <td>-1.094478</td>\n",
       "      <td>-3.069421</td>\n",
       "      <td>1.253318</td>\n",
       "      <td>3.619744</td>\n",
       "      <td>1.094468</td>\n",
       "      <td>1.136612</td>\n",
       "      <td>1.028258</td>\n",
       "      <td>1.190112</td>\n",
       "      <td>3.252864</td>\n",
       "      <td>2.835117</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.161854</td>\n",
       "      <td>0.161854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>bestimmte</td>\n",
       "      <td>476</td>\n",
       "      <td>1.292782</td>\n",
       "      <td>24.629007</td>\n",
       "      <td>24.191372</td>\n",
       "      <td>-1.306316</td>\n",
       "      <td>-1.432796</td>\n",
       "      <td>1.493228</td>\n",
       "      <td>4.708645</td>\n",
       "      <td>1.306297</td>\n",
       "      <td>1.858184</td>\n",
       "      <td>1.281976</td>\n",
       "      <td>1.432772</td>\n",
       "      <td>0.266867</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.150797</td>\n",
       "      <td>0.150797</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Trigram  Direction  Correct token dla  Last token logit encoded  \\\n",
       "186       innergeme       4072           1.157572                 21.290022   \n",
       "266      Handelsver        584           1.118627                 18.120243   \n",
       "10     China forder         62           1.345419                 20.573227   \n",
       "374     isse forder         62           1.345419                 21.364992   \n",
       "340      fer forder         62           1.345419                 21.535933   \n",
       "188       innergeme       3197           0.832652                 21.290022   \n",
       "382    ktion forder         62           1.345419                 21.015951   \n",
       "273     agen forder         62           1.345419                 21.917458   \n",
       "19    Franco forder         62           1.345419                 22.303532   \n",
       "357        ige Abge        384           0.599724                 22.893623   \n",
       "424      ung forder         62           1.345419                 22.731911   \n",
       "436        wei Abge        384           0.599724                 23.983540   \n",
       "255        wiegende        393           0.960913                 23.917593   \n",
       "212       selbstver        753           0.648003                 18.173252   \n",
       "423     ung bestimm       2550           1.209204                 21.953039   \n",
       "203         schafft       2131           1.184455                 22.205004   \n",
       "295         bstätig       3139           1.083079                 24.361597   \n",
       "243      verhandelt       2570           1.009165                 26.756870   \n",
       "61         Welt ihr       3965           1.741047                 20.625347   \n",
       "294       bestimmte        476           1.292782                 24.629007   \n",
       "\n",
       "     Last token logit zeroed  Last token logprob encoded  \\\n",
       "186                16.642542                   -3.099989   \n",
       "266                15.997128                   -2.905767   \n",
       "10                 14.025286                   -1.399669   \n",
       "374                14.421988                   -1.300429   \n",
       "340                14.893001                   -1.214864   \n",
       "188                20.563368                   -3.099989   \n",
       "382                14.548864                   -1.446094   \n",
       "273                15.129575                   -1.178817   \n",
       "19                 14.885261                   -0.786667   \n",
       "357                16.822409                   -2.662203   \n",
       "424                15.661656                   -0.879690   \n",
       "436                17.832367                   -2.625005   \n",
       "255                23.148394                   -1.563304   \n",
       "212                15.647492                   -3.141588   \n",
       "423                15.788952                   -2.810383   \n",
       "203                20.676662                   -2.083634   \n",
       "295                20.565302                   -2.391039   \n",
       "243                26.353092                   -1.551715   \n",
       "61                 18.003666                   -1.094478   \n",
       "294                24.191372                   -1.306316   \n",
       "\n",
       "     Last token logprob zeroed  Context active loss  Context ablated loss  \\\n",
       "186                  -6.182594             3.167385              5.402523   \n",
       "266                  -4.615692             2.688336              5.865644   \n",
       "10                   -6.900258             1.867226              4.890429   \n",
       "374                  -6.731775             1.781804              4.321985   \n",
       "340                  -6.132004             1.510983              3.983179   \n",
       "188                  -3.590939             3.167385              5.402523   \n",
       "382                  -6.514494             1.728582              4.491117   \n",
       "273                  -5.867035             1.520251              3.618384   \n",
       "19                   -6.230092             1.400424              3.575026   \n",
       "357                  -5.835008             2.695361              5.055069   \n",
       "424                  -5.423447             0.992219              3.258415   \n",
       "436                  -5.653204             2.646649              5.260794   \n",
       "255                  -1.829517             2.600226              5.077466   \n",
       "212                  -5.513366             2.951469              5.574380   \n",
       "423                  -5.488988             3.031411              5.282207   \n",
       "203                  -2.672441             2.345168              5.323439   \n",
       "295                  -4.909102             2.646295              6.061925   \n",
       "243                  -1.921748             1.969084              4.524234   \n",
       "61                   -3.069421             1.253318              3.619744   \n",
       "294                  -1.432796             1.493228              4.708645   \n",
       "\n",
       "     Encoder context active loss  Encoder context inactive loss  \\\n",
       "186                     3.100044                       4.449189   \n",
       "266                     2.905791                       3.953487   \n",
       "10                      1.398350                       2.367307   \n",
       "374                     1.300446                       1.988589   \n",
       "340                     1.214860                       2.006253   \n",
       "188                     3.100044                       4.449189   \n",
       "382                     1.446125                       1.951242   \n",
       "273                     1.178815                       1.698394   \n",
       "19                      0.786667                       1.445746   \n",
       "357                     2.662180                       3.176529   \n",
       "424                     0.879722                       1.304010   \n",
       "436                     2.625017                       3.147731   \n",
       "255                     1.563264                       2.833772   \n",
       "212                     3.141505                       3.728266   \n",
       "423                     2.810365                       2.969649   \n",
       "203                     2.083606                       3.061061   \n",
       "295                     2.391018                       2.236691   \n",
       "243                     1.551808                       1.946767   \n",
       "61                      1.094468                       1.136612   \n",
       "294                     1.306297                       1.858184   \n",
       "\n",
       "     Encoder direction active loss  Encoder direction inactive loss  \\\n",
       "186                       3.085035                         3.690939   \n",
       "266                       2.866349                         3.577502   \n",
       "10                        1.383643                         1.885301   \n",
       "374                       1.292664                         1.779058   \n",
       "340                       1.185222                         1.666187   \n",
       "188                       3.062783                         3.541084   \n",
       "382                       1.420306                         1.888897   \n",
       "273                       1.166545                         1.518345   \n",
       "19                        0.764717                         1.108495   \n",
       "357                       2.664087                         2.975869   \n",
       "424                       0.868446                         1.161737   \n",
       "436                       2.621516                         2.900934   \n",
       "255                       1.568108                         1.796915   \n",
       "212                       3.135017                         3.539271   \n",
       "423                       2.799627                         3.006994   \n",
       "203                       2.052392                         2.229801   \n",
       "295                       2.392107                         2.566377   \n",
       "243                       1.513257                         1.677322   \n",
       "61                        1.028258                         1.190112   \n",
       "294                       1.281976                         1.432772   \n",
       "\n",
       "     Direction activation context active  \\\n",
       "186                             3.282212   \n",
       "266                             2.664534   \n",
       "10                              6.459126   \n",
       "374                             6.286462   \n",
       "340                             6.036119   \n",
       "188                             0.529961   \n",
       "382                             5.752903   \n",
       "273                             6.298723   \n",
       "19                              6.745555   \n",
       "357                             5.693321   \n",
       "424                             6.471165   \n",
       "436                             5.622843   \n",
       "255                             0.906874   \n",
       "212                             7.334629   \n",
       "423                             7.096591   \n",
       "203                             1.512561   \n",
       "295                             3.915552   \n",
       "243                             0.599582   \n",
       "61                              3.252864   \n",
       "294                             0.266867   \n",
       "\n",
       "     Direction activation context inactive  Encoder loss increase  \\\n",
       "186                               2.547564               0.000000   \n",
       "266                               1.429002               0.178014   \n",
       "10                                5.351360               0.000000   \n",
       "374                               5.231203               0.000000   \n",
       "340                               4.929015               0.000000   \n",
       "188                               0.047972               0.000000   \n",
       "382                               4.808158               0.000000   \n",
       "273                               5.314038               0.000000   \n",
       "19                                5.714249               0.000000   \n",
       "357                               4.817931               0.000000   \n",
       "424                               5.418386               0.000000   \n",
       "436                               4.730890               0.000000   \n",
       "255                               0.098913               0.000000   \n",
       "212                               6.018028               0.183548   \n",
       "423                               5.901746               0.000000   \n",
       "203                               1.024729               0.000000   \n",
       "295                               3.592585               0.000000   \n",
       "243                               0.344359               0.000000   \n",
       "61                                2.835117               0.000000   \n",
       "294                               0.000000               0.000000   \n",
       "\n",
       "     Direction ablation loss increase  Interest  \n",
       "186                          0.605904  0.605904  \n",
       "266                          0.711152  0.533139  \n",
       "10                           0.501658  0.501658  \n",
       "374                          0.486395  0.486395  \n",
       "340                          0.480965  0.480965  \n",
       "188                          0.478301  0.478301  \n",
       "382                          0.468591  0.468591  \n",
       "273                          0.351800  0.351800  \n",
       "19                           0.343777  0.343777  \n",
       "357                          0.311782  0.311782  \n",
       "424                          0.293291  0.293291  \n",
       "436                          0.279418  0.279418  \n",
       "255                          0.228806  0.228806  \n",
       "212                          0.404254  0.220706  \n",
       "423                          0.207367  0.207367  \n",
       "203                          0.177410  0.177410  \n",
       "295                          0.174270  0.174270  \n",
       "243                          0.164065  0.164065  \n",
       "61                           0.161854  0.161854  \n",
       "294                          0.150797  0.150797  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_df.sort_values(\"Interest\", ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mats",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
