{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Identify all SAE directions active before quotation marks are predicted.\n",
    "## 2. Visualize them at various levels of activation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import pickle\n",
    "import os\n",
    "import sys\n",
    "import requests\n",
    "import logging\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformer_lens import HookedTransformer\n",
    "import plotly.io as pio\n",
    "import numpy as np\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import wandb\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "import torch.nn.init as init\n",
    "from pathlib import Path\n",
    "from jaxtyping import Int, Float\n",
    "from torch import Tensor\n",
    "import einops\n",
    "from collections import Counter\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from ipywidgets import interact, IntSlider\n",
    "from process_tiny_stories_data import load_tinystories_validation_prompts, load_tinystories_tokens\n",
    "from typing import Literal\n",
    "from transformer_lens.utils import test_prompt\n",
    "import pickle\n",
    "from ipywidgets import interact, IntSlider, SelectionSlider\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "pio.renderers.default = \"notebook_connected\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "torch.autograd.set_grad_enabled(False)\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "logging.basicConfig(format='(%(levelname)s) %(asctime)s: %(message)s', level=logging.INFO, datefmt='%I:%M:%S')\n",
    "sys.path.append('../')  # Add the parent directory to the system path\n",
    "\n",
    "import utils.haystack_utils as haystack_utils\n",
    "from sparse_coding.train_autoencoder import AutoEncoder\n",
    "from utils.autoencoder_utils import (\n",
    "    AutoEncoderConfig, \n",
    "    get_all_activating_test_prompts, \n",
    "    eval_direction_tokens_global, \n",
    "    get_encode_activations_hook, \n",
    "    get_activations, \n",
    "    get_acts, \n",
    "    load_encoder, \n",
    "    eval_ablation_token_rank, \n",
    "    get_direction_ablation_hook, \n",
    "    get_top_activating_examples_for_direction, \n",
    "    evaluate_direction_ablation_single_prompt,\n",
    "    eval_encoder_reconstruction_single_position,\n",
    "    get_top_direction_ablation_df,\n",
    "    get_mean_component_wise_mlp,\n",
    "    get_custom_forward_hook\n",
    ")\n",
    "from utils.plotting_utils import line, multiple_line\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model tiny-stories-2L-33M into HookedTransformer\n",
      "{'cfg_file': None, 'data_path': '/workspace/data/tinystories', 'save_path': '/workspace', 'use_wandb': True, 'num_eval_tokens': 800000, 'num_training_tokens': 500000000.0, 'batch_size': 4096, 'buffer_mult': 128, 'seq_len': 128, 'model': 'tiny-stories-2L-33M', 'layer': 0, 'act': 'mlp.hook_post', 'expansion_factor': 4, 'seed': 47, 'lr': 0.0001, 'l1_coeff': 0.0003, 'l1_target': None, 'wd': 0.01, 'beta1': 0.9, 'beta2': 0.99, 'num_eval_prompts': 200, 'save_checkpoint_models': False, 'reg': 'l1', 'finetune_encoder': None, 'dead_direction_frequency': 0.0005, 'model_batch_size': 32, 'buffer_size': 524288, 'buffer_batches': 4096, 'num_eval_batches': 195, 'd_in': 4096, 'wandb_name': 'serene-plasma-54', 'save_name': '54_serene_plasma'}\n",
      "54_serene_plasma 0 0.0003\n",
      "{'cfg_file': None, 'data_path': '/workspace/data/tinystories', 'save_path': '/workspace', 'use_wandb': True, 'num_eval_tokens': 800000, 'num_training_tokens': 500000000.0, 'batch_size': 5080, 'buffer_mult': 128, 'seq_len': 127, 'model': 'tiny-stories-2L-33M', 'layer': 1, 'act': 'mlp.hook_post', 'expansion_factor': 4, 'seed': 47, 'lr': 0.0001, 'l1_coeff': [0.0001, 0.00015], 'l1_target': None, 'wd': 0.01, 'beta1': 0.9, 'beta2': 0.99, 'num_eval_prompts': 200, 'save_checkpoint_models': False, 'reg': 'combined_hoyer_sqrt', 'finetune_encoder': None, 'dead_direction_frequency': 1e-05, 'model_batch_size': 40, 'buffer_size': 650240, 'buffer_batches': 5120, 'num_eval_batches': 157, 'd_in': 4096, 'wandb_name': 'giddy-water-189', 'save_name': '189_giddy_water'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(INFO) 03:55:13: Loaded 21990 TinyStories validation prompts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "189_giddy_water 1 [0.0001, 0.00015]\n"
     ]
    }
   ],
   "source": [
    "model_name = \"tiny-stories-2L-33M\"\n",
    "model = HookedTransformer.from_pretrained(\n",
    "    model_name,\n",
    "    center_unembed=True,\n",
    "    center_writing_weights=True,\n",
    "    fold_ln=True,\n",
    "    device=device,\n",
    ")\n",
    "model.set_use_attn_result(True)\n",
    "\n",
    "run_names = [\"54_serene_plasma\", \"189_giddy_water\"] \n",
    "encoders = []\n",
    "for run_name in run_names:\n",
    "    encoder, cfg = load_encoder(run_name, model_name, model)\n",
    "    cfg.run_name = run_name\n",
    "    print(cfg.run_name, cfg.layer, cfg.l1_coeff)\n",
    "    encoders.append((encoder, cfg))\n",
    "    \n",
    "prompts = load_tinystories_validation_prompts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAE direction activation examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_examples(prompts: list[str], activations: Float[Tensor, \"n_prompts d_enc\"], direction: int, encoder: AutoEncoder, cfg: AutoEncoderConfig, n=5):\n",
    "    top_idxs = activations[:, direction].argsort(descending=True)[:n].cpu().tolist()\n",
    "    for prompt_index in top_idxs:\n",
    "        prompt = prompts[prompt_index]\n",
    "        prompt_tokens = model.to_str_tokens(model.to_tokens(prompt))\n",
    "        acts = get_acts(prompt, model, encoder, cfg)\n",
    "        direction_act = acts[:, direction].cpu().tolist()\n",
    "        max_direction_act = max(direction_act)\n",
    "        if max_direction_act > 0:\n",
    "            print(f\"Prompt: {prompt_index}\")\n",
    "            haystack_utils.clean_print_strings_as_html(prompt_tokens, direction_act, max_value=max_direction_act, pretty_print=True)\n",
    "\n",
    "\n",
    "def get_quotation_test_prompts(model, prompts):\n",
    "    # Filter test prompts following 'said, \" [...] .\"' pattern\n",
    "    # '.\"' '?\"' and '!\"' are single tokens\n",
    "    test_prompts = []\n",
    "    for prompt in tqdm(prompts):\n",
    "        if \"said, \\\"\" in prompt:\n",
    "            start_index = prompt.index(\"said, \\\"\") + 7\n",
    "            end_index = prompt.find(\".\\\"\", start_index)\n",
    "            # Exclude long prompts - model performance degrades\n",
    "            if (end_index != -1) and (end_index < 1600):\n",
    "                subprompt = prompt[:end_index + 2]\n",
    "                tokens = model.to_tokens(subprompt)\n",
    "                last_token = model.to_single_str_token(tokens[0, -1].item())\n",
    "                if (subprompt[-2:] == \".\\\"\") and (last_token == \".\\\"\"):\n",
    "                    test_prompts.append(subprompt)\n",
    "    print(len(test_prompts), \"test prompts\")\n",
    "    return test_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Any\n",
    "def get_cached_or_build(path: str, build: Callable[[str], Any]):\n",
    "    if os.path.isfile(path):\n",
    "        return pd.read_csv(path)\n",
    "    df = build()\n",
    "    df.to_csv(path)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_and_visualize_quotation_directions():\n",
    "    max_activation_data = {}\n",
    "    for encoder, cfg in encoders:\n",
    "        run_name = cfg.run_name\n",
    "        max_activations, max_activation_token_indices = get_activations(encoder, cfg, run_name, prompts, model)\n",
    "        max_activation_data[run_name] = {\n",
    "            \"max_activations\": max_activations.cpu(),\n",
    "            \"max_activation_token_indices\": max_activation_token_indices.cpu()\n",
    "        }\n",
    "\n",
    "    test_prompts = get_quotation_test_prompts(model, prompts)\n",
    "\n",
    "    dfs = []\n",
    "    for encoder, cfg in encoders:\n",
    "        def build():\n",
    "            activating_test_prompts_l1 = get_all_activating_test_prompts(test_prompts, encoder, model, cfg, active_threshold=0.1)\n",
    "            df, loss_increases_l1 = get_top_direction_ablation_df(activating_test_prompts_l1, test_prompts, model, encoder, cfg, max_activation_data[cfg.run_name]['max_activations'])\n",
    "            df = df.sort_values(\"Loss increase\", ascending=False)\n",
    "            return df\n",
    "        \n",
    "        df = get_cached_or_build(f\"/workspace/data/{cfg.run_name}-direction-ablation-df.csv\", build)\n",
    "        top_directions = df[\"Direction\"].tolist()\n",
    "        for direction in top_directions[:5]:\n",
    "            print(f\"Direction {direction} max activating examples\")\n",
    "            print_top_examples(prompts, max_activation_data[cfg.run_name]['max_activations'], direction, encoder, cfg, 5)\n",
    "        dfs.append(df)\n",
    "    \n",
    "    return max_activation_data, test_prompts, dfs\n",
    "\n",
    "\n",
    "max_activation_data, test_prompts, dfs = find_and_visualize_quotation_directions()\n",
    "\n",
    "\n",
    "mae_interpretations = {\n",
    "    1154: \"I can't see a strong relation to quotation marks. \\\n",
    "        There are dry-prefix words, outfit-related words, and toy-related words.\",\n",
    "    4776: \"\"\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
