{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import pickle\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import torch\n",
    "from transformer_lens import HookedTransformer\n",
    "import plotly.io as pio\n",
    "import pandas as pd\n",
    "from ipywidgets import interact, IntSlider\n",
    "from process_tiny_stories_data import load_tinystories_validation_prompts, load_tinystories_tokens\n",
    "from typing import Literal\n",
    "from transformer_lens.utils import test_prompt\n",
    "\n",
    "pio.renderers.default = \"notebook_connected\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "torch.autograd.set_grad_enabled(False)\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "sys.path.append('../')  # Add the parent directory to the system path\n",
    "\n",
    "import utils.haystack_utils as haystack_utils\n",
    "from sparse_coding.train_autoencoder import AutoEncoder\n",
    "from utils.autoencoder_utils import custom_forward, AutoEncoderConfig, evaluate_autoencoder_reconstruction, get_encoder_feature_frequencies, load_encoder, generate_with_encoder\n",
    "import utils.haystack_utils as haystack_utils\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c5197cea3554083afb5ffa34096fcb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)TinyStories-33M/resolve/main/config.json:   0%|          | 0.00/968 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "886e3675aba743ee810ea9bfaaa0d4da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/291M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "304bdbee36434ffa85a4a4f9f14d8bb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)s-33M/resolve/main/tokenizer_config.json:   0%|          | 0.00/722 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfec6216514a42f5b1539f74196e9dad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)/TinyStories-33M/resolve/main/vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd0547c39da84c3683250a3b5c839b60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)/TinyStories-33M/resolve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "999e619500464b48acdb2283f9a1ebe4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)yStories-33M/resolve/main/tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bdf35d03be24680851779091792339a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)33M/resolve/main/special_tokens_map.json:   0%|          | 0.00/438 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model tiny-stories-33M into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "model_name = \"tiny-stories-33M\"\n",
    "model = HookedTransformer.from_pretrained(\n",
    "    model_name,\n",
    "    center_unembed=True,\n",
    "    center_writing_weights=True,\n",
    "    fold_ln=True,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "l0_encoder, l0_config = load_encoder('2_silvery_smoke', model_name, model)\n",
    "l1_encoder, l1_config = load_encoder('2_soft_monkey', model_name, model)\n",
    "prompts = load_tinystories_validation_prompts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SqueezeBackward1 object at 0x7f323bab1780>\n",
      "<AddBackward0 object at 0x7f323bab1780>\n",
      "None\n",
      "<UnsqueezeBackward0 object at 0x7f323bab33d0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1926/386258236.py:20: UserWarning:\n",
      "\n",
      "The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at aten/src/ATen/core/TensorBody.h:489.)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "torch.autograd.set_grad_enabled(True)\n",
    "torch.set_grad_enabled(True)\n",
    "\n",
    "# For single prompt: compare the derivative of the L1 feature wrt L0 mlp input to layer, to linear map (assuming GELU is identity)\n",
    "prompt = prompts[0]\n",
    "\n",
    "def get_encode_activations_hooks(encoder, config):\n",
    "    def encode_activations_hook(value, hook):\n",
    "        value = value.squeeze(0)\n",
    "        _, x_reconstruct, _, _, _ = encoder(value)\n",
    "        print(value.grad_fn)\n",
    "        print(x_reconstruct.grad_fn)\n",
    "        return x_reconstruct.unsqueeze(0)\n",
    "    reconstruct_hook_label = f'blocks.{config.layer}.{config.act_name}'\n",
    "    return [(reconstruct_hook_label, encode_activations_hook)]\n",
    "\n",
    "grads = []\n",
    "grad_fns = []\n",
    "def save_grad(value, hook):\n",
    "    grads.append(value.grad)\n",
    "    grad_fns.append(value.grad_fn)\n",
    "\n",
    "save_grads = [('blocks.0.mlp.hook_post', save_grad), ('blocks.1.mlp.hook_post', save_grad)]\n",
    "with model.hooks(get_encode_activations_hooks(l0_encoder, l0_config) + save_grads): #, get_encode_activations_hooks(l1_encoder, l1_config)):\n",
    "    loss = model(prompt, return_type='loss')\n",
    "\n",
    "print(grads[0])\n",
    "print(grad_fns[0])\n",
    "loss.backward()\n",
    "\n",
    "# The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. \n",
    "# Its .grad attribute won't be populated during autograd.backward(). \n",
    "# If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. \n",
    "# If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. \n",
    "# See github.com/pytorch/pytorch/pull/30531 for more informations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
