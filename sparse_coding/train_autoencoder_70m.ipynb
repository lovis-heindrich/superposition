{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from transformer_lens import HookedTransformer, ActivationCache, utils\n",
    "from jaxtyping import Float, Int, Bool\n",
    "from torch import Tensor\n",
    "from tqdm.auto import tqdm\n",
    "import plotly.io as pio\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "import argparse\n",
    "import json\n",
    "import pprint\n",
    "import numpy as np\n",
    "import random\n",
    "import torch.nn as nn\n",
    "\n",
    "pio.renderers.default = \"notebook_connected\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.autograd.set_grad_enabled(False)\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "from haystack_utils import get_mlp_activations\n",
    "import haystack_utils\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc3fc673789540ca99a5fbcbb082d1ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/166M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\heind\\miniconda3\\envs\\mats\\Lib\\site-packages\\huggingface_hub\\file_download.py:137: UserWarning:\n",
      "\n",
      "`huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\heind\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "\n",
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model EleutherAI/pythia-70m into HookedTransformer\n",
      "data/german_europarl.json: Loaded 2000 examples with 152 to 2000 characters each.\n",
      "data/english_europarl.json: Loaded 2000 examples with 165 to 2000 characters each.\n"
     ]
    }
   ],
   "source": [
    "model = HookedTransformer.from_pretrained(\"EleutherAI/pythia-70m\",\n",
    "    center_unembed=True,\n",
    "    center_writing_weights=True,\n",
    "    fold_ln=True,\n",
    "    device=device)\n",
    "\n",
    "german_data = haystack_utils.load_json_data(\"data/german_europarl.json\")\n",
    "english_data = haystack_utils.load_json_data(\"data/english_europarl.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cfg = {\n",
    "    \"seed\": 47,\n",
    "    \"batch_size\": 32,\n",
    "    \"model_batch_size\": 128,\n",
    "    \"lr\": 1e-4,\n",
    "    \"num_tokens\": int(1e7),\n",
    "    \"l1_coeff\": 3e-3,\n",
    "    \"wd\": 1e-2,\n",
    "    \"beta1\": 0.9,\n",
    "    \"beta2\": 0.99,\n",
    "    \"dict_mult\": 8,\n",
    "    \"seq_len\": 128,\n",
    "}\n",
    "cfg[\"model_batch_size\"] = cfg[\"batch_size\"] // cfg[\"seq_len\"] * 16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x257a9d1f5d0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SEED = cfg[\"seed\"]\n",
    "GENERATOR = torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "torch.set_grad_enabled(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_layers = model.cfg.n_layers\n",
    "d_model = model.cfg.d_model\n",
    "n_heads = model.cfg.n_heads\n",
    "d_head = model.cfg.d_head\n",
    "d_mlp = model.cfg.d_mlp\n",
    "d_vocab = model.cfg.d_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 2048]), torch.Size([60, 2048]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def get_mlp_acts(tokens, batch_size=1024, layer=5):\n",
    "    _, cache = model.run_with_cache(tokens, names_filter=f\"blocks.{layer}.mlp.hook_post\")\n",
    "    mlp_acts = cache[f\"blocks.{layer}.mlp.hook_post\"]\n",
    "    mlp_acts = mlp_acts.reshape(-1, d_mlp)\n",
    "    subsample = torch.randperm(mlp_acts.shape[0], generator=GENERATOR)[:batch_size]\n",
    "    subsampled_mlp_acts = mlp_acts[subsample, :]\n",
    "    return subsampled_mlp_acts, mlp_acts\n",
    "\n",
    "sub, acts = get_mlp_acts(german_data[0], batch_size=1)\n",
    "sub.shape, acts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, d_hidden, l1_coeff, dtype=torch.float32, seed=47):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(seed)\n",
    "        self.W_enc = nn.Parameter(torch.nn.init.kaiming_uniform_(torch.empty(d_mlp, d_hidden, dtype=dtype)))\n",
    "        self.W_dec = nn.Parameter(torch.nn.init.kaiming_uniform_(torch.empty(d_hidden, d_mlp, dtype=dtype)))\n",
    "        self.b_enc = nn.Parameter(torch.zeros(d_hidden, dtype=dtype))\n",
    "        self.b_dec = nn.Parameter(torch.zeros(d_mlp, dtype=dtype))\n",
    "\n",
    "        self.W_dec.data[:] = self.W_dec / self.W_dec.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        self.d_hidden = d_hidden\n",
    "        self.l1_coeff = l1_coeff\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x_cent = x - self.b_dec\n",
    "        acts = F.relu(x_cent @ self.W_enc + self.b_enc)\n",
    "        x_reconstruct = acts @ self.W_dec + self.b_dec\n",
    "        l2_loss = (x_reconstruct - x).pow(2).sum(-1).mean(0)\n",
    "        l1_loss = self.l1_coeff * (acts.abs().sum())\n",
    "        loss = l2_loss + l1_loss\n",
    "        return loss, x_reconstruct, acts, l2_loss, l1_loss\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def remove_parallel_component_of_grads(self):\n",
    "        W_dec_normed = self.W_dec / self.W_dec.norm(dim=-1, keepdim=True)\n",
    "        W_dec_grad_proj = (self.W_dec.grad * W_dec_normed).sum(-1, keepdim=True) * W_dec_normed\n",
    "        self.W_dec.grad -= W_dec_grad_proj"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mats",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
