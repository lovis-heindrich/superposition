{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from transformer_lens import HookedTransformer\n",
    "from tqdm.auto import tqdm\n",
    "import plotly.io as pio\n",
    "import numpy as np\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import wandb\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "import torch.nn.init as init\n",
    "import pickle\n",
    "\n",
    "\n",
    "pio.renderers.default = \"notebook_connected\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "#torch.autograd.set_grad_enabled(False)\n",
    "#torch.set_grad_enabled(False)\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')  # Add the parent directory to the system path\n",
    "import utils.haystack_utils as haystack_utils\n",
    "from sparse_coding.train_autoencoder import AutoEncoder\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "german_data = haystack_utils.load_json_data(\"data/german_europarl.json\")\n",
    "\n",
    "model = HookedTransformer.from_pretrained(\"EleutherAI/pythia-160m\",\n",
    "    center_unembed=True,\n",
    "    center_writing_weights=True,\n",
    "    fold_ln=True,\n",
    "    device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_in = model.cfg.d_model\n",
    "expansion_factor = 4\n",
    "autoencoder_dim = d_in * expansion_factor\n",
    "l1_coeff = 0.01\n",
    "\n",
    "our_autoencoder = AutoEncoder(autoencoder_dim, l1_coeff, d_in)\n",
    "our_autoencoder_filename = \"pythia-160m/hook_mlp_out_l8.pt\"\n",
    "our_autoencoder.load_state_dict(torch.load(our_autoencoder_filename))\n",
    "our_autoencoder.to(device)\n",
    "\n",
    "\n",
    "with open(\"pythia-160m/hook_mlp_out_l8.pkl\", \"wb\") as f:\n",
    "    pickle.dump(our_autoencoder, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate our dict\n",
    "with torch.no_grad():\n",
    "    def encode_mlp_activations_hook(value, hook):\n",
    "        value = value.squeeze(0)\n",
    "        _, x_reconstruct, _, _, _ = our_autoencoder(value)\n",
    "        return x_reconstruct.unsqueeze(0)\n",
    "\n",
    "    hooks = [(\"blocks.8.hook_mlp_out\", encode_mlp_activations_hook)]\n",
    "\n",
    "    original_losses = []\n",
    "    reconstruct_losses = []\n",
    "    for prompt in tqdm(german_data[:200]):\n",
    "        original_loss = model(prompt, return_type=\"loss\")\n",
    "        with model.hooks(hooks):\n",
    "            reconstruct_loss = model(prompt, return_type=\"loss\")\n",
    "        original_losses.append(original_loss.item())\n",
    "        reconstruct_losses.append(reconstruct_loss.item())\n",
    "\n",
    "print(f\"Average loss increase after encoding: {(np.mean(reconstruct_losses) - np.mean(original_losses)):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Logan's one\n",
    "autoencoder2.to_device(device)\n",
    "model.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    def encode_mlp_activations_hook(value, hook):\n",
    "        value = value.squeeze(0)\n",
    "        acts = autoencoder2.encode(value)\n",
    "        out = autoencoder2.decode(acts)\n",
    "        return out.unsqueeze(0)\n",
    "\n",
    "    hooks = [(\"blocks.8.hook_mlp_out\", encode_mlp_activations_hook)]\n",
    "\n",
    "    original_losses = []\n",
    "    reconstruct_losses = []\n",
    "    for prompt in tqdm(german_data[:200]):\n",
    "        original_loss = model(prompt, return_type=\"loss\")\n",
    "        with model.hooks(hooks):\n",
    "            reconstruct_loss = model(prompt, return_type=\"loss\")\n",
    "        original_losses.append(original_loss.item())\n",
    "        reconstruct_losses.append(reconstruct_loss.item())\n",
    "\n",
    "print(f\"Average loss increase after encoding: {(np.mean(reconstruct_losses) - np.mean(original_losses)):.4f}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
