{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import pickle\n",
    "import os\n",
    "import sys\n",
    "import requests\n",
    "import logging\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from transformer_lens import HookedTransformer\n",
    "from tqdm.auto import tqdm\n",
    "import plotly.io as pio\n",
    "import numpy as np\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import wandb\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "import torch.nn.init as init\n",
    "from pathlib import Path\n",
    "from jaxtyping import Int, Float\n",
    "from torch import Tensor\n",
    "import einops\n",
    "from collections import Counter\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from ipywidgets import interact, IntSlider\n",
    "from process_tiny_stories_data import load_tinystories_validation_prompts, load_tinystories_tokens\n",
    "\n",
    "pio.renderers.default = \"notebook_connected\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "torch.autograd.set_grad_enabled(False)\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "logging.basicConfig(format='(%(levelname)s) %(asctime)s: %(message)s', level=logging.INFO, datefmt='%I:%M:%S')\n",
    "sys.path.append('../')  # Add the parent directory to the system path\n",
    "\n",
    "import utils.haystack_utils as haystack_utils\n",
    "from sparse_coding.train_autoencoder import AutoEncoder\n",
    "from utils.autoencoder_utils import custom_forward, AutoEncoderConfig, evaluate_autoencoder_reconstruction, get_encoder_feature_frequencies, load_encoder\n",
    "import utils.haystack_utils as haystack_utils\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load interesting encoder, find token features, generate histogram of activations for that token. Park features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model tiny-stories-2L-33M into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "# Run overview\n",
    "model_name = \"tiny-stories-2L-33M\"\n",
    "layer_name = \"L0\"\n",
    "print_model_name = f\"{model_name}-{layer_name}\"\n",
    "\n",
    "model = HookedTransformer.from_pretrained(\n",
    "        model_name,\n",
    "        center_unembed=True,\n",
    "        center_writing_weights=True,\n",
    "        fold_ln=True,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "prompts = load_tinystories_validation_prompts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_names = [f.split(\".\")[0] for f in os.listdir(model_name) if f.endswith('.pt')]\n",
    "l0_encoder, l0_cfg = load_encoder('18_morning_sun', model_name, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "febcfe69f2a2477ea6109caadd74abc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21990 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 94.00 MiB. GPU 0 has a total capacty of 47.54 GiB of which 89.75 MiB is free. Process 5405 has 16.40 GiB memory in use. Process 32528 has 31.06 GiB memory in use. Of the allocated memory 27.63 GiB is allocated by PyTorch, and 2.34 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m/root/superposition/sparse_coding/check_monosemanticity.ipynb Cell 5\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B135.181.65.54/root/superposition/sparse_coding/check_monosemanticity.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=31'>32</a>\u001b[0m         \u001b[39mif\u001b[39;00m max_direction_act \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B135.181.65.54/root/superposition/sparse_coding/check_monosemanticity.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=32'>33</a>\u001b[0m             haystack_utils\u001b[39m.\u001b[39mclean_print_strings_as_html(prompt_tokens, direction_act, max_value\u001b[39m=\u001b[39mmax_direction_act)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B135.181.65.54/root/superposition/sparse_coding/check_monosemanticity.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=34'>35</a>\u001b[0m max_activations, max_activation_token_indices \u001b[39m=\u001b[39m get_max_activations(prompts, model, l0_encoder, l0_cfg)\n",
      "\u001b[1;32m/root/superposition/sparse_coding/check_monosemanticity.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B135.181.65.54/root/superposition/sparse_coding/check_monosemanticity.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m indices \u001b[39m=\u001b[39m []\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B135.181.65.54/root/superposition/sparse_coding/check_monosemanticity.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mfor\u001b[39;00m prompt \u001b[39min\u001b[39;00m tqdm(prompts):\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B135.181.65.54/root/superposition/sparse_coding/check_monosemanticity.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m     \u001b[39m# get encoder activations at final position\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B135.181.65.54/root/superposition/sparse_coding/check_monosemanticity.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m     acts \u001b[39m=\u001b[39m get_acts(prompt, model, encoder, cfg)[\u001b[39m0\u001b[39m, :\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B135.181.65.54/root/superposition/sparse_coding/check_monosemanticity.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m     activations\u001b[39m.\u001b[39mappend(acts)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B135.181.65.54/root/superposition/sparse_coding/check_monosemanticity.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m max_activation_per_prompt \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstack(activations)  \u001b[39m# n_prompt x d_enc\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[1;32m/root/superposition/sparse_coding/check_monosemanticity.ipynb Cell 5\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B135.181.65.54/root/superposition/sparse_coding/check_monosemanticity.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m@torch\u001b[39m\u001b[39m.\u001b[39mno_grad()\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B135.181.65.54/root/superposition/sparse_coding/check_monosemanticity.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_acts\u001b[39m(prompt: \u001b[39mstr\u001b[39m, model: HookedTransformer, encoder: AutoEncoder, cfg: AutoEncoderConfig):\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B135.181.65.54/root/superposition/sparse_coding/check_monosemanticity.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     _, cache \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mrun_with_cache(prompt, names_filter\u001b[39m=\u001b[39;49mcfg\u001b[39m.\u001b[39;49mencoder_hook_point)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B135.181.65.54/root/superposition/sparse_coding/check_monosemanticity.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     acts \u001b[39m=\u001b[39m cache[cfg\u001b[39m.\u001b[39mencoder_hook_point]\u001b[39m.\u001b[39msqueeze(\u001b[39m0\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B135.181.65.54/root/superposition/sparse_coding/check_monosemanticity.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m     _, _, mid_acts, _, _ \u001b[39m=\u001b[39m encoder(acts)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformer_lens/HookedTransformer.py:552\u001b[0m, in \u001b[0;36mHookedTransformer.run_with_cache\u001b[0;34m(self, return_cache_object, remove_batch_dim, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun_with_cache\u001b[39m(\n\u001b[1;32m    537\u001b[0m     \u001b[39mself\u001b[39m, \u001b[39m*\u001b[39mmodel_args, return_cache_object\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, remove_batch_dim\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[1;32m    538\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    545\u001b[0m     Union[ActivationCache, Dict[\u001b[39mstr\u001b[39m, torch\u001b[39m.\u001b[39mTensor]],\n\u001b[1;32m    546\u001b[0m ]:\n\u001b[1;32m    547\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    548\u001b[0m \u001b[39m    Wrapper around run_with_cache in HookedRootModule. If return_cache_object is True, this will return an\u001b[39;00m\n\u001b[1;32m    549\u001b[0m \u001b[39m    ActivationCache object, with a bunch of useful HookedTransformer specific methods, otherwise it will return a\u001b[39;00m\n\u001b[1;32m    550\u001b[0m \u001b[39m    dictionary of activations as in HookedRootModule.\u001b[39;00m\n\u001b[1;32m    551\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 552\u001b[0m     out, cache_dict \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mrun_with_cache(\n\u001b[1;32m    553\u001b[0m         \u001b[39m*\u001b[39;49mmodel_args, remove_batch_dim\u001b[39m=\u001b[39;49mremove_batch_dim, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    554\u001b[0m     )\n\u001b[1;32m    555\u001b[0m     \u001b[39mif\u001b[39;00m return_cache_object:\n\u001b[1;32m    556\u001b[0m         cache \u001b[39m=\u001b[39m ActivationCache(\n\u001b[1;32m    557\u001b[0m             cache_dict, \u001b[39mself\u001b[39m, has_batch_dim\u001b[39m=\u001b[39m\u001b[39mnot\u001b[39;00m remove_batch_dim\n\u001b[1;32m    558\u001b[0m         )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformer_lens/hook_points.py:459\u001b[0m, in \u001b[0;36mHookedRootModule.run_with_cache\u001b[0;34m(self, names_filter, device, remove_batch_dim, incl_bwd, reset_hooks_end, clear_contexts, *model_args, **model_kwargs)\u001b[0m\n\u001b[1;32m    449\u001b[0m cache_dict, fwd, bwd \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_caching_hooks(\n\u001b[1;32m    450\u001b[0m     names_filter, incl_bwd, device, remove_batch_dim\u001b[39m=\u001b[39mremove_batch_dim\n\u001b[1;32m    451\u001b[0m )\n\u001b[1;32m    453\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhooks(\n\u001b[1;32m    454\u001b[0m     fwd_hooks\u001b[39m=\u001b[39mfwd,\n\u001b[1;32m    455\u001b[0m     bwd_hooks\u001b[39m=\u001b[39mbwd,\n\u001b[1;32m    456\u001b[0m     reset_hooks_end\u001b[39m=\u001b[39mreset_hooks_end,\n\u001b[1;32m    457\u001b[0m     clear_contexts\u001b[39m=\u001b[39mclear_contexts,\n\u001b[1;32m    458\u001b[0m ):\n\u001b[0;32m--> 459\u001b[0m     model_out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(\u001b[39m*\u001b[39;49mmodel_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs)\n\u001b[1;32m    460\u001b[0m     \u001b[39mif\u001b[39;00m incl_bwd:\n\u001b[1;32m    461\u001b[0m         model_out\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformer_lens/HookedTransformer.py:498\u001b[0m, in \u001b[0;36mHookedTransformer.forward\u001b[0;34m(self, input, return_type, loss_per_token, prepend_bos, padding_side, start_at_layer, tokens, shortformer_pos_embed, left_attention_mask, stop_at_layer, past_kv_cache, past_left_attention_mask)\u001b[0m\n\u001b[1;32m    496\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    497\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 498\u001b[0m     logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49munembed(residual)  \u001b[39m# [batch, pos, d_vocab]\u001b[39;00m\n\u001b[1;32m    499\u001b[0m     \u001b[39mif\u001b[39;00m return_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mlogits\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    500\u001b[0m         \u001b[39mreturn\u001b[39;00m logits\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformer_lens/components.py:61\u001b[0m, in \u001b[0;36mUnembed.forward\u001b[0;34m(self, residual)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m     58\u001b[0m     \u001b[39mself\u001b[39m, residual: Float[torch\u001b[39m.\u001b[39mTensor, \u001b[39m\"\u001b[39m\u001b[39mbatch pos d_model\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m     59\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Float[torch\u001b[39m.\u001b[39mTensor, \u001b[39m\"\u001b[39m\u001b[39mbatch pos d_vocab_out\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[1;32m     60\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m---> 61\u001b[0m         einsum(\n\u001b[1;32m     62\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mbatch pos d_model, d_model vocab -> batch pos vocab\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     63\u001b[0m             residual,\n\u001b[1;32m     64\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mW_U,\n\u001b[1;32m     65\u001b[0m         )\n\u001b[1;32m     66\u001b[0m         \u001b[39m+\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mb_U\n\u001b[1;32m     67\u001b[0m     )\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 94.00 MiB. GPU 0 has a total capacty of 47.54 GiB of which 89.75 MiB is free. Process 5405 has 16.40 GiB memory in use. Process 32528 has 31.06 GiB memory in use. Of the allocated memory 27.63 GiB is allocated by PyTorch, and 2.34 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def get_acts(prompt: str, model: HookedTransformer, encoder: AutoEncoder, cfg: AutoEncoderConfig):\n",
    "    _, cache = model.run_with_cache(prompt, names_filter=cfg.encoder_hook_point)\n",
    "    acts = cache[cfg.encoder_hook_point].squeeze(0)\n",
    "    _, _, mid_acts, _, _ = encoder(acts)\n",
    "    return mid_acts\n",
    "\n",
    "\n",
    "def get_max_activations(prompts: list[str], model: HookedTransformer, encoder: AutoEncoder, cfg: AutoEncoderConfig):\n",
    "    activations = []\n",
    "    indices = []\n",
    "    for prompt in tqdm(prompts):\n",
    "        # get encoder activations at final position\n",
    "        acts = get_acts(prompt, model, encoder, cfg)[0, :-1]\n",
    "        activations.append(acts)\n",
    "\n",
    "    max_activation_per_prompt = torch.stack(activations)  # n_prompt x d_enc\n",
    "    max_activation_token_index = torch.stack(indices)\n",
    "\n",
    "    total_activations = max_activation_per_prompt.sum(0)\n",
    "    print(f\"Active directions on validation data: {total_activations.nonzero().shape[0]} out of {total_activations.shape[0]}\")\n",
    "    return max_activation_per_prompt, max_activation_token_index\n",
    "\n",
    "def print_top_examples(prompts: list[str], activations: Float[Tensor, \"n_prompts d_enc\"], direction: int, cfg: AutoEncoderConfig, encoder: AutoEncoder, n=5):\n",
    "    top_idxs = activations[:, direction].argsort(descending=True)[:n].cpu().tolist()\n",
    "    for prompt_index in top_idxs:\n",
    "        prompt = prompts[prompt_index]\n",
    "        prompt_tokens = model.to_str_tokens(model.to_tokens(prompt))\n",
    "        acts = get_acts(prompt, model, encoder, cfg)\n",
    "        direction_act = acts[:, direction].cpu().tolist()\n",
    "        max_direction_act = max(direction_act)\n",
    "        if max_direction_act > 0:\n",
    "            haystack_utils.clean_print_strings_as_html(prompt_tokens, direction_act, max_value=max_direction_act)\n",
    "\n",
    "max_activations, max_activation_token_indices = get_max_activations(prompts, model, l0_encoder, l0_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"One day, a little girl named Lily went for a walk in the park\"\n",
    "acts = get_acts(prompt, model, l0_encoder, l0_cfg)[-1] # d_enc\n",
    "print(f\"Active directions on last token: {acts.nonzero().shape[0]} out of {acts.shape[0]}\")\n",
    "active_directions = acts.nonzero().squeeze(1)\n",
    "highly_active_directions = torch.argwhere((acts > 0.5)).squeeze(1)\n",
    "for active_direction in highly_active_directions:\n",
    "    print(f\"Direction {active_direction}\")\n",
    "    print_top_examples(prompts, max_activations, active_direction, l0_cfg, l0_encoder, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = model.to_single_token(\" park\")\n",
    "token_prompts = []\n",
    "for prompt in prompts[:1000]:\n",
    "    tokens = model.to_tokens(prompt)\n",
    "    if token in tokens:\n",
    "        token_prompts.append(prompt)\n",
    "print(len(token_prompts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_direction_frequency(data: list[str], direction: int, cfg: AutoEncoderConfig, encoder: AutoEncoder):\n",
    "    activations = []\n",
    "    for prompt in tqdm(data):\n",
    "        tokens = model.to_tokens(prompt)\n",
    "        _, cache = model.run_with_cache(\n",
    "            tokens, names_filter=f\"blocks.{cfg.layer}.{cfg.act_name}\"\n",
    "            )\n",
    "        acts = cache[f\"blocks.{cfg.layer}.{cfg.act_name}\"].squeeze(0)\n",
    "        _, _, mid_acts, _, _ = encoder(acts)\n",
    "        activations.append(mid_acts[:, direction])\n",
    "    activations = torch.cat(activations)\n",
    "    print(activations.shape)\n",
    "\n",
    "    fig = px.histogram(activations.tolist(), \n",
    "                       title=f\"{print_model_name} L1={cfg.l1_coeff}: Activations for direction {direction}\", \n",
    "                       histnorm=\"probability\")\n",
    "    fig.update_layout(\n",
    "        xaxis_title=\"Activation\",\n",
    "        yaxis_title=\"Probability\",\n",
    "        width = 600,\n",
    "        showlegend=False\n",
    "    )\n",
    "    fig.update_yaxes(type='log')\n",
    "    fig.show()\n",
    "    \n",
    "for direction in active_directions[:10]:\n",
    "    plot_direction_frequency(prompts[:50], direction, l0_cfg, l0_encoder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
