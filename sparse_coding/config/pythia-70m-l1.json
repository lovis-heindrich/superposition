{
    "data_path": "/workspace/data/pile",
    "save_path": "/workspace",
    "num_eval_tokens": 800000, 
    "num_training_tokens": 5e8,
    "batch_size": 8192,
    "model": "pythia-70m",
    "layer": 5,
    "act": "mlp.hook_post",
    "expansion_factor": 4,
    "lr": 1e-4,
    "l1_coeff": 0.0003,
    "reg": "l1",
    "l1_target": 30,
    "dead_direction_frequency": 1e-5
}