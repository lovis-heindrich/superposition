{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm.auto import tqdm, trange\n",
    "from transformer_lens import HookedTransformer\n",
    "from jaxtyping import Float, Int, Bool\n",
    "from torch import Tensor\n",
    "from tqdm.auto import tqdm\n",
    "import plotly.io as pio\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px \n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from IPython.display import display, HTML\n",
    "from datasets import load_dataset\n",
    "from collections import Counter\n",
    "import pickle\n",
    "import os\n",
    "import haystack_utils\n",
    "from transformer_lens import utils\n",
    "from fancy_einsum import einsum\n",
    "import einops\n",
    "import json\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from datasets import load_dataset\n",
    "import random\n",
    "import math\n",
    "import random\n",
    "import neel.utils as nutils\n",
    "from neel_plotly import *\n",
    "import circuitsvis as cv\n",
    "\n",
    "import hook_utils\n",
    "import haystack_utils\n",
    "\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "pio.renderers.default = \"notebook_connected+notebook\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.autograd.set_grad_enabled(False)\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-large into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7083"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model: HookedTransformer = HookedTransformer.from_pretrained(\"gpt2-large\")\n",
    "\n",
    "data = load_dataset(\"stas/openwebtext-10k\", split=\"train\")\n",
    "strings = [i for i in data[\"text\"] if len(i)>2000]\n",
    "len(strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batched_dot_product = torch.vmap(torch.dot)\n",
    "batched_projection = torch.vmap(haystack_utils.get_collinear_component, (0, None))\n",
    "\n",
    "def neuron_to_context_neuron_DLA(\n",
    "        model: HookedTransformer, \n",
    "        prompt: str | list[str], \n",
    "        pos=np.s_[-1:], \n",
    "        context_neuron=tuple[int, int]\n",
    ") -> tuple[Float[Tensor, \"component\"], list[str]]:\n",
    "    '''Gets full resid decomposition including all neurons. Unbatched.'''\n",
    "    _, cache = model.run_with_cache(prompt)\n",
    "    layer, neuron = context_neuron\n",
    "    neuron_attrs, neuron_labels = cache.stack_neuron_results(layer, apply_ln=True, return_labels=True, pos_slice=pos)\n",
    "    neuron_attrs = neuron_attrs.squeeze(1)\n",
    "    answer_residual_direction = model.W_in[layer, :, neuron]\n",
    "\n",
    "    results = []\n",
    "    for i in range(neuron_attrs.shape[1]):\n",
    "        results.append(batched_projection(neuron_attrs[:, i], answer_residual_direction).norm(dim=-1))\n",
    "    return torch.stack(results), neuron_labels\n",
    "\n",
    "def components_to_context_neuron_DLA(\n",
    "        model: HookedTransformer, \n",
    "        prompt: str | list[str], \n",
    "        pos=np.s_[-1:], \n",
    "        context_neuron=tuple[int, int]\n",
    ") -> tuple[Float[Tensor, \"component\"], list[str]]:\n",
    "    '''Gets full resid decomposition including all neurons. Unbatched.'''\n",
    "    _, cache = model.run_with_cache(prompt)\n",
    "    layer, neuron = context_neuron\n",
    "    attrs, labels = cache.get_full_resid_decomposition(layer, apply_ln=True, return_labels=True, pos_slice=pos, expand_neurons=False)\n",
    "    attrs = attrs.squeeze(1)\n",
    "    answer_residual_direction = model.W_in[layer, :, neuron]\n",
    "\n",
    "    results = []\n",
    "    for i in range(attrs.shape[1]):\n",
    "        results.append(batched_projection(attrs[:, i], answer_residual_direction).norm(dim=-1))\n",
    "    return torch.stack(results), labels\n",
    "\n",
    "def resid_to_context_neuron_DLA(\n",
    "        model: HookedTransformer, \n",
    "        prompt: str | list[str], \n",
    "        pos=np.s_[-1:], \n",
    "        context_neuron:tuple[int, int]=(0,0)\n",
    ") -> tuple[Float[Tensor, \"component\"], list[str]]:\n",
    "    '''Gets full resid decomposition including all neurons. Unbatched.'''\n",
    "    _, cache = model.run_with_cache(prompt)\n",
    "    layer, neuron = context_neuron\n",
    "    all_attrs, labels = cache.get_full_resid_decomposition(layer, apply_ln=True, return_labels=True, pos_slice=pos)\n",
    "    all_attrs = all_attrs.squeeze(1)\n",
    "    \n",
    "    answer_residual_direction = model.W_in[layer, :, neuron]\n",
    "\n",
    "    results = []\n",
    "    for i in range(all_attrs.shape[1]):\n",
    "        results.append(batched_projection(all_attrs[:, i], answer_residual_direction).norm(dim=-1))\n",
    "    return torch.stack(results), labels\n",
    "\n",
    "def get_neuron_mean_acts(model: HookedTransformer, data: list[str], layer_neuron_dict: dict[int, list[int]]) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    sorted_layer_neuron_tuples = []\n",
    "    sorted_acts = []\n",
    "\n",
    "    for layer, neurons in layer_neuron_dict.items():\n",
    "        mean_acts = haystack_utils.get_mlp_activations(data, layer, model, context_crop_start=0, hook_pre=False, neurons=neurons, disable_tqdm=True)\n",
    "        sorted_layer_neuron_tuples.extend([(layer, neuron) for neuron in neurons])\n",
    "        sorted_acts.extend(mean_acts)\n",
    "        assert len(sorted_layer_neuron_tuples) == len(sorted_acts)\n",
    "\n",
    "    return sorted_layer_neuron_tuples, sorted_acts\n",
    "\n",
    "def get_unspecified_neurons(model: HookedTransformer, layer_neuron_dict: dict[int, list[int]]):\n",
    "    unspecified = []\n",
    "    for layer in range(model.cfg.n_layers):\n",
    "        for neuron in range(model.cfg.d_mlp):\n",
    "            if not neuron in layer_neuron_dict[layer]:\n",
    "                unspecified.append((layer, neuron))\n",
    "    return unspecified\n",
    "\n",
    "def get_neuron_loss_increases(model: HookedTransformer, data: list[str], prompt: str, positionwise: bool=False) -> torch.Tensor:\n",
    "    n_tokens = model.to_tokens(prompt).shape[1] - 1\n",
    "    original_loss = model([prompt], return_type='loss', loss_per_token=positionwise)\n",
    "    \n",
    "    losses = []\n",
    "    for layer in trange(model.cfg.n_layers):\n",
    "        mean_acts = haystack_utils.get_mlp_activations(data[:200], layer, model, disable_tqdm=True, context_crop_start=0)\n",
    "        for neuron in range(model.cfg.d_mlp):\n",
    "            hook = hook_utils.get_ablate_neuron_hook(layer, neuron, mean_acts[neuron])\n",
    "            with model.hooks([hook]):\n",
    "                ablated_loss = model([prompt], return_type='loss', loss_per_token=positionwise)\n",
    "                losses.append((ablated_loss - original_loss)[0])\n",
    "    return torch.stack(losses).reshape(n_tokens, model.cfg.n_layers * model.cfg.d_mlp)\n",
    "\n",
    "def compare_dla_and_ablation(model: HookedTransformer, dla_attrs_by_neuron: torch.Tensor, ablation_losses_by_neuron: torch.Tensor, num_neurons=20):\n",
    "    print(\"DLA:\")\n",
    "    values, indices = torch.topk(dla_attrs_by_neuron, num_neurons, dim=-1)\n",
    "    layer_indices, neuron_indices = np.unravel_index(indices.cpu().numpy(), (model.cfg.n_layers, model.cfg.d_mlp))\n",
    "    print(list(zip(layer_indices.tolist(), neuron_indices.tolist())))\n",
    "    print(dla_attrs_by_neuron[indices.tolist()])\n",
    "\n",
    "    print(\"Ablation:\")\n",
    "    loss_increases_by_neuron = ablation_losses_by_neuron\n",
    "    values, indices = torch.topk(loss_increases_by_neuron, num_neurons)\n",
    "    layer_indices, neuron_indices = np.unravel_index(indices.cpu().numpy()[:num_neurons], (model.cfg.n_layers, model.cfg.d_mlp))\n",
    "    print(list(zip(layer_indices.tolist(), neuron_indices.tolist())))\n",
    "    print(dla_attrs_by_neuron[indices.tolist()])\n",
    "\n",
    "def get_hook_inputs_for_token_index(model: HookedTransformer, data: list[str], loss_increases_by_neuron: torch.Tensor, k=40):\n",
    "    values, indices = torch.topk(loss_increases_by_neuron, k)\n",
    "\n",
    "    layer_indices, neuron_indices = np.unravel_index(indices.cpu().numpy(), (model.cfg.n_layers, model.cfg.d_mlp))\n",
    "    layer_neuron_dict = defaultdict(list)\n",
    "    for layer, neuron in zip(layer_indices, neuron_indices):\n",
    "        layer_neuron_dict[layer].append(neuron)\n",
    "\n",
    "    sorted_dla_layer_neuron_tuples = []\n",
    "    sorted_acts = []\n",
    "    for layer, neurons in layer_neuron_dict.items():\n",
    "        mean_acts = haystack_utils.get_mlp_activations(data, layer, model, context_crop_start=0, neurons=neurons, disable_tqdm=True)\n",
    "        sorted_dla_layer_neuron_tuples.extend([(layer, neuron) for neuron in neurons])\n",
    "        sorted_acts.extend(mean_acts)\n",
    "        assert len(sorted_dla_layer_neuron_tuples) == len(sorted_acts)\n",
    "\n",
    "    return sorted_dla_layer_neuron_tuples, sorted_acts\n",
    "\n",
    "def unravel_top_k(neuron_attrs: torch.Tensor, k: int=10):\n",
    "    values, indices = torch.topk(neuron_attrs, k)\n",
    "    layer_indices, neuron_indices = np.unravel_index(indices.cpu().numpy(), (model.cfg.n_layers, model.cfg.d_mlp))\n",
    "    return list(zip(layer_indices.tolist(), neuron_indices.tolist()))\n",
    "\n",
    "def resid_to_head_DLA(\n",
    "        model: HookedTransformer, \n",
    "        prompt: str | list[str], \n",
    "        head: tuple[int, int],\n",
    "        pos=np.s_[-1:], \n",
    "        \n",
    ") -> tuple[Float[Tensor, \"component\"], list[str]]:\n",
    "    '''Gets full resid decomposition and return the composition of each element of the given K matrix. Unbatched.'''\n",
    "    tokens = model.to_tokens(prompt)\n",
    "    _, cache = model.run_with_cache(prompt)\n",
    "    layer, head_index = head\n",
    "    all_attrs, labels = cache.get_full_resid_decomposition(layer, apply_ln=True, return_labels=True, pos_slice=pos)\n",
    "    all_attrs = all_attrs.squeeze(1)\n",
    "    answer_residual_direction = model.W_K[layer, head_index, :]\n",
    "    results = torch.zeros(all_attrs.shape[1], all_attrs.shape[0], answer_residual_direction.shape[1])\n",
    "    for i in range(all_attrs.shape[1]): # for each token\n",
    "        for j in range(answer_residual_direction.shape[1]): # for each direction in head input\n",
    "            token_attrs = all_attrs[:, i]\n",
    "            answer = answer_residual_direction[:, j]\n",
    "            results[i, :, j] = batched_projection(token_attrs, answer).norm(dim=-1)\n",
    "    return results, labels\n",
    "\n",
    "\n",
    "def mask_scores(attn_scores: Float[Tensor, \"query_nctx key_nctx\"]):\n",
    "    '''Mask the attention scores so that tokens don't attend to previous tokens.'''\n",
    "    # assert attn_scores.shape == (model.cfg.n_ctx, model.cfg.n_ctx)\n",
    "    mask = torch.tril(torch.ones_like(attn_scores)).bool()\n",
    "    neg_inf = torch.tensor(-1.0e6).to(attn_scores.device)\n",
    "    masked_attn_scores = torch.where(mask, attn_scores, neg_inf)\n",
    "    return masked_attn_scores\n",
    "    \n",
    "def resid_to_head_DLA_custom(\n",
    "        model: HookedTransformer, \n",
    "        prompt: str | list[str], \n",
    "        head: tuple[int, int]\n",
    "        \n",
    ") -> tuple[Float[Tensor, \"component\"], list[str]]:\n",
    "    '''For last two tokens, figure out which components contribute the most to them paying attention to each other.'''\n",
    "    _, cache = model.run_with_cache(prompt)\n",
    "    layer, head_index = head\n",
    "\n",
    "    all_attrs, labels = cache.get_full_resid_decomposition(layer, apply_ln=True, return_labels=True, pos_slice=np.s_[-2:], expand_neurons=False)\n",
    "    all_attrs = all_attrs.squeeze(1).permute(1, 0, 2)\n",
    "\n",
    "    W_QK = model.W_Q[layer, head_index] @ model.W_K[layer, head_index].T\n",
    "\n",
    "    pos_by_pos_scores = all_attrs[0] @ W_QK @ all_attrs[1].T\n",
    "    # masked_scaled = mask_scores(pos_by_pos_scores / model.cfg.d_head ** 0.5)\n",
    "    # pos_by_pos_pattern = torch.softmax(masked_scaled, dim=-1)\n",
    "    return pos_by_pos_scores, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    }
   ],
   "source": [
    "# Redo the upstream neurons method to include full MLP components in addition to the individual neuron break down\n",
    "# L0N1595\n",
    "\n",
    "attrs, labels = components_to_context_neuron_DLA(model, \" An eye for an\", pos=np.s_[-1:], context_neuron=(0, 1595))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3])\n"
     ]
    }
   ],
   "source": [
    "print(attrs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
