{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from transformer_lens import HookedTransformer\n",
    "from jaxtyping import Float, Int, Bool\n",
    "from torch import Tensor\n",
    "from tqdm.auto import tqdm\n",
    "import plotly.io as pio\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px \n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from IPython.display import display, HTML\n",
    "from datasets import load_dataset\n",
    "from collections import Counter\n",
    "import pickle\n",
    "import os\n",
    "import haystack_utils\n",
    "from transformer_lens import utils\n",
    "\n",
    "\n",
    "pio.renderers.default = \"notebook_connected+notebook\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.autograd.set_grad_enabled(False)\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model roneneldan/TinyStories-33M into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "model = HookedTransformer.from_pretrained(\n",
    "    \"roneneldan/TinyStories-33M\",\n",
    "    #center_unembed=True,\n",
    "    #center_writing_weights=True,\n",
    "    #fold_ln=True,\n",
    "    #refactor_factored_attn_matrices=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' The', ' elephant', ' is', ' grey']\n"
     ]
    }
   ],
   "source": [
    "haystack_utils.print_tokenized_word(\" The elephant is grey\", model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Color of animals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test = '\"Strawberries are red\", Bob said. \"Correct\", his mother answered. \"Do you also know what color the sky is?\" \"Of course\", Bob answered proudly, \"it is'\n",
    "test = \"'Strawberries are red', Bob said. 'Correct', his mother answered. 'Do you also know what color the sky is?' 'Of course', Bob answered proudly, 'it is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<|endoftext|>', \"'\", 'St', 'raw', 'berries', ' are', ' red', \"',\", ' Bob', ' said', '.', \" '\", 'Correct', \"',\", ' his', ' mother', ' answered', '.', \" '\", 'Do', ' you', ' also', ' know', ' what', ' color', ' the', ' sky', ' is', \"?'\", \" '\", 'Of', ' course', \"',\", ' Bob', ' answered', ' proudly', ',', \" '\", 'it', ' is']\n",
      "Tokenized answer: [' blue']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">        Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">18.04</span><span style=\"font-weight: bold\"> Prob: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">33.08</span><span style=\"font-weight: bold\">% Token: | blue|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m        Logit: \u001b[0m\u001b[1;36m18.04\u001b[0m\u001b[1m Prob: \u001b[0m\u001b[1;36m33.08\u001b[0m\u001b[1m% Token: | blue|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 18.04 Prob: 33.08% Token: | blue|\n",
      "Top 1th token. Logit: 17.46 Prob: 18.39% Token: | red|\n",
      "Top 2th token. Logit: 16.04 Prob:  4.44% Token: | strawberry|\n",
      "Top 3th token. Logit: 15.80 Prob:  3.51% Token: | a|\n",
      "Top 4th token. Logit: 15.73 Prob:  3.26% Token: |!'|\n",
      "Top 5th token. Logit: 15.70 Prob:  3.17% Token: | the|\n",
      "Top 6th token. Logit: 15.23 Prob:  1.99% Token: | pink|\n",
      "Top 7th token. Logit: 15.07 Prob:  1.70% Token: | purple|\n",
      "Top 8th token. Logit: 14.92 Prob:  1.46% Token: |...|\n",
      "Top 9th token. Logit: 14.90 Prob:  1.43% Token: | my|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' blue'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' blue'\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "utils.test_prompt(test, \" blue\", model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"'Strawberries are red', Bob said. 'Correct', his mother answered. 'Do you also know what color sky is?' 'Of course', Bob answered proudly, 'it is\", ' blue')\n"
     ]
    }
   ],
   "source": [
    "objects = [\"sky\", \"grass\", \"the sun\", \"an apple\", \"a carrot\", \"coal\", \"a strawberry\", \"the ocean\", \"a pumpkin\", \"a lemon\", \"a tomato\"]\n",
    "colors = [\"blue\", \"green\", \"yellow\", \"red\", \"orange\", \"black\", \"red\", \"blue\", \"orange\", \"yellow\", \"red\"]\n",
    "\n",
    "def make_prompt(object, color):\n",
    "    prompt = f\"'Strawberries are red', Bob said. 'Correct', his mother answered. 'Do you also know what color {object} is?' 'Of course', Bob answered proudly, 'it is\"\n",
    "    answer = f\" {color}\"\n",
    "    return prompt, answer\n",
    "\n",
    "print(make_prompt(objects[0], colors[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sky ->  blue: Rank 1 (p=0.32)\n",
      "grass ->  green: Rank 3 (p=0.10)\n",
      "the sun ->  yellow: Rank 9 (p=0.02)\n",
      "an apple ->  red: Rank 1 (p=0.37)\n",
      "a carrot ->  orange: Rank 5 (p=0.05)\n",
      "coal ->  black: Rank 2 (p=0.14)\n",
      "a strawberry ->  red: Rank 1 (p=0.28)\n",
      "the ocean ->  blue: Rank 1 (p=0.30)\n",
      "a pumpkin ->  orange: Rank 4 (p=0.09)\n",
      "a lemon ->  yellow: Rank 2 (p=0.11)\n",
      "a tomato ->  red: Rank 1 (p=0.43)\n"
     ]
    }
   ],
   "source": [
    "def test_prompt(prompt, answer, model, object):\n",
    "    logits = model(prompt)\n",
    "    answer_token = model.to_single_token(answer)\n",
    "    pred = logits[0, -1, :].softmax(dim=-1)\n",
    "    prob = pred[answer_token].item()\n",
    "    rank = (pred >= pred[answer_token]).sum().item()\n",
    "    print(f\"{object} -> {answer}: Rank {rank} (p={prob:.2f})\")\n",
    "\n",
    "for object, color in zip(objects, colors):\n",
    "    prompt, answer = make_prompt(object, color)\n",
    "    test_prompt(prompt, answer, model, object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sizes of animals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prompt(animal_1, animal_2):\n",
    "    prompt = f'Once upon a time there was a boy named Bob. Bob loved animals and was proud that he knew all the sizes of his favorite animals. \"A tiger is larger than a cat\", Bob said. \"Correct\", his mother answered. \"Do you also know if {animal_1} is larger than {animal_2}?\" \"Of course\", Bob answered proudly, \"The'\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' dog']\n",
      "[' cat']\n",
      "[' cow']\n",
      "[' horse']\n",
      "[' sheep']\n",
      "[' elephant']\n",
      "[' lion']\n",
      "[' tiger']\n",
      "[' bear']\n",
      "[' duck']\n",
      "[' chicken']\n",
      "[' fish']\n",
      "[' turtle']\n",
      "[' rabbit']\n",
      "[' monkey']\n"
     ]
    }
   ],
   "source": [
    "animals = [\"dog\", \"cat\", \"cow\", \"horse\", \"sheep\", \"elephant\", \"lion\", \"tiger\", \"bear\", \"duck\", \"chicken\", \"fish\", \"turtle\", \"rabbit\", \"monkey\"]\n",
    "\n",
    "for animal in animals:\n",
    "    haystack_utils.print_tokenized_word(\" \"+animal, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " horse> dog: Rank 1/5 (p=0.81, 0.01)\n",
      " horse> dog: Rank 54/1 (p=0.00, 0.34)\n",
      " bear> duck: Rank 1/2 (p=0.40, 0.13)\n",
      " bear> duck: Rank 4/1 (p=0.00, 0.91)\n",
      " elephant> rabbit: Rank 1/5 (p=0.74, 0.02)\n",
      " elephant> rabbit: Rank 2/1 (p=0.07, 0.74)\n",
      " sheep> bug: Rank 1/223 (p=0.32, 0.00)\n",
      " sheep> bug: Rank 271/3 (p=0.00, 0.09)\n",
      " cow> fish: Rank 1/27 (p=0.79, 0.00)\n",
      " cow> fish: Rank 9/7 (p=0.01, 0.01)\n"
     ]
    }
   ],
   "source": [
    "animal_pairs = [\n",
    "    (\"a horse\", \"a dog\", \" horse\", \" dog\"),\n",
    "    (\"a bear\", \"a duck\", \" bear\", \" duck\"),\n",
    "    (\"an elephant\", \"a rabbit\", \" elephant\", \" rabbit\"),\n",
    "    (\"a sheep\", \"a bug\", \" sheep\", \" bug\"),\n",
    "    (\"a cow\", \"a fish\", \" cow\", \" fish\"),\n",
    "]\n",
    "\n",
    "def get_ranks(prompt, correct, incorrect):\n",
    "    logits = model(prompt)\n",
    "    answer_token = model.to_single_token(correct)\n",
    "    incorrect_token = model.to_single_token(incorrect)\n",
    "    pred = logits[0, -1, :].softmax(dim=-1)\n",
    "    prob = pred[answer_token].item()\n",
    "    rank = (pred >= pred[answer_token]).sum().item()\n",
    "    incorrect_prob = pred[incorrect_token].item()\n",
    "    incorrect_rank = (pred >= pred[incorrect_token]).sum().item()\n",
    "    print(f\"{correct}>{incorrect}: Rank {rank}/{incorrect_rank} (p={prob:.2f}, {incorrect_prob:.2f})\")\n",
    "\n",
    "\n",
    "for animal_1, animal_2, correct, incorrect in animal_pairs:\n",
    "    prompt_1 = make_prompt(animal_1, animal_2)\n",
    "    prompt_2 = make_prompt(animal_2, animal_1)\n",
    "    correct_token = model.to_single_token(correct)\n",
    "    incorrect_token = model.to_single_token(incorrect)\n",
    "\n",
    "    get_ranks(prompt_1, correct, incorrect)\n",
    "    get_ranks(prompt_2, correct, incorrect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideas\n",
    "- Opposites\n",
    "- Categorize fruit or vegetable\n",
    "- Match animals to habitates\n",
    "- analogies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<|endoftext|>', 'A', ' bird', ' has', ' wings', ' and', ' a', ' cat', ' has']\n",
      "Tokenized answer: [' paws']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">52</span><span style=\"font-weight: bold\">       Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12.15</span><span style=\"font-weight: bold\"> Prob:  </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.14</span><span style=\"font-weight: bold\">% Token: | paws|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m52\u001b[0m\u001b[1m       Logit: \u001b[0m\u001b[1;36m12.15\u001b[0m\u001b[1m Prob:  \u001b[0m\u001b[1;36m0.14\u001b[0m\u001b[1m% Token: | paws|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 17.92 Prob: 43.50% Token: | a|\n",
      "Top 1th token. Logit: 16.25 Prob:  8.20% Token: | wings|\n",
      "Top 2th token. Logit: 15.94 Prob:  6.03% Token: | no|\n",
      "Top 3th token. Logit: 14.92 Prob:  2.16% Token: | stripes|\n",
      "Top 4th token. Logit: 14.69 Prob:  1.72% Token: | many|\n",
      "Top 5th token. Logit: 14.63 Prob:  1.63% Token: | feathers|\n",
      "Top 6th token. Logit: 14.48 Prob:  1.39% Token: | an|\n",
      "Top 7th token. Logit: 14.42 Prob:  1.32% Token: | been|\n",
      "Top 8th token. Logit: 14.36 Prob:  1.23% Token: | four|\n",
      "Top 9th token. Logit: 14.20 Prob:  1.06% Token: | big|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' paws'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">52</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' paws'\u001b[0m, \u001b[1;36m52\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "utils.test_prompt(\"A bird has wings and a cat has\", \" paws\", model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<|endoftext|>', 'Once', ' upon', ' a', ' time', ' there', ' was', ' a', ' boy', ' named', ' Bob', '.', ' Bob', ' loved', ' playing', ' games', ' with', ' words', '.', \" '\", 'Let', \"'s\", ' play', ' a', ' game', \"'\", ' his', ' mother', ' said', '.', \" '\", 'I', ' will', ' tell', ' you', ' a', ' word', ' and', ' you', ' have', ' to', ' tell', ' me', ' the', ' opposite', '.', ' Ready', \"?'\", \" '\", 'Yes', '!', ' That', ' sounds', ' fun', \"',\", ' answered', ' Bob', '.', 'His', ' mother', ' nodded', '.', \" '\", 'Ok', ',', ' great', '.', ' What', ' is', ' the', ' opposite', ' of', ' loud', \"?'\", ' Bob', ' answered', ' immediately', ':', \" '\", 'It', \"'s\"]\n",
      "Tokenized answer: [' cold']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">191</span><span style=\"font-weight: bold\">      Logit:  </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9.72</span><span style=\"font-weight: bold\"> Prob:  </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.02</span><span style=\"font-weight: bold\">% Token: | cold|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m191\u001b[0m\u001b[1m      Logit:  \u001b[0m\u001b[1;36m9.72\u001b[0m\u001b[1m Prob:  \u001b[0m\u001b[1;36m0.02\u001b[0m\u001b[1m% Token: | cold|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 16.88 Prob: 25.39% Token: | a|\n",
      "Top 1th token. Logit: 16.30 Prob: 14.20% Token: | '|\n",
      "Top 2th token. Logit: 15.63 Prob:  7.29% Token: | the|\n",
      "Top 3th token. Logit: 15.48 Prob:  6.30% Token: | called|\n",
      "Top 4th token. Logit: 15.41 Prob:  5.84% Token: | two|\n",
      "Top 5th token. Logit: 15.40 Prob:  5.78% Token: | so|\n",
      "Top 6th token. Logit: 14.42 Prob:  2.18% Token: | not|\n",
      "Top 7th token. Logit: 14.41 Prob:  2.15% Token: | very|\n",
      "Top 8th token. Logit: 14.21 Prob:  1.77% Token: | my|\n",
      "Top 9th token. Logit: 13.86 Prob:  1.24% Token: | an|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' cold'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">191</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' cold'\u001b[0m, \u001b[1;36m191\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"Once upon a time there was a boy named Bob. Bob loved playing games with words. 'Let's play a game' his mother said. 'I will tell you a word and you have to tell me the opposite. Ready?' 'Yes! That sounds fun', answered Bob.\" + \\\n",
    "    \"His mother nodded. 'Ok, great. What is the opposite of loud?' Bob answered immediately: 'It's\"\n",
    "\n",
    "utils.test_prompt(prompt, \" cold\", model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<|endoftext|>', 'I', ' know', ' a', ' lot', ' about', ' animals', '.', ' For', ' example', ',', ' monkeys', ' like', ' to', ' eat']\n",
      "Tokenized answer: [' grass']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">30</span><span style=\"font-weight: bold\">       Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">15.73</span><span style=\"font-weight: bold\"> Prob:  </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.02</span><span style=\"font-weight: bold\">% Token: | grass|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m30\u001b[0m\u001b[1m       Logit: \u001b[0m\u001b[1;36m15.73\u001b[0m\u001b[1m Prob:  \u001b[0m\u001b[1;36m0.02\u001b[0m\u001b[1m% Token: | grass|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 24.13 Prob: 94.55% Token: | bananas|\n",
      "Top 1th token. Logit: 19.70 Prob:  1.13% Token: | fruit|\n",
      "Top 2th token. Logit: 19.17 Prob:  0.67% Token: | apples|\n",
      "Top 3th token. Logit: 18.68 Prob:  0.41% Token: | fruits|\n",
      "Top 4th token. Logit: 18.68 Prob:  0.41% Token: | peanuts|\n",
      "Top 5th token. Logit: 18.30 Prob:  0.28% Token: | banana|\n",
      "Top 6th token. Logit: 18.28 Prob:  0.27% Token: | the|\n",
      "Top 7th token. Logit: 18.15 Prob:  0.24% Token: | nuts|\n",
      "Top 8th token. Logit: 18.07 Prob:  0.22% Token: | a|\n",
      "Top 9th token. Logit: 17.23 Prob:  0.10% Token: | leaves|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' grass'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">30</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' grass'\u001b[0m, \u001b[1;36m30\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"I know a lot about animals. For example, cows live in the\"\n",
    "prompt = \"I know a lot about animals. For example, monkeys like to eat\"\n",
    "utils.test_prompt(prompt, \" grass\", model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<|endoftext|>', 'Once', ' upon', ' a', ' time', ' there', ' was', ' a', ' girl', ' named', ' Sarah', '.', ' Sarah', ' loved', ' animals', ' and', ' really', ' wanted', ' a', ' pet', '.', ' Sarah', ' wanted', ' to', ' get', ' a', ' gir', 'affe', ' or', ' a', ' rabbit', '.', ' Her', ' mother', ' hates', ' gir', 'aff', 'es', ' so', ' she', ' got', ' Sarah', ' a']\n",
      "Tokenized answer: [' rabbit']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">13</span><span style=\"font-weight: bold\">       Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">15.30</span><span style=\"font-weight: bold\"> Prob:  </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.30</span><span style=\"font-weight: bold\">% Token: | rabbit|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m13\u001b[0m\u001b[1m       Logit: \u001b[0m\u001b[1;36m15.30\u001b[0m\u001b[1m Prob:  \u001b[0m\u001b[1;36m1.30\u001b[0m\u001b[1m% Token: | rabbit|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 17.73 Prob: 14.75% Token: | dog|\n",
      "Top 1th token. Logit: 17.70 Prob: 14.34% Token: | gir|\n",
      "Top 2th token. Logit: 16.65 Prob:  5.00% Token: | monkey|\n",
      "Top 3th token. Logit: 16.46 Prob:  4.14% Token: | par|\n",
      "Top 4th token. Logit: 16.44 Prob:  4.05% Token: | toy|\n",
      "Top 5th token. Logit: 16.41 Prob:  3.92% Token: | pig|\n",
      "Top 6th token. Logit: 16.32 Prob:  3.60% Token: | big|\n",
      "Top 7th token. Logit: 16.23 Prob:  3.30% Token: | stuffed|\n",
      "Top 8th token. Logit: 16.07 Prob:  2.81% Token: | pet|\n",
      "Top 9th token. Logit: 15.95 Prob:  2.49% Token: | puppy|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' rabbit'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">13</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' rabbit'\u001b[0m, \u001b[1;36m13\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"Once upon a time there was a girl named Sarah. Sarah loved animals and really wanted a pet. Sarah wanted to get a giraffe or a rabbit. Her mother hates giraffes so she got Sarah a\"\n",
    "utils.test_prompt(prompt, \" rabbit\", model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.8930225372314453\n"
     ]
    }
   ],
   "source": [
    "prompt = 'Once upon a time there was a girl named Alice. Alice loved learning about different animals. \"Are chicken smaller than cows?\", Alice asked her mother. \"'\n",
    "\n",
    "yes_token = model.to_single_token(\"Yes\")\n",
    "no_token = model.to_single_token(\"No\")\n",
    "def get_yes_no_logits(prompt):\n",
    "    logits = model(prompt, return_type=\"logits\")\n",
    "    yes_logits = logits[0, -1, yes_token].item()\n",
    "    no_logits = logits[0, -1, no_token].item()\n",
    "    return yes_logits - no_logits\n",
    "\n",
    "print(get_yes_no_logits(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<|endoftext|>', '\"', 'Are', ' cows', ' smaller', ' than', ' chicken', '?\",', ' Alice', ' asked', ' her', ' mother', '.', ' \"']\n",
      "Tokenized answer: ['No']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">        Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">18.10</span><span style=\"font-weight: bold\"> Prob: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">13.57</span><span style=\"font-weight: bold\">% Token: |No|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m        Logit: \u001b[0m\u001b[1;36m18.10\u001b[0m\u001b[1m Prob: \u001b[0m\u001b[1;36m13.57\u001b[0m\u001b[1m% Token: |No|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 18.77 Prob: 26.39% Token: |Yes|\n",
      "Top 1th token. Logit: 18.10 Prob: 13.57% Token: |No|\n",
      "Top 2th token. Logit: 17.47 Prob:  7.17% Token: |What|\n",
      "Top 3th token. Logit: 17.30 Prob:  6.09% Token: |I|\n",
      "Top 4th token. Logit: 17.28 Prob:  5.93% Token: |Why|\n",
      "Top 5th token. Logit: 16.89 Prob:  4.02% Token: |That|\n",
      "Top 6th token. Logit: 16.87 Prob:  3.96% Token: |It|\n",
      "Top 7th token. Logit: 16.62 Prob:  3.08% Token: |We|\n",
      "Top 8th token. Logit: 16.46 Prob:  2.62% Token: |The|\n",
      "Top 9th token. Logit: 16.17 Prob:  1.97% Token: |Well|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">'No'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'No'\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "utils.test_prompt(prompt, \"No\", model, prepend_space_to_answer=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<|endoftext|>', 'Once', ' upon', ' a', ' time', ' there', ' was', ' a', ' girl', ' named', ' Alice', '.', ' Alice', ' had', ' a', ' carrot', ' and', ' a', ' banana', '.', ' She', ' ate', ' the', ' banana', '.', ' Then', ',', ' she', ' only', ' had', ' the']\n",
      "Tokenized answer: [' banana']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span><span style=\"font-weight: bold\">        Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16.04</span><span style=\"font-weight: bold\"> Prob:  </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.43</span><span style=\"font-weight: bold\">% Token: | banana|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m8\u001b[0m\u001b[1m        Logit: \u001b[0m\u001b[1;36m16.04\u001b[0m\u001b[1m Prob:  \u001b[0m\u001b[1;36m1.43\u001b[0m\u001b[1m% Token: | banana|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 18.71 Prob: 20.61% Token: | apple|\n",
      "Top 1th token. Logit: 18.05 Prob: 10.68% Token: | small|\n",
      "Top 2th token. Logit: 17.12 Prob:  4.22% Token: | peel|\n",
      "Top 3th token. Logit: 17.10 Prob:  4.13% Token: | little|\n",
      "Top 4th token. Logit: 16.97 Prob:  3.61% Token: | carrot|\n",
      "Top 5th token. Logit: 16.55 Prob:  2.39% Token: | two|\n",
      "Top 6th token. Logit: 16.10 Prob:  1.52% Token: | left|\n",
      "Top 7th token. Logit: 16.05 Prob:  1.45% Token: | original|\n",
      "Top 8th token. Logit: 16.04 Prob:  1.43% Token: | banana|\n",
      "Top 9th token. Logit: 15.97 Prob:  1.34% Token: | toy|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' banana'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' banana'\u001b[0m, \u001b[1;36m8\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"Once upon a time there was a girl named Alice. Alice had a carrot and a banana. She ate the banana. Then, she only had the\"\n",
    "#prompt = \"Once upon a time there was a girl named Alice. Alice had a carrot and a banana. She gave the banana to Bob. Then, Bob had the\"\n",
    "utils.test_prompt(prompt, \" banana\", model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<|endoftext|>', 'Once', ' upon', ' a', ' time', ' there', ' was', ' a', ' boy', ' named', ' Jack', '.', ' On', ' Sunday', ',', ' Jack', ' visited', ' a', ' lot', ' of', ' family', ' members', '.', ' In', ' the', ' morning', ',', ' Jack', ' visited', ' his', ' grandmother', '.', ' At', ' noon', ',', ' he', ' played', ' with', ' his', ' father', '.', ' In', ' the', ' evening', ',', ' he', ' ate', ' dinner', ' with', ' his', ' mother', '.', ' The', ' next', ' day', ' in', ' school', ',', ' he', ' told', ' his', ' best', ' friend', ' about', ' his', ' weekend', '.', ' \"', 'Yesterday', ' evening', ',', ' I', ' had', ' dinner', ' with', ' my']\n",
      "Tokenized answer: [' mother']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">15</span><span style=\"font-weight: bold\">       Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16.68</span><span style=\"font-weight: bold\"> Prob:  </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.49</span><span style=\"font-weight: bold\">% Token: | mother|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m15\u001b[0m\u001b[1m       Logit: \u001b[0m\u001b[1;36m16.68\u001b[0m\u001b[1m Prob:  \u001b[0m\u001b[1;36m0.49\u001b[0m\u001b[1m% Token: | mother|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 21.07 Prob: 39.04% Token: | grandmother|\n",
      "Top 1th token. Logit: 20.61 Prob: 24.77% Token: | family|\n",
      "Top 2th token. Logit: 19.37 Prob:  7.18% Token: | grand|\n",
      "Top 3th token. Logit: 18.85 Prob:  4.24% Token: | grandma|\n",
      "Top 4th token. Logit: 18.65 Prob:  3.48% Token: | grandfather|\n",
      "Top 5th token. Logit: 18.39 Prob:  2.67% Token: | friend|\n",
      "Top 6th token. Logit: 18.37 Prob:  2.63% Token: | friends|\n",
      "Top 7th token. Logit: 18.16 Prob:  2.14% Token: | father|\n",
      "Top 8th token. Logit: 18.11 Prob:  2.02% Token: | dad|\n",
      "Top 9th token. Logit: 18.08 Prob:  1.96% Token: | Grand|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' mother'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">15</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' mother'\u001b[0m, \u001b[1;36m15\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = 'Once upon a time there was a boy named Jack. On Sunday, Jack visited a lot of family members. In the morning, Jack visited his grandmother.' + \\\n",
    "    ' At noon, he played with his father.' +\\\n",
    "    ' In the evening, he ate dinner with his mother.' + \\\n",
    "    ' The next day in school, he told his best friend about his weekend.' + \\\n",
    "    ' \"Yesterday evening, I had dinner with my'\n",
    "\n",
    "utils.test_prompt(prompt, \" mother\", model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<|endoftext|>', 'Once', ' upon', ' a', ' time', ' there', ' was', ' a', ' girl', ' named', ' Alice', '.', ' Alice', ' went', ' to', ' nearby', ' apple', ' tree', ' to', ' collect', ' some', ' apples', '.', ' Alice', ' had', ' four', ' apples', '.', ' She', ' ate', ' two', ' of', ' them', '.', ' Then', ',', ' she', ' had', ' exactly']\n",
      "Tokenized answer: [' three']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span><span style=\"font-weight: bold\">        Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">18.77</span><span style=\"font-weight: bold\"> Prob:  </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.35</span><span style=\"font-weight: bold\">% Token: | three|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m7\u001b[0m\u001b[1m        Logit: \u001b[0m\u001b[1;36m18.77\u001b[0m\u001b[1m Prob:  \u001b[0m\u001b[1;36m3.35\u001b[0m\u001b[1m% Token: | three|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 20.84 Prob: 26.38% Token: | the|\n",
      "Top 1th token. Logit: 20.37 Prob: 16.57% Token: | four|\n",
      "Top 2th token. Logit: 20.25 Prob: 14.70% Token: | enough|\n",
      "Top 3th token. Logit: 20.02 Prob: 11.69% Token: | one|\n",
      "Top 4th token. Logit: 19.05 Prob:  4.44% Token: | where|\n",
      "Top 5th token. Logit: 19.00 Prob:  4.21% Token: | five|\n",
      "Top 6th token. Logit: 18.83 Prob:  3.56% Token: | ten|\n",
      "Top 7th token. Logit: 18.77 Prob:  3.35% Token: | three|\n",
      "Top 8th token. Logit: 18.53 Prob:  2.62% Token: | seven|\n",
      "Top 9th token. Logit: 18.37 Prob:  2.24% Token: | what|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' three'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' three'\u001b[0m, \u001b[1;36m7\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"Once upon a time there was a girl named Alice. Alice went to nearby apple tree to collect some apples. Alice had four apples. She ate two of them. Then, she had exactly\"\n",
    "#prompt = \"Alice had two apples. She found one more apple. Then, she had\"\n",
    "#prompt = \"Jack had three apples. He gave two to Jill. Then, he had\"\n",
    "utils.test_prompt(prompt, \" three\", model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<|endoftext|>', 'Once', ' upon', ' a', ' time', ',', ' there', ' was', ' a', ' fish', ' called', ' Craig', '.', ' He', ' was', ' very', ' different', ' from', ' the', ' other', ' fish', '.', ' All', ' the', ' other', ' fish', ' were', ' fast', ' sw', 'immers', ',', ' but', ' Craig', ' was']\n",
      "Tokenized answer: [' weak']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">59</span><span style=\"font-weight: bold\">       Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">15.04</span><span style=\"font-weight: bold\"> Prob:  </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.04</span><span style=\"font-weight: bold\">% Token: | weak|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m59\u001b[0m\u001b[1m       Logit: \u001b[0m\u001b[1;36m15.04\u001b[0m\u001b[1m Prob:  \u001b[0m\u001b[1;36m0.04\u001b[0m\u001b[1m% Token: | weak|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 21.87 Prob: 40.34% Token: | the|\n",
      "Top 1th token. Logit: 20.64 Prob: 11.84% Token: | very|\n",
      "Top 2th token. Logit: 20.62 Prob: 11.63% Token: | still|\n",
      "Top 3th token. Logit: 19.97 Prob:  6.06% Token: | always|\n",
      "Top 4th token. Logit: 19.43 Prob:  3.54% Token: | not|\n",
      "Top 5th token. Logit: 19.18 Prob:  2.76% Token: | slow|\n",
      "Top 6th token. Logit: 19.15 Prob:  2.67% Token: | special|\n",
      "Top 7th token. Logit: 18.85 Prob:  1.97% Token: | a|\n",
      "Top 8th token. Logit: 18.80 Prob:  1.88% Token: | really|\n",
      "Top 9th token. Logit: 18.79 Prob:  1.86% Token: | fast|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' weak'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">59</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' weak'\u001b[0m, \u001b[1;36m59\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"Once upon a time, there was a fish called Craig. He was very different from the other fish. All the other fish were fast swimmers, but Craig was\"\n",
    "\n",
    "utils.test_prompt(prompt, \" weak\", model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
