{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from transformer_lens import HookedTransformer, ActivationCache, utils, patching\n",
    "from jaxtyping import Float, Int, Bool\n",
    "from torch import Tensor\n",
    "from tqdm.auto import tqdm\n",
    "import plotly.io as pio\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "pio.renderers.default = \"notebook_connected\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.autograd.set_grad_enabled(False)\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "from haystack_utils import get_mlp_activations\n",
    "import haystack_utils\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model EleutherAI/pythia-70m into HookedTransformer\n",
      "data/german_europarl.json: Loaded 2000 examples with 152 to 2000 characters each.\n",
      "data/english_europarl.json: Loaded 2000 examples with 165 to 2000 characters each.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a47b19dbc454d1487f0e25a0499848d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9d80c35a15c4fb1a6ffef4cfca05609",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/german_europarl.json: Loaded 2000 examples with 152 to 2000 characters each.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a835680115ea41e1aba11d2bebb6f014",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ngram = \"orschlägen\"\n",
    "model = HookedTransformer.from_pretrained(\"EleutherAI/pythia-70m\",\n",
    "    center_unembed=True,\n",
    "    center_writing_weights=True,\n",
    "    fold_ln=True,\n",
    "    device=device)\n",
    "\n",
    "activate_neurons_fwd_hooks, deactivate_neurons_fwd_hooks = haystack_utils.get_context_ablation_hooks(3, [669], model)\n",
    "all_ignore, _ = haystack_utils.get_weird_tokens(model, plot_norms=False)\n",
    "\n",
    "german_data = haystack_utils.load_json_data(\"data/german_europarl.json\")[:200]\n",
    "common_tokens = haystack_utils.get_common_tokens(german_data, model, all_ignore, k=100)\n",
    "\n",
    "# Sort tokens into new word vs continuation\n",
    "new_word_tokens = []\n",
    "continuation_tokens = []\n",
    "for token in common_tokens:\n",
    "    str_token = model.to_single_str_token(token.item())\n",
    "    if str_token.startswith(\" \"):\n",
    "        new_word_tokens.append(token)\n",
    "    else:\n",
    "        continuation_tokens.append(token)\n",
    "new_word_tokens = torch.stack(new_word_tokens)\n",
    "continuation_tokens = torch.stack(continuation_tokens)\n",
    "\n",
    "context_direction = model.W_out[3, 669, :]\n",
    "\n",
    "def get_cosine_sim(direction: Float[Tensor, \"d_res\"], layer=5) -> Float[Tensor, \"d_mlp\"]:\n",
    "    cosine = torch.nn.CosineSimilarity(dim=1)\n",
    "    return cosine(model.W_in[layer].T, direction.unsqueeze(0))\n",
    "\n",
    "def plot_histogram(t1, t2, t3, name1, name2, name3):\n",
    "    t1 = t1.cpu().numpy()\n",
    "    t2 = t2.cpu().numpy()\n",
    "    t3 = t3.cpu().numpy()\n",
    "    fig = go.Figure()\n",
    "    bin_width= 0.01\n",
    "    fig.add_trace(go.Histogram(x=t1, name=name1, opacity=0.5, histnorm='probability density', xbins=dict(size=bin_width)))\n",
    "    fig.add_trace(go.Histogram(x=t2, name=name2, opacity=0.5 , histnorm='probability density', xbins=dict(size=bin_width)))\n",
    "    fig.add_trace(go.Histogram(x=t3, name=name3, opacity=0.5, histnorm='probability density', xbins=dict(size=bin_width)))\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=\"Individual MLP5 similarities to direction vectors\",\n",
    "        xaxis_title=\"Cosine Similarity\",\n",
    "        yaxis_title=\"Probability Density\",\n",
    "        barmode=\"overlay\",\n",
    "    )\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "def compute_mlp_loss(prompts, df, neurons, ablate_mode=\"NNN\", layer=5, compute_original_loss=False):\n",
    "\n",
    "    mean_activations = torch.Tensor(df[df.index.isin(neurons.tolist())][ablate_mode].tolist()).cuda()\n",
    "    def ablate_mlp_hook(value, hook):\n",
    "        value[:, :, neurons] = mean_activations\n",
    "        return value\n",
    "\n",
    "    with model.hooks(fwd_hooks=[(f\"blocks.{layer}.mlp.hook_pre\", ablate_mlp_hook)]):\n",
    "        ablated_loss = model(prompts, return_type=\"loss\", loss_per_token=True)[:, -1].mean().item()\n",
    "\n",
    "    if compute_original_loss:\n",
    "        loss = model(prompts, return_type=\"loss\", loss_per_token=True)[:, -1].mean().item()\n",
    "        return loss, ablated_loss\n",
    "    return ablated_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss change for different AND thresholds\n",
    "option = \"orschlägen\"\n",
    "df = pd.read_pickle(f\"data/and_neurons/df_{option}.pkl\") \n",
    "\n",
    "with open(f\"data/and_neurons/set_losses.json\", \"r\") as f:\n",
    "    all_losses = json.load(f)\n",
    "\n",
    "prompts = haystack_utils.generate_random_prompts(option, model, common_tokens, 500, length=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_token = model.to_single_token(\"gen\")\n",
    "ge_token = model.to_single_token(\"ge\")\n",
    "gen_dir = model.tokens_to_residual_directions(gen_token)\n",
    "ge_dir = model.tokens_to_residual_directions(ge_token)\n",
    "\n",
    "cos = torch.nn.CosineSimilarity(dim=1)\n",
    "gen_sims = cos(model.W_out[5], gen_dir.unsqueeze(0)).cpu().numpy()\n",
    "ge_sims = cos(model.W_out[5], ge_dir.unsqueeze(0)).cpu().numpy()\n",
    "\n",
    "df[\"GenSim\"] = gen_sims\n",
    "df[\"GeSim\"] = ge_sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prev/Curr/Context\n",
      "AblationDiff    0.434787\n",
      "GenSim          0.002992\n",
      "GeSim          -0.000410\n",
      "dtype: float64\n",
      "Prev/Curr/Context\n",
      "AblationDiff   -0.431541\n",
      "GenSim          0.000029\n",
      "GeSim           0.004855\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(df[df[\"AblationDiff\"]>0.2][[\"AblationDiff\", \"GenSim\", \"GeSim\"]].mean())\n",
    "print(df[df[\"AblationDiff\"]<-0.2][[\"AblationDiff\", \"GenSim\", \"GeSim\"]].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102\n",
      "1.4273631572723389 7.775740146636963\n"
     ]
    }
   ],
   "source": [
    "df_tmp = df.copy()\n",
    "ablation_mode = \"YYN\"\n",
    "\n",
    "df_tmp[\"Custom\"] = (df[\"YYY\"]>0) & (df[\"YYY\"]>df[\"NNN\"]) & (df[\"GenSim\"]>df[\"GeSim\"]) &\\\n",
    "    (df[\"YYY\"]>df[\"YYN\"]) & (df[\"YYY\"]>df[\"YNY\"]) & (df[\"YYY\"]>df[\"NYY\"]) &\\\n",
    "    (df[\"YYY\"]>df[\"NYN\"]) & (df[\"YYY\"]>df[\"NNY\"]) & (df[\"YYY\"]>df[\"YNN\"])# & df[\"PosSim\"]\n",
    "\n",
    "print(df_tmp[\"Custom\"].sum())\n",
    "pos_and_neurons = torch.LongTensor(df_tmp[df_tmp[\"Custom\"]].index.tolist()).cuda()\n",
    "\n",
    "#df_tmp[\"context_diff\"] = df_tmp[\"YYY\"] - df_tmp[\"YYN\"]\n",
    "#df_tmp = df_tmp.sort_values(by=[\"Custom\", \"context_diff\"], ascending=False)\n",
    "#pos_and_neurons = torch.LongTensor(df_tmp.index.tolist()[:30]).cuda()\n",
    "\n",
    "original_loss, ablated_loss = compute_mlp_loss(prompts, df, pos_and_neurons, ablate_mode=ablation_mode, compute_original_loss=True)\n",
    "print(original_loss, ablated_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model.hooks(deactivate_neurons_fwd_hooks):\n",
    "    ablated_loss, ablated_cache = model.run_with_cache(prompts, return_type=\"loss\")\n",
    "\n",
    "def get_ablate_neurons_hook(neuron: int | list[int], ablated_cache, layer=5):\n",
    "    def ablate_neurons_hook(value, hook):\n",
    "        value[:, :, neuron] = ablated_cache[f'blocks.{layer}.mlp.hook_post'][:, :, neuron]\n",
    "        return value\n",
    "    return [(f'blocks.{layer}.mlp.hook_post', ablate_neurons_hook)]\n",
    "\n",
    "ablate_top_neurons_hook = get_ablate_neurons_hook([i for i in range(2048)], ablated_cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([500, 23])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/conda-bld/pytorch_1682343967769/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [7347,0,0], thread: [64,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1682343967769/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [7347,0,0], thread: [65,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1682343967769/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [7347,0,0], thread: [66,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1682343967769/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [7347,0,0], thread: [67,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1682343967769/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [7347,0,0], thread: [68,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1682343967769/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [7347,0,0], thread: [69,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1682343967769/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [7347,0,0], thread: [70,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1682343967769/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [7347,0,0], thread: [71,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1682343967769/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [7347,0,0], thread: [72,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1682343967769/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [7347,0,0], thread: [73,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1682343967769/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [7347,0,0], thread: [74,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1682343967769/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [7347,0,0], thread: [75,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1682343967769/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [7347,0,0], thread: [76,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1682343967769/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [7347,0,0], thread: [77,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1682343967769/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [7347,0,0], thread: [78,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1682343967769/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [7347,0,0], thread: [79,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1682343967769/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [7347,0,0], thread: [80,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1682343967769/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [7347,0,0], thread: [81,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1682343967769/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [7347,0,0], thread: [82,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1682343967769/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [7347,0,0], thread: [83,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1682343967769/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [7347,0,0], thread: [84,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1682343967769/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [7347,0,0], thread: [85,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1682343967769/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [7347,0,0], thread: [86,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1682343967769/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [7347,0,0], thread: [87,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1682343967769/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [7347,0,0], thread: [88,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1682343967769/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [7347,0,0], thread: [89,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1682343967769/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [7347,0,0], thread: [90,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1682343967769/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [7347,0,0], thread: [91,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1682343967769/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [7347,0,0], thread: [92,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1682343967769/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [7347,0,0], thread: [93,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1682343967769/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [7347,0,0], thread: [94,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1682343967769/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [7347,0,0], thread: [95,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1682343967769/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [7536,0,0], thread: [0,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1682343967769/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [7536,0,0], thread: [1,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1682343967769/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [7536,0,0], thread: [2,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1682343967769/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [7536,0,0], thread: [3,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1682343967769/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [7536,0,0], thread: [4,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1682343967769/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [7536,0,0], thread: [5,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1682343967769/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [7536,0,0], thread: [6,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1682343967769/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [7536,0,0], thread: [7,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1682343967769/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [7536,0,0], thread: [8,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1682343967769/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [7536,0,0], thread: [9,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1682343967769/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [7536,0,0], thread: [10,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1682343967769/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [7536,0,0], thread: [11,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1682343967769/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [7536,0,0], thread: [12,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1682343967769/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [7536,0,0], thread: [13,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1682343967769/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [7536,0,0], thread: [14,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1682343967769/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [7536,0,0], thread: [15,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1682343967769/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [7536,0,0], thread: [16,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1682343967769/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [7536,0,0], thread: [17,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1682343967769/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [7536,0,0], thread: [18,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1682343967769/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [7536,0,0], thread: [19,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1682343967769/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [7536,0,0], thread: [20,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1682343967769/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [7536,0,0], thread: [21,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1682343967769/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [7536,0,0], thread: [22,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1682343967769/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [7536,0,0], thread: [23,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1682343967769/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [7536,0,0], thread: [24,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1682343967769/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [7536,0,0], thread: [25,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1682343967769/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [7536,0,0], thread: [26,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1682343967769/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [7536,0,0], thread: [27,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1682343967769/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [7536,0,0], thread: [28,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1682343967769/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [7536,0,0], thread: [29,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1682343967769/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [7536,0,0], thread: [30,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1682343967769/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [7536,0,0], thread: [31,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m diffs[original_logprobs\u001b[39m<\u001b[39m\u001b[39m-\u001b[39m\u001b[39m7\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m      6\u001b[0m top_diff, top_token \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtopk(diffs, \u001b[39m20\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m \u001b[39mprint\u001b[39;49m(top_diff)\n\u001b[1;32m      8\u001b[0m \u001b[39mprint\u001b[39m(model\u001b[39m.\u001b[39mto_str_tokens(top_token))\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:426\u001b[0m, in \u001b[0;36mTensor.__repr__\u001b[0;34m(self, tensor_contents)\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    423\u001b[0m         Tensor\u001b[39m.\u001b[39m\u001b[39m__repr__\u001b[39m, (\u001b[39mself\u001b[39m,), \u001b[39mself\u001b[39m, tensor_contents\u001b[39m=\u001b[39mtensor_contents\n\u001b[1;32m    424\u001b[0m     )\n\u001b[1;32m    425\u001b[0m \u001b[39m# All strings are unicode in Python 3.\u001b[39;00m\n\u001b[0;32m--> 426\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_tensor_str\u001b[39m.\u001b[39;49m_str(\u001b[39mself\u001b[39;49m, tensor_contents\u001b[39m=\u001b[39;49mtensor_contents)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor_str.py:636\u001b[0m, in \u001b[0;36m_str\u001b[0;34m(self, tensor_contents)\u001b[0m\n\u001b[1;32m    634\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad(), torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39m_python_dispatch\u001b[39m.\u001b[39m_disable_current_modes():\n\u001b[1;32m    635\u001b[0m     guard \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_DisableFuncTorch()\n\u001b[0;32m--> 636\u001b[0m     \u001b[39mreturn\u001b[39;00m _str_intern(\u001b[39mself\u001b[39;49m, tensor_contents\u001b[39m=\u001b[39;49mtensor_contents)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor_str.py:567\u001b[0m, in \u001b[0;36m_str_intern\u001b[0;34m(inp, tensor_contents)\u001b[0m\n\u001b[1;32m    565\u001b[0m                     tensor_str \u001b[39m=\u001b[39m _tensor_str(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mto_dense(), indent)\n\u001b[1;32m    566\u001b[0m                 \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 567\u001b[0m                     tensor_str \u001b[39m=\u001b[39m _tensor_str(\u001b[39mself\u001b[39;49m, indent)\n\u001b[1;32m    569\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayout \u001b[39m!=\u001b[39m torch\u001b[39m.\u001b[39mstrided:\n\u001b[1;32m    570\u001b[0m     suffixes\u001b[39m.\u001b[39mappend(\u001b[39m\"\u001b[39m\u001b[39mlayout=\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayout))\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor_str.py:327\u001b[0m, in \u001b[0;36m_tensor_str\u001b[0;34m(self, indent)\u001b[0m\n\u001b[1;32m    323\u001b[0m     \u001b[39mreturn\u001b[39;00m _tensor_str_with_formatter(\n\u001b[1;32m    324\u001b[0m         \u001b[39mself\u001b[39m, indent, summarize, real_formatter, imag_formatter\n\u001b[1;32m    325\u001b[0m     )\n\u001b[1;32m    326\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 327\u001b[0m     formatter \u001b[39m=\u001b[39m _Formatter(get_summarized_data(\u001b[39mself\u001b[39;49m) \u001b[39mif\u001b[39;49;00m summarize \u001b[39melse\u001b[39;49;00m \u001b[39mself\u001b[39;49m)\n\u001b[1;32m    328\u001b[0m     \u001b[39mreturn\u001b[39;00m _tensor_str_with_formatter(\u001b[39mself\u001b[39m, indent, summarize, formatter)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor_str.py:115\u001b[0m, in \u001b[0;36m_Formatter.__init__\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_width \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_width, \u001b[39mlen\u001b[39m(value_str))\n\u001b[1;32m    114\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 115\u001b[0m     nonzero_finite_vals \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmasked_select(\n\u001b[1;32m    116\u001b[0m         tensor_view, torch\u001b[39m.\u001b[39misfinite(tensor_view) \u001b[39m&\u001b[39m tensor_view\u001b[39m.\u001b[39mne(\u001b[39m0\u001b[39m)\n\u001b[1;32m    117\u001b[0m     )\n\u001b[1;32m    119\u001b[0m     \u001b[39mif\u001b[39;00m nonzero_finite_vals\u001b[39m.\u001b[39mnumel() \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    120\u001b[0m         \u001b[39m# no valid number, do nothing\u001b[39;00m\n\u001b[1;32m    121\u001b[0m         \u001b[39mreturn\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "original_logprobs, ablated_logprobs, _, all_MLP5_logprobs = haystack_utils.get_direct_effect(prompts, model, pos=-2, context_ablation_hooks=deactivate_neurons_fwd_hooks, context_activation_hooks=activate_neurons_fwd_hooks, return_type='logprobs')\n",
    "\n",
    "diffs = original_logprobs - ablated_logprobs\n",
    "diffs[all_ignore] = 0\n",
    "# diffs[original_logprobs<-7] = 0\n",
    "# top_diff, top_token = torch.topk(diffs, 20)\n",
    "# print(top_diff)\n",
    "# print(model.to_str_tokens(top_token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %%\n",
    "\n",
    "option = \"orschlägen\"\n",
    "ablation_mode = \"YYN\"\n",
    "prompts = haystack_utils.generate_random_prompts(option, model, common_tokens, 1000, length=20)\n",
    "\n",
    "names = list(all_losses[option][ablation_mode].keys())\n",
    "losses = [[all_losses[option][ablation_mode][name]] for name in names]\n",
    "\n",
    "print(len(names), len(losses))\n",
    "print([len(x) for x in losses])\n",
    "haystack_utils.plot_barplot(losses, names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "layer = 2\n",
    "ngram = \"orschlägen\"\n",
    "prompts = haystack_utils.generate_random_prompts(ngram, model, common_tokens, 500, length=20)\n",
    "\n",
    "if ngram.startswith(\" \"):\n",
    "    prompt_tuple = haystack_utils.get_trigram_prompts(prompts, new_word_tokens, continuation_tokens)\n",
    "else:\n",
    "    prompt_tuple = haystack_utils.get_trigram_prompts(prompts, continuation_tokens, continuation_tokens)\n",
    "prev_token_direction, curr_token_direction = haystack_utils.get_residual_trigram_directions(prompt_tuple, model, layer-1)\n",
    "\n",
    "prev_token_sim = get_cosine_sim(prev_token_direction, layer)\n",
    "curr_token_sim = get_cosine_sim(curr_token_direction, layer)\n",
    "context_sim = get_cosine_sim(context_direction, layer)\n",
    "\n",
    "plot_histogram(prev_token_sim, curr_token_sim, context_sim, \"Prev Token\", \"Curr Token\", \"Context\")\n",
    "# %%\n",
    "prev_sim_neurons = torch.argwhere(prev_token_sim>0.05)\n",
    "curr_sim_neurons = torch.argwhere(curr_token_sim>0.03)\n",
    "\n",
    "print(len(prev_sim_neurons), len(curr_sim_neurons))\n",
    "union = haystack_utils.union_where([prev_token_sim, curr_token_sim], 0.07)\n",
    "print(union)\n",
    "# %%\n",
    "\n",
    "# Get random mean cache\n",
    "random_prompts = haystack_utils.generate_random_prompts(ngram, model, common_tokens, 500, length=20)[:, :-3]\n",
    "_, random_cache = model.run_with_cache(random_prompts)\n",
    "\n",
    "\n",
    "# %%\n",
    "# Define ablate neuron hook\n",
    "\n",
    "# Layer 1\n",
    "# orschlägen: tensor([  61,  188, 1011], device='cuda:0')\n",
    "# häufig: 268 (almost doubles loss)\n",
    "# beweglich: neurons decrease loss - maybe they boost alternative completion\n",
    "\n",
    "def get_ablate_neurons_hook(neurons, layer):\n",
    "    print(neurons)\n",
    "    def ablate_neurons_hook(value, hook):\n",
    "        value[:, :, neurons] = random_cache[f'blocks.{layer}.mlp.hook_post'][:, :, neurons].mean((0, 1))\n",
    "        return value\n",
    "    return [(f'blocks.{layer}.mlp.hook_post', ablate_neurons_hook)]\n",
    "\n",
    "# Check loss increase\n",
    "original_loss, original_ablated_loss = compute_mlp_loss(prompts, df, torch.LongTensor([i for i in range(model.cfg.d_mlp)]).cuda(), compute_original_loss=True)\n",
    "\n",
    "with model.hooks(fwd_hooks=get_ablate_neurons_hook([1789], layer)):\n",
    "    ablated_loss = model(prompts, return_type=\"loss\", loss_per_token=True)[:, -1].mean().item()\n",
    "\n",
    "print(original_loss, original_ablated_loss, ablated_loss)\n",
    "# %%\n",
    "\n",
    "# 1011 increases loss on both \"gen\" and \"ge\"\n",
    "# Either it boosts both completions (trigram table)\n",
    "# Or it combines \"orsch\" and \"lä\" into a single representation that later components use\n",
    "\n",
    "# Check if trigram table by looking at the direct effect\n",
    "# Total effect of L1N1011\n",
    "with model.hooks(fwd_hooks=get_ablate_neurons_hook([1406], layer)):\n",
    "    _, ablated_cache = model.run_with_cache(prompts)\n",
    "\n",
    "def ablate_component_hook(value, hook):\n",
    "    value = ablated_cache[hook.name]\n",
    "    return value\n",
    "\n",
    "components = [f\"blocks.{layer}.mlp.hook_post\" for layer in range(3, 6)] + [f\"blocks.{layer}.attn.hook_z\" for layer in range(3, 6)]\n",
    "hooks = [(component, ablate_component_hook) for component in components]\n",
    "\n",
    "with model.hooks(fwd_hooks=hooks):\n",
    "    ablated_logits = model(prompts, return_type=\"logits\", loss_per_token=True)[:, -2].log_softmax(-1).mean(0)\n",
    "\n",
    "original_logits = model(prompts, return_type=\"logits\", loss_per_token=True)[:, -2].log_softmax(-1).mean(0)\n",
    "\n",
    "print(ablated_logits.shape, original_logits.shape)\n",
    "\n",
    "prob_diff = original_logits - ablated_logits\n",
    "prob_diff[all_ignore] = 0\n",
    "prob_diff[original_logits < -7] = 0\n",
    "diffs, tokens = torch.topk(prob_diff, 20)\n",
    "print(diffs)\n",
    "print(tokens)\n",
    "print(model.to_str_tokens(tokens))\n",
    "\n",
    "# %% \n",
    "# Direct effect\n",
    "_, original_cache = model.run_with_cache(prompts)\n",
    "\n",
    "def activate_component_hook(value, hook):\n",
    "    value = original_cache[hook.name]\n",
    "    return value\n",
    "\n",
    "activate_hooks = [(component, activate_component_hook) for component in components]\n",
    "\n",
    "with model.hooks(fwd_hooks=activate_hooks + get_ablate_neurons_hook([1406], layer)):\n",
    "    activated_logits = model(prompts, return_type=\"logits\", loss_per_token=True)[:, -2].log_softmax(-1).mean(0)\n",
    "\n",
    "prob_diff = original_logits - activated_logits\n",
    "prob_diff[all_ignore] = 0\n",
    "prob_diff[original_logits < -7] = 0\n",
    "diffs, tokens = torch.topk(prob_diff, 20)\n",
    "print(diffs)\n",
    "print(tokens)\n",
    "print(model.to_str_tokens(tokens))\n",
    "\n",
    "# Check later components + context neuron effects of 1011\n",
    "# %%\n",
    "\n",
    "#output_direction = model.W_out[1, 1011]\n",
    "output_direction = model.W_out[2, 1406]\n",
    "context_direction = model.W_out[3, 669]\n",
    "\n",
    "output_sims = get_cosine_sim(output_direction, 5)\n",
    "context_sims = get_cosine_sim(context_direction, 5)\n",
    "\n",
    "plot_histogram(output_sims, context_sims, torch.zeros_like(output_sims), \"Output\", \"Context\", \"Zero\")\n",
    "# %%\n",
    "union = haystack_utils.union_where([output_sims, context_sims], 0.05)\n",
    "len(union)\n",
    "# %%\n",
    "ngram = \"orschlägen\"\n",
    "prompts = haystack_utils.generate_random_prompts(ngram, model, common_tokens, 1000, length=20)\n",
    "\n",
    "original_loss, original_ablated_loss = compute_mlp_loss(prompts, df, torch.LongTensor([i for i in range(model.cfg.d_mlp)]).cuda(), compute_original_loss=True)\n",
    "\n",
    "with model.hooks(fwd_hooks=get_ablate_neurons_hook(union, 5)): #712, 394, 287\n",
    "    ablated_loss = model(prompts, return_type=\"loss\", loss_per_token=True)[:, -1].mean().item()\n",
    "\n",
    "print(original_loss, original_ablated_loss, ablated_loss)\n",
    "# %%\n",
    "ngram = \" meine Vorschläge\"\n",
    "prompts = haystack_utils.generate_random_prompts(ngram, model, common_tokens, 1000, length=20)\n",
    "original_loss, original_ablated_loss = compute_mlp_loss(prompts, df, torch.LongTensor([i for i in range(model.cfg.d_mlp)]).cuda(), compute_original_loss=True)\n",
    "print(original_loss, original_ablated_loss)\n",
    "# %%\n",
    "model.to_str_tokens(model.to_tokens(\" deinen Vorschläge\", prepend_bos=False))\n",
    "# %%\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
