{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'base' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n base ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformer_lens import HookedTransformer\n",
    "from jaxtyping import Float\n",
    "from torch import Tensor\n",
    "import plotly.io as pio\n",
    "import numpy as np\n",
    "from tqdm import trange\n",
    "from collections import defaultdictr\n",
    "\n",
    "pio.renderers.default = \"notebook_connected+notebook\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.autograd.set_grad_enabled(False)\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "import haystack_utils\n",
    "import hook_utils\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HookedTransformer.from_pretrained(\"gpt2-small\",\n",
    "    center_unembed=True,\n",
    "    center_writing_weights=True,\n",
    "    fold_ln=True,\n",
    "    device=\"cuda\")\n",
    "\n",
    "data = haystack_utils.load_json_data('english_europarl.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batched_dot_product(x: torch.Tensor, y: torch.Tensor):\n",
    "    return torch.vmap(torch.dot)(x, y)\n",
    "    \n",
    "def neuron_DLA(model: HookedTransformer, prompt: str, pos=np.s_[-1:]) -> tuple[Float[Tensor, \"component\"], list[str]]:\n",
    "    '''Gets full resid decomposition including all neurons'''\n",
    "    tokens = model.to_tokens(prompt)\n",
    "    answers = tokens[:, 1:]\n",
    "    tokens = tokens[:, :-1]\n",
    "    \n",
    "    _, cache = model.run_with_cache(tokens)\n",
    "    attrs, labels = cache.get_full_resid_decomposition(-1, expand_neurons=True, apply_ln=True, return_labels=True, pos_slice=pos)\n",
    "    \n",
    "    # I think it removes the batch dimension if it's one\n",
    "    answer_residual_directions = model.tokens_to_residual_directions(answers)\n",
    "    if answer_residual_directions.ndim == 1:\n",
    "        answer_residual_directions = answer_residual_directions.unsqueeze(0)  # [1 d_model]\n",
    "    elif answer_residual_directions.ndim == 3:\n",
    "        answer_residual_directions = answer_residual_directions[0]  # [pos d_model]\n",
    "    answer_residual_directions = answer_residual_directions[pos]  # [pos d_model]\n",
    "\n",
    "    neuron_indices = [i for i in range(len(labels)) if 'N' in labels[i]]\n",
    "    neuron_labels = [labels[i] for i in neuron_indices]\n",
    "    neuron_attrs = attrs[neuron_indices, :].squeeze(1)\n",
    "    results = []\n",
    "    for i in range(neuron_attrs.shape[1]):\n",
    "        results.append(batched_dot_product(neuron_attrs[:, i], answer_residual_directions[[i]].repeat(neuron_attrs.shape[0], 1)))\n",
    "    return torch.stack(results), neuron_labels\n",
    "\n",
    "def neuron_to_context_neuron_DLA(\n",
    "        model: HookedTransformer, \n",
    "        prompt: str | list[str], \n",
    "        pos=np.s_[-1:], \n",
    "        context_neuron=tuple[int, int]\n",
    ") -> tuple[Float[Tensor, \"component\"], list[str]]:\n",
    "    '''Gets full resid decomposition including all neurons. Unbatched.'''\n",
    "    tokens = model.to_tokens(prompt)\n",
    "    _, cache = model.run_with_cache(prompt)\n",
    "    layer, neuron = context_neuron\n",
    "    neuron_attrs, neuron_labels = cache.stack_neuron_results(layer, apply_ln=True, return_labels=True, pos_slice=pos)\n",
    "    neuron_attrs = neuron_attrs.squeeze(1)\n",
    "    \n",
    "    answer_residual_direction = model.W_in[layer, :, neuron].unsqueeze(0)  # [1 d_model]\n",
    "\n",
    "    results = []\n",
    "    for i in range(neuron_attrs.shape[1]):\n",
    "        results.append(batched_dot_product(neuron_attrs[:, i], answer_residual_direction.repeat(neuron_attrs.shape[0], 1)))\n",
    "    return torch.stack(results), neuron_labels\n",
    "\n",
    "def resid_to_context_neuron_DLA(\n",
    "        model: HookedTransformer, \n",
    "        prompt: str | list[str], \n",
    "        pos=np.s_[-1:], \n",
    "        context_neuron:tuple[int, int]=(0,0)\n",
    ") -> tuple[Float[Tensor, \"component\"], list[str]]:\n",
    "    '''Gets full resid decomposition including all neurons. Unbatched.'''\n",
    "    tokens = model.to_tokens(prompt)\n",
    "    _, cache = model.run_with_cache(prompt)\n",
    "    layer, neuron = context_neuron\n",
    "    all_attrs, labels = cache.get_full_resid_decomposition(layer, apply_ln=True, return_labels=True, pos_slice=pos)\n",
    "    all_attrs = all_attrs.squeeze(1)\n",
    "    \n",
    "    answer_residual_direction = model.W_in[layer, :, neuron].unsqueeze(0)  # [1 d_model]\n",
    "\n",
    "    results = []\n",
    "    for i in range(all_attrs.shape[1]):\n",
    "        results.append(batched_dot_product(all_attrs[:, i], answer_residual_direction.repeat(all_attrs.shape[0], 1)))\n",
    "    return torch.stack(results), labels\n",
    "\n",
    "def get_neuron_mean_acts(model: HookedTransformer, data: list[str], layer_neuron_dict: dict[int, list[int]]) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    sorted_layer_neuron_tuples = []\n",
    "    sorted_acts = []\n",
    "\n",
    "    for layer, neurons in layer_neuron_dict.items():\n",
    "        mean_acts = haystack_utils.get_mlp_activations(data, layer, model, context_crop_start=0, hook_pre=False, neurons=neurons, disable_tqdm=True)\n",
    "        sorted_layer_neuron_tuples.extend([(layer, neuron) for neuron in neurons])\n",
    "        sorted_acts.extend(mean_acts)\n",
    "        assert len(sorted_layer_neuron_tuples) == len(sorted_acts)\n",
    "\n",
    "    return sorted_layer_neuron_tuples, sorted_acts\n",
    "\n",
    "def get_unspecified_neurons(model: HookedTransformer, layer_neuron_dict: dict[int, list[int]]):\n",
    "    unspecified = []\n",
    "    for layer in range(model.cfg.n_layers):\n",
    "        for neuron in range(model.cfg.d_mlp):\n",
    "            if not neuron in layer_neuron_dict[layer]:\n",
    "                unspecified.append((layer, neuron))\n",
    "    return unspecified\n",
    "\n",
    "def get_neuron_loss_increases(model: HookedTransformer, data: list[str], prompt: str, positionwise: bool=False) -> torch.Tensor:\n",
    "    n_tokens = model.to_tokens(prompt).shape[1] - 1\n",
    "    original_loss = model([prompt], return_type='loss', loss_per_token=positionwise)\n",
    "    \n",
    "    losses = []\n",
    "    for layer in trange(model.cfg.n_layers):\n",
    "        mean_acts = haystack_utils.get_mlp_activations(data[:200], layer, model, disable_tqdm=True, context_crop_start=0)\n",
    "        for neuron in range(model.cfg.d_mlp):\n",
    "            hook = hook_utils.get_ablate_neuron_hook(layer, neuron, mean_acts[neuron])\n",
    "            with model.hooks([hook]):\n",
    "                ablated_loss = model([prompt], return_type='loss', loss_per_token=positionwise)\n",
    "                losses.append((ablated_loss - original_loss)[0])\n",
    "    return torch.stack(losses).reshape(n_tokens, model.cfg.n_layers * model.cfg.d_mlp)\n",
    "\n",
    "def compare_dla_and_ablation(model: HookedTransformer, dla_attrs_by_neuron: torch.Tensor, ablation_losses_by_neuron: torch.Tensor, num_neurons=20):\n",
    "    print(\"DLA:\")\n",
    "    values, indices = torch.topk(dla_attrs_by_neuron, num_neurons, dim=-1)\n",
    "    layer_indices, neuron_indices = np.unravel_index(indices.cpu().numpy(), (model.cfg.n_layers, model.cfg.d_mlp))\n",
    "    print(list(zip(layer_indices.tolist(), neuron_indices.tolist())))\n",
    "    print(dla_attrs_by_neuron[indices.tolist()])\n",
    "\n",
    "    print(\"Ablation:\")\n",
    "    loss_increases_by_neuron = ablation_losses_by_neuron\n",
    "    values, indices = torch.topk(loss_increases_by_neuron, num_neurons)\n",
    "    layer_indices, neuron_indices = np.unravel_index(indices.cpu().numpy()[:num_neurons], (model.cfg.n_layers, model.cfg.d_mlp))\n",
    "    print(list(zip(layer_indices.tolist(), neuron_indices.tolist())))\n",
    "    print(dla_attrs_by_neuron[indices.tolist()])\n",
    "\n",
    "def get_hook_inputs_for_token_index(model: HookedTransformer, data: list[str], loss_increases_by_neuron: torch.Tensor, k=40):\n",
    "    values, indices = torch.topk(loss_increases_by_neuron, k)\n",
    "\n",
    "    layer_indices, neuron_indices = np.unravel_index(indices.cpu().numpy(), (model.cfg.n_layers, model.cfg.d_mlp))\n",
    "    layer_neuron_dict = defaultdict(list)\n",
    "    for layer, neuron in zip(layer_indices, neuron_indices):\n",
    "        layer_neuron_dict[layer].append(neuron)\n",
    "\n",
    "    sorted_dla_layer_neuron_tuples = []\n",
    "    sorted_acts = []\n",
    "    for layer, neurons in layer_neuron_dict.items():\n",
    "        mean_acts = haystack_utils.get_mlp_activations(data, layer, model, context_crop_start=0, neurons=neurons, disable_tqdm=True)\n",
    "        sorted_dla_layer_neuron_tuples.extend([(layer, neuron) for neuron in neurons])\n",
    "        sorted_acts.extend(mean_acts)\n",
    "        assert len(sorted_dla_layer_neuron_tuples) == len(sorted_acts)\n",
    "\n",
    "    return sorted_dla_layer_neuron_tuples, sorted_acts\n",
    "\n",
    "def unravel_top_k(neuron_attrs: torch.Tensor, k: int=10):\n",
    "    values, indices = torch.topk(neuron_attrs, k)\n",
    "    layer_indices, neuron_indices = np.unravel_index(indices.cpu().numpy(), (model.cfg.n_layers, model.cfg.d_mlp))\n",
    "    return list(zip(layer_indices.tolist(), neuron_indices.tolist()))\n",
    "\n",
    "def resid_to_head_DLA(\n",
    "        model: HookedTransformer, \n",
    "        prompt: str | list[str], \n",
    "        head: tuple[int, int],\n",
    "        pos=np.s_[-1:], \n",
    "        \n",
    ") -> tuple[Float[Tensor, \"component\"], list[str]]:\n",
    "    '''Gets full resid decomposition and return the composition of each element of the given K matrix. Unbatched.'''\n",
    "    tokens = model.to_tokens(prompt)\n",
    "    _, cache = model.run_with_cache(prompt)\n",
    "    layer, head_index = head\n",
    "    all_attrs, labels = cache.get_full_resid_decomposition(layer, apply_ln=True, return_labels=True, pos_slice=pos)\n",
    "    all_attrs = all_attrs.squeeze(1)\n",
    "    answer_residual_direction = model.W_K[layer, head_index, :]\n",
    "    results = torch.zeros(all_attrs.shape[1], all_attrs.shape[0], answer_residual_direction.shape[1])\n",
    "    for i in range(all_attrs.shape[1]): # for each token\n",
    "        for j in range(answer_residual_direction.shape[1]): # for each direction in head input\n",
    "            token_attrs = all_attrs[:, i]\n",
    "            answer = answer_residual_direction[:, j].unsqueeze(0).repeat(token_attrs.shape[0], 1)\n",
    "            results[i, :, j] = batched_dot_product(token_attrs, answer)\n",
    "    return results, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attrs, labels = neuron_DLA(model, \"Once upon a time\", pos=np.s_[-10:])\n",
    "haystack_utils.line(attrs[0].cpu().numpy(), xlabel=\"Correct logit\", ylabel=\"\", title=\"DLA per neuron in layer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_neuron_dict = defaultdict(list)\n",
    "for token_index in range(4):\n",
    "    values, indices = torch.topk(attrs[token_index], 3, dim=-1)\n",
    "    layer_indices, neuron_indices = np.unravel_index(indices.cpu().numpy(), (model.cfg.n_layers, model.cfg.d_mlp))\n",
    "    for layer_index, neuron_index in zip(layer_indices.tolist(), neuron_indices.tolist()):\n",
    "        layer_neuron_dict[layer_index].append(neuron_index)\n",
    "\n",
    "sorted_dla_tuples, sorted_acts = get_neuron_mean_acts(model, filtered_prompts[:200], layer_neuron_dict)\n",
    "hooks = hook_utils.get_ablate_context_neurons_hooks(sorted_dla_tuples, sorted_acts)\n",
    "with model.hooks(hooks):\n",
    "    print(model.generate(\"Once\", 10, temperature=0, use_past_kv_cache=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
