{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch import einsum\n",
    "from tqdm.auto import tqdm\n",
    "import seaborn as sns\n",
    "from transformer_lens import HookedTransformer, ActivationCache, utils\n",
    "from datasets import load_dataset\n",
    "from einops import einsum\n",
    "import pandas as pd\n",
    "from transformer_lens import utils\n",
    "from rich.table import Table, Column\n",
    "from rich import print as rprint\n",
    "from jaxtyping import Float, Int, Bool\n",
    "from torch import Tensor\n",
    "import einops\n",
    "import functools\n",
    "from transformer_lens.hook_points import HookPoint\n",
    "# import circuitsvis\n",
    "from IPython.display import HTML\n",
    "from plotly.express import line\n",
    "import plotly.express as px\n",
    "from tqdm.auto import tqdm\n",
    "import json\n",
    "import gc\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from plotly.subplots import make_subplots\n",
    "# Plotly needs a different renderer for VSCode/Notebooks vs Colab argh\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = \"notebook_connected\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.autograd.set_grad_enabled(False)\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "from haystack_utils import load_txt_data, get_mlp_activations, line\n",
    "import haystack_utils\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAYER_TO_ABLATE = 3\n",
    "NEURONS_TO_ABLATE = [609]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m-v0 into HookedTransformer\n",
      "kde4_french.txt: Loaded 1007 examples with 505 to 5345 characters each.\n",
      "kde4_english.txt: Loaded 1007 examples with 501 to 5295 characters each.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b69f27f0ceb249a28120b36b1b47e2b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21d047ad08a94dbbbad74377a069c963",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = HookedTransformer.from_pretrained(\"pythia-70m-v0\", fold_ln=True, device=device)\n",
    "\n",
    "kde_french = load_txt_data(\"kde4_french.txt\")\n",
    "kde_english = load_txt_data(\"kde4_english.txt\")\n",
    "\n",
    "french_activations = get_mlp_activations(kde_french, LAYER_TO_ABLATE, model, num_prompts=100, mean=True)\n",
    "english_activations = get_mlp_activations(kde_english, LAYER_TO_ABLATE, model, num_prompts=100, mean=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare whole model on bigram prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[' person', 'ne'], [' ann', 'ée'], [' part', 'ie'], [' hom', 'me'], [' fem', 'me'], [' enf', 'ant'], [' ma', 'ison'], [' am', 'our'], [' am', 'i'], [' fam', 'ille'], [' arg', 'ent'], [' mat', 'in'], [' so', 'ir'], [' n', 'uit'], [' am', 'i'], [' fr', 'ère'], [' p', 'ère'], [' m', 'ère'], [' fil', 'le']]\n"
     ]
    }
   ],
   "source": [
    "# Find some single words embedded as two tokens - those are very likely bigrams\n",
    "test_words = [' happily', ' beautiful', ' suddenly', ' exciting', ' surprisingly', ' wonderfully', ' generally', ' delicious', ' basically', ' carefully', ' suddenly', ' absolutely', ' interestingly', ' naturally', ' thankfully', ' quickly', ' usually', ' definitely', ' eventually', ' fortunately']\n",
    "test_words = [' absolutely', ' basically', ' carefully', ' delicious', ' eventually', ' fortunately', ' generally', ' happily', ' interestingly', ' naturally', ' surprisingly', ' thankfully', ' usually', ' wonderfully']\n",
    "test_words = [' significantly', ' occasionally', ' consistently', ' effectively', ' importantly', ' basically', ' incredibly', ' typically', ' ideally', ' genuinely', ' certainly', ' previously', ' frequently', ' gradually', ' personally', ' ultimately', ' currently', ' traditionally', ' basically', ' fortunately']\n",
    "test_words = ['temps', 'personne', 'jour', 'année', 'vie', 'chose', 'partie', 'homme', 'femme', 'enfant', 'maison', 'ville', 'amour', 'travail', 'ami', 'famille', 'étudiant', 'cœur', 'argent', 'temps', 'matin', 'soir', 'nuit', 'ami', 'frère', 'sœur', 'père', 'mère', 'fille', 'garçon']\n",
    "\n",
    "bigrams = []\n",
    "for word in test_words:\n",
    "    if word[0] != \" \":\n",
    "        word = \" \" + word\n",
    "    length = len(model.to_tokens(word)[0])-1\n",
    "    if length == 2:\n",
    "        str_tokens = model.to_str_tokens(word)\n",
    "        bigrams.append([str_tokens[1], str_tokens[2]])\n",
    "print(bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer_token_logit_difference(\n",
    "    prompt: str, \n",
    "    answer: str, \n",
    "    model: HookedTransformer = model, \n",
    "    mean_disabled_activation: Float[Tensor, \"d_mlp\"] = english_activations, \n",
    "    mean_enabled_activation: Float[Tensor, \"d_mlp\"] = french_activations, \n",
    "    layer_to_ablate: int = LAYER_TO_ABLATE, \n",
    "    neurons_to_ablate: list[int] = NEURONS_TO_ABLATE, \n",
    "    log=True, \n",
    "    apply_softmax=True\n",
    "):\n",
    "    \"\"\" Get the difference in logits between the answer token when the context neurons are enabled and when they're disabled. \"\"\"\n",
    "    def get_answer_rank(logits: Float[Tensor, \"d_mlp\"], answer_token: int):\n",
    "        return torch.sum(logits > logits[answer_token]).item() + 1\n",
    "    \n",
    "    neurons = torch.LongTensor(neurons_to_ablate)\n",
    "    def disable_neurons_hook(value, hook):\n",
    "        value[:, :, neurons] = mean_disabled_activation[neurons]\n",
    "        return value\n",
    "    def enable_neurons_hook(value, hook):\n",
    "        value[:, :, neurons] = mean_enabled_activation[neurons]\n",
    "        return value\n",
    "\n",
    "    position = -1\n",
    "    original_logits, disabled_logits, _, _ = haystack_utils.get_caches_single_prompt(prompt, model, fwd_hooks=[(f'blocks.{layer_to_ablate}.mlp.hook_post', disable_neurons_hook)], return_type=\"logits\")\n",
    "    _, enabled_logits, _, _ = haystack_utils.get_caches_single_prompt(prompt, model, fwd_hooks=[(f'blocks.{layer_to_ablate}.mlp.hook_post', enable_neurons_hook)], return_type=\"logits\")\n",
    "    answer_token = model.to_single_token(answer)\n",
    "\n",
    "    if apply_softmax:\n",
    "        original_logits = original_logits.softmax(dim=-1)\n",
    "        enabled_logits = enabled_logits.softmax(dim=-1)\n",
    "        disabled_logits = disabled_logits.softmax(dim=-1)\n",
    "\n",
    "    contribution_difference = (enabled_logits[0, position, answer_token] - disabled_logits[0, position, answer_token]).item()\n",
    "\n",
    "    if log:\n",
    "        print(f\"Prediction of \\\"{prompt}\\\" + \\\"{answer}\\\"\")\n",
    "        print(f\"Original logits probs: {original_logits[0, position, answer_token]:.6f} (Rank {get_answer_rank(original_logits[0, position], answer_token)})\")\n",
    "        print(f\"Enabled context neuron probs: {enabled_logits[0, position, answer_token]:.6f} (Rank {get_answer_rank(enabled_logits[0, position], answer_token)})\")\n",
    "        print(f\"Disabled context neuron probs: {disabled_logits[0, position, answer_token]:.6f} (Rank {get_answer_rank(disabled_logits[0, position], answer_token)})\")\n",
    "        print(f\"Contribution difference: {contribution_difference:.6f}\")\n",
    "    return contribution_difference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note the listed bigrams aren't necessarily the most common completions for the language\n",
    "french_bigrams = [[\" rend\", \"re\"], [\" met\", \"tre\"], [\" demand\", \"er\"], [\" pou\", \"voir\"], [' person', 'ne'], [' hom', 'me'], [' am', 'our']]\n",
    "english_bigrams = [[' firef', 'ighter'], [' waterm', 'elon'], [' straw', 'berry'], [' fortun', 'ately'], [' thank', 'fully'], [' interesting', 'ly']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.002872240231876536\n"
     ]
    }
   ],
   "source": [
    "logit_differences = []\n",
    "for prompt, answer in french_bigrams:\n",
    "    diff = get_answer_token_logit_difference(prompt, answer, log=False)\n",
    "    logit_differences.append(diff)\n",
    "\n",
    "print(np.mean(logit_differences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.09110676303195457\n"
     ]
    }
   ],
   "source": [
    "logit_differences = []\n",
    "for prompt, answer in english_bigrams:\n",
    "    diff = get_answer_token_logit_difference(prompt, answer, log=False)\n",
    "    logit_differences.append(diff)\n",
    "\n",
    "print(np.mean(logit_differences))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results\n",
    "- We compare the logit difference of the correct bigram token between activating the French neuron and deactivating the French neuron\n",
    "- A positive logit difference means the model has a higher activation with the French neuron active\n",
    "- On French bigrams, the logit difference is positive\n",
    "    - This means the French neuron boosts French bigrams\n",
    "- On English bigrams, the logit difference is negative\n",
    "    - This means the French neuron inhibits English bigrams"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare use of context before bigram occurrence\n",
    "Motivation: see how the context neuron loss difference responds to different contexts before the bigram we care about\n",
    "- Start with random number tokens (i.e. they shouldn't matter for either language) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repeated unrelated padding tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_padded_logit_difference(bigram_prompts, random_number_str_token, num_pad):\n",
    "    logit_differences = []\n",
    "    for prompt, answer in bigram_prompts:\n",
    "        prompt = random_number_str_token * num_pad + prompt\n",
    "        assert len(model.to_tokens(prompt)[0]) == 2 + num_pad, model.to_str_tokens(prompt)\n",
    "        diff = get_answer_token_logit_difference(prompt, answer, log=False)\n",
    "        logit_differences.append(diff)\n",
    "    return logit_differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use numbers as language-agnostic prompts\n",
    "random_number_str_token = \" 42\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d75b753798484c1ea285483d8631d0fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-2.24.1.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>                            <div id=\"06986009-941c-42a2-9be7-e25ecda02e1f\" class=\"plotly-graph-div\" style=\"height:525px; width:800px;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"06986009-941c-42a2-9be7-e25ecda02e1f\")) {                    Plotly.newPlot(                        \"06986009-941c-42a2-9be7-e25ecda02e1f\",                        [{\"hovertemplate\":\"variable=French Differences\\u003cbr\\u003eNumber of Padded Tokens=%{x}\\u003cbr\\u003evalue=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"French Differences\",\"line\":{\"color\":\"#636efa\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"French Differences\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19],\"xaxis\":\"x\",\"y\":[0.002872240231876536,0.0018753475117136986,0.0030692985913317117,0.004780500139012085,0.00488006748024158,0.004857826547387438,0.004700962520863479,0.004969807101328375,0.005743209518966198,0.006098717440311573,0.0066336547655997235,0.00743387550496014,0.008389411728525633,0.00867654648131325,0.009003273596265769,0.009424480006990572,0.009833415848074114,0.010547704286831763,0.011703852273058146,0.011328327057916405],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"variable=English Differences\\u003cbr\\u003eNumber of Padded Tokens=%{x}\\u003cbr\\u003evalue=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"English Differences\",\"line\":{\"color\":\"#EF553B\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"English Differences\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19],\"xaxis\":\"x\",\"y\":[-0.09110676303195457,-0.05700070950357864,-0.055658732793138675,-0.05215518632515644,-0.04986447926300267,-0.045185892444957666,-0.038604481727816164,-0.033911809315516926,-0.034867503544470914,-0.036747539231631286,-0.03934221791375118,-0.04142560415978854,-0.04208501482692858,-0.04116593200402955,-0.04080276421154849,-0.041492574712416776,-0.0426440399120717,-0.04458096056866149,-0.043755985340491556,-0.04355355283284249],\"yaxis\":\"y\",\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Number of Padded Tokens\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"value\"}},\"legend\":{\"title\":{\"text\":\"variable\"},\"tracegroupgap\":0},\"title\":{\"text\":\"Logit Differences for French and English Bigrams with Padded Tokens\"},\"width\":800},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('06986009-941c-42a2-9be7-e25ecda02e1f');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "french_differences = []\n",
    "english_differences = []\n",
    "for num_pad in tqdm(range(20)):\n",
    "    padded_french_differences = get_padded_logit_difference(french_bigrams, random_number_str_token, num_pad)\n",
    "    padded_english_differences = get_padded_logit_difference(english_bigrams, random_number_str_token, num_pad)\n",
    "    french_differences.append(np.mean(padded_french_differences))\n",
    "    english_differences.append(np.mean(padded_english_differences))\n",
    "\n",
    "df = pd.DataFrame({\"French Differences\": french_differences, \"English Differences\": english_differences, \"Number of Padded Tokens\": range(20)})\n",
    "px.line(df, x=\"Number of Padded Tokens\", y=[\"French Differences\", \"English Differences\"], title=\"Logit Differences for French and English Bigrams with Padded Tokens\", width=800)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results\n",
    "- We are comparing the difference in logits for the correct bigram completion between model runs with the French neuron activated or deactivated\n",
    "- For French bigrams, the difference in logits gets larger with increased context length\n",
    "- For English bigrams, the difference in logits gets smaller with increased context length"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare related padding to unrelated padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meaningful_pad = \" il\"\n",
    "meaningless_pad = \" 42\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0df5eb7be284c66a08e6cb36bec5f76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>                            <div id=\"63d10f68-dfb6-4797-b66f-fd5db28a6ff7\" class=\"plotly-graph-div\" style=\"height:525px; width:800px;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"63d10f68-dfb6-4797-b66f-fd5db28a6ff7\")) {                    Plotly.newPlot(                        \"63d10f68-dfb6-4797-b66f-fd5db28a6ff7\",                        [{\"hovertemplate\":\"variable=Meaningful pad\\u003cbr\\u003eNumber of Padded Tokens=%{x}\\u003cbr\\u003evalue=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"Meaningful pad\",\"line\":{\"color\":\"#636efa\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"Meaningful pad\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19],\"xaxis\":\"x\",\"y\":[0.002872240231876536,0.03056422597936554,0.03897304252106031,0.04751387104001229,0.06171308308382452,0.0705430231921907,0.07388991759424764,0.07007566073610048,0.06644800152987175,0.06634206836627397,0.06826783150401232,0.06937596415186167,0.0682830906838977,0.06557090455316938,0.061921475969679056,0.0598591115745616,0.05922937706470423,0.057502731636174885,0.05457707504891524,0.05488504397882415],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"variable=Meaningless pad\\u003cbr\\u003eNumber of Padded Tokens=%{x}\\u003cbr\\u003evalue=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"Meaningless pad\",\"line\":{\"color\":\"#EF553B\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"Meaningless pad\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19],\"xaxis\":\"x\",\"y\":[0.002872240231876536,0.0018753475117136986,0.0030692985913317117,0.004780500139012085,0.00488006748024158,0.004857826547387438,0.004700962520863479,0.004969807101328375,0.005743209518966198,0.006098717440311573,0.0066336547655997235,0.00743387550496014,0.008389411728525633,0.00867654648131325,0.009003273596265769,0.009424480006990572,0.009833415848074114,0.010547704286831763,0.011703852273058146,0.011328327057916405],\"yaxis\":\"y\",\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Number of Padded Tokens\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"value\"}},\"legend\":{\"title\":{\"text\":\"variable\"},\"tracegroupgap\":0},\"title\":{\"text\":\"Logit Differences for French padding with either meaningless numbers or meaningful french words\"},\"width\":800},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('63d10f68-dfb6-4797-b66f-fd5db28a6ff7');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "meaningful_pad_differences = []\n",
    "meaningless_pad_differences = []\n",
    "\n",
    "for num_pad in tqdm(range(20)):\n",
    "    meaningful_pad_difference = get_padded_logit_difference(french_bigrams, meaningful_pad, num_pad)\n",
    "    meaningless_pad_difference = get_padded_logit_difference(french_bigrams, meaningless_pad, num_pad)\n",
    "    meaningful_pad_differences.append(np.mean(meaningful_pad_difference))\n",
    "    meaningless_pad_differences.append(np.mean(meaningless_pad_difference))\n",
    "\n",
    "df = pd.DataFrame({\"Meaningful pad\": meaningful_pad_differences, \"Meaningless pad\": meaningless_pad_differences, \"Number of Padded Tokens\": range(20)})\n",
    "px.line(df, x=\"Number of Padded Tokens\", y=[\"Meaningful pad\", \"Meaningless pad\"], title=\"Logit Differences for French padding with either meaningless numbers or meaningful french words\", width=800)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results\n",
    "- When padding with meaningful values, the overall positive impact of the French neuron seems to be slightly smaller\n",
    "    - This could mean that other components can also identify the context as French in this scenario?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check contribution differences on complete sentence prompts in both languages\n",
    "- Prediction: \n",
    "    - Enabling the French neuron should boost the French token on both French and English sentences\n",
    "    - Enabling the French neuron should inhibit the English token on both French and English sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_biliangual_prompt(english_prompt, english_answer, french_prompt, french_answer):\n",
    "    print(\"French prompt, French answer\")\n",
    "    _ = get_answer_token_logit_difference(french_prompt, french_answer)\n",
    "    print(\"\\nEnglish prompt, French answer\")\n",
    "    _ = get_answer_token_logit_difference(english_prompt, french_answer)\n",
    "    print(\"\\nEnglish prompt, English answer\")\n",
    "    _ = get_answer_token_logit_difference(english_prompt, english_answer)\n",
    "    print(\"\\nFrench prompt, English answer\")\n",
    "    _ = get_answer_token_logit_difference(french_prompt, english_answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "French prompt, French answer\n",
      "Prediction of \"J'adore les animaux. J'aime jouer avec mon\" + \" chat\"\n",
      "Original logits probs: 0.000065 (Rank 1281)\n",
      "Enabled context neuron probs: 0.000068 (Rank 1262)\n",
      "Disabled context neuron probs: 0.000077 (Rank 1336)\n",
      "Contribution difference: -0.000010\n",
      "\n",
      "English prompt, French answer\n",
      "Prediction of \"I love animals. I enjoy playing with my\" + \" chat\"\n",
      "Original logits probs: 0.000001 (Rank 7562)\n",
      "Enabled context neuron probs: 0.000002 (Rank 6590)\n",
      "Disabled context neuron probs: 0.000001 (Rank 7568)\n",
      "Contribution difference: 0.000001\n",
      "\n",
      "English prompt, English answer\n",
      "Prediction of \"I love animals. I enjoy playing with my\" + \" cat\"\n",
      "Original logits probs: 0.001720 (Rank 54)\n",
      "Enabled context neuron probs: 0.002983 (Rank 37)\n",
      "Disabled context neuron probs: 0.001692 (Rank 54)\n",
      "Contribution difference: 0.001291\n",
      "\n",
      "French prompt, English answer\n",
      "Prediction of \"J'adore les animaux. J'aime jouer avec mon\" + \" cat\"\n",
      "Original logits probs: 0.000232 (Rank 535)\n",
      "Enabled context neuron probs: 0.000234 (Rank 540)\n",
      "Disabled context neuron probs: 0.000148 (Rank 829)\n",
      "Contribution difference: 0.000086\n"
     ]
    }
   ],
   "source": [
    "english_prompt = \"I love animals. I enjoy playing with my\"\n",
    "english_answer = \" cat\"\n",
    "french_prompt = \"J'adore les animaux. J'aime jouer avec mon\"\n",
    "french_answer = \" chat\"\n",
    "\n",
    "compare_biliangual_prompt(english_prompt, english_answer, french_prompt, french_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "French prompt, French answer\n",
      "Prediction of \"Je vais au supermarché pour acheter du\" + \" pain\"\n",
      "Original logits probs: 0.002075 (Rank 92)\n",
      "Enabled context neuron probs: 0.002107 (Rank 91)\n",
      "Disabled context neuron probs: 0.001291 (Rank 139)\n",
      "Contribution difference: 0.000816\n",
      "\n",
      "English prompt, French answer\n",
      "Prediction of \"I'm going to the supermarket to buy some\" + \" pain\"\n",
      "Original logits probs: 0.000061 (Rank 1619)\n",
      "Enabled context neuron probs: 0.000048 (Rank 1945)\n",
      "Disabled context neuron probs: 0.000061 (Rank 1616)\n",
      "Contribution difference: -0.000013\n",
      "\n",
      "English prompt, English answer\n",
      "Prediction of \"I'm going to the supermarket to buy some\" + \" bread\"\n",
      "Original logits probs: 0.007390 (Rank 12)\n",
      "Enabled context neuron probs: 0.010748 (Rank 9)\n",
      "Disabled context neuron probs: 0.007394 (Rank 12)\n",
      "Contribution difference: 0.003354\n",
      "\n",
      "French prompt, English answer\n",
      "Prediction of \"Je vais au supermarché pour acheter du\" + \" bread\"\n",
      "Original logits probs: 0.000003 (Rank 6241)\n",
      "Enabled context neuron probs: 0.000003 (Rank 5967)\n",
      "Disabled context neuron probs: 0.000003 (Rank 7916)\n",
      "Contribution difference: 0.000001\n"
     ]
    }
   ],
   "source": [
    "english_prompt = \"I'm going to the supermarket to buy some\"\n",
    "english_answer = \" bread\"\n",
    "french_prompt = \"Je vais au supermarché pour acheter du\"\n",
    "french_answer = \" pain\"\n",
    "\n",
    "compare_biliangual_prompt(english_prompt, english_answer, french_prompt, french_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "French prompt, French answer\n",
      "Prediction of \"Les parents de Bob vivent dans un petit village. Son\" + \" p\"\n",
      "Original logits probs: 0.011118 (Rank 6)\n",
      "Enabled context neuron probs: 0.012117 (Rank 5)\n",
      "Disabled context neuron probs: 0.008631 (Rank 12)\n",
      "Contribution difference: 0.003486\n",
      "\n",
      "English prompt, French answer\n",
      "Prediction of \"Bob's parents live in a small village. His\" + \" p\"\n",
      "Original logits probs: 0.000152 (Rank 412)\n",
      "Enabled context neuron probs: 0.000292 (Rank 215)\n",
      "Disabled context neuron probs: 0.000152 (Rank 412)\n",
      "Contribution difference: 0.000139\n",
      "\n",
      "English prompt, English answer\n",
      "Prediction of \"Bob's parents live in a small village. His\" + \" father\"\n",
      "Original logits probs: 0.149794 (Rank 1)\n",
      "Enabled context neuron probs: 0.176889 (Rank 2)\n",
      "Disabled context neuron probs: 0.149605 (Rank 1)\n",
      "Contribution difference: 0.027284\n",
      "\n",
      "French prompt, English answer\n",
      "Prediction of \"Les parents de Bob vivent dans un petit village. Son\" + \" father\"\n",
      "Original logits probs: 0.000526 (Rank 297)\n",
      "Enabled context neuron probs: 0.000473 (Rank 316)\n",
      "Disabled context neuron probs: 0.000536 (Rank 295)\n",
      "Contribution difference: -0.000063\n"
     ]
    }
   ],
   "source": [
    "english_prompt = \"Bob's parents live in a small village. His\"\n",
    "english_answer = \" father\"\n",
    "french_prompt = \"Les parents de Bob vivent dans un petit village. Son\"\n",
    "french_answer = ' p' # père\n",
    "\n",
    "compare_biliangual_prompt(english_prompt, english_answer, french_prompt, french_answer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results\n",
    "- We are only testing on a few manual prompts but the results seem NOT in line with the previous bigram experiments\n",
    "- Specifically, enabling the french neuron seems to boost the correct english continuation for the \"bread\" sentence but not the French continuation\n",
    "- Activating the french neuron could have an overall positive effect on logits, I switched to softmax to prevent this\n",
    "- Why does the french neuron boost \"cat\" in the english sentence\n",
    "    - This seems like the opposite of what we expected\n",
    "    - I asked chatGPT if \"if there are french words beginning with \"cat\" and apparently there aren't"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There are many French words beginning with cat: https://educalingo.com/en/dic-fr/abc/cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
