{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch import einsum\n",
    "from tqdm.auto import tqdm\n",
    "import seaborn as sns\n",
    "from transformer_lens import HookedTransformer, ActivationCache, utils\n",
    "from datasets import load_dataset\n",
    "from einops import einsum\n",
    "import pandas as pd\n",
    "from transformer_lens import utils\n",
    "from rich.table import Table, Column\n",
    "from rich import print as rprint\n",
    "from jaxtyping import Float, Int, Bool\n",
    "from torch import Tensor\n",
    "import einops\n",
    "import functools\n",
    "from transformer_lens.hook_points import HookPoint\n",
    "# import circuitsvis\n",
    "from IPython.display import HTML\n",
    "from plotly.express import line\n",
    "import plotly.express as px\n",
    "from tqdm.auto import tqdm\n",
    "import json\n",
    "import gc\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from plotly.subplots import make_subplots\n",
    "# Plotly needs a different renderer for VSCode/Notebooks vs Colab argh\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = \"notebook_connected\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.autograd.set_grad_enabled(False)\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "from haystack_utils import load_txt_data, get_mlp_activations, line\n",
    "import haystack_utils\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAYER_TO_ABLATE = 3\n",
    "NEURONS_TO_ABLATE = [609]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m-v0 into HookedTransformer\n",
      "kde4_french.txt: Loaded 1007 examples with 505 to 5345 characters each.\n",
      "kde4_english.txt: Loaded 1007 examples with 501 to 5295 characters each.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8227ed95cc4b4f7a9337c857e4288b63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edaec552b54f4ccabdadeae8d3c31cbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = HookedTransformer.from_pretrained(\"pythia-70m-v0\", fold_ln=True, device=device)\n",
    "\n",
    "kde_french = load_txt_data(\"kde4_french.txt\")\n",
    "kde_english = load_txt_data(\"kde4_english.txt\")\n",
    "\n",
    "french_activations = get_mlp_activations(kde_french, LAYER_TO_ABLATE, model, num_prompts=100, mean=True)\n",
    "english_activations = get_mlp_activations(kde_english, LAYER_TO_ABLATE, model, num_prompts=100, mean=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare whole model on bigram prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find some single words embedded as two tokens - those are very likely bigrams\n",
    "test_words = [' happily', ' beautiful', ' suddenly', ' exciting', ' surprisingly', ' wonderfully', ' generally', ' delicious', ' basically', ' carefully', ' suddenly', ' absolutely', ' interestingly', ' naturally', ' thankfully', ' quickly', ' usually', ' definitely', ' eventually', ' fortunately']\n",
    "test_words = [' absolutely', ' basically', ' carefully', ' delicious', ' eventually', ' fortunately', ' generally', ' happily', ' interestingly', ' naturally', ' surprisingly', ' thankfully', ' usually', ' wonderfully']\n",
    "test_words = [' significantly', ' occasionally', ' consistently', ' effectively', ' importantly', ' basically', ' incredibly', ' typically', ' ideally', ' genuinely', ' certainly', ' previously', ' frequently', ' gradually', ' personally', ' ultimately', ' currently', ' traditionally', ' basically', ' fortunately']\n",
    "test_words = ['temps', 'personne', 'jour', 'année', 'vie', 'chose', 'partie', 'homme', 'femme', 'enfant', 'maison', 'ville', 'amour', 'travail', 'ami', 'famille', 'étudiant', 'cœur', 'argent', 'temps', 'matin', 'soir', 'nuit', 'ami', 'frère', 'sœur', 'père', 'mère', 'fille', 'garçon']\n",
    "\n",
    "bigrams = []\n",
    "for word in test_words:\n",
    "    if word[0] != \" \":\n",
    "        word = \" \" + word\n",
    "    length = len(model.to_tokens(word)[0])-1\n",
    "    if length == 2:\n",
    "        str_tokens = model.to_str_tokens(word)\n",
    "        bigrams.append([str_tokens[1], str_tokens[2]])\n",
    "print(bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer_token_logit_difference(prompt: str, answer: str, model: HookedTransformer = model, mean_disabled_activation: Float[Tensor, \"d_mlp\"] = english_activations, mean_enabled_activation: Float[Tensor, \"d_mlp\"] = french_activations, layer_to_ablate: int = LAYER_TO_ABLATE, neurons_to_ablate: list[int] = NEURONS_TO_ABLATE, log=True, apply_softmax=True):\n",
    "    def get_answer_rank(logits: Float[Tensor, \"d_mlp\"], answer_token: int):\n",
    "        return torch.sum(logits > logits[answer_token]).item() + 1\n",
    "    \n",
    "    position = -1\n",
    "    original_logits, disabled_logits, _, _ = haystack_utils.get_caches_single_prompt(prompt, model, mean_neuron_activations=mean_disabled_activation, neurons=neurons_to_ablate, layer_to_ablate=layer_to_ablate, return_type=\"logits\")\n",
    "    _, enabled_logits, _, _ = haystack_utils.get_caches_single_prompt(prompt, model, mean_neuron_activations=mean_enabled_activation, neurons=neurons_to_ablate, layer_to_ablate=layer_to_ablate, return_type=\"logits\")\n",
    "    answer_token = model.to_single_token(answer)\n",
    "\n",
    "    if apply_softmax:\n",
    "        original_logits = original_logits.softmax(dim=-1)\n",
    "        enabled_logits = enabled_logits.softmax(dim=-1)\n",
    "        disabled_logits = disabled_logits.softmax(dim=-1)\n",
    "\n",
    "    contribution_difference = (enabled_logits[0, position, answer_token] - disabled_logits[0, position, answer_token]).item()\n",
    "\n",
    "    if log:\n",
    "        print(f\"Prediction of \\\"{prompt}\\\" + \\\"{answer}\\\"\")\n",
    "        print(f\"Original logits probs: {original_logits[0, position, answer_token]:.6f} (Rank {get_answer_rank(original_logits[0, position], answer_token)})\")\n",
    "        print(f\"Enabled context neuron probs: {enabled_logits[0, position, answer_token]:.6f} (Rank {get_answer_rank(enabled_logits[0, position], answer_token)})\")\n",
    "        print(f\"Disabled context neuron probs: {disabled_logits[0, position, answer_token]:.6f} (Rank {get_answer_rank(disabled_logits[0, position], answer_token)})\")\n",
    "        print(f\"Contribution difference: {contribution_difference:.6f}\")\n",
    "    return contribution_difference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "french_bigrams = [[\" rend\", \"re\"], [\" met\", \"tre\"], [\" demand\", \"er\"], [\" pou\", \"voir\"], [' person', 'ne'], [' hom', 'me'], [' am', 'our']]\n",
    "english_bigrams = [[' firef', 'ighter'], [' waterm', 'elon'], [' straw', 'berry'], [' fortun', 'ately'], [' thank', 'fully'], [' interesting', 'ly']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.002872340549531925\n"
     ]
    }
   ],
   "source": [
    "logit_differences = []\n",
    "for prompt, answer in french_bigrams:\n",
    "    diff = get_answer_token_logit_difference(prompt, answer, log=False)\n",
    "    logit_differences.append(diff)\n",
    "\n",
    "print(np.mean(logit_differences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.09110652764017384\n"
     ]
    }
   ],
   "source": [
    "logit_differences = []\n",
    "for prompt, answer in english_bigrams:\n",
    "    diff = get_answer_token_logit_difference(prompt, answer, log=False)\n",
    "    logit_differences.append(diff)\n",
    "\n",
    "print(np.mean(logit_differences))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results\n",
    "- We compare the logit difference of the correct bigram token between activating the French neuron and deactivating the French neuron\n",
    "- A positive logit difference means the model has a higher activation with the French neuron active\n",
    "- On French bigrams, the logit difference is positive\n",
    "    - This means the French neuron boosts French bigrams\n",
    "- On English bigrams, the logit difference is negative\n",
    "    - This means the French neuron inhibits English bigrams"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare use of context before bigram occurrence\n",
    "Motivation: see how the context neuron loss difference responds to different contexts before the bigram we care about\n",
    "- Start with random number tokens (i.e. they shouldn't matter for either language) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repeated unrelated padding tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_padded_logit_difference(bigram_prompts, random_number_str_token, num_pad):\n",
    "    logit_differences = []\n",
    "    for prompt, answer in bigram_prompts:\n",
    "        prompt = random_number_str_token * num_pad + prompt\n",
    "        assert len(model.to_tokens(prompt)[0]) == 2 + num_pad, model.to_str_tokens(prompt)\n",
    "        diff = get_answer_token_logit_difference(prompt, answer, log=False)\n",
    "        logit_differences.append(diff)\n",
    "    return logit_differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use numbers as language-agnostic prompts\n",
    "random_number_str_token = \" 42\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad02b6e03b334b929743b8ef6a4058fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>                            <div id=\"42a666a9-6936-4261-a986-3d028efc2dcf\" class=\"plotly-graph-div\" style=\"height:525px; width:800px;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"42a666a9-6936-4261-a986-3d028efc2dcf\")) {                    Plotly.newPlot(                        \"42a666a9-6936-4261-a986-3d028efc2dcf\",                        [{\"hovertemplate\":\"variable=French Differences\\u003cbr\\u003eNumber of Padded Tokens=%{x}\\u003cbr\\u003evalue=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"French Differences\",\"line\":{\"color\":\"#636efa\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"French Differences\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19],\"xaxis\":\"x\",\"y\":[0.002872340549531925,0.0018753197272027527,0.003069273227278505,0.004780567538611129,0.004880134227018321,0.004857744468998655,0.004700959190553736,0.004969813646182176,0.005743282921685022,0.006098711362418336,0.006633676763937858,0.00743390809891155,0.008389454296515655,0.008676720474146091,0.009003274092330165,0.009424292601709437,0.009833450886487429,0.010546739229799382,0.011702686711942079,0.011328106494017578],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"variable=English Differences\\u003cbr\\u003eNumber of Padded Tokens=%{x}\\u003cbr\\u003evalue=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"English Differences\",\"line\":{\"color\":\"#EF553B\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"English Differences\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19],\"xaxis\":\"x\",\"y\":[-0.09110652764017384,-0.05700081280277421,-0.055660068057477474,-0.052155754393121846,-0.04986122676442998,-0.04518873347357536,-0.03860340487638799,-0.033913516284277044,-0.03486576828678759,-0.036746257149692006,-0.03934192020096816,-0.04142400023799079,-0.04208491478736202,-0.04116532297727341,-0.04080501889499525,-0.0414919370765953,-0.042644250264856964,-0.044582024827832356,-0.04375760771411782,-0.0435518096977224],\"yaxis\":\"y\",\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Number of Padded Tokens\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"value\"}},\"legend\":{\"title\":{\"text\":\"variable\"},\"tracegroupgap\":0},\"title\":{\"text\":\"Logit Differences for French and English Bigrams with Padded Tokens\"},\"width\":800},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('42a666a9-6936-4261-a986-3d028efc2dcf');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "french_differences = []\n",
    "english_differences = []\n",
    "for num_pad in tqdm(range(20)):\n",
    "    padded_french_differences = get_padded_logit_difference(french_bigrams, random_number_str_token, num_pad)\n",
    "    padded_english_differences = get_padded_logit_difference(english_bigrams, random_number_str_token, num_pad)\n",
    "    french_differences.append(np.mean(padded_french_differences))\n",
    "    english_differences.append(np.mean(padded_english_differences))\n",
    "\n",
    "df = pd.DataFrame({\"French Differences\": french_differences, \"English Differences\": english_differences, \"Number of Padded Tokens\": range(20)})\n",
    "px.line(df, x=\"Number of Padded Tokens\", y=[\"French Differences\", \"English Differences\"], title=\"Logit Differences for French and English Bigrams with Padded Tokens\", width=800)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results\n",
    "- We are comparing the difference in logits for the correct bigram completion between model runs with the French neuron activated or deactivated\n",
    "- For French bigrams, the difference in logits gets larger with increased context length\n",
    "- For English bigrams, the difference in logits gets smaller with increased context length"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare related padding to unrelated padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "meaningful_pad = \" il\"\n",
    "meaningless_pad = \" 42\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f71b793912a9433d80aed41c8aede0ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>                            <div id=\"4b4d0608-ef4e-48bb-a999-9233fc3f9ce4\" class=\"plotly-graph-div\" style=\"height:525px; width:800px;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"4b4d0608-ef4e-48bb-a999-9233fc3f9ce4\")) {                    Plotly.newPlot(                        \"4b4d0608-ef4e-48bb-a999-9233fc3f9ce4\",                        [{\"hovertemplate\":\"variable=Meaningful pad\\u003cbr\\u003eNumber of Padded Tokens=%{x}\\u003cbr\\u003evalue=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"Meaningful pad\",\"line\":{\"color\":\"#636efa\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"Meaningful pad\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19],\"xaxis\":\"x\",\"y\":[1.1598856789725167,1.7172227587018694,1.6702642440795898,1.644899640764509,1.620840072631836,1.550445829119001,1.5072212219238281,1.4509680611746651,1.4165353775024414,1.403986930847168,1.4105463027954102,1.4290783745901925,1.4452994210379464,1.4420343126569475,1.4304922648838587,1.4234417506626673,1.4236510140555245,1.430462156023298,1.4375956399100167,1.4262418746948242],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"variable=Meaningless pad\\u003cbr\\u003eNumber of Padded Tokens=%{x}\\u003cbr\\u003evalue=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"Meaningless pad\",\"line\":{\"color\":\"#EF553B\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"Meaningless pad\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19],\"xaxis\":\"x\",\"y\":[1.1598856789725167,1.3565054280417306,1.6664141927446638,1.7990663392203194,1.8111144815172469,1.7739160060882568,1.707775252205985,1.680325950895037,1.6348573139735632,1.5778890677860804,1.5372400965009416,1.5246336119515556,1.5252151148659843,1.4939555100032262,1.4590716702597482,1.435622044972011,1.4293474129268102,1.4571547848837716,1.4599534102848597,1.4545477799006872],\"yaxis\":\"y\",\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Number of Padded Tokens\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"value\"}},\"legend\":{\"title\":{\"text\":\"variable\"},\"tracegroupgap\":0},\"title\":{\"text\":\"Logit Differences for French padding with either meaningless numbers or meaningful french words\"},\"width\":800},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('4b4d0608-ef4e-48bb-a999-9233fc3f9ce4');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "meaningful_pad_differences = []\n",
    "meaningless_pad_differences = []\n",
    "\n",
    "for num_pad in tqdm(range(20)):\n",
    "    meaningful_pad_difference = get_padded_logit_difference(french_bigrams, meaningful_pad, num_pad)\n",
    "    meaningless_pad_difference = get_padded_logit_difference(french_bigrams, meaningless_pad, num_pad)\n",
    "    meaningful_pad_differences.append(np.mean(meaningful_pad_difference))\n",
    "    meaningless_pad_differences.append(np.mean(meaningless_pad_difference))\n",
    "\n",
    "df = pd.DataFrame({\"Meaningful pad\": meaningful_pad_differences, \"Meaningless pad\": meaningless_pad_differences, \"Number of Padded Tokens\": range(20)})\n",
    "px.line(df, x=\"Number of Padded Tokens\", y=[\"Meaningful pad\", \"Meaningless pad\"], title=\"Logit Differences for French padding with either meaningless numbers or meaningful french words\", width=800)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results\n",
    "- When padding with meaningful values, the overall positive impact of the French neuron seems to be slightly smaller\n",
    "    - This could mean that other components can also identify the context as French in this scenario?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check contribution differences on complete sentence prompts in both languages\n",
    "- Prediction: \n",
    "    - Enabling the French neuron should boost the French token on both French and English sentences\n",
    "    - Enabling the French neuron should inhibit the English token on both French and English sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_biliangual_prompt(english_prompt, english_answer, french_prompt, french_answer):\n",
    "    print(\"French prompt, French answer\")\n",
    "    _ = get_answer_token_logit_difference(french_prompt, french_answer)\n",
    "    print(\"\\nEnglish prompt, French answer\")\n",
    "    _ = get_answer_token_logit_difference(english_prompt, french_answer)\n",
    "    print(\"\\nEnglish prompt, English answer\")\n",
    "    _ = get_answer_token_logit_difference(english_prompt, english_answer)\n",
    "    print(\"\\nFrench prompt, English answer\")\n",
    "    _ = get_answer_token_logit_difference(french_prompt, english_answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "French prompt, French answer\n",
      "Prediction of \"J'adore les animaux. J'aime jouer avec mon\" + \" chat\"\n",
      "Original logits probs: 0.000065 (Rank 1281)\n",
      "Enabled context neuron probs: 0.000068 (Rank 1262)\n",
      "Disabled context neuron probs: 0.000077 (Rank 1336)\n",
      "Contribution difference: -0.000010\n",
      "\n",
      "English prompt, French answer\n",
      "Prediction of \"I love animals. I enjoy playing with my\" + \" chat\"\n",
      "Original logits probs: 0.000001 (Rank 7561)\n",
      "Enabled context neuron probs: 0.000002 (Rank 6590)\n",
      "Disabled context neuron probs: 0.000001 (Rank 7568)\n",
      "Contribution difference: 0.000001\n",
      "\n",
      "English prompt, English answer\n",
      "Prediction of \"I love animals. I enjoy playing with my\" + \" cat\"\n",
      "Original logits probs: 0.001720 (Rank 54)\n",
      "Enabled context neuron probs: 0.002982 (Rank 37)\n",
      "Disabled context neuron probs: 0.001692 (Rank 54)\n",
      "Contribution difference: 0.001291\n",
      "\n",
      "French prompt, English answer\n",
      "Prediction of \"J'adore les animaux. J'aime jouer avec mon\" + \" cat\"\n",
      "Original logits probs: 0.000232 (Rank 535)\n",
      "Enabled context neuron probs: 0.000234 (Rank 540)\n",
      "Disabled context neuron probs: 0.000148 (Rank 829)\n",
      "Contribution difference: 0.000086\n"
     ]
    }
   ],
   "source": [
    "english_prompt = \"I love animals. I enjoy playing with my\"\n",
    "english_answer = \" cat\"\n",
    "french_prompt = \"J'adore les animaux. J'aime jouer avec mon\"\n",
    "french_answer = \" chat\"\n",
    "\n",
    "compare_biliangual_prompt(english_prompt, english_answer, french_prompt, french_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "French prompt, French answer\n",
      "Prediction of \"Je vais au supermarché pour acheter du\" + \" pain\"\n",
      "Original logits probs: 0.002075 (Rank 92)\n",
      "Enabled context neuron probs: 0.002107 (Rank 91)\n",
      "Disabled context neuron probs: 0.001291 (Rank 139)\n",
      "Contribution difference: 0.000816\n",
      "\n",
      "English prompt, French answer\n",
      "Prediction of \"I'm going to the supermarket to buy some\" + \" pain\"\n",
      "Original logits probs: 0.000061 (Rank 1619)\n",
      "Enabled context neuron probs: 0.000048 (Rank 1945)\n",
      "Disabled context neuron probs: 0.000061 (Rank 1616)\n",
      "Contribution difference: -0.000013\n",
      "\n",
      "English prompt, English answer\n",
      "Prediction of \"I'm going to the supermarket to buy some\" + \" bread\"\n",
      "Original logits probs: 0.007390 (Rank 12)\n",
      "Enabled context neuron probs: 0.010748 (Rank 9)\n",
      "Disabled context neuron probs: 0.007395 (Rank 12)\n",
      "Contribution difference: 0.003353\n",
      "\n",
      "French prompt, English answer\n",
      "Prediction of \"Je vais au supermarché pour acheter du\" + \" bread\"\n",
      "Original logits probs: 0.000003 (Rank 6241)\n",
      "Enabled context neuron probs: 0.000003 (Rank 5967)\n",
      "Disabled context neuron probs: 0.000003 (Rank 7916)\n",
      "Contribution difference: 0.000001\n"
     ]
    }
   ],
   "source": [
    "english_prompt = \"I'm going to the supermarket to buy some\"\n",
    "english_answer = \" bread\"\n",
    "french_prompt = \"Je vais au supermarché pour acheter du\"\n",
    "french_answer = \" pain\"\n",
    "\n",
    "compare_biliangual_prompt(english_prompt, english_answer, french_prompt, french_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "French prompt, French answer\n",
      "Prediction of \"Les parents de Bob vivent dans un petit village. Son\" + \" p\"\n",
      "Original logits probs: 0.011118 (Rank 6)\n",
      "Enabled context neuron probs: 0.012117 (Rank 5)\n",
      "Disabled context neuron probs: 0.008631 (Rank 12)\n",
      "Contribution difference: 0.003486\n",
      "\n",
      "English prompt, French answer\n",
      "Prediction of \"Bob's parents live in a small village. His\" + \" p\"\n",
      "Original logits probs: 0.000152 (Rank 412)\n",
      "Enabled context neuron probs: 0.000292 (Rank 215)\n",
      "Disabled context neuron probs: 0.000152 (Rank 412)\n",
      "Contribution difference: 0.000139\n",
      "\n",
      "English prompt, English answer\n",
      "Prediction of \"Bob's parents live in a small village. His\" + \" father\"\n",
      "Original logits probs: 0.149795 (Rank 1)\n",
      "Enabled context neuron probs: 0.176887 (Rank 2)\n",
      "Disabled context neuron probs: 0.149605 (Rank 1)\n",
      "Contribution difference: 0.027282\n",
      "\n",
      "French prompt, English answer\n",
      "Prediction of \"Les parents de Bob vivent dans un petit village. Son\" + \" father\"\n",
      "Original logits probs: 0.000526 (Rank 297)\n",
      "Enabled context neuron probs: 0.000473 (Rank 316)\n",
      "Disabled context neuron probs: 0.000536 (Rank 295)\n",
      "Contribution difference: -0.000063\n"
     ]
    }
   ],
   "source": [
    "english_prompt = \"Bob's parents live in a small village. His\"\n",
    "english_answer = \" father\"\n",
    "french_prompt = \"Les parents de Bob vivent dans un petit village. Son\"\n",
    "french_answer = ' p' # père\n",
    "\n",
    "compare_biliangual_prompt(english_prompt, english_answer, french_prompt, french_answer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results\n",
    "- We are only testing on a few manual prompts but the results seem NOT in line with the previous bigram experiments\n",
    "- Specifically, enabling the french neuron seems to boost the correct english continuation for the \"bread\" sentence but not the French continuation\n",
    "- Activating the french neuron could have an overall positive effect on logits, I switched to softmax to prevent this\n",
    "- Why does the french neuron boost \"cat\" in the english sentence\n",
    "    - This seems like the opposite of what we expected\n",
    "    - I asked chatGPT if \"if there are french words beginning with \"cat\" and apparently there aren't"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
