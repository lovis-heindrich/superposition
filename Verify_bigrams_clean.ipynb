{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from transformer_lens import HookedTransformer, ActivationCache, utils, patching\n",
    "from jaxtyping import Float, Int, Bool\n",
    "from torch import Tensor\n",
    "from tqdm.auto import tqdm\n",
    "import plotly.io as pio\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "pio.renderers.default = \"notebook_connected\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.autograd.set_grad_enabled(False)\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "from haystack_utils import get_mlp_activations\n",
    "import haystack_utils\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model EleutherAI/pythia-70m into HookedTransformer\n",
      "data/german_europarl.json: Loaded 2000 examples with 152 to 2000 characters each.\n",
      "data/english_europarl.json: Loaded 2000 examples with 165 to 2000 characters each.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e88842a24b94240a03b9e0544328f9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bed1c5ce2c5a48d08eb1fbbc968ccdcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = HookedTransformer.from_pretrained(\"EleutherAI/pythia-70m\",\n",
    "    center_unembed=True,\n",
    "    center_writing_weights=True,\n",
    "    fold_ln=True,\n",
    "    device=device)\n",
    "\n",
    "german_data = haystack_utils.load_json_data(\"data/german_europarl.json\")[:200]\n",
    "english_data = haystack_utils.load_json_data(\"data/english_europarl.json\")[:200]\n",
    "\n",
    "\n",
    "english_activations = {}\n",
    "german_activations = {}\n",
    "for layer in range(3, 4):\n",
    "    english_activations[layer] = get_mlp_activations(english_data, layer, model, mean=False)\n",
    "    german_activations[layer] = get_mlp_activations(german_data, layer, model, mean=False)\n",
    "\n",
    "LAYER_TO_ABLATE = 3\n",
    "NEURONS_TO_ABLATE = [669]\n",
    "MEAN_ACTIVATION_ACTIVE = german_activations[LAYER_TO_ABLATE][:, NEURONS_TO_ABLATE].mean()\n",
    "MEAN_ACTIVATION_INACTIVE = english_activations[LAYER_TO_ABLATE][:, NEURONS_TO_ABLATE].mean()\n",
    "\n",
    "def deactivate_neurons_hook(value, hook):\n",
    "    value[:, :, NEURONS_TO_ABLATE] = MEAN_ACTIVATION_INACTIVE\n",
    "    return value\n",
    "deactivate_neurons_fwd_hooks=[(f'blocks.{LAYER_TO_ABLATE}.mlp.hook_post', deactivate_neurons_hook)]\n",
    "\n",
    "def activate_neurons_hook(value, hook):\n",
    "    value[:, :, NEURONS_TO_ABLATE] = MEAN_ACTIVATION_ACTIVE\n",
    "    return value\n",
    "activate_neurons_fwd_hooks=[(f'blocks.{LAYER_TO_ABLATE}.mlp.hook_post', activate_neurons_hook)]\n",
    "\n",
    "all_ignore, not_ignore = haystack_utils.get_weird_tokens(model, plot_norms=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find top common German tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "893c694c7f0d43ea8d449438c8d75ecd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' der', 'en', ' die', ' und', 'ung', 'ä', ' in', ' den', ' des', ' zu', 'ch', 'n', 'st', 're', 'z', ' von', ' für', 'äsident', ' Pr', 'ischen', 't', 'ü', 'icht', 'gen', ' ist', ' auf', ' dass', 'ge', 'ig', ' im', 'in', ' über', 'g', ' das', 'te', ' er', 'men', ' w', 'es', ' an', 'ß', ' wir', ' eine', 'f', ' W', 'hen', 'w', ' Europ', ' ich', 'ungen', 'ren', 'le', ' dem', 'ten', ' ein', 'e', ' Z', ' Ver', 'der', ' B', ' mit', ' dies', 'h', ' nicht', 'ungs', 's', ' G', ' z', 'it', ' Herr', ' es', 'l', ' S', 'ich', 'lich', ' An', 'heit', 'ie', ' Er', ' zur', ' V', ' ver', 'u', 'hr', 'chaft', 'Der', ' Ich', ' Ab', ' haben', 'i', 'ant', 'chte', ' mö', 'er', ' K', 'igen', ' Ber', 'ür', ' Fra', 'em']\n"
     ]
    }
   ],
   "source": [
    "# Get top common german tokens excluding punctuation\n",
    "token_counts = torch.zeros(model.cfg.d_vocab).cuda()\n",
    "for example in tqdm(german_data):\n",
    "    tokens = model.to_tokens(example)\n",
    "    for token in tokens[0]:\n",
    "        token_counts[token.item()] += 1\n",
    "\n",
    "punctuation = [\"\\n\", \".\", \",\", \"!\", \"?\", \";\", \":\", \"-\", \"(\", \")\", \"[\", \"]\", \"{\", \"}\", \"<\", \">\", \"/\", \"\\\\\", \"\\\"\", \"'\"]\n",
    "leading_space_punctuation = [\" \" + char for char in punctuation]\n",
    "punctuation_tokens = model.to_tokens(punctuation + leading_space_punctuation + [' –', \" \", '  ', \"<|endoftext|>\"])[:, 1].flatten()\n",
    "token_counts[punctuation_tokens] = 0\n",
    "token_counts[all_ignore] = 0\n",
    "\n",
    "top_counts, top_tokens = torch.topk(token_counts, 100)\n",
    "print(model.to_str_tokens(top_tokens[:100]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of ngrams preceded by random prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_selection(tensor, n=12):\n",
    "    # Hacky replacement for np.random.choice\n",
    "    return tensor[torch.randperm(len(tensor))[:n]]\n",
    "\n",
    "def generate_random_prompts(end_string, n=50, length=12):\n",
    "    # Generate a batch of random prompts ending with a specific ngram\n",
    "    end_tokens = model.to_tokens(end_string).flatten()[1:]\n",
    "    prompts = []\n",
    "    for i in range(n):\n",
    "        prompt = get_random_selection(top_tokens[:max(50, length)], n=length).cuda()\n",
    "        prompt = torch.cat([prompt, end_tokens])\n",
    "        prompts.append(prompt)\n",
    "    prompts = torch.stack(prompts)\n",
    "    return prompts\n",
    "\n",
    "def replace_column(prompts: Int[Tensor, \"n_prompts n_tokens\"], token_index: int):\n",
    "    # Replaces a specific token position in a batch of prompts with random common German tokens\n",
    "    new_prompts = prompts.clone()\n",
    "    random_tokens = get_random_selection(top_tokens[:max(50, prompts.shape[0])], n=prompts.shape[0]).cuda()\n",
    "    new_prompts[:, token_index] = random_tokens\n",
    "    return new_prompts \n",
    "\n",
    "def loss_analysis(prompts: Tensor, title=\"\"):\n",
    "    # Loss plot for a batch of prompts\n",
    "    names = [\"Original\", \"Ablated\", \"MLP5 path patched\"]\n",
    "    original_loss, ablated_loss, _, only_activated_loss = \\\n",
    "        haystack_utils.get_direct_effect(prompts, model, pos=-1,\n",
    "                                        context_ablation_hooks=deactivate_neurons_fwd_hooks, \n",
    "                                        context_activation_hooks=activate_neurons_fwd_hooks, \n",
    "                                        )\n",
    "    haystack_utils.plot_barplot([original_loss.tolist(), ablated_loss.tolist(), only_activated_loss.tolist()], names, ylabel=\"Loss\", title=title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_analysis_random_prompts(end_string, n=50, length=12, replace_columns: list[int] | None = None):\n",
    "    # Loss plot for a batch of random prompts ending with a specific ngram and optionally replacing specific tokens\n",
    "    prompts = generate_random_prompts(end_string, n=n, length=length)\n",
    "    title=f\"Average last token loss on {length} random tokens ending in '{end_string}'\"\n",
    "    if replace_columns is not None:\n",
    "        replaced_tokens = model.to_str_tokens(prompts[0, replace_columns])\n",
    "        title += f\" replacing {replaced_tokens}\"\n",
    "        for column in replace_columns:\n",
    "            prompts = replace_column(prompts, column)\n",
    "    \n",
    "    loss_analysis(prompts, title=title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "# loss_analysis_random_prompts(\" Vorschlägen\", n=100, length=20)\n",
    "# loss_analysis_random_prompts(\" Vorschlägen\", n=100, length=20, replace_columns=[-2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77afaa0a937f42139202d41a5db81ff4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Ngram: ', options=(' Vorschlägen', ' Vorschläge', ' häufig', ' schließt', ' beweglich'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "982dfe8ca47944e8a9ab4aa19e533a39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SelectMultiple(description='Replace Columns:', index=(3,), options=('-2', '-3', '-4', 'None'), value=('None',)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74220c781ba34a48be59b0fde7ec2bc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a dropdown menu widget\n",
    "dropdown = widgets.Dropdown(\n",
    "    options = [\" Vorschlägen\", \" Vorschläge\", \" häufig\", \" schließt\", \" beweglich\"], \n",
    "    value = \" Vorschlägen\",\n",
    "    description = 'Ngram: ',\n",
    ")\n",
    "\n",
    "replace_columns_dropdown = widgets.SelectMultiple(\n",
    "    options = ['-2', '-3', '-4', 'None'],\n",
    "    value = ['None'],  # default selected value\n",
    "    description = 'Replace Columns:',\n",
    ")\n",
    "\n",
    "# Create an output widget to hold the plot\n",
    "output = widgets.Output()\n",
    "\n",
    "# Define a function to call when the widget's value changes\n",
    "def update_plot(*args):\n",
    "    # Clear the old plot from the output\n",
    "    with output:\n",
    "        clear_output(wait=True)\n",
    "\n",
    "        if 'None' in replace_columns_dropdown.value:\n",
    "            replace_columns = None\n",
    "        else:\n",
    "            # If 'None' not selected, convert the selected values to integers\n",
    "            replace_columns = [int(val) for val in replace_columns_dropdown.value]\n",
    "        \n",
    "        # Call your function with the values from the widgets\n",
    "        loss_analysis_random_prompts(dropdown.value, n=100, length=20, replace_columns=replace_columns)\n",
    "\n",
    "# Set the function to be called when the widget's value changes\n",
    "dropdown.observe(update_plot, 'value')\n",
    "replace_columns_dropdown.observe(update_plot, 'value')\n",
    "\n",
    "# Display the widget and the output\n",
    "display(dropdown, replace_columns_dropdown, output)\n",
    "\n",
    "# Run once at startup\n",
    "update_plot()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify relevant neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 24])\n"
     ]
    }
   ],
   "source": [
    "prompts = generate_random_prompts(\" Vorschlägen\", n=100, length=20)\n",
    "print(prompts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea86ac1e2abf4dfeb16e5237413d9d59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2048 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0007)\n"
     ]
    }
   ],
   "source": [
    "haystack_utils.clean_cache\n",
    "# Calculate neuron-wise loss change\n",
    "with model.hooks(deactivate_neurons_fwd_hooks):\n",
    "    _, ablated_cache = model.run_with_cache(prompts)\n",
    "\n",
    "def get_ablate_neurons_hook(neuron: int | list[int], ablated_cache):\n",
    "    def ablate_neurons_hook(value, hook):\n",
    "        value[:, :, neuron] = ablated_cache['blocks.5.mlp.hook_post'][:, :, neuron]\n",
    "        return value\n",
    "    return [('blocks.5.mlp.hook_post', ablate_neurons_hook)]\n",
    "\n",
    "diffs = torch.zeros(2048, prompts.shape[0])\n",
    "# Loss with path patched MLP5 neurons\n",
    "_, _, _, baseline_loss = haystack_utils.get_direct_effect(prompts, model, pos=-1, context_ablation_hooks=deactivate_neurons_fwd_hooks, context_activation_hooks=activate_neurons_fwd_hooks)\n",
    "for neuron in tqdm(range(2048)):\n",
    "    ablate_single_neuron_hook = get_ablate_neurons_hook(neuron, ablated_cache)\n",
    "    # Loss with path patched MLP5 neurons but a single neuron changed back to original ablated value\n",
    "    _, _, _, only_activated_loss = haystack_utils.get_direct_effect(prompts, model, pos=-1, context_ablation_hooks=deactivate_neurons_fwd_hooks, context_activation_hooks=activate_neurons_fwd_hooks+ablate_single_neuron_hook)\n",
    "    diffs[neuron] = only_activated_loss - baseline_loss\n",
    "\n",
    "print(diffs.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>                            <div id=\"631c8de1-6cb5-4a55-9e55-7f02d6e2b1b2\" class=\"plotly-graph-div\" style=\"height:525px; width:800px;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"631c8de1-6cb5-4a55-9e55-7f02d6e2b1b2\")) {                    Plotly.newPlot(                        \"631c8de1-6cb5-4a55-9e55-7f02d6e2b1b2\",                        [{\"hovertemplate\":\"variable=0\\u003cbr\\u003eindex=%{x}\\u003cbr\\u003evalue=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"0\",\"line\":{\"color\":\"#636efa\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"0\",\"showlegend\":true,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542,543,544,545,546,547,548,549,550,551,552,553,554,555,556,557,558,559,560,561,562,563,564,565,566,567,568,569,570,571,572,573,574,575,576,577,578,579,580,581,582,583,584,585,586,587,588,589,590,591,592,593,594,595,596,597,598,599,600,601,602,603,604,605,606,607,608,609,610,611,612,613,614,615,616,617,618,619,620,621,622,623,624,625,626,627,628,629,630,631,632,633,634,635,636,637,638,639,640,641,642,643,644,645,646,647,648,649,650,651,652,653,654,655,656,657,658,659,660,661,662,663,664,665,666,667,668,669,670,671,672,673,674,675,676,677,678,679,680,681,682,683,684,685,686,687,688,689,690,691,692,693,694,695,696,697,698,699,700,701,702,703,704,705,706,707,708,709,710,711,712,713,714,715,716,717,718,719,720,721,722,723,724,725,726,727,728,729,730,731,732,733,734,735,736,737,738,739,740,741,742,743,744,745,746,747,748,749,750,751,752,753,754,755,756,757,758,759,760,761,762,763,764,765,766,767,768,769,770,771,772,773,774,775,776,777,778,779,780,781,782,783,784,785,786,787,788,789,790,791,792,793,794,795,796,797,798,799,800,801,802,803,804,805,806,807,808,809,810,811,812,813,814,815,816,817,818,819,820,821,822,823,824,825,826,827,828,829,830,831,832,833,834,835,836,837,838,839,840,841,842,843,844,845,846,847,848,849,850,851,852,853,854,855,856,857,858,859,860,861,862,863,864,865,866,867,868,869,870,871,872,873,874,875,876,877,878,879,880,881,882,883,884,885,886,887,888,889,890,891,892,893,894,895,896,897,898,899,900,901,902,903,904,905,906,907,908,909,910,911,912,913,914,915,916,917,918,919,920,921,922,923,924,925,926,927,928,929,930,931,932,933,934,935,936,937,938,939,940,941,942,943,944,945,946,947,948,949,950,951,952,953,954,955,956,957,958,959,960,961,962,963,964,965,966,967,968,969,970,971,972,973,974,975,976,977,978,979,980,981,982,983,984,985,986,987,988,989,990,991,992,993,994,995,996,997,998,999,1000,1001,1002,1003,1004,1005,1006,1007,1008,1009,1010,1011,1012,1013,1014,1015,1016,1017,1018,1019,1020,1021,1022,1023,1024,1025,1026,1027,1028,1029,1030,1031,1032,1033,1034,1035,1036,1037,1038,1039,1040,1041,1042,1043,1044,1045,1046,1047,1048,1049,1050,1051,1052,1053,1054,1055,1056,1057,1058,1059,1060,1061,1062,1063,1064,1065,1066,1067,1068,1069,1070,1071,1072,1073,1074,1075,1076,1077,1078,1079,1080,1081,1082,1083,1084,1085,1086,1087,1088,1089,1090,1091,1092,1093,1094,1095,1096,1097,1098,1099,1100,1101,1102,1103,1104,1105,1106,1107,1108,1109,1110,1111,1112,1113,1114,1115,1116,1117,1118,1119,1120,1121,1122,1123,1124,1125,1126,1127,1128,1129,1130,1131,1132,1133,1134,1135,1136,1137,1138,1139,1140,1141,1142,1143,1144,1145,1146,1147,1148,1149,1150,1151,1152,1153,1154,1155,1156,1157,1158,1159,1160,1161,1162,1163,1164,1165,1166,1167,1168,1169,1170,1171,1172,1173,1174,1175,1176,1177,1178,1179,1180,1181,1182,1183,1184,1185,1186,1187,1188,1189,1190,1191,1192,1193,1194,1195,1196,1197,1198,1199,1200,1201,1202,1203,1204,1205,1206,1207,1208,1209,1210,1211,1212,1213,1214,1215,1216,1217,1218,1219,1220,1221,1222,1223,1224,1225,1226,1227,1228,1229,1230,1231,1232,1233,1234,1235,1236,1237,1238,1239,1240,1241,1242,1243,1244,1245,1246,1247,1248,1249,1250,1251,1252,1253,1254,1255,1256,1257,1258,1259,1260,1261,1262,1263,1264,1265,1266,1267,1268,1269,1270,1271,1272,1273,1274,1275,1276,1277,1278,1279,1280,1281,1282,1283,1284,1285,1286,1287,1288,1289,1290,1291,1292,1293,1294,1295,1296,1297,1298,1299,1300,1301,1302,1303,1304,1305,1306,1307,1308,1309,1310,1311,1312,1313,1314,1315,1316,1317,1318,1319,1320,1321,1322,1323,1324,1325,1326,1327,1328,1329,1330,1331,1332,1333,1334,1335,1336,1337,1338,1339,1340,1341,1342,1343,1344,1345,1346,1347,1348,1349,1350,1351,1352,1353,1354,1355,1356,1357,1358,1359,1360,1361,1362,1363,1364,1365,1366,1367,1368,1369,1370,1371,1372,1373,1374,1375,1376,1377,1378,1379,1380,1381,1382,1383,1384,1385,1386,1387,1388,1389,1390,1391,1392,1393,1394,1395,1396,1397,1398,1399,1400,1401,1402,1403,1404,1405,1406,1407,1408,1409,1410,1411,1412,1413,1414,1415,1416,1417,1418,1419,1420,1421,1422,1423,1424,1425,1426,1427,1428,1429,1430,1431,1432,1433,1434,1435,1436,1437,1438,1439,1440,1441,1442,1443,1444,1445,1446,1447,1448,1449,1450,1451,1452,1453,1454,1455,1456,1457,1458,1459,1460,1461,1462,1463,1464,1465,1466,1467,1468,1469,1470,1471,1472,1473,1474,1475,1476,1477,1478,1479,1480,1481,1482,1483,1484,1485,1486,1487,1488,1489,1490,1491,1492,1493,1494,1495,1496,1497,1498,1499,1500,1501,1502,1503,1504,1505,1506,1507,1508,1509,1510,1511,1512,1513,1514,1515,1516,1517,1518,1519,1520,1521,1522,1523,1524,1525,1526,1527,1528,1529,1530,1531,1532,1533,1534,1535,1536,1537,1538,1539,1540,1541,1542,1543,1544,1545,1546,1547,1548,1549,1550,1551,1552,1553,1554,1555,1556,1557,1558,1559,1560,1561,1562,1563,1564,1565,1566,1567,1568,1569,1570,1571,1572,1573,1574,1575,1576,1577,1578,1579,1580,1581,1582,1583,1584,1585,1586,1587,1588,1589,1590,1591,1592,1593,1594,1595,1596,1597,1598,1599,1600,1601,1602,1603,1604,1605,1606,1607,1608,1609,1610,1611,1612,1613,1614,1615,1616,1617,1618,1619,1620,1621,1622,1623,1624,1625,1626,1627,1628,1629,1630,1631,1632,1633,1634,1635,1636,1637,1638,1639,1640,1641,1642,1643,1644,1645,1646,1647,1648,1649,1650,1651,1652,1653,1654,1655,1656,1657,1658,1659,1660,1661,1662,1663,1664,1665,1666,1667,1668,1669,1670,1671,1672,1673,1674,1675,1676,1677,1678,1679,1680,1681,1682,1683,1684,1685,1686,1687,1688,1689,1690,1691,1692,1693,1694,1695,1696,1697,1698,1699,1700,1701,1702,1703,1704,1705,1706,1707,1708,1709,1710,1711,1712,1713,1714,1715,1716,1717,1718,1719,1720,1721,1722,1723,1724,1725,1726,1727,1728,1729,1730,1731,1732,1733,1734,1735,1736,1737,1738,1739,1740,1741,1742,1743,1744,1745,1746,1747,1748,1749,1750,1751,1752,1753,1754,1755,1756,1757,1758,1759,1760,1761,1762,1763,1764,1765,1766,1767,1768,1769,1770,1771,1772,1773,1774,1775,1776,1777,1778,1779,1780,1781,1782,1783,1784,1785,1786,1787,1788,1789,1790,1791,1792,1793,1794,1795,1796,1797,1798,1799,1800,1801,1802,1803,1804,1805,1806,1807,1808,1809,1810,1811,1812,1813,1814,1815,1816,1817,1818,1819,1820,1821,1822,1823,1824,1825,1826,1827,1828,1829,1830,1831,1832,1833,1834,1835,1836,1837,1838,1839,1840,1841,1842,1843,1844,1845,1846,1847,1848,1849,1850,1851,1852,1853,1854,1855,1856,1857,1858,1859,1860,1861,1862,1863,1864,1865,1866,1867,1868,1869,1870,1871,1872,1873,1874,1875,1876,1877,1878,1879,1880,1881,1882,1883,1884,1885,1886,1887,1888,1889,1890,1891,1892,1893,1894,1895,1896,1897,1898,1899,1900,1901,1902,1903,1904,1905,1906,1907,1908,1909,1910,1911,1912,1913,1914,1915,1916,1917,1918,1919,1920,1921,1922,1923,1924,1925,1926,1927,1928,1929,1930,1931,1932,1933,1934,1935,1936,1937,1938,1939,1940,1941,1942,1943,1944,1945,1946,1947,1948,1949,1950,1951,1952,1953,1954,1955,1956,1957,1958,1959,1960,1961,1962,1963,1964,1965,1966,1967,1968,1969,1970,1971,1972,1973,1974,1975,1976,1977,1978,1979,1980,1981,1982,1983,1984,1985,1986,1987,1988,1989,1990,1991,1992,1993,1994,1995,1996,1997,1998,1999,2000,2001,2002,2003,2004,2005,2006,2007,2008,2009,2010,2011,2012,2013,2014,2015,2016,2017,2018,2019,2020,2021,2022,2023,2024,2025,2026,2027,2028,2029,2030,2031,2032,2033,2034,2035,2036,2037,2038,2039,2040,2041,2042,2043,2044,2045,2046,2047],\"xaxis\":\"x\",\"y\":[-0.5908506512641907,-0.3176130950450897,-0.21662813425064087,-0.16440452635288239,-0.16333068907260895,-0.1567901074886322,-0.14458847045898438,-0.12905961275100708,-0.12081574648618698,-0.11865899711847305,-0.1149710863828659,-0.1100044995546341,-0.10267341881990433,-0.09881996363401413,-0.09545677155256271,-0.08476755023002625,-0.08334609866142273,-0.08221772313117981,-0.08182232081890106,-0.08063283562660217,-0.07961014658212662,-0.07827641814947128,-0.07642104476690292,-0.07227037847042084,-0.07178755104541779,-0.07136926800012589,-0.07031063735485077,-0.0693088173866272,-0.06773466616868973,-0.06478271633386612,-0.0646742507815361,-0.06383261829614639,-0.06136587634682655,-0.06119929254055023,-0.06087018921971321,-0.05994008481502533,-0.059821244329214096,-0.058735307306051254,-0.05850876867771149,-0.05815461650490761,-0.05810093507170677,-0.05636400356888771,-0.05591978505253792,-0.05515286326408386,-0.05366083234548569,-0.052415743470191956,-0.05239997059106827,-0.051502540707588196,-0.05082610249519348,-0.0493461936712265,-0.0477001890540123,-0.047524623572826385,-0.04749568924307823,-0.046315666288137436,-0.04456621780991554,-0.04253493249416351,-0.041350092738866806,-0.04049190133810043,-0.040270403027534485,-0.040248479694128036,-0.03949795290827751,-0.03906689211726189,-0.03847965598106384,-0.038146354258060455,-0.037805963307619095,-0.03748519346117973,-0.03730660304427147,-0.03722841292619705,-0.036849018186330795,-0.03668180853128433,-0.03663286939263344,-0.03555474802851677,-0.03525175899267197,-0.034499939531087875,-0.03435399383306503,-0.03430438041687012,-0.03338676691055298,-0.033037617802619934,-0.03294592350721359,-0.032803624868392944,-0.03249654918909073,-0.032275889068841934,-0.031461022794246674,-0.03109496645629406,-0.030867796391248703,-0.03067931719124317,-0.03014484792947769,-0.03008127398788929,-0.030048465356230736,-0.029064927250146866,-0.02904260717332363,-0.02889636904001236,-0.02874951809644699,-0.028729844838380814,-0.02866770513355732,-0.02849782444536686,-0.028451647609472275,-0.028381090611219406,-0.028332261368632317,-0.028286365792155266,-0.02816944569349289,-0.027902230620384216,-0.02774100936949253,-0.02709699235856533,-0.027067270129919052,-0.02700662612915039,-0.026861870661377907,-0.026488997042179108,-0.026075543835759163,-0.026059310883283615,-0.025938615202903748,-0.02585994452238083,-0.02553197182714939,-0.025211086496710777,-0.02499811351299286,-0.02487366273999214,-0.02445860579609871,-0.024430353194475174,-0.02405623346567154,-0.02388746291399002,-0.02378719300031662,-0.023184821009635925,-0.02304401434957981,-0.022349877282977104,-0.022290825843811035,-0.02203436940908432,-0.021821318194270134,-0.021588873118162155,-0.02155802771449089,-0.02151309885084629,-0.021135959774255753,-0.021038895472884178,-0.02100072056055069,-0.020737193524837494,-0.020378153771162033,-0.020364660769701004,-0.02027101442217827,-0.019883621484041214,-0.019666435196995735,-0.01958107203245163,-0.019417785108089447,-0.019127512350678444,-0.018718715757131577,-0.018657540902495384,-0.01858271285891533,-0.018427545204758644,-0.0180455781519413,-0.017965778708457947,-0.01758008450269699,-0.017447059974074364,-0.01739877462387085,-0.017337091267108917,-0.017298230901360512,-0.017237940803170204,-0.017193736508488655,-0.017126290127635002,-0.01680726371705532,-0.01669805310666561,-0.016639038920402527,-0.016615867614746094,-0.01660877652466297,-0.01657607965171337,-0.016475176438689232,-0.016313470900058746,-0.015798434615135193,-0.015641694888472557,-0.015515229664742947,-0.01551415678113699,-0.015489206649363041,-0.015471446327865124,-0.015330913476645947,-0.015194293111562729,-0.014936239458620548,-0.014812558889389038,-0.01462512742727995,-0.014620637521147728,-0.014473807998001575,-0.014357378706336021,-0.01431801076978445,-0.01417387742549181,-0.014165986329317093,-0.014146068133413792,-0.0138515904545784,-0.013819713145494461,-0.013558166101574898,-0.013484759256243706,-0.013468123972415924,-0.013312291353940964,-0.013192703947424889,-0.012948415242135525,-0.012899576686322689,-0.012876096181571484,-0.012758949771523476,-0.012568707577884197,-0.012553856708109379,-0.012527953833341599,-0.012436517514288425,-0.012405673041939735,-0.012293550185859203,-0.012204055674374104,-0.01218742597848177,-0.012030317448079586,-0.012020736001431942,-0.011806385591626167,-0.0117710642516613,-0.011745128780603409,-0.011662345379590988,-0.011613045819103718,-0.011503901332616806,-0.011268586851656437,-0.011223101988434792,-0.011163678020238876,-0.011026990599930286,-0.010882985778152943,-0.01053632516413927,-0.010447549633681774,-0.010441422462463379,-0.01043617632240057,-0.010138358920812607,-0.010128212161362171,-0.01007732842117548,-0.010036579333245754,-0.009990603663027287,-0.009900511242449284,-0.009831535629928112,-0.009820017032325268,-0.009708617813885212,-0.009624890983104706,-0.009560611099004745,-0.009513390250504017,-0.009433374740183353,-0.00938479695469141,-0.009366258047521114,-0.00933937355875969,-0.009282498620450497,-0.009250990115106106,-0.00921292882412672,-0.009168011136353016,-0.009116488508880138,-0.009090056642889977,-0.009053346700966358,-0.009044894948601723,-0.009032148867845535,-0.009015620686113834,-0.009001714177429676,-0.008860624395310879,-0.008825033903121948,-0.008627749979496002,-0.008480175398290157,-0.008476666174829006,-0.008384679444134235,-0.008347664959728718,-0.008342846296727657,-0.008193248882889748,-0.00811164639890194,-0.00793303269892931,-0.007924564182758331,-0.007920334115624428,-0.007837248966097832,-0.007835152558982372,-0.007756662555038929,-0.0077496604062616825,-0.007742479909211397,-0.0077057224698364735,-0.007681069429963827,-0.007665610872209072,-0.007665380369871855,-0.007664107717573643,-0.007616505026817322,-0.007611530367285013,-0.0075560747645795345,-0.007489215116947889,-0.007449746131896973,-0.007391876075416803,-0.007377827540040016,-0.007330028340220451,-0.007261523045599461,-0.007120184134691954,-0.007058460731059313,-0.006965680047869682,-0.0069467113353312016,-0.006913949269801378,-0.0068771434016525745,-0.006873678416013718,-0.006821844726800919,-0.0067956168204545975,-0.006771531887352467,-0.006751011125743389,-0.0067411451600492,-0.0067053139209747314,-0.006672987714409828,-0.006613643374294043,-0.006587623152881861,-0.00655656773597002,-0.006531383842229843,-0.006519302260130644,-0.006496146786957979,-0.006462071090936661,-0.006461791228502989,-0.006456468254327774,-0.00644741952419281,-0.006429644767194986,-0.006418019533157349,-0.0063795666210353374,-0.0063685281202197075,-0.006301038432866335,-0.00626274524256587,-0.006260348949581385,-0.006223826203495264,-0.006200027652084827,-0.0061905900947749615,-0.006087592802941799,-0.006081572733819485,-0.006067326292395592,-0.006057317368686199,-0.006051175761967897,-0.006011219695210457,-0.006000418681651354,-0.00594933470711112,-0.0059394435957074165,-0.005904478020966053,-0.0059015084989368916,-0.005877143237739801,-0.005825652275234461,-0.005814367439597845,-0.005787325091660023,-0.005757845006883144,-0.005737379193305969,-0.005707600153982639,-0.005678217392414808,-0.005675413645803928,-0.00562837952747941,-0.0055937194265425205,-0.005570952780544758,-0.005536102689802647,-0.0055187116377055645,-0.005485361907631159,-0.00545128108933568,-0.00545036094263196,-0.005429923068732023,-0.005422079470008612,-0.005407651886343956,-0.005404691677540541,-0.005401227623224258,-0.005398139823228121,-0.005369729828089476,-0.0053545706905424595,-0.005350072402507067,-0.005330990068614483,-0.005279941484332085,-0.005264258477836847,-0.005226803012192249,-0.005187945440411568,-0.005172278732061386,-0.005153847858309746,-0.005088804289698601,-0.005014913156628609,-0.004989582113921642,-0.004907876718789339,-0.004895443096756935,-0.004881925415247679,-0.0048284996300935745,-0.004791472107172012,-0.004778378643095493,-0.004728711675852537,-0.004723063670098782,-0.004657608922570944,-0.00464651919901371,-0.00464233523234725,-0.004620998166501522,-0.00460297055542469,-0.004592020530253649,-0.00458794878795743,-0.004565319512039423,-0.0045617506839334965,-0.0045058755204081535,-0.0044963182881474495,-0.00448077917098999,-0.0044165910221636295,-0.004334774799644947,-0.0043041761964559555,-0.0042931558564305305,-0.0042688120156526566,-0.004235472064465284,-0.004229536280035973,-0.004212695173919201,-0.004079041071236134,-0.004076783545315266,-0.004071331582963467,-0.004039806313812733,-0.004026174079626799,-0.003997886087745428,-0.003980160225182772,-0.003978573717176914,-0.003976911772042513,-0.0039582084864377975,-0.003936965949833393,-0.003936333116143942,-0.0039304206147789955,-0.003908676095306873,-0.0039061778225004673,-0.0038973919581621885,-0.0038953274488449097,-0.003888443810865283,-0.0038851385470479727,-0.003864831058308482,-0.0037706270813941956,-0.003748999908566475,-0.0037458690349012613,-0.003726798575371504,-0.0037185801193118095,-0.0036664435174316168,-0.0036335012409836054,-0.0036243265494704247,-0.0036170678213238716,-0.0036055154632776976,-0.0035688108764588833,-0.0035272198729217052,-0.003470857162028551,-0.0034548186231404543,-0.003440934931859374,-0.0034323660656809807,-0.0034218011423945427,-0.0034164651297032833,-0.003409999655559659,-0.0034046194050461054,-0.0034032482653856277,-0.0033960402943193913,-0.0033744806423783302,-0.0033725339453667402,-0.0033642391208559275,-0.003356538014486432,-0.003351307474076748,-0.003342950949445367,-0.0033390806056559086,-0.0033340933732688427,-0.0033040016423910856,-0.003273132722824812,-0.003257298842072487,-0.0031871171668171883,-0.003170907264575362,-0.0031608417630195618,-0.003156025428324938,-0.0031290098559111357,-0.003128071315586567,-0.0031276836525648832,-0.0031141056679189205,-0.003111798083409667,-0.0030929683707654476,-0.0030804539564996958,-0.003079173620790243,-0.003078627632930875,-0.0030568845104426146,-0.003002179553732276,-0.0029980493709445,-0.0029782273340970278,-0.002966406289488077,-0.0029618472326546907,-0.0029499102383852005,-0.0029394382145255804,-0.0029358845204114914,-0.0029316991567611694,-0.002928279573097825,-0.0029167854227125645,-0.0029124596621841192,-0.002893500728532672,-0.0028659082017838955,-0.0028553945012390614,-0.002851995639503002,-0.002832722617313266,-0.00282182265073061,-0.002807192737236619,-0.002800488146021962,-0.0027876580134034157,-0.0027634170837700367,-0.0027286845725029707,-0.0027266042307019234,-0.002706505125388503,-0.002704777754843235,-0.002698938362300396,-0.0026987006422132254,-0.0026977246161550283,-0.002696371404454112,-0.0026896733324974775,-0.0026891320012509823,-0.0026791461277753115,-0.00263806339353323,-0.002622745232656598,-0.002620189217850566,-0.002615445526316762,-0.0026101856492459774,-0.002609708346426487,-0.002593585290014744,-0.0025888485834002495,-0.0025795488618314266,-0.0025695532094687223,-0.0025646029971539974,-0.002522856928408146,-0.002520601265132427,-0.0024946234188973904,-0.00249318964779377,-0.0024837839882820845,-0.0024563875049352646,-0.0024552508257329464,-0.0024505164474248886,-0.0024374716449528933,-0.002433087909594178,-0.002415986964479089,-0.002401953563094139,-0.0023944429121911526,-0.0023876375053077936,-0.0023593674413859844,-0.0023433577734977007,-0.002342300722375512,-0.00234137917868793,-0.002339881146326661,-0.002322631888091564,-0.0022935625165700912,-0.0022931525018066168,-0.0022889950778335333,-0.0022750876378268003,-0.002261716639623046,-0.0022594956681132317,-0.0022517398465424776,-0.002246169839054346,-0.002233704086393118,-0.0022105907555669546,-0.0022030421532690525,-0.002197587862610817,-0.002196128247305751,-0.0021959359291940928,-0.0021885696332901716,-0.0021800559479743242,-0.002174814697355032,-0.002159262076020241,-0.002145608887076378,-0.0021372877527028322,-0.0021367655135691166,-0.0021330418530851603,-0.002128934022039175,-0.002124700229614973,-0.002121730474755168,-0.002119106939062476,-0.0021116482093930244,-0.002108326181769371,-0.0020991419441998005,-0.002092218492180109,-0.002088329289108515,-0.0020772151183336973,-0.002061079489067197,-0.0020577367395162582,-0.002054245676845312,-0.002045577624812722,-0.002037630183622241,-0.002031683688983321,-0.0020217730198055506,-0.002016744576394558,-0.0020105894654989243,-0.0020092781633138657,-0.0019884281791746616,-0.00198442954570055,-0.0019835736602544785,-0.0019689477048814297,-0.0019533345475792885,-0.0019376660929992795,-0.0019346242770552635,-0.0019309495110064745,-0.001926217693835497,-0.0019246322335675359,-0.001913420739583671,-0.0018986454233527184,-0.0018962313188239932,-0.0018903219606727362,-0.0018620460759848356,-0.001851942972280085,-0.0018333628540858626,-0.0018237227341160178,-0.0018208344699814916,-0.0017705821665003896,-0.001766867469996214,-0.001763885491527617,-0.0017526570009067655,-0.0017460533417761326,-0.0017411421285942197,-0.0017392628360539675,-0.0017296542646363378,-0.0017258222214877605,-0.0017191290389746428,-0.001710238168016076,-0.0016800250159576535,-0.0016649223398417234,-0.001658521592617035,-0.001615861663594842,-0.001595045323483646,-0.0015948530053719878,-0.0015915298135951161,-0.001579220755957067,-0.0015748864971101284,-0.0015738552901893854,-0.0015650741988793015,-0.0015642690705135465,-0.0015513210091739893,-0.0015483981696888804,-0.0015454472741112113,-0.0015372041380032897,-0.0015316077042371035,-0.0015281866071745753,-0.0015224888920783997,-0.0015091383829712868,-0.0015032683731988072,-0.001501510851085186,-0.0015001153806224465,-0.0014874041080474854,-0.001486470689997077,-0.0014853290049359202,-0.0014695158461108804,-0.0014688321389257908,-0.0014672065153717995,-0.0014402043307200074,-0.001439069164916873,-0.0014265403151512146,-0.0014168977504596114,-0.001407866133376956,-0.001403405680321157,-0.0013861103216186166,-0.0013765230542048812,-0.001364974770694971,-0.0013526753755286336,-0.0013505988754332066,-0.001349366270005703,-0.0013465643860399723,-0.0013449036050587893,-0.0013410650426521897,-0.0013399514136835933,-0.001338647911325097,-0.0013373622205108404,-0.0013309064088389277,-0.0013205829309299588,-0.0013164725387468934,-0.0013137107016518712,-0.001312747597694397,-0.0012943774927407503,-0.001281056203879416,-0.0012726378627121449,-0.0012706943089142442,-0.001268247957341373,-0.0012559687020257115,-0.0012542068725451827,-0.0012468427885323763,-0.0012426460161805153,-0.0012371520278975368,-0.0012236193288117647,-0.0012226119870319963,-0.0012206831015646458,-0.0012170764384791255,-0.001212551025673747,-0.0012124532368034124,-0.0012089788215234876,-0.001203838037326932,-0.0012025097385048866,-0.0011879598023369908,-0.0011757435277104378,-0.0011644689366221428,-0.0011621045414358377,-0.0011556923855096102,-0.0011543238069862127,-0.0011543005239218473,-0.0011496127117425203,-0.0011447422439232469,-0.0011233808472752571,-0.0011220021406188607,-0.0011165121104568243,-0.0011068352032452822,-0.001095674466341734,-0.0010934628080576658,-0.0010858576279133558,-0.0010784356854856014,-0.0010594089981168509,-0.0010468080872669816,-0.0010380237363278866,-0.001036653178744018,-0.0010337982093915343,-0.0010296227410435677,-0.0010238798568025231,-0.0010213684290647507,-0.001016572117805481,-0.0009980868780985475,-0.0009877578122541308,-0.0009876451222226024,-0.0009876438416540623,-0.0009872325463220477,-0.0009867482585832477,-0.0009717540233395994,-0.0009572939015924931,-0.0009500777814537287,-0.0009469412034377456,-0.0009457599371671677,-0.0009309035376645625,-0.0009298262884840369,-0.0009262785315513611,-0.0009230494615621865,-0.0009227711125276983,-0.0009225572575815022,-0.0009197613690048456,-0.0009135811706073582,-0.0009127853554673493,-0.000911981740500778,-0.0009095519781112671,-0.0009036091505549848,-0.0008955870289355516,-0.0008906216826289892,-0.0008844509720802307,-0.0008826415869407356,-0.0008710048859938979,-0.0008579224813729525,-0.0008573089726269245,-0.0008568407502025366,-0.0008509616018272936,-0.0008493517525494099,-0.0008436951320618391,-0.000843005022034049,-0.0008395157055929303,-0.000838319887407124,-0.0008374095195904374,-0.0008334700250998139,-0.0008276526350528002,-0.0008113741641864181,-0.0008051714394241571,-0.0008009860175661743,-0.0007950247963890433,-0.000792358536273241,-0.0007896554307080805,-0.0007806281791999936,-0.0007682395516894758,-0.0007657325477339327,-0.0007637040107510984,-0.0007621729164384305,-0.0007568771252408624,-0.0007565103005617857,-0.0007543308311142027,-0.0007517421036027372,-0.0007514091557823122,-0.0007504337118007243,-0.000746962265111506,-0.0007419803878292441,-0.0007403442868962884,-0.0007272469811141491,-0.0007260488928295672,-0.0007235088851302862,-0.0007147344294935465,-0.0007099038339219987,-0.0006983394850976765,-0.0006959114107303321,-0.00069284881465137,-0.0006908340146765113,-0.0006876292754895985,-0.0006821245769970119,-0.0006811453495174646,-0.0006805432494729757,-0.0006717222277075052,-0.0006689080037176609,-0.0006578234606422484,-0.0006543456111103296,-0.0006526985671371222,-0.0006491143722087145,-0.0006434990209527314,-0.0006404542364180088,-0.0006294330814853311,-0.000622306193690747,-0.0006220011273398995,-0.0006174884038046002,-0.0006156951421871781,-0.0006155198207125068,-0.0006113081471994519,-0.0005848375149071217,-0.0005809353897348046,-0.0005766890244558454,-0.0005654941778630018,-0.0005654061096720397,-0.0005608964711427689,-0.0005424395203590393,-0.0005343359080143273,-0.0005331242573447526,-0.0005312243010848761,-0.0005301414639689028,-0.0005245517240837216,-0.0005242393235675991,-0.0005241084727458656,-0.0005215503042563796,-0.0005166070768609643,-0.0005144025199115276,-0.0005111637874506414,-0.0005072557250969112,-0.0005008158041164279,-0.0004960731603205204,-0.0004954481264576316,-0.0004931285511702299,-0.0004930566065013409,-0.0004878458275925368,-0.00048276653978973627,-0.00048052496276795864,-0.0004788746009580791,-0.00047458961489610374,-0.00046170211862772703,-0.0004511126899160445,-0.00045038669486530125,-0.00044201366836205125,-0.0004406149673741311,-0.0004323961620684713,-0.0004278366977814585,-0.00042197303264401853,-0.0004176054790150374,-0.0004115068295504898,-0.0004095339681953192,-0.00040715583600103855,-0.0003971093101426959,-0.00039232493145391345,-0.0003907509963028133,-0.00038829774712212384,-0.00038353397394530475,-0.00038116349605843425,-0.00038034937460906804,-0.0003801925340667367,-0.0003800502454396337,-0.0003794998046942055,-0.0003791704075410962,-0.00037833809619769454,-0.00037722871638834476,-0.000376187264919281,-0.000375765492208302,-0.0003725138958543539,-0.0003652409359347075,-0.00036467969766817987,-0.0003626856196206063,-0.00036031962372362614,-0.0003560677287168801,-0.00035556487273424864,-0.0003503817424643785,-0.00034863001201301813,-0.0003418466367293149,-0.0003385409072507173,-0.0003367131866980344,-0.00033613404957577586,-0.0003339369723107666,-0.0003306790313217789,-0.00032635487150400877,-0.00032568551250733435,-0.0003241006925236434,-0.00032328293309547007,-0.0003232246672268957,-0.00031933977152220905,-0.00031693599885329604,-0.00031083880458027124,-0.0003079098532907665,-0.0003074444830417633,-0.0003065230557695031,-0.0003059586160816252,-0.0003025371697731316,-0.0003001043223775923,-0.000298630737233907,-0.0002979232231155038,-0.0002968408225569874,-0.0002919952676165849,-0.00028775178361684084,-0.00028145991382189095,-0.0002811343874782324,-0.00027841239352710545,-0.00027823506388813257,-0.0002776319452095777,-0.00027616001898422837,-0.0002664826170075685,-0.00026287988293915987,-0.0002605455229058862,-0.0002583684108685702,-0.0002551464131101966,-0.0002521395799703896,-0.0002457959926687181,-0.00024340502568520606,-0.00023907035938464105,-0.0002365791006013751,-0.0002344380336580798,-0.00023230366059578955,-0.00022095143503975123,-0.00022024690406396985,-0.00021490738436114043,-0.00020762503845617175,-0.00020647428755182773,-0.00020521588157862425,-0.00020432710880413651,-0.00019939683261327446,-0.00019460648763924837,-0.00019421301840338856,-0.00019396380230318755,-0.00019234880164731294,-0.00019208535377401859,-0.00019106925174128264,-0.00018885165627580136,-0.0001817738957470283,-0.00017779492191039026,-0.00017710648535285145,-0.00017548240430187434,-0.00017506786389276385,-0.0001734647958073765,-0.0001711549557512626,-0.00016568116552662104,-0.00016039445472415537,-0.00015726193669252098,-0.00015336528304032981,-0.00015217751206364483,-0.00015186630480457097,-0.0001500148355262354,-0.00014960482076276094,-0.00014789841952733696,-0.00014654405822511762,-0.00014614127576351166,-0.0001445180969312787,-0.0001421141641912982,-0.00014162294974084944,-0.00014159649435896426,-0.00014041915710549802,-0.00013510607823263854,-0.00013112678425386548,-0.00013025150110479444,-0.00012764267739839852,-0.0001261941361008212,-0.00012507640349213034,-0.00012027554475935176,-0.00011618316057138145,-0.00011364177044015378,-0.00011184707545908168,-0.00011176057159900665,-0.00011141225695610046,-0.00011031582835130394,-0.00011009007721440867,-0.00010947026021312922,-0.00010930791177088395,-0.00010544098768150434,-0.00010303400631528348,-0.00010164953710045666,-9.701557428343222e-05,-9.496197162661701e-05,-9.338281961390749e-05,-9.294010669691488e-05,-9.203582885675132e-05,-9.064957703230903e-05,-8.941352280089632e-05,-8.908912423066795e-05,-8.690126560395584e-05,-8.430421439697966e-05,-8.196629642043263e-05,-7.680997077841312e-05,-7.529370486736298e-05,-7.40627947379835e-05,-6.794177170377225e-05,-6.715059134876356e-05,-6.662145460722968e-05,-6.46736443741247e-05,-6.433702947106212e-05,-6.0665308410534635e-05,-5.990967110847123e-05,-5.920976400375366e-05,-5.612284076050855e-05,-5.284182770992629e-05,-5.161769513506442e-05,-5.1535444072214887e-05,-4.6904460759833455e-05,-4.625879228115082e-05,-4.597410588758066e-05,-4.380278187454678e-05,-4.2418017983436584e-05,-4.2214171116938815e-05,-4.114702460356057e-05,-3.991141784354113e-05,-3.655217733466998e-05,-3.369003388797864e-05,-3.262579411966726e-05,-3.194376768078655e-05,-3.1800420401850715e-05,-2.893522469094023e-05,-2.843938818841707e-05,-2.5423392798984423e-05,-2.4476721591781825e-05,-2.398051401542034e-05,-2.2351368897943757e-05,-2.138361378456466e-05,-2.065338230750058e-05,-1.922570118040312e-05,-1.791156864783261e-05,-1.642309143790044e-05,-1.616492954781279e-05,-1.5637204342056066e-05,-1.56163423525868e-05,-1.5241801520460285e-05,-1.4110207303019706e-05,-1.3992115782457404e-05,-1.2831241292587947e-05,-1.2315288586250972e-05,-1.1448115401435643e-05,-1.0765194929263089e-05,-1.0688081601983868e-05,-1.0667443348211236e-05,-9.41015787248034e-06,-8.310452358273324e-06,-5.728230007662205e-06,-5.333647095540073e-06,-3.96646555600455e-06,-3.852322606689995e-06,-3.2906980322877644e-06,-2.889856659749057e-06,-2.667903800102067e-06,-1.4959275631554192e-06,-1.4277547961683013e-06,-6.143003474790021e-07,0.0,1.357346718577901e-06,2.274364305776544e-06,3.3158808037114795e-06,3.4893303109129192e-06,3.7262589103193022e-06,3.917962203559e-06,4.988610726286424e-06,5.774870714958524e-06,7.735863619018346e-06,9.040087206813041e-06,9.709820005809888e-06,9.902864803734701e-06,1.114867609430803e-05,1.1180713954672683e-05,1.164078730653273e-05,1.1644438018265646e-05,1.1980608178419061e-05,1.2378320207062643e-05,1.2388303730404004e-05,1.4015063243277837e-05,1.4476627256954089e-05,1.6030966435209848e-05,1.658380097069312e-05,1.661971145949792e-05,1.6881824194570072e-05,1.853696994658094e-05,2.084486186504364e-05,2.1680294594261795e-05,2.306498572579585e-05,2.640776256157551e-05,2.9534399800468236e-05,2.9580742193502374e-05,3.2719821319915354e-05,3.454350007814355e-05,3.5016088077099994e-05,3.732286495505832e-05,3.742978151421994e-05,3.785736771533266e-05,3.9609818486496806e-05,4.410967085277662e-05,4.502363663050346e-05,4.641689520212822e-05,4.651926428778097e-05,4.699394048657268e-05,4.7556906793033704e-05,4.814751446247101e-05,4.839226676267572e-05,5.4069609177531675e-05,5.633331966237165e-05,5.804002285003662e-05,5.972124563413672e-05,5.9785099438158795e-05,6.381675484590232e-05,6.554029823746532e-05,6.930530071258545e-05,6.960861355764791e-05,7.191650365712121e-05,7.267355977091938e-05,7.274970266735181e-05,7.450401608366519e-05,7.513984746765345e-05,7.569022272946313e-05,7.682569412281737e-05,7.880479097366333e-05,8.126087777782232e-05,8.292510756291449e-05,8.432925096713006e-05,8.615828846814111e-05,8.660100138513371e-05,8.664831693749875e-05,8.756235183682293e-05,8.822500967653468e-05,9.132236300501972e-05,9.200834028888494e-05,9.263701940653846e-05,9.306818537879735e-05,9.340100223198533e-05,0.00010172859037993476,0.00010222695709671825,0.00010246790770906955,0.00010368593211751431,0.00010404139902675524,0.00010701447899919003,0.00010785534686874598,0.00010999471123795956,0.0001153398334281519,0.0001197757592308335,0.00012000105925835669,0.00012107469228794798,0.00012602575588971376,0.00012614198203664273,0.00012755468196701258,0.0001297622948186472,0.00013454444706439972,0.00013538234634324908,0.00013753160601481795,0.0001385043578920886,0.00013937220501247793,0.00013958505587652326,0.00014148645277600735,0.0001423360372427851,0.00014294877473730594,0.00014606796321459115,0.0001461090869270265,0.00014678433944936842,0.0001479434286011383,0.0001492758165113628,0.0001508235145593062,0.0001509764842921868,0.00015736691420897841,0.0001601736294105649,0.00016857206355780363,0.00017493129416834563,0.0001763983746059239,0.00018244922102894634,0.00018328525766264647,0.00019018053717445582,0.0001946256379596889,0.0001949123980011791,0.00019519210036378354,0.00019747234182432294,0.0002000550157390535,0.00020011536253150553,0.000201931077754125,0.0002051324408967048,0.0002054180222330615,0.00021413312060758471,0.00021428547916002572,0.00021831750927958637,0.00022380366863217205,0.0002266819792566821,0.00022911294945515692,0.000231069847359322,0.00023426100960932672,0.00023883103858679533,0.00024041325377766043,0.00024203836801461875,0.00024719253997318447,0.0002500572882127017,0.0002549753407947719,0.0002583403838798404,0.000261629291344434,0.00027096859412267804,0.000277035083854571,0.0002772067382466048,0.00028283268329687417,0.00028747148462571204,0.0002885182329919189,0.0002953095827251673,0.0002987544285133481,0.0003026752092409879,0.00030682579381391406,0.0003139518084935844,0.00031491703703068197,0.0003168572438880801,0.0003169194678775966,0.000326355395372957,0.0003265458217356354,0.00032676331466063857,0.00032861606450751424,0.00033379413071088493,0.00033438258105888963,0.0003354227519594133,0.0003359448164701462,0.00033671632991172373,0.0003371216298546642,0.00033716141479089856,0.0003462257154751569,0.00034972064895555377,0.0003504875930957496,0.0003549467073753476,0.0003553943242877722,0.0003576760063879192,0.00035994790960103273,0.00036031202762387693,0.0003610698040574789,0.00036308905691839755,0.00036337986239232123,0.0003637179615907371,0.0003652606101240963,0.0003665840486064553,0.00036963963066227734,0.00037312894710339606,0.000373530900105834,0.0003833197697531432,0.00038477659109048545,0.00038792946725152433,0.0003884828183799982,0.0003898715949617326,0.0003906208148691803,0.00039164931513369083,0.0003920353192370385,0.0003951609251089394,0.00040842138696461916,0.0004140987293794751,0.00041697503183968365,0.0004192316555418074,0.0004251391510479152,0.0004300414875615388,0.00043012015521526337,0.0004426462110131979,0.0004461161734070629,0.00044772468390874565,0.000448944338131696,0.0004495934408623725,0.0004578383232001215,0.00045899764518253505,0.0004668254405260086,0.00046945438953116536,0.0004751436354126781,0.0004821403417736292,0.0004829005047213286,0.00048380388761870563,0.000489800120703876,0.0004905491950921714,0.0004911127616651356,0.0004936532932333648,0.0005121944122947752,0.0005191494710743427,0.0005223083426244557,0.0005345353274606168,0.0005416799103841186,0.000543837493751198,0.0005451198085211217,0.0005470924079418182,0.0005625419435091317,0.0005722654750570655,0.0005737043102271855,0.0005749118863604963,0.0005799080245196819,0.0005815618205815554,0.0005892267799936235,0.0006054628756828606,0.0006139278993941844,0.00061434181407094,0.0006183285149745643,0.0006199798081070185,0.0006203741650097072,0.0006353099597617984,0.0006432882510125637,0.0006434444221667945,0.0006582295172847807,0.0006589490221813321,0.0006615373422391713,0.0006621557986363769,0.0006638323538936675,0.0006645385874435306,0.0006855289684608579,0.0006902877357788384,0.0006948186201043427,0.0006994217401370406,0.000704537087585777,0.0007049187552183867,0.0007260563434101641,0.0007328742067329586,0.0007344140321947634,0.000740072107873857,0.0007421698537655175,0.0007565818959847093,0.0007568866130895913,0.0007601314573548734,0.0007683828589506447,0.0007721541915088892,0.00078534334897995,0.0007889930857345462,0.0007958124624565244,0.0008043959969654679,0.0008061351836659014,0.0008106004679575562,0.0008203613688237965,0.000829244265332818,0.0008372284355573356,0.000837573257740587,0.0008445807034149766,0.000857082661241293,0.0008693136041983962,0.0008697226876392961,0.0008871658355928957,0.0008892425685189664,0.0008921437547542155,0.0008955097873695195,0.0009244272951036692,0.0009338504169136286,0.0009381590643897653,0.000946611340623349,0.0009563902858644724,0.0009606255334801972,0.0009614331065677106,0.0009723493712954223,0.0009779655374586582,0.0009794054785743356,0.0009826814057305455,0.0009833513759076595,0.0009879156714305282,0.0009971163235604763,0.0010247215395793319,0.0010290032951161265,0.0010291454382240772,0.0010346597991883755,0.0010465063387528062,0.0010514239547774196,0.0010628331219777465,0.0010656585218384862,0.0010705881286412477,0.0010729609057307243,0.0010926435934379697,0.0011050275061279535,0.0011132006766274571,0.0011139980051666498,0.0011162011651322246,0.0011201866436749697,0.0011246700305491686,0.0011251955293118954,0.0011294514406472445,0.0011364856036379933,0.0011387803824618459,0.0011403149692341685,0.001151320873759687,0.0011655684793367982,0.001169180846773088,0.00117065804079175,0.001178637845441699,0.0011811560252681375,0.001182836713269353,0.001183103071525693,0.0011968498583883047,0.001201465493068099,0.0012075054692104459,0.001212664763443172,0.0012175235897302628,0.0012270451989024878,0.0012293594190850854,0.0012389039620757103,0.001240187557414174,0.0012515794951468706,0.0012617718894034624,0.0012639283668249846,0.0012707109563052654,0.001272564404644072,0.0012728488072752953,0.0012791645713150501,0.0012815042864531279,0.0012927077477797866,0.001312946667894721,0.0013155201449990273,0.001328921876847744,0.0013290590140968561,0.0013302245642989874,0.0013414148706942797,0.0013646635925397277,0.001378301647491753,0.0013841396430507302,0.0013850736431777477,0.0013899155892431736,0.0013905352680012584,0.0014228768413886428,0.0014298059977591038,0.0014373284066095948,0.0014569781487807631,0.0014675314305350184,0.0014728521928191185,0.001474048476666212,0.001474081538617611,0.001481499057263136,0.0014941260451450944,0.0014964358415454626,0.0014967444585636258,0.0015008890768513083,0.0015167552046477795,0.0015172804705798626,0.001523988088592887,0.0015246288385242224,0.0015306546119973063,0.0015374907525256276,0.0015465038595721126,0.0015537221916019917,0.0015753692714497447,0.0015821829438209534,0.0015880158171057701,0.0015925465850159526,0.001616338617168367,0.0016353923128917813,0.001639669411815703,0.0016580611700192094,0.0016642591217532754,0.0016657719388604164,0.0016693389043211937,0.0016729425406083465,0.0016815625131130219,0.0016840300522744656,0.001685908529907465,0.0017145293531939387,0.0017528773751109838,0.0017708834493532777,0.0017745275981724262,0.0017800801433622837,0.0017832990270107985,0.001786037697456777,0.0017935585929080844,0.0017965195002034307,0.0017969642067328095,0.001813698559999466,0.0018205012893304229,0.0018220085185021162,0.0018235737225040793,0.0018283738754689693,0.0018306825077161193,0.0018327635480090976,0.0018336846260353923,0.001833842834457755,0.0018394759390503168,0.0018406967865303159,0.0018407633760944009,0.0018411859637126327,0.001843737903982401,0.0018481964943930507,0.001855623908340931,0.0018776414217427373,0.0018861457938328385,0.001887572230771184,0.001899059396237135,0.0019024790963158011,0.001913301064632833,0.0019523693481460214,0.0019594889599829912,0.0019599497318267822,0.0019617860671132803,0.0019718315452337265,0.001971856225281954,0.0019769915379583836,0.0019789787475019693,0.0019878828898072243,0.001990176737308502,0.002021131571382284,0.0020337356254458427,0.00203489838168025,0.002035438548773527,0.002044277498498559,0.002050529234111309,0.0020665971096605062,0.0020961554255336523,0.002110067056491971,0.0021107655484229326,0.0021180626936256886,0.0021185078658163548,0.0021304674446582794,0.002144523663446307,0.0021545093040913343,0.002156719332560897,0.0021594101563096046,0.0021698998752981424,0.002173148561269045,0.0021829165052622557,0.0021915435791015625,0.0021931964438408613,0.002195704495534301,0.002198964124545455,0.0022212823387235403,0.0022281373385339975,0.002231774851679802,0.002246648306027055,0.0022478920873254538,0.002272418700158596,0.0022760469000786543,0.002281807130202651,0.0022852234542369843,0.002286734292283654,0.0022881641052663326,0.0022963692899793386,0.0023011656012386084,0.002303383080288768,0.0023104774300009012,0.002328231930732727,0.002336476230993867,0.0023399367928504944,0.002343729604035616,0.0023536337539553642,0.0023623707238584757,0.002363216131925583,0.0023744828067719936,0.0023869299329817295,0.0024003388825803995,0.002411272143945098,0.002412055153399706,0.002413487294688821,0.002416551811620593,0.0024196142330765724,0.002449748106300831,0.0024512100499123335,0.002455578651279211,0.002468253718689084,0.0024691412691026926,0.0024723978713154793,0.0024807238951325417,0.0024875765666365623,0.002492827596142888,0.002511947648599744,0.0025173381436616182,0.002533185528591275,0.002538422355428338,0.002539076143875718,0.0025482121855020523,0.002556025981903076,0.0025588669814169407,0.002568645402789116,0.0025711539201438427,0.0025749187916517258,0.002590073738247156,0.002616911893710494,0.002669288543984294,0.0026736995205283165,0.0026760816108435392,0.0026887536514550447,0.002722111064940691,0.0027407268062233925,0.0027410450857132673,0.0027412206400185823,0.0027505618054419756,0.0027547182980924845,0.0027806588914245367,0.0027876105159521103,0.002833656733855605,0.0028529285918921232,0.0028601600788533688,0.002892331685870886,0.0029000192880630493,0.0029305582866072655,0.002941293641924858,0.0029493379406630993,0.0029576034285128117,0.002987237647175789,0.003013388253748417,0.003019877942278981,0.003032670821994543,0.0030545280314981937,0.003074832959100604,0.003133850172162056,0.00314957182854414,0.003153007011860609,0.0031597355846315622,0.003167699556797743,0.0032064777333289385,0.0032366793602705,0.003257930977270007,0.0032761648762971163,0.0032790242694318295,0.003282822435721755,0.0033154182601720095,0.0033176455181092024,0.0033379853703081608,0.003354395041242242,0.003362270537763834,0.003363874740898609,0.003407303709536791,0.003416151972487569,0.0034250502940267324,0.0034278484527021646,0.0034358000848442316,0.0034466320648789406,0.0034557217732071877,0.0034907713998109102,0.0034976161550730467,0.0035190752241760492,0.0035211776848882437,0.0035347205121070147,0.003560095326974988,0.0035959624219685793,0.0036185679491609335,0.003648401005193591,0.003652027575299144,0.003678605891764164,0.003695645835250616,0.0036972942762076855,0.0036988735664635897,0.003713515354320407,0.0037222253158688545,0.003728277748450637,0.0037377646658569574,0.003792604897171259,0.003799459431320429,0.003852939698845148,0.003867590334266424,0.0038823282811790705,0.00391138531267643,0.0039694602601230145,0.004000274930149317,0.004017986822873354,0.0040184990502893925,0.0040685152634978294,0.004095087293535471,0.004111700225621462,0.004183579236268997,0.004188972525298595,0.004227155353873968,0.004277541767805815,0.004360485821962357,0.004379418678581715,0.004466320853680372,0.004467673134058714,0.004478500224649906,0.004501001909375191,0.004513703752309084,0.004544468130916357,0.0045564789324998856,0.004586805123835802,0.004601543769240379,0.00460659759119153,0.004610052797943354,0.004630001727491617,0.004632547497749329,0.004705965518951416,0.004710481967777014,0.004716680850833654,0.004738297779113054,0.0047782291658222675,0.004806214943528175,0.004807828459888697,0.004831282887607813,0.004840816371142864,0.004851349163800478,0.0049080150201916695,0.004925585351884365,0.004946505650877953,0.004984171595424414,0.005001761484891176,0.0050227586179971695,0.005045351572334766,0.005094835534691811,0.005122297443449497,0.0051712095737457275,0.00517720403149724,0.005189275369048119,0.00519603444263339,0.005205722525715828,0.0052695078775286674,0.0052733574993908405,0.00533241406083107,0.005362203810364008,0.005381447263062,0.005411931313574314,0.005438994616270065,0.005443505011498928,0.0055119614116847515,0.005513196811079979,0.00552785350009799,0.005535261705517769,0.005538478959351778,0.005550895351916552,0.005555006675422192,0.005587129388004541,0.005602917168289423,0.0056223501451313496,0.005635906010866165,0.005700274836272001,0.005725985858589411,0.005745492409914732,0.0057544042356312275,0.005757548380643129,0.005869308486580849,0.005939452443271875,0.005967335309833288,0.005969650577753782,0.005972709506750107,0.006010253913700581,0.006049188785254955,0.0060903169214725494,0.0061104088090360165,0.0061480277217924595,0.006197757553309202,0.006213597021996975,0.0062216417863965034,0.0062548075802624226,0.006269747391343117,0.006278566084802151,0.006319544743746519,0.0063592554070055485,0.006413821596652269,0.006417240481823683,0.006426338106393814,0.006467659492045641,0.00647064158692956,0.006568365264683962,0.006569232326000929,0.006579199340194464,0.00666849734261632,0.006690378300845623,0.006748816929757595,0.006778012495487928,0.006782184354960918,0.006807235069572926,0.006813336163759232,0.006817839574068785,0.006832588464021683,0.006832906510680914,0.006833180785179138,0.006917997729033232,0.006929235532879829,0.006939230486750603,0.00695120170712471,0.006978842429816723,0.006987025961279869,0.0070019871927797794,0.007039362099021673,0.007100293412804604,0.007142431102693081,0.0071617234498262405,0.0071937995962798595,0.007265855558216572,0.007303408347070217,0.00730420695617795,0.007318771444261074,0.007322898134589195,0.007323680445551872,0.007389429956674576,0.007585042621940374,0.0075860293582081795,0.007596559356898069,0.007654767483472824,0.007656022906303406,0.007674872409552336,0.007686701603233814,0.007724346127361059,0.007788411807268858,0.00781519990414381,0.007838072255253792,0.007936766371130943,0.007990368641912937,0.008032005280256271,0.008077193051576614,0.008101475425064564,0.008114972151815891,0.008128028362989426,0.008262545801699162,0.008277743123471737,0.008306937292218208,0.008330625481903553,0.00833128672093153,0.008579774759709835,0.00860764179378748,0.008644020184874535,0.008676066994667053,0.008756086230278015,0.00887351669371128,0.008917286060750484,0.008927052840590477,0.008977978490293026,0.009016401134431362,0.00902210921049118,0.009023267775774002,0.009137850254774094,0.009140508249402046,0.009250668808817863,0.009282086044549942,0.009304378181695938,0.009330663830041885,0.009369582869112492,0.009386959485709667,0.009435053914785385,0.009466019459068775,0.009515499696135521,0.009598677977919579,0.00962764397263527,0.009634759277105331,0.0096363490447402,0.00976470485329628,0.009778792038559914,0.009782182052731514,0.00993671640753746,0.009968744590878487,0.00997209269553423,0.01026980858296156,0.010295486077666283,0.010366281494498253,0.010405424982309341,0.010411262512207031,0.010449839755892754,0.010532192885875702,0.01061930414289236,0.010634583421051502,0.01066531427204609,0.010781672783195972,0.010871866717934608,0.010893324390053749,0.010948892682790756,0.010977254249155521,0.01103752851486206,0.011053323745727539,0.011194064281880856,0.011198217049241066,0.011213082820177078,0.011223454028367996,0.011290457099676132,0.011331658810377121,0.011356127448379993,0.011358858086168766,0.011419227346777916,0.011456498876214027,0.011494488455355167,0.011665534228086472,0.01170069444924593,0.011740627698600292,0.012070639058947563,0.012115887366235256,0.012125780805945396,0.01238205935806036,0.012414229102432728,0.012421119958162308,0.012479317374527454,0.012489440850913525,0.012548139318823814,0.012694379314780235,0.012747070752084255,0.012778349220752716,0.012889056466519833,0.01300123706459999,0.013002499006688595,0.013081026263535023,0.01309156697243452,0.013138816691935062,0.013157009147107601,0.013159201480448246,0.013240034691989422,0.013391494750976562,0.01362990215420723,0.013636564835906029,0.013657464645802975,0.013722013682126999,0.01374023873358965,0.013986286707222462,0.01403276901692152,0.014037528075277805,0.014243343845009804,0.014286455698311329,0.014334878884255886,0.01438827533274889,0.014490008354187012,0.014693737961351871,0.01483468059450388,0.014893502928316593,0.014921355061233044,0.015221062116324902,0.015256824903190136,0.015743477270007133,0.01574859395623207,0.016414087265729904,0.016644112765789032,0.016822343692183495,0.016902849078178406,0.017053969204425812,0.017243728041648865,0.017341705039143562,0.01743817888200283,0.017622463405132294,0.01767432875931263,0.01767810992896557,0.017777657136321068,0.017893169075250626,0.01790662482380867,0.017908325418829918,0.0179580245167017,0.017961964011192322,0.018113398924469948,0.018260454759001732,0.01832389272749424,0.018388809636235237,0.01851528324186802,0.018702756613492966,0.018713945522904396,0.01872270368039608,0.018807342275977135,0.01888761855661869,0.018998289480805397,0.01917373575270176,0.01921878382563591,0.019429238513112068,0.019744599238038063,0.01974680833518505,0.019803138449788094,0.01995782181620598,0.020121216773986816,0.020330818369984627,0.020592330023646355,0.02059841901063919,0.02111159637570381,0.021210959181189537,0.02143671363592148,0.02144458331167698,0.021458875387907028,0.021668870002031326,0.022027600556612015,0.022252798080444336,0.02228703536093235,0.02232905849814415,0.02249225601553917,0.022748608142137527,0.022914284840226173,0.022983329370617867,0.02310791239142418,0.02311483770608902,0.023341141641139984,0.023610897362232208,0.023833252489566803,0.02418113499879837,0.02445770986378193,0.0245061032474041,0.0246063694357872,0.025792812928557396,0.02591521292924881,0.025955013930797577,0.026018518954515457,0.02612098678946495,0.026879597455263138,0.026930920779705048,0.02699187584221363,0.027072662487626076,0.027150418609380722,0.027562938630580902,0.027865126729011536,0.027921831235289574,0.02795586548745632,0.02799483761191368,0.028247112408280373,0.028342807665467262,0.02839680388569832,0.02960893139243126,0.030660200864076614,0.03067411482334137,0.03079233132302761,0.031044907867908478,0.031430188566446304,0.03172405809164047,0.031939003616571426,0.03195855766534805,0.032323259860277176,0.03259812667965889,0.03299618139863014,0.03327321633696556,0.033495329320430756,0.03363422676920891,0.03377005085349083,0.033814724534749985,0.03390031307935715,0.034094229340553284,0.03414235636591911,0.03570673614740372,0.03687800467014313,0.037859536707401276,0.03802095353603363,0.03837410360574722,0.03839835897088051,0.03877057880163193,0.03890704736113548,0.03959444537758827,0.04030419513583183,0.040313221514225006,0.04179083928465843,0.04192216694355011,0.04212801158428192,0.042550552636384964,0.042879510670900345,0.0431046299636364,0.04358682036399841,0.0441933311522007,0.04434584081172943,0.04509257897734642,0.04598454385995865,0.04603494331240654,0.04648483917117119,0.046828579157590866,0.04764602705836296,0.047930993139743805,0.04807387292385101,0.04870906472206116,0.0492439791560173,0.049395207315683365,0.04941613972187042,0.04971703886985779,0.05087464675307274,0.05172969773411751,0.05227000638842583,0.052710648626089096,0.05368893966078758,0.056285277009010315,0.056698061525821686,0.057623282074928284,0.0579242929816246,0.05807225778698921,0.06271978467702866,0.0659932866692543,0.06611721217632294,0.06646441668272018,0.06689860671758652,0.06720148026943207,0.0688035786151886,0.06952101737260818,0.07130877673625946,0.07397492229938507,0.0753302127122879,0.07552966475486755,0.07698405534029007,0.07847829908132553,0.07915660738945007,0.0805872455239296,0.08086816966533661,0.08237899839878082,0.08455191552639008,0.08558090031147003,0.08858448266983032,0.08899861574172974,0.08948905766010284,0.09034135192632675,0.0919472873210907,0.09231073409318924,0.09300432354211807,0.09402266144752502,0.10143940150737762,0.10267216712236404,0.10546979308128357,0.11083605885505676,0.11873766034841537,0.1203351765871048,0.12047170847654343,0.12417421489953995,0.13373875617980957,0.1391717940568924,0.1413986086845398,0.15257756412029266,0.16256611049175262,0.16534888744354248,0.1661081165075302,0.17176620662212372,0.1927006095647812,0.21796908974647522,0.22104370594024658],\"yaxis\":\"y\",\"type\":\"scattergl\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Sorted neurons\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Loss change\"}},\"legend\":{\"title\":{\"text\":\"variable\"},\"tracegroupgap\":0},\"title\":{\"text\":\"Loss change from ablating MLP5 neuron\"},\"width\":800},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('631c8de1-6cb5-4a55-9e55-7f02d6e2b1b2');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sorted_means, indices = torch.sort(diffs.mean(1))\n",
    "sorted_means = sorted_means.tolist()\n",
    "haystack_utils.line(sorted_means, xlabel=\"Sorted neurons\", ylabel=\"Loss change\", title=\"Loss change from ablating MLP5 neuron\") # xticks=indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>                            <div id=\"37ecf30c-af38-4284-a30b-1e72f65920d3\" class=\"plotly-graph-div\" style=\"height:525px; width:1000px;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"37ecf30c-af38-4284-a30b-1e72f65920d3\")) {                    Plotly.newPlot(                        \"37ecf30c-af38-4284-a30b-1e72f65920d3\",                        [{\"error_y\":{\"array\":[1.2347346308337255],\"type\":\"data\",\"visible\":true},\"name\":\"Original\",\"x\":[\"Original\"],\"y\":[1.4070041203126311],\"type\":\"bar\"},{\"error_y\":{\"array\":[2.0610106578827225],\"type\":\"data\",\"visible\":true},\"name\":\"Ablated\",\"x\":[\"Ablated\"],\"y\":[3.684828568845987],\"type\":\"bar\"},{\"error_y\":{\"array\":[1.3511615310340028],\"type\":\"data\",\"visible\":true},\"name\":\"MLP5 path patched\",\"x\":[\"MLP5 path patched\"],\"y\":[2.069063015952706],\"type\":\"bar\"},{\"error_y\":{\"array\":[1.72087086095148],\"type\":\"data\",\"visible\":true},\"name\":\"MLP5 path patched + Top 10 MLP5 neurons ablated\",\"x\":[\"Top MLP5 removed\"],\"y\":[4.057379254102707],\"type\":\"bar\"},{\"error_y\":{\"array\":[0.7521015897163567],\"type\":\"data\",\"visible\":true},\"name\":\"MLP5 path patched + Bottom 10 MLP5 neurons ablated\",\"x\":[\"Bottom MLP5 removed\"],\"y\":[0.5377582565695047],\"type\":\"bar\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"title\":{\"text\":\"Average last token loss when removing top \\u002f bottom neurons from path patching\"},\"xaxis\":{\"title\":{\"text\":\"\"}},\"yaxis\":{\"title\":{\"text\":\"Loss\"}},\"barmode\":\"group\",\"width\":1000},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('37ecf30c-af38-4284-a30b-1e72f65920d3');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check loss change when ablating top / bottom neurons\n",
    "\n",
    "top_neurons_count = 10\n",
    "top_neurons = indices[-top_neurons_count:]\n",
    "bottom_neurons = indices[:top_neurons_count]\n",
    "\n",
    "with model.hooks(deactivate_neurons_fwd_hooks):\n",
    "    ablated_loss, ablated_cache = model.run_with_cache(prompts, return_type=\"loss\")\n",
    "\n",
    "ablate_top_neurons_hook = get_ablate_neurons_hook(top_neurons, ablated_cache)\n",
    "ablate_bottom_neurons_hook = get_ablate_neurons_hook(bottom_neurons, ablated_cache)\n",
    "\n",
    "original_loss, ablated_loss, _, all_MLP5_loss = haystack_utils.get_direct_effect(prompts, model, pos=-1, context_ablation_hooks=deactivate_neurons_fwd_hooks, context_activation_hooks=activate_neurons_fwd_hooks)\n",
    "_, _, _, top_MLP5_ablated_loss = haystack_utils.get_direct_effect(prompts, model, pos=-1, context_ablation_hooks=deactivate_neurons_fwd_hooks, context_activation_hooks=activate_neurons_fwd_hooks+ablate_top_neurons_hook)\n",
    "_, _, _, bottom_MLP5_ablated_loss = haystack_utils.get_direct_effect(prompts, model, pos=-1, context_ablation_hooks=deactivate_neurons_fwd_hooks, context_activation_hooks=activate_neurons_fwd_hooks+ablate_bottom_neurons_hook)\n",
    "\n",
    "names = [\"Original\", \"Ablated\", \"MLP5 path patched\", f\"MLP5 path patched + Top {top_neurons_count} MLP5 neurons ablated\", f\"MLP5 path patched + Bottom {top_neurons_count} MLP5 neurons ablated\"]\n",
    "short_names = [\"Original\", \"Ablated\", \"MLP5 path patched\", f\"Top MLP5 removed\", f\"Bottom MLP5 removed\"]\n",
    "\n",
    "values = [original_loss.tolist(), ablated_loss.tolist(), all_MLP5_loss.tolist(), top_MLP5_ablated_loss.tolist(), bottom_MLP5_ablated_loss.tolist()]\n",
    "haystack_utils.plot_barplot(values, names, short_names=short_names, ylabel=\"Loss\", title=f\"Average last token loss when removing top / bottom neurons from path patching\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigate top / bottom neuron boosts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 462.00 MiB (GPU 0; 22.19 GiB total capacity; 5.79 GiB already allocated; 216.50 MiB free; 5.83 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m ablate_top_neurons_hook \u001b[39m=\u001b[39m get_ablate_neurons_hook(top_neurons, ablated_cache)\n\u001b[1;32m      7\u001b[0m ablate_bottom_neurons_hook \u001b[39m=\u001b[39m get_ablate_neurons_hook(bottom_neurons, ablated_cache)\n\u001b[0;32m----> 9\u001b[0m original_logits, ablated_logprobs, _, all_MLP5_logprobs \u001b[39m=\u001b[39m haystack_utils\u001b[39m.\u001b[39;49mget_direct_effect(prompts, model, pos\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m2\u001b[39;49m, context_ablation_hooks\u001b[39m=\u001b[39;49mdeactivate_neurons_fwd_hooks, context_activation_hooks\u001b[39m=\u001b[39;49mactivate_neurons_fwd_hooks, return_type\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mlogprobs\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m     10\u001b[0m _, _, _, top_MLP5_ablated_logprobs \u001b[39m=\u001b[39m haystack_utils\u001b[39m.\u001b[39mget_direct_effect(prompts, model, pos\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m, context_ablation_hooks\u001b[39m=\u001b[39mdeactivate_neurons_fwd_hooks, context_activation_hooks\u001b[39m=\u001b[39mactivate_neurons_fwd_hooks\u001b[39m+\u001b[39mablate_top_neurons_hook, return_type\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mlogprobs\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     11\u001b[0m _, _, _, bottom_MLP5_ablated_logprobs \u001b[39m=\u001b[39m haystack_utils\u001b[39m.\u001b[39mget_direct_effect(prompts, model, pos\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m, context_ablation_hooks\u001b[39m=\u001b[39mdeactivate_neurons_fwd_hooks, context_activation_hooks\u001b[39m=\u001b[39mactivate_neurons_fwd_hooks\u001b[39m+\u001b[39mablate_bottom_neurons_hook, return_type\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mlogprobs\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/haystack_utils.py:1099\u001b[0m, in \u001b[0;36mget_direct_effect\u001b[0;34m(prompt, model, context_ablation_hooks, context_activation_hooks, pos, deactivated_components, activated_components, return_type)\u001b[0m\n\u001b[1;32m   1097\u001b[0m activate_components_hooks \u001b[39m=\u001b[39m [(freeze_act_name, activate_components_hook) \u001b[39mfor\u001b[39;00m freeze_act_name \u001b[39min\u001b[39;00m activated_components]\n\u001b[1;32m   1098\u001b[0m \u001b[39mwith\u001b[39;00m model\u001b[39m.\u001b[39mhooks(fwd_hooks\u001b[39m=\u001b[39mactivate_components_hooks\u001b[39m+\u001b[39mcontext_ablation_hooks\u001b[39m+\u001b[39mdeactivate_components_hooks):\n\u001b[0;32m-> 1099\u001b[0m     only_activated_metric \u001b[39m=\u001b[39m model(prompt, return_type\u001b[39m=\u001b[39;49mmetric_return_type, loss_per_token\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m   1101\u001b[0m \u001b[39m# convert logits metric to logprobs metric\u001b[39;00m\n\u001b[1;32m   1102\u001b[0m \u001b[39mif\u001b[39;00m return_type \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mlogprobs\u001b[39m\u001b[39m'\u001b[39m:\n",
      "File \u001b[0;32m~/Documents/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/.venv/lib/python3.10/site-packages/transformer_lens/HookedTransformer.py:375\u001b[0m, in \u001b[0;36mHookedTransformer.forward\u001b[0;34m(self, input, return_type, loss_per_token, prepend_bos, stop_at_layer, past_kv_cache)\u001b[0m\n\u001b[1;32m    373\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    374\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 375\u001b[0m     logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49munembed(residual)  \u001b[39m# [batch, pos, d_vocab]\u001b[39;00m\n\u001b[1;32m    376\u001b[0m     \u001b[39mif\u001b[39;00m return_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mlogits\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    377\u001b[0m         \u001b[39mreturn\u001b[39;00m logits\n",
      "File \u001b[0;32m~/Documents/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/.venv/lib/python3.10/site-packages/transformer_lens/components.py:58\u001b[0m, in \u001b[0;36mUnembed.forward\u001b[0;34m(self, residual)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m     55\u001b[0m     \u001b[39mself\u001b[39m, residual: Float[torch\u001b[39m.\u001b[39mTensor, \u001b[39m\"\u001b[39m\u001b[39mbatch pos d_model\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m     56\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Float[torch\u001b[39m.\u001b[39mTensor, \u001b[39m\"\u001b[39m\u001b[39mbatch pos d_vocab_out\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[1;32m     57\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m---> 58\u001b[0m         einsum(\n\u001b[1;32m     59\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mbatch pos d_model, d_model vocab -> batch pos vocab\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     60\u001b[0m             residual,\n\u001b[1;32m     61\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mW_U,\n\u001b[1;32m     62\u001b[0m         )\n\u001b[1;32m     63\u001b[0m         \u001b[39m+\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mb_U\n\u001b[1;32m     64\u001b[0m     )\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 462.00 MiB (GPU 0; 22.19 GiB total capacity; 5.79 GiB already allocated; 216.50 MiB free; 5.83 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "# Patch in MLP5 neurons, ablated bottom neurons, compare with patched MLP5 neurons without ablating bottom neurons\n",
    "\n",
    "with model.hooks(deactivate_neurons_fwd_hooks):\n",
    "    ablated_logits, ablated_cache = model.run_with_cache(prompts)\n",
    "\n",
    "ablate_top_neurons_hook = get_ablate_neurons_hook(top_neurons, ablated_cache)\n",
    "ablate_bottom_neurons_hook = get_ablate_neurons_hook(bottom_neurons, ablated_cache)\n",
    "\n",
    "original_logits, ablated_logprobs, _, all_MLP5_logprobs = haystack_utils.get_direct_effect(prompts, model, pos=-2, context_ablation_hooks=deactivate_neurons_fwd_hooks, context_activation_hooks=activate_neurons_fwd_hooks, return_type='logprobs')\n",
    "_, _, _, top_MLP5_ablated_logprobs = haystack_utils.get_direct_effect(prompts, model, pos=-2, context_ablation_hooks=deactivate_neurons_fwd_hooks, context_activation_hooks=activate_neurons_fwd_hooks+ablate_top_neurons_hook, return_type='logprobs')\n",
    "_, _, _, bottom_MLP5_ablated_logprobs = haystack_utils.get_direct_effect(prompts, model, pos=-2, context_ablation_hooks=deactivate_neurons_fwd_hooks, context_activation_hooks=activate_neurons_fwd_hooks+ablate_bottom_neurons_hook, return_type='logprobs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_difference_neurons(baseline_logprobs, ablated_logprops, positive=True, logprob_threshold=-7, k=50):\n",
    "    neuron_logprob_difference = (baseline_logprobs - ablated_logprops).mean(0)\n",
    "    neuron_logprob_difference[baseline_logprobs.mean(0) < logprob_threshold] = 0\n",
    "    if positive:\n",
    "        non_zero_count = (neuron_logprob_difference > 0).sum()\n",
    "    else:\n",
    "        non_zero_count = (neuron_logprob_difference < 0).sum()\n",
    "    top_logprob_difference, top_neurons = haystack_utils.top_k_with_exclude(neuron_logprob_difference, min(non_zero_count, k), all_ignore, largest=positive)\n",
    "    return top_logprob_difference, top_neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boosted tokens\n",
    "bottom_neuron_pos_difference_logprobs, bottom_pos_indices = get_top_difference_neurons(all_MLP5_logprobs, bottom_MLP5_ablated_logprobs, positive=True)\n",
    "top_neuron_pos_difference_logprobs, top_pos_indices = get_top_difference_neurons(all_MLP5_logprobs, top_MLP5_ablated_logprobs, positive=True)\n",
    "# Deboosted tokens\n",
    "bottom_neuron_neg_difference_logprobs, bottom_neg_indices = get_top_difference_neurons(all_MLP5_logprobs, bottom_MLP5_ablated_logprobs, positive=False)\n",
    "top_neuron_neg_difference_logprobs, top_neg_indices = get_top_difference_neurons(all_MLP5_logprobs, top_MLP5_ablated_logprobs, positive=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdfb5fba991e4556b8371d33fe86bef7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='Tokens:', options=('Boosted', 'Deboosted'), value='Boosted'), Drop…"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def plot_data(boosted_deboosted, top_bottom):\n",
    "    if boosted_deboosted == 'Boosted' and top_bottom == 'Top':\n",
    "        logprobs = top_neuron_pos_difference_logprobs\n",
    "        indices = top_pos_indices\n",
    "        title_change = 'increase'\n",
    "    elif boosted_deboosted == 'Boosted' and top_bottom == 'Bottom':\n",
    "        logprobs = bottom_neuron_pos_difference_logprobs\n",
    "        indices = bottom_pos_indices\n",
    "        title_change = 'increase'\n",
    "    elif boosted_deboosted == 'Deboosted' and top_bottom == 'Top':\n",
    "        logprobs = top_neuron_neg_difference_logprobs\n",
    "        indices = top_neg_indices\n",
    "        title_change = 'decrease'\n",
    "    else:  # 'Deboosted' and 'Bottom'\n",
    "        logprobs = bottom_neuron_neg_difference_logprobs\n",
    "        indices = bottom_neg_indices\n",
    "        title_change = 'decrease'\n",
    "\n",
    "    xlabel = boosted_deboosted + \" tokens\"\n",
    "    ylabel = \"full_logprob - ablated_logprop\"\n",
    "    title = 'Logprob ' + title_change + ' from ' + top_bottom.lower() + ' neurons'\n",
    "    xticks = [model.to_str_tokens([i])[0] for i in indices]\n",
    "\n",
    "    haystack_utils.line(logprobs.cpu().numpy(), xlabel=xlabel, ylabel=ylabel, title=title, xticks=xticks, show_legend=False)\n",
    "\n",
    "\n",
    "boosted_deboosted = widgets.Dropdown(options=['Boosted', 'Deboosted'], value='Boosted', description='Tokens:')\n",
    "top_bottom = widgets.Dropdown(options=['Top', 'Bottom'], value='Top', description='Neurons:')\n",
    "widgets.interactive(plot_data, boosted_deboosted=boosted_deboosted, top_bottom=top_bottom)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decompose neuron effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get logprob change for individual neuron\n",
    "\n",
    "def get_individual_neuron_logprob_effect(sorted_indices, neuron_pos=0, top=True, positive=True, plot=True):\n",
    "    if top:\n",
    "        neurons = sorted_indices[-(neuron_pos+1)]\n",
    "    else:\n",
    "        neurons = sorted_indices[neuron_pos]\n",
    "    neurons_hook = get_ablate_neurons_hook([neurons], ablated_cache)\n",
    "\n",
    "    original_logits, ablated_logprobs, _, all_MLP5_logprobs = haystack_utils.get_direct_effect(prompts, model, pos=-2, context_ablation_hooks=deactivate_neurons_fwd_hooks, context_activation_hooks=activate_neurons_fwd_hooks, return_type='logprobs')\n",
    "    _, _, _, neuron_ablated_logprobs = haystack_utils.get_direct_effect(prompts, model, pos=-2, context_ablation_hooks=deactivate_neurons_fwd_hooks, context_activation_hooks=activate_neurons_fwd_hooks+neurons_hook, return_type='logprobs')\n",
    "    neuron_difference_logprobs, difference_indices = \\\n",
    "        get_top_difference_neurons(all_MLP5_logprobs, neuron_ablated_logprobs, positive=positive)\n",
    "    \n",
    "    if not plot:\n",
    "        return neuron_difference_logprobs, difference_indices\n",
    "    \n",
    "    if positive:\n",
    "        xlabel = 'Boosted tokens'\n",
    "        title = 'Logprob increase from neuron ' + str(neurons.item())\n",
    "    else: \n",
    "        xlabel = 'Deboosted tokens'\n",
    "        title = 'Logprob decrease from neuron ' + str(neurons.item())\n",
    "    if top:\n",
    "        title += f' (top {neuron_pos+1} neuron)'\n",
    "    else:\n",
    "        title += f' (bottom {neuron_pos+1} neuron)'\n",
    "\n",
    "    xticks = [model.to_str_tokens([i])[0] for i in difference_indices]\n",
    "    haystack_utils.line(neuron_difference_logprobs.cpu().numpy(), xlabel=xlabel, ylabel=\"full_logprob - ablated_logprop\", title=title, xticks=xticks, show_legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b5975daae984b81aa589a447a9e4dc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='Neuron Pos:', max=20), Dropdown(description='Neurons:', …"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a function to call your function with widget inputs\n",
    "def plot_individual_neuron(neuron_pos, top_bottom, pos_neg):\n",
    "    top_bool = True if top_bottom == 'Top' else False\n",
    "    positive_bool = True if pos_neg == 'Positive' else False\n",
    "    get_individual_neuron_logprob_effect(indices, neuron_pos=neuron_pos, top=top_bool, positive=positive_bool)\n",
    "\n",
    "# Define widgets\n",
    "neuron_pos_slider = widgets.IntSlider(min=0, max=20, step=1, value=0, description='Neuron Pos:')\n",
    "top_bottom_dropdown = widgets.Dropdown(options=['Top', 'Bottom'], value='Top', description='Neurons:')\n",
    "pos_neg_dropdown = widgets.Dropdown(options=['Positive', 'Negative'], value='Positive', description='Pos/Neg:')\n",
    "\n",
    "# Use interactive function to bind the widgets to the plotting function\n",
    "widgets.interactive(plot_individual_neuron, neuron_pos=neuron_pos_slider, top_bottom=top_bottom_dropdown, pos_neg=pos_neg_dropdown)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum individual weighted positive / negative effects\n",
    "def summed_neuron_differences(num_neurons=10, top=True, positive=True, k=50):\n",
    "    summed_neuron_diffs = torch.zeros(model.cfg.d_vocab).cuda()\n",
    "    for neuron_pos in range(num_neurons):\n",
    "        # It would probably be more principle to do our filtering on the summed diffs instead of per neuron\n",
    "        neuron_diff, token_indices = get_individual_neuron_logprob_effect(indices, neuron_pos=neuron_pos, top=top, positive=positive, plot=False)\n",
    "        summed_neuron_diffs[token_indices] += neuron_diff\n",
    "    if positive:\n",
    "        non_zero_count = (summed_neuron_diffs > 0).sum()\n",
    "    else:\n",
    "        non_zero_count = (summed_neuron_diffs < 0).sum()\n",
    "    top_logprob_difference, top_tokens = haystack_utils.top_k_with_exclude(summed_neuron_diffs, min(non_zero_count, k), all_ignore, largest=positive)\n",
    "    return top_logprob_difference, top_tokens\n",
    "\n",
    "def plot_summed_neuron_differences(num_neurons=10, top=True, positive=True):\n",
    "    top_logprob_difference, top_logprob_tokens = summed_neuron_differences(num_neurons=num_neurons, top=top, positive=positive)\n",
    "    xticks = [model.to_str_tokens([i])[0] for i in top_logprob_tokens]\n",
    "    \n",
    "    if positive:\n",
    "        title = \"Summed individual boosts by\"\n",
    "        xlabel = \"Boosted tokens\"\n",
    "    else:\n",
    "        title = \"Summed individual deboosts by\"\n",
    "        xlabel = \"Deboosted tokens\"\n",
    "    if top:\n",
    "        title += f\" top {num_neurons} neurons\"\n",
    "    else:\n",
    "        title += f\" bottom {num_neurons} neurons\"\n",
    "    haystack_utils.line(top_logprob_difference.cpu().numpy(), xlabel=xlabel, ylabel=\"full_logprob - ablated_logprop\", title=title, xticks=xticks, show_legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af77c67c0ad24f8d8a185844af918a85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=10, description='Num Neurons:', max=20, min=1), Dropdown(description='Ne…"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a function to call your function with widget inputs\n",
    "def plot_summed_neurons_widget(num_neurons, top_bottom, pos_neg):\n",
    "    top_bool = True if top_bottom == 'Top' else False\n",
    "    positive_bool = True if pos_neg == 'Positive' else False\n",
    "    plot_summed_neuron_differences(num_neurons=num_neurons, top=top_bool, positive=positive_bool)\n",
    "\n",
    "# Define widgets\n",
    "num_neuron_slider = widgets.IntSlider(min=1, max=20, step=1, value=10, description='Num Neurons:')\n",
    "top_bottom_dropdown = widgets.Dropdown(options=['Top', 'Bottom'], value='Top', description='Neurons:')\n",
    "pos_neg_dropdown = widgets.Dropdown(options=['Positive', 'Negative'], value='Positive', description='Pos/Neg:')\n",
    "\n",
    "# Use interactive function to bind the widgets to the plotting function\n",
    "widgets.interactive(plot_summed_neurons_widget, num_neurons=num_neuron_slider, top_bottom=top_bottom_dropdown, pos_neg=pos_neg_dropdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neuron -> Token Logit Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0237,  0.0346, -0.0347, -0.0274,  0.0595,  0.0397,  0.0501,  0.1405,\n",
      "         0.0417, -0.0392], device='cuda:0')\n",
      "tensor([0.1634, 0.1461, 0.1405, 0.1399, 0.1349, 0.1207, 0.1196, 0.1161, 0.1156,\n",
      "        0.1139, 0.1130, 0.1128, 0.1120, 0.1104, 0.1096, 0.1070, 0.1056, 0.1048,\n",
      "        0.1042, 0.1028], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Our top neurons are selected by the difference in their boost in gen based on the context neuron\n",
    "# Many other neurons boost gen more per unit of activation\n",
    "\n",
    "cosine_sim = torch.nn.CosineSimilarity(dim=1)\n",
    "answer_residual_direction = model.tokens_to_residual_directions(\"gen\")\n",
    "neuron_weights = model.state_dict()['blocks.5.mlp.W_out'][top_neurons]\n",
    "\n",
    "cosine_sims = cosine_sim(neuron_weights, answer_residual_direction.unsqueeze(0))\n",
    "print(cosine_sims)\n",
    "\n",
    "cosine_sim = torch.nn.CosineSimilarity(dim=1)\n",
    "answer_residual_direction = model.tokens_to_residual_directions(\"gen\")\n",
    "neuron_weights = model.state_dict()['blocks.5.mlp.W_out']\n",
    "\n",
    "cosine_sims = cosine_sim(neuron_weights, answer_residual_direction.unsqueeze(0))\n",
    "top, indices = torch.topk(cosine_sims, 20)\n",
    "print(top)\n",
    "\n",
    "# cosine sim and different aspects of the neurons\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dae76fefc29c442692a4e49a111e4bd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 522.00 MiB (GPU 0; 22.19 GiB total capacity; 4.58 GiB already allocated; 480.50 MiB free; 5.57 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m all_activations_l5 \u001b[39m=\u001b[39m {}\n\u001b[1;32m      5\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m5\u001b[39m, \u001b[39m6\u001b[39m):\n\u001b[0;32m----> 6\u001b[0m     english_activations_l5[layer] \u001b[39m=\u001b[39m get_mlp_activations(english_data, layer, model, mean\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m      7\u001b[0m     german_activations_l5[layer] \u001b[39m=\u001b[39m get_mlp_activations(german_data, layer, model, mean\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m      8\u001b[0m     all_activations_l5[layer] \u001b[39m=\u001b[39m get_mlp_activations(german_data[:\u001b[39m50\u001b[39m] \u001b[39m+\u001b[39m english_data[:\u001b[39m150\u001b[39m], layer, model, mean\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/Documents/haystack_utils.py:75\u001b[0m, in \u001b[0;36mget_mlp_activations\u001b[0;34m(prompts, layer, model, num_prompts, context_crop_start, context_crop_end, mean, hook_pre)\u001b[0m\n\u001b[1;32m     73\u001b[0m     act \u001b[39m=\u001b[39m einops\u001b[39m.\u001b[39mrearrange(act, \u001b[39m\"\u001b[39m\u001b[39mbatch pos d_mlp -> (batch pos) d_mlp\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     74\u001b[0m     acts\u001b[39m.\u001b[39mappend(act)\n\u001b[0;32m---> 75\u001b[0m acts \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mconcat(acts, dim\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n\u001b[1;32m     76\u001b[0m \u001b[39mif\u001b[39;00m mean:\n\u001b[1;32m     77\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mmean(acts, dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 522.00 MiB (GPU 0; 22.19 GiB total capacity; 4.58 GiB already allocated; 480.50 MiB free; 5.57 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "haystack_utils.clean_cache()\n",
    "english_activations_l5 = {}\n",
    "german_activations_l5 = {}\n",
    "all_activations_l5 = {}\n",
    "for layer in range(5, 6):\n",
    "    english_activations_l5[layer] = get_mlp_activations(english_data, layer, model, mean=True)\n",
    "    german_activations_l5[layer] = get_mlp_activations(german_data, layer, model, mean=True)\n",
    "    all_activations_l5[layer] = get_mlp_activations(german_data[:50] + english_data[:150], layer, model, mean=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_activations_l5' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 28\u001b[0m\n\u001b[1;32m     26\u001b[0m W_out \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mstate_dict()[\u001b[39m'\u001b[39m\u001b[39mblocks.5.mlp.W_out\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     27\u001b[0m W_U \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mW_U\n\u001b[0;32m---> 28\u001b[0m \u001b[39mprint\u001b[39m(W_out\u001b[39m.\u001b[39mshape, W_U\u001b[39m.\u001b[39mshape, all_activations_l5\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mshape)\n\u001b[1;32m     30\u001b[0m average_boost \u001b[39m=\u001b[39m W_out \u001b[39m*\u001b[39m all_activations_l5\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m) \u001b[39m*\u001b[39m W_U\n\u001b[1;32m     31\u001b[0m \u001b[39mprint\u001b[39m(average_boost\u001b[39m.\u001b[39mshape) \u001b[39m# d_mlp d_vocab\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'all_activations_l5' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# token based measures:\n",
    "# index\n",
    "# next index\n",
    "\n",
    "# neuron based measures:\n",
    "# index\n",
    "# cosine sim with gen\n",
    "# average activation when context neuron enabled\n",
    "# average activation when context neuron disabled\n",
    "# 5 if cosine sim > 0 else -5\n",
    "# loss change when neuron ablated\n",
    "\n",
    "# boost_gen_acts = cosine_sims\n",
    "# boost_gen_acts[cosine_sims > 0] = 5.0\n",
    "# boost_gen_acts[cosine_sims <= 0] = -5.0\n",
    "\n",
    "# def get_deactivate_neuron_hook(neuron):\n",
    "#     def deactivate_neurons_hook(value, hook):\n",
    "#         value[:, :, neuron] = MEAN_ACTIVATION_INACTIVE\n",
    "#         return value\n",
    "# deactivate_neurons_fwd_hooks=[(f'blocks.{LAYER_TO_ABLATE}.mlp.hook_post', deactivate_neurons_hook)]\n",
    "# loss_diffs = []\n",
    "# for i in range(2048):\n",
    "\n",
    "W_out = model.state_dict()['blocks.5.mlp.W_out']\n",
    "W_U = model.W_U\n",
    "print(W_out.shape, W_U.shape, all_activations_l5.unsqueeze(1).shape)\n",
    "\n",
    "average_boost = W_out * all_activations_l5.unsqueeze(1) * W_U\n",
    "print(average_boost.shape) # d_mlp d_vocab\n",
    "\n",
    "# 'Average gen boost': [i zip(all_activations_l5,\n",
    "\n",
    "# mean activation\n",
    "data = {\n",
    "    'Neuron index': list(range(2048)),\n",
    "    'Cosine similarity with \\\"gen\\\"': cosine_sims.tolist(),\n",
    "    'Average act - context neuron enabled': german_activations_l5.tolist(),\n",
    "    'Average act - context neuron disabled': english_activations_l5.tolist(),\n",
    "    'Average act': all_activations_l5.tolist()\n",
    "    # 'Boost \\\"gen\\\" act': boost_gen_acts.tolist(),\n",
    "    # 'Loss change when ablated': \n",
    "}\n",
    "\n",
    "# Create the dataframe\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Print the dataframe\n",
    "print(df)\n",
    "\n",
    "# More complete picture of neurons that boost gen.\n",
    "# Line plot of each neuron's gen boost with dotted vertical lines where the German context neuron-boosted ones are"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brainstorm notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to decompose an MLP5 neuron's effect into its boost to the correct logit and its deboost of other logits. I want to discover how these two effects change the log prob.\n",
    "\n",
    "metric like loss reduction vs. token boost\n",
    "\n",
    "~~run direct effect, patch each neuron in top 10 individually, get overall loss reduction from neuron controlled by context neuron (equivalent to logprob increase for correct token)~~\n",
    "decompose loss reduction into two parts:\n",
    "run direct effect, patch the correct token logit boost from the neuron (removing the neuron's other effects), get overall loss reduction from neuron (equivalent to logprob increase for correct token)\n",
    "destructive interference loss reduction = overall loss reduction - loss reduction from correct token logit boost (component of the logprob increase for correct token due to it deboosting incorrect token)\n",
    "\n",
    "Patch the correct token logit boost from the neuron (removing the neuron's other effects).\n",
    "1. Get baseline logprobs for a prompt\n",
    "2. Get difference in logits from activating the neuron under test. Can use get_direct_effects with return_type='logits'.\n",
    "3. Run the model with return_type='logits' and the neuron under test zero ablated. \n",
    "4. Add the correct token logit from step 1 to a copy of the output logits. Convert to logprobs\n",
    "5. Add the incorrect token logits from step 1 to a copy of the output logits. Convert to logprobs\n",
    "6. Compare A. lobprobs with correct answer token logit increase, B. logprobs with incorrect answer token logit increases, and C. baseline logprobs\n",
    "\n",
    "~~If the context neuron gives each neuron a flat boost then if we decompose the resulting flat boost to one MLP5 neuron into a boost to one logit vs. boosts to all other logits it will change the logprobs (first will increase answer probability and second will reduce answer probability). \n",
    "\n",
    "the two resulting log probs at the correct answer token won't add up to the original log probs (?). \n",
    "\n",
    "If the boost is flat the correct percentage decomposition is 1/50000 and 49999/50000? In practice/all other factors being equal\n",
    "\n",
    "New plan:\n",
    "\n",
    "Difference between baseline log prob and neuron log prob?\n",
    "Classify individual neurons by percentage constructive vs destructive by looking at their log probs and summing the incorrect token log probs\n",
    "\n",
    "Correct log prob difference\n",
    "Incorrect log prob difference (summed over every plausible token?)\n",
    "\n",
    "Largest boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the log prob for an incorrect token is significantly lower then that's where the extra probability density on the correct answer is coming from \n",
    "# Constructive interference increases correct token log prob and uniformly decreases other log probs\n",
    "# Destructive interference decreases specific other log probs and uniformly increases other log probs\n",
    " \n",
    "# Most neurons are a mixture of the above\n",
    "# Decompose neurons into what % of their effect is each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original_logits, ablated_logprobs, _, all_MLP5_logprobs = haystack_utils.get_direct_effect(prompts, model, pos=-2, context_ablation_hooks=deactivate_neurons_fwd_hooks, context_activation_hooks=activate_neurons_fwd_hooks, return_type='logprobs')\n",
    "# _, _, _, top_MLP5_ablated_logprobs = haystack_utils.get_direct_effect(prompts, model, pos=-2, context_ablation_hooks=deactivate_neurons_fwd_hooks, context_activation_hooks=activate_neurons_fwd_hooks+ablate_top_neurons_hook, return_type='logprobs')\n",
    "# _, _, _, bottom_MLP5_ablated_logprobs = haystack_utils.get_direct_effect(prompts, model, pos=-2, context_ablation_hooks=deactivate_neurons_fwd_hooks, context_activation_hooks=activate_neurons_fwd_hooks+ablate_bottom_neurons_hook, return_type='logprobs')\n",
    "\n",
    "# bottom_neuron_high_difference_logprobs = (all_MLP5_logprobs - bottom_MLP5_ablated_logprobs)\n",
    "# bottom_neuron_high_difference_logprobs[bottom_neuron_high_difference_logprobs < 0] = 0\n",
    "# bottom_neuron_high_difference_logprobs = bottom_neuron_high_difference_logprobs.mean(0)\n",
    "# bottom_neuron_high_difference_logprobs[all_MLP5_logprobs.mean(0) < -7] = 0\n",
    "\n",
    "# bottom_non_zero_count = (bottom_neuron_high_difference_logprobs > 0).sum()\n",
    "# bottom_neuron_high_difference_logprobs, bottom_indices = haystack_utils.top_k_with_exclude(bottom_neuron_high_difference_logprobs, min(bottom_non_zero_count, 50), all_ignore)\n",
    "# haystack_utils.line(bottom_neuron_high_difference_logprobs.cpu().numpy(), title='Largest positive difference in log probs for tokens when bottom neurons are not ablated', xticks=[model.to_str_tokens([i])[0] for i in bottom_indices])\n",
    "\n",
    "\n",
    "# top_neuron_high_difference_logprobs = (all_MLP5_logprobs - top_MLP5_ablated_logprobs)\n",
    "# top_neuron_high_difference_logprobs[top_neuron_high_difference_logprobs < 0] = 0\n",
    "# top_neuron_high_difference_logprobs = top_neuron_high_difference_logprobs.mean(0)\n",
    "# top_neuron_high_difference_logprobs[all_MLP5_logprobs.mean(0) < -7] = 0\n",
    "\n",
    "# top_non_zero_count = (top_neuron_high_difference_logprobs > 0).sum()\n",
    "# top_neuron_high_difference_logprobs, top_indices = haystack_utils.top_k_with_exclude(top_neuron_high_difference_logprobs, min(top_non_zero_count, 50), all_ignore)\n",
    "# haystack_utils.line(top_neuron_high_difference_logprobs.cpu().numpy(), title='Largest positive difference in log probs for tokens when bottom neurons are not ablated', xticks=[model.to_str_tokens([i])[0] for i in top_indices])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
