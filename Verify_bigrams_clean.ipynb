{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from transformer_lens import HookedTransformer, ActivationCache, utils, patching\n",
    "from jaxtyping import Float, Int, Bool\n",
    "from torch import Tensor\n",
    "from tqdm.auto import tqdm\n",
    "import plotly.io as pio\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "pio.renderers.default = \"notebook_connected\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.autograd.set_grad_enabled(False)\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "from haystack_utils import get_mlp_activations\n",
    "import haystack_utils\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "817f0604129e4fae92cc0606c1de781d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/567 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e453a22c86f43d3900aaa21276b8b21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/166M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57596baaf40d48c4ad0c8d800084b158",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/396 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b32910ae66854e2e97fd811a000837c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b7525e3a8684423820d3d9d6456d53f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model EleutherAI/pythia-70m into HookedTransformer\n",
      "data/german_europarl.json: Loaded 2000 examples with 152 to 2000 characters each.\n",
      "data/english_europarl.json: Loaded 2000 examples with 165 to 2000 characters each.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd0c85f3be324f6bba7881297bc89a19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "410264e3ab4948239c467a7a3120e0d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = HookedTransformer.from_pretrained(\"EleutherAI/pythia-70m\",\n",
    "    center_unembed=True,\n",
    "    center_writing_weights=True,\n",
    "    fold_ln=True,\n",
    "    device=device)\n",
    "\n",
    "german_data = haystack_utils.load_json_data(\"data/german_europarl.json\")[:200]\n",
    "english_data = haystack_utils.load_json_data(\"data/english_europarl.json\")[:200]\n",
    "\n",
    "\n",
    "english_activations = {}\n",
    "german_activations = {}\n",
    "for layer in range(3, 4):\n",
    "    english_activations[layer] = get_mlp_activations(english_data, layer, model, mean=False)\n",
    "    german_activations[layer] = get_mlp_activations(german_data, layer, model, mean=False)\n",
    "\n",
    "LAYER_TO_ABLATE = 3\n",
    "NEURONS_TO_ABLATE = [669]\n",
    "MEAN_ACTIVATION_ACTIVE = german_activations[LAYER_TO_ABLATE][:, NEURONS_TO_ABLATE].mean()\n",
    "MEAN_ACTIVATION_INACTIVE = english_activations[LAYER_TO_ABLATE][:, NEURONS_TO_ABLATE].mean()\n",
    "\n",
    "def deactivate_neurons_hook(value, hook):\n",
    "    value[:, :, NEURONS_TO_ABLATE] = MEAN_ACTIVATION_INACTIVE\n",
    "    return value\n",
    "deactivate_neurons_fwd_hooks=[(f'blocks.{LAYER_TO_ABLATE}.mlp.hook_post', deactivate_neurons_hook)]\n",
    "\n",
    "def activate_neurons_hook(value, hook):\n",
    "    value[:, :, NEURONS_TO_ABLATE] = MEAN_ACTIVATION_ACTIVE\n",
    "    return value\n",
    "activate_neurons_fwd_hooks=[(f'blocks.{LAYER_TO_ABLATE}.mlp.hook_post', activate_neurons_hook)]\n",
    "\n",
    "all_ignore, not_ignore = haystack_utils.get_weird_tokens(model, plot_norms=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find top common German tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00b0b6ad1f354fd798312d91f26a69bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' der', 'en', ' die', ' und', 'ung', 'ä', ' in', ' den', ' des', ' zu', 'ch', 'n', 'st', 're', 'z', ' von', ' für', 'äsident', ' Pr', 'ischen', 't', 'ü', 'icht', 'gen', ' ist', ' auf', ' dass', 'ge', 'ig', ' im', 'in', ' über', 'g', ' das', 'te', ' er', 'men', ' w', 'es', ' an', 'ß', ' wir', ' eine', 'f', ' W', 'hen', 'w', ' Europ', ' ich', 'ungen', 'ren', 'le', ' dem', 'ten', ' ein', 'e', ' Z', ' Ver', 'der', ' B', ' mit', ' dies', 'h', ' nicht', 'ungs', 's', ' G', ' z', 'it', ' Herr', ' es', 'l', ' S', 'ich', 'lich', ' An', 'heit', 'ie', ' Er', ' zur', ' V', ' ver', 'u', 'hr', 'chaft', 'Der', ' Ich', ' Ab', ' haben', 'i', 'ant', 'chte', ' mö', 'er', ' K', 'igen', ' Ber', 'ür', ' Fra', 'em']\n"
     ]
    }
   ],
   "source": [
    "# Get top common german tokens excluding punctuation\n",
    "token_counts = torch.zeros(model.cfg.d_vocab).cuda()\n",
    "for example in tqdm(german_data):\n",
    "    tokens = model.to_tokens(example)\n",
    "    for token in tokens[0]:\n",
    "        token_counts[token.item()] += 1\n",
    "\n",
    "punctuation = [\"\\n\", \".\", \",\", \"!\", \"?\", \";\", \":\", \"-\", \"(\", \")\", \"[\", \"]\", \"{\", \"}\", \"<\", \">\", \"/\", \"\\\\\", \"\\\"\", \"'\"]\n",
    "leading_space_punctuation = [\" \" + char for char in punctuation]\n",
    "punctuation_tokens = model.to_tokens(punctuation + leading_space_punctuation + [' –', \" \", '  ', \"<|endoftext|>\"])[:, 1].flatten()\n",
    "token_counts[punctuation_tokens] = 0\n",
    "token_counts[all_ignore] = 0\n",
    "\n",
    "top_counts, top_tokens = torch.topk(token_counts, 100)\n",
    "print(model.to_str_tokens(top_tokens[:100]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of ngrams preceded by random prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_selection(tensor, n=12):\n",
    "    # Hacky replacement for np.random.choice\n",
    "    return tensor[torch.randperm(len(tensor))[:n]]\n",
    "\n",
    "def generate_random_prompts(end_string, n=50, length=12):\n",
    "    # Generate a batch of random prompts ending with a specific ngram\n",
    "    end_tokens = model.to_tokens(end_string).flatten()[1:]\n",
    "    prompts = []\n",
    "    for i in range(n):\n",
    "        prompt = get_random_selection(top_tokens[:max(50, length)], n=length).cuda()\n",
    "        prompt = torch.cat([prompt, end_tokens])\n",
    "        prompts.append(prompt)\n",
    "    prompts = torch.stack(prompts)\n",
    "    return prompts\n",
    "\n",
    "def replace_column(prompts: Int[Tensor, \"n_prompts n_tokens\"], token_index: int):\n",
    "    # Replaces a specific token position in a batch of prompts with random common German tokens\n",
    "    new_prompts = prompts.clone()\n",
    "    random_tokens = get_random_selection(top_tokens[:max(50, prompts.shape[0])], n=prompts.shape[0]).cuda()\n",
    "    new_prompts[:, token_index] = random_tokens\n",
    "    return new_prompts \n",
    "\n",
    "def loss_analysis(prompts: Tensor, title=\"\"):\n",
    "    # Loss plot for a batch of prompts\n",
    "    names = [\"Original\", \"Ablated\", \"MLP5 path patched\"]\n",
    "    original_loss, ablated_loss, _, only_activated_loss = \\\n",
    "        haystack_utils.get_direct_effect(prompts, model, pos=-1,\n",
    "                                        context_ablation_hooks=deactivate_neurons_fwd_hooks, \n",
    "                                        context_activation_hooks=activate_neurons_fwd_hooks, \n",
    "                                        )\n",
    "    haystack_utils.plot_barplot([original_loss.tolist(), ablated_loss.tolist(), only_activated_loss.tolist()], names, ylabel=\"Loss\", title=title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_analysis_random_prompts(end_string, n=50, length=12, replace_columns: list[int] | None = None):\n",
    "    # Loss plot for a batch of random prompts ending with a specific ngram and optionally replacing specific tokens\n",
    "    prompts = generate_random_prompts(end_string, n=n, length=length)\n",
    "    title=f\"Average last token loss on {length} random tokens ending in '{end_string}'\"\n",
    "    if replace_columns is not None:\n",
    "        replaced_tokens = model.to_str_tokens(prompts[0, replace_columns])\n",
    "        title += f\" replacing {replaced_tokens}\"\n",
    "        for column in replace_columns:\n",
    "            prompts = replace_column(prompts, column)\n",
    "    \n",
    "    loss_analysis(prompts, title=title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "# loss_analysis_random_prompts(\" Vorschlägen\", n=100, length=20)\n",
    "# loss_analysis_random_prompts(\" Vorschlägen\", n=100, length=20, replace_columns=[-2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc77443c308f4464b58f602290dbfd57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Ngram: ', options=(' Vorschlägen', ' Vorschläge', ' häufig', ' schließt', ' beweglich'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e74f607b1b6544f5aad8ca681b49eb89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SelectMultiple(description='Replace Columns:', index=(3,), options=('-2', '-3', '-4', 'None'), value=('None',)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7f7205d633a4d82b25b697b427726a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a dropdown menu widget\n",
    "dropdown = widgets.Dropdown(\n",
    "    options = [\" Vorschlägen\", \" Vorschläge\", \" häufig\", \" schließt\", \" beweglich\"], \n",
    "    value = \" Vorschlägen\",\n",
    "    description = 'Ngram: ',\n",
    ")\n",
    "\n",
    "replace_columns_dropdown = widgets.SelectMultiple(\n",
    "    options = ['-2', '-3', '-4', 'None'],\n",
    "    value = ['None'],  # default selected value\n",
    "    description = 'Replace Columns:',\n",
    ")\n",
    "\n",
    "# Create an output widget to hold the plot\n",
    "output = widgets.Output()\n",
    "\n",
    "# Define a function to call when the widget's value changes\n",
    "def update_plot(*args):\n",
    "    # Clear the old plot from the output\n",
    "    with output:\n",
    "        clear_output(wait=True)\n",
    "\n",
    "        if 'None' in replace_columns_dropdown.value:\n",
    "            replace_columns = None\n",
    "        else:\n",
    "            # If 'None' not selected, convert the selected values to integers\n",
    "            replace_columns = [int(val) for val in replace_columns_dropdown.value]\n",
    "        \n",
    "        # Call your function with the values from the widgets\n",
    "        loss_analysis_random_prompts(dropdown.value, n=100, length=20, replace_columns=replace_columns)\n",
    "\n",
    "# Set the function to be called when the widget's value changes\n",
    "dropdown.observe(update_plot, 'value')\n",
    "replace_columns_dropdown.observe(update_plot, 'value')\n",
    "\n",
    "# Display the widget and the output\n",
    "display(dropdown, replace_columns_dropdown, output)\n",
    "\n",
    "# Run once at startup\n",
    "update_plot()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify relevant neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 24])\n"
     ]
    }
   ],
   "source": [
    "prompts = generate_random_prompts(\" Vorschlägen\", n=100, length=20)\n",
    "print(prompts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cff05c7fa1754d2dba25f203d70eee10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2048 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0007)\n"
     ]
    }
   ],
   "source": [
    "with model.hooks(deactivate_neurons_fwd_hooks):\n",
    "    _, ablated_cache = model.run_with_cache(prompts)\n",
    "\n",
    "def get_ablate_neurons_hook(neuron: int | list[int], ablated_cache, layer=5):\n",
    "    def ablate_neurons_hook(value, hook):\n",
    "        value[:, :, neuron] = ablated_cache[f'blocks.{layer}.mlp.hook_post'][:, :, neuron]\n",
    "        return value\n",
    "    return [(f'blocks.{layer}.mlp.hook_post', ablate_neurons_hook)]\n",
    "\n",
    "diffs = torch.zeros(2048, prompts.shape[0])\n",
    "# Loss with path patched MLP5 neurons\n",
    "_, _, _, baseline_loss = haystack_utils.get_direct_effect(prompts, model, pos=-1, context_ablation_hooks=deactivate_neurons_fwd_hooks, context_activation_hooks=activate_neurons_fwd_hooks)\n",
    "for neuron in tqdm(range(2048)):\n",
    "    ablate_single_neuron_hook = get_ablate_neurons_hook(neuron, ablated_cache)\n",
    "    # Loss with path patched MLP5 neurons but a single neuron changed back to original ablated value\n",
    "    _, _, _, only_deactivated_loss = haystack_utils.get_direct_effect(prompts, model, pos=-1, context_ablation_hooks=deactivate_neurons_fwd_hooks, context_activation_hooks=activate_neurons_fwd_hooks+ablate_single_neuron_hook)\n",
    "    diffs[neuron] = only_deactivated_loss - baseline_loss\n",
    "\n",
    "print(diffs.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>                            <div id=\"91c57798-7571-4209-8710-44df61105c34\" class=\"plotly-graph-div\" style=\"height:525px; width:800px;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"91c57798-7571-4209-8710-44df61105c34\")) {                    Plotly.newPlot(                        \"91c57798-7571-4209-8710-44df61105c34\",                        [{\"hovertemplate\":\"variable=0\\u003cbr\\u003eindex=%{x}\\u003cbr\\u003evalue=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"0\",\"line\":{\"color\":\"#636efa\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"0\",\"showlegend\":true,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542,543,544,545,546,547,548,549,550,551,552,553,554,555,556,557,558,559,560,561,562,563,564,565,566,567,568,569,570,571,572,573,574,575,576,577,578,579,580,581,582,583,584,585,586,587,588,589,590,591,592,593,594,595,596,597,598,599,600,601,602,603,604,605,606,607,608,609,610,611,612,613,614,615,616,617,618,619,620,621,622,623,624,625,626,627,628,629,630,631,632,633,634,635,636,637,638,639,640,641,642,643,644,645,646,647,648,649,650,651,652,653,654,655,656,657,658,659,660,661,662,663,664,665,666,667,668,669,670,671,672,673,674,675,676,677,678,679,680,681,682,683,684,685,686,687,688,689,690,691,692,693,694,695,696,697,698,699,700,701,702,703,704,705,706,707,708,709,710,711,712,713,714,715,716,717,718,719,720,721,722,723,724,725,726,727,728,729,730,731,732,733,734,735,736,737,738,739,740,741,742,743,744,745,746,747,748,749,750,751,752,753,754,755,756,757,758,759,760,761,762,763,764,765,766,767,768,769,770,771,772,773,774,775,776,777,778,779,780,781,782,783,784,785,786,787,788,789,790,791,792,793,794,795,796,797,798,799,800,801,802,803,804,805,806,807,808,809,810,811,812,813,814,815,816,817,818,819,820,821,822,823,824,825,826,827,828,829,830,831,832,833,834,835,836,837,838,839,840,841,842,843,844,845,846,847,848,849,850,851,852,853,854,855,856,857,858,859,860,861,862,863,864,865,866,867,868,869,870,871,872,873,874,875,876,877,878,879,880,881,882,883,884,885,886,887,888,889,890,891,892,893,894,895,896,897,898,899,900,901,902,903,904,905,906,907,908,909,910,911,912,913,914,915,916,917,918,919,920,921,922,923,924,925,926,927,928,929,930,931,932,933,934,935,936,937,938,939,940,941,942,943,944,945,946,947,948,949,950,951,952,953,954,955,956,957,958,959,960,961,962,963,964,965,966,967,968,969,970,971,972,973,974,975,976,977,978,979,980,981,982,983,984,985,986,987,988,989,990,991,992,993,994,995,996,997,998,999,1000,1001,1002,1003,1004,1005,1006,1007,1008,1009,1010,1011,1012,1013,1014,1015,1016,1017,1018,1019,1020,1021,1022,1023,1024,1025,1026,1027,1028,1029,1030,1031,1032,1033,1034,1035,1036,1037,1038,1039,1040,1041,1042,1043,1044,1045,1046,1047,1048,1049,1050,1051,1052,1053,1054,1055,1056,1057,1058,1059,1060,1061,1062,1063,1064,1065,1066,1067,1068,1069,1070,1071,1072,1073,1074,1075,1076,1077,1078,1079,1080,1081,1082,1083,1084,1085,1086,1087,1088,1089,1090,1091,1092,1093,1094,1095,1096,1097,1098,1099,1100,1101,1102,1103,1104,1105,1106,1107,1108,1109,1110,1111,1112,1113,1114,1115,1116,1117,1118,1119,1120,1121,1122,1123,1124,1125,1126,1127,1128,1129,1130,1131,1132,1133,1134,1135,1136,1137,1138,1139,1140,1141,1142,1143,1144,1145,1146,1147,1148,1149,1150,1151,1152,1153,1154,1155,1156,1157,1158,1159,1160,1161,1162,1163,1164,1165,1166,1167,1168,1169,1170,1171,1172,1173,1174,1175,1176,1177,1178,1179,1180,1181,1182,1183,1184,1185,1186,1187,1188,1189,1190,1191,1192,1193,1194,1195,1196,1197,1198,1199,1200,1201,1202,1203,1204,1205,1206,1207,1208,1209,1210,1211,1212,1213,1214,1215,1216,1217,1218,1219,1220,1221,1222,1223,1224,1225,1226,1227,1228,1229,1230,1231,1232,1233,1234,1235,1236,1237,1238,1239,1240,1241,1242,1243,1244,1245,1246,1247,1248,1249,1250,1251,1252,1253,1254,1255,1256,1257,1258,1259,1260,1261,1262,1263,1264,1265,1266,1267,1268,1269,1270,1271,1272,1273,1274,1275,1276,1277,1278,1279,1280,1281,1282,1283,1284,1285,1286,1287,1288,1289,1290,1291,1292,1293,1294,1295,1296,1297,1298,1299,1300,1301,1302,1303,1304,1305,1306,1307,1308,1309,1310,1311,1312,1313,1314,1315,1316,1317,1318,1319,1320,1321,1322,1323,1324,1325,1326,1327,1328,1329,1330,1331,1332,1333,1334,1335,1336,1337,1338,1339,1340,1341,1342,1343,1344,1345,1346,1347,1348,1349,1350,1351,1352,1353,1354,1355,1356,1357,1358,1359,1360,1361,1362,1363,1364,1365,1366,1367,1368,1369,1370,1371,1372,1373,1374,1375,1376,1377,1378,1379,1380,1381,1382,1383,1384,1385,1386,1387,1388,1389,1390,1391,1392,1393,1394,1395,1396,1397,1398,1399,1400,1401,1402,1403,1404,1405,1406,1407,1408,1409,1410,1411,1412,1413,1414,1415,1416,1417,1418,1419,1420,1421,1422,1423,1424,1425,1426,1427,1428,1429,1430,1431,1432,1433,1434,1435,1436,1437,1438,1439,1440,1441,1442,1443,1444,1445,1446,1447,1448,1449,1450,1451,1452,1453,1454,1455,1456,1457,1458,1459,1460,1461,1462,1463,1464,1465,1466,1467,1468,1469,1470,1471,1472,1473,1474,1475,1476,1477,1478,1479,1480,1481,1482,1483,1484,1485,1486,1487,1488,1489,1490,1491,1492,1493,1494,1495,1496,1497,1498,1499,1500,1501,1502,1503,1504,1505,1506,1507,1508,1509,1510,1511,1512,1513,1514,1515,1516,1517,1518,1519,1520,1521,1522,1523,1524,1525,1526,1527,1528,1529,1530,1531,1532,1533,1534,1535,1536,1537,1538,1539,1540,1541,1542,1543,1544,1545,1546,1547,1548,1549,1550,1551,1552,1553,1554,1555,1556,1557,1558,1559,1560,1561,1562,1563,1564,1565,1566,1567,1568,1569,1570,1571,1572,1573,1574,1575,1576,1577,1578,1579,1580,1581,1582,1583,1584,1585,1586,1587,1588,1589,1590,1591,1592,1593,1594,1595,1596,1597,1598,1599,1600,1601,1602,1603,1604,1605,1606,1607,1608,1609,1610,1611,1612,1613,1614,1615,1616,1617,1618,1619,1620,1621,1622,1623,1624,1625,1626,1627,1628,1629,1630,1631,1632,1633,1634,1635,1636,1637,1638,1639,1640,1641,1642,1643,1644,1645,1646,1647,1648,1649,1650,1651,1652,1653,1654,1655,1656,1657,1658,1659,1660,1661,1662,1663,1664,1665,1666,1667,1668,1669,1670,1671,1672,1673,1674,1675,1676,1677,1678,1679,1680,1681,1682,1683,1684,1685,1686,1687,1688,1689,1690,1691,1692,1693,1694,1695,1696,1697,1698,1699,1700,1701,1702,1703,1704,1705,1706,1707,1708,1709,1710,1711,1712,1713,1714,1715,1716,1717,1718,1719,1720,1721,1722,1723,1724,1725,1726,1727,1728,1729,1730,1731,1732,1733,1734,1735,1736,1737,1738,1739,1740,1741,1742,1743,1744,1745,1746,1747,1748,1749,1750,1751,1752,1753,1754,1755,1756,1757,1758,1759,1760,1761,1762,1763,1764,1765,1766,1767,1768,1769,1770,1771,1772,1773,1774,1775,1776,1777,1778,1779,1780,1781,1782,1783,1784,1785,1786,1787,1788,1789,1790,1791,1792,1793,1794,1795,1796,1797,1798,1799,1800,1801,1802,1803,1804,1805,1806,1807,1808,1809,1810,1811,1812,1813,1814,1815,1816,1817,1818,1819,1820,1821,1822,1823,1824,1825,1826,1827,1828,1829,1830,1831,1832,1833,1834,1835,1836,1837,1838,1839,1840,1841,1842,1843,1844,1845,1846,1847,1848,1849,1850,1851,1852,1853,1854,1855,1856,1857,1858,1859,1860,1861,1862,1863,1864,1865,1866,1867,1868,1869,1870,1871,1872,1873,1874,1875,1876,1877,1878,1879,1880,1881,1882,1883,1884,1885,1886,1887,1888,1889,1890,1891,1892,1893,1894,1895,1896,1897,1898,1899,1900,1901,1902,1903,1904,1905,1906,1907,1908,1909,1910,1911,1912,1913,1914,1915,1916,1917,1918,1919,1920,1921,1922,1923,1924,1925,1926,1927,1928,1929,1930,1931,1932,1933,1934,1935,1936,1937,1938,1939,1940,1941,1942,1943,1944,1945,1946,1947,1948,1949,1950,1951,1952,1953,1954,1955,1956,1957,1958,1959,1960,1961,1962,1963,1964,1965,1966,1967,1968,1969,1970,1971,1972,1973,1974,1975,1976,1977,1978,1979,1980,1981,1982,1983,1984,1985,1986,1987,1988,1989,1990,1991,1992,1993,1994,1995,1996,1997,1998,1999,2000,2001,2002,2003,2004,2005,2006,2007,2008,2009,2010,2011,2012,2013,2014,2015,2016,2017,2018,2019,2020,2021,2022,2023,2024,2025,2026,2027,2028,2029,2030,2031,2032,2033,2034,2035,2036,2037,2038,2039,2040,2041,2042,2043,2044,2045,2046,2047],\"xaxis\":\"x\",\"y\":[-0.6033958792686462,-0.31149494647979736,-0.21057458221912384,-0.16562816500663757,-0.16521412134170532,-0.16455577313899994,-0.14445212483406067,-0.12185119837522507,-0.11951565742492676,-0.11702213436365128,-0.11349071562290192,-0.111394502222538,-0.09946679323911667,-0.09574508666992188,-0.09440818428993225,-0.09090440720319748,-0.08115145564079285,-0.07940669357776642,-0.07685365527868271,-0.07594120502471924,-0.07532858103513718,-0.0740765631198883,-0.07326208800077438,-0.07317975163459778,-0.07310781627893448,-0.07209232449531555,-0.0716722309589386,-0.06989538669586182,-0.0686962679028511,-0.06862448900938034,-0.06700707226991653,-0.0653308853507042,-0.06379242986440659,-0.06350760906934738,-0.061733148992061615,-0.06166892498731613,-0.06085330992937088,-0.06062319874763489,-0.059969186782836914,-0.05902688950300217,-0.05858337879180908,-0.05416354537010193,-0.05414102226495743,-0.05397837609052658,-0.05313220992684364,-0.052769847214221954,-0.052061185240745544,-0.050920773297548294,-0.04954272136092186,-0.048981208354234695,-0.04850725084543228,-0.047763943672180176,-0.046935733407735825,-0.045491207391023636,-0.04418486729264259,-0.042328644543886185,-0.04138300567865372,-0.04101642966270447,-0.040700361132621765,-0.04015059396624565,-0.039763156324625015,-0.039748720824718475,-0.0390423908829689,-0.03792055323719978,-0.03789736330509186,-0.03773251548409462,-0.03758709505200386,-0.03723788633942604,-0.03706599771976471,-0.036987751722335815,-0.03600093722343445,-0.0359494611620903,-0.035611752420663834,-0.035538867115974426,-0.035366982221603394,-0.03529400750994682,-0.035131700336933136,-0.03394994139671326,-0.03267485275864601,-0.03255583345890045,-0.03226839005947113,-0.03160177543759346,-0.031455148011446,-0.03140001371502876,-0.030946878716349602,-0.03092333860695362,-0.030655469745397568,-0.030190205201506615,-0.030066875740885735,-0.03000195510685444,-0.029902150854468346,-0.029853282496333122,-0.029760729521512985,-0.029554180800914764,-0.029505204409360886,-0.029501639306545258,-0.029309773817658424,-0.02907978929579258,-0.02902274578809738,-0.02864731289446354,-0.02849651128053665,-0.028393171727657318,-0.028219880536198616,-0.027983034029603004,-0.027409903705120087,-0.02718358300626278,-0.027021246030926704,-0.02667362056672573,-0.02623932808637619,-0.026212535798549652,-0.025917258113622665,-0.025893254205584526,-0.025630876421928406,-0.025411393493413925,-0.025328706949949265,-0.025185272097587585,-0.0248796958476305,-0.024558208882808685,-0.02397376298904419,-0.02379484847187996,-0.02345796301960945,-0.02329510636627674,-0.023012306541204453,-0.022991500794887543,-0.022780396044254303,-0.022659054026007652,-0.022380443289875984,-0.021967383101582527,-0.021839380264282227,-0.021624119952321053,-0.0214423518627882,-0.021403510123491287,-0.020477576181292534,-0.020318064838647842,-0.019939500838518143,-0.019893422722816467,-0.0195954367518425,-0.01940261572599411,-0.01924789510667324,-0.019212570041418076,-0.01902242936193943,-0.018833572044968605,-0.018590135499835014,-0.018388669937849045,-0.01807408034801483,-0.018021300435066223,-0.017928846180438995,-0.017879247665405273,-0.017623860388994217,-0.01757754758000374,-0.01749432645738125,-0.017416365444660187,-0.01740332320332527,-0.017064442858099937,-0.01696649380028248,-0.01689443551003933,-0.01686316728591919,-0.01685451716184616,-0.016440806910395622,-0.016424797475337982,-0.016376987099647522,-0.016078555956482887,-0.015922309830784798,-0.015881450846791267,-0.015783920884132385,-0.015431339852511883,-0.01529774907976389,-0.015029272995889187,-0.014916682615876198,-0.014909949153661728,-0.014751851558685303,-0.014734889380633831,-0.0146526750177145,-0.01462836004793644,-0.014447151683270931,-0.014267721213400364,-0.014212839305400848,-0.013951070606708527,-0.013914682902395725,-0.013761051930487156,-0.013680797070264816,-0.013643018901348114,-0.013550778850913048,-0.013421483337879181,-0.013326261192560196,-0.01321719866245985,-0.013063544407486916,-0.013025249354541302,-0.012875876389443874,-0.012803768739104271,-0.012773267924785614,-0.012590494006872177,-0.01257634162902832,-0.01233628299087286,-0.012272786349058151,-0.01210720557719469,-0.012002570554614067,-0.011927556246519089,-0.011743385344743729,-0.011712081730365753,-0.011605777777731419,-0.011433519423007965,-0.011426949873566628,-0.011424516327679157,-0.011245611123740673,-0.011017024517059326,-0.01099293865263462,-0.010964048095047474,-0.010823500342667103,-0.010797991417348385,-0.010611805133521557,-0.01056017354130745,-0.01054630521684885,-0.01053635124117136,-0.01022328156977892,-0.01007717102766037,-0.010010958649218082,-0.010007232427597046,-0.009987927041947842,-0.009977476671338081,-0.009977050125598907,-0.009900128468871117,-0.009889070875942707,-0.009851261042058468,-0.009774422273039818,-0.009733128361403942,-0.009603317826986313,-0.009602097794413567,-0.009489469230175018,-0.009423647075891495,-0.009358917362987995,-0.009327681735157967,-0.009239818900823593,-0.009159034118056297,-0.009138593450188637,-0.009127773344516754,-0.009127701632678509,-0.009076266549527645,-0.009063259698450565,-0.0090274503454566,-0.009000133723020554,-0.008904684334993362,-0.008809459395706654,-0.00874639954417944,-0.008710849098861217,-0.008689030073583126,-0.008664070628583431,-0.008639434352517128,-0.008628712967038155,-0.008610662072896957,-0.008433126844465733,-0.00841442123055458,-0.008232813328504562,-0.008150937035679817,-0.008039858192205429,-0.008004081435501575,-0.007948043756186962,-0.0079186437651515,-0.007857739925384521,-0.0078033865429461,-0.007788058370351791,-0.007785856258124113,-0.007730039767920971,-0.007685090880841017,-0.007592542562633753,-0.007550172042101622,-0.007543541025370359,-0.007484755013138056,-0.00748227396979928,-0.007434416562318802,-0.007416652515530586,-0.007404318079352379,-0.007380306720733643,-0.00730169890448451,-0.0072755650617182255,-0.007269497029483318,-0.007246576715260744,-0.007215344347059727,-0.007158799562603235,-0.00715161208063364,-0.007145253475755453,-0.0071440041065216064,-0.007135407533496618,-0.007123963441699743,-0.007098472211509943,-0.007081198506057262,-0.007001414429396391,-0.006964841391891241,-0.006925087422132492,-0.006918388418853283,-0.00686708465218544,-0.0068169827573001385,-0.006800340488553047,-0.006760535761713982,-0.006679042708128691,-0.0066603790037333965,-0.0065770759247243404,-0.0065609668381512165,-0.006468965206295252,-0.006408865563571453,-0.006390796508640051,-0.006380749866366386,-0.006363266613334417,-0.006306407507508993,-0.006294114515185356,-0.006233295891433954,-0.006203154101967812,-0.006192418280988932,-0.006157700438052416,-0.006146290339529514,-0.0061348252929747105,-0.006128626875579357,-0.006081567611545324,-0.006056306418031454,-0.006043621804565191,-0.005978562403470278,-0.005926954094320536,-0.005900917109102011,-0.005895181093364954,-0.0058855172246694565,-0.0058633494190871716,-0.0058381520211696625,-0.005819580052047968,-0.005811347160488367,-0.005801255814731121,-0.005779219791293144,-0.00577494315803051,-0.0057584987953305244,-0.005733108147978783,-0.005711323581635952,-0.0056532700546085835,-0.00563226779922843,-0.00563204288482666,-0.005629906430840492,-0.00562540115788579,-0.005619331728667021,-0.005590521264821291,-0.00552914896979928,-0.0055240001529455185,-0.005499620456248522,-0.005470235366374254,-0.005400650668889284,-0.005383308045566082,-0.0053793275728821754,-0.005358029156923294,-0.005330758169293404,-0.005316315684467554,-0.00525654898956418,-0.005240178667008877,-0.005209252703934908,-0.005145607516169548,-0.005090005230158567,-0.00508522056043148,-0.0050179376266896725,-0.004958620294928551,-0.004943688865751028,-0.004908674396574497,-0.004859492648392916,-0.004847690928727388,-0.004737452138215303,-0.004734775982797146,-0.004730916116386652,-0.004724489524960518,-0.004694255068898201,-0.004683545790612698,-0.0046580079942941666,-0.004629296716302633,-0.004627301823347807,-0.004602926783263683,-0.004591107368469238,-0.004583329427987337,-0.004567594733089209,-0.004561771173030138,-0.004551510792225599,-0.004518311005085707,-0.004515142645686865,-0.004499733913689852,-0.004498744383454323,-0.004424081649631262,-0.0043940795585513115,-0.004388021305203438,-0.004356319084763527,-0.004327442497014999,-0.004305241163820028,-0.004301657900214195,-0.004301608074456453,-0.004287635907530785,-0.004254916682839394,-0.004244478419423103,-0.004200604744255543,-0.0041953446343541145,-0.00415655504912138,-0.004153379704803228,-0.00414631050080061,-0.004134745337069035,-0.0041116089560091496,-0.004082872532308102,-0.004064538981765509,-0.004052996169775724,-0.00403245072811842,-0.004022760782390833,-0.004015875980257988,-0.004006571136415005,-0.003967976663261652,-0.003951360937207937,-0.003930940758436918,-0.003911033738404512,-0.0038835429586470127,-0.0038212714716792107,-0.0038037276826798916,-0.0037911010440438986,-0.003779417835175991,-0.0037673788610845804,-0.00374769838526845,-0.003724495181813836,-0.0037068992387503386,-0.003654695115983486,-0.003636918030679226,-0.003634045133367181,-0.003613907378166914,-0.0035867018159478903,-0.003572864457964897,-0.0035704628098756075,-0.0035296049900352955,-0.0035074669867753983,-0.003485489869490266,-0.003455868922173977,-0.003449941985309124,-0.003440814558416605,-0.0034399940632283688,-0.003437716979533434,-0.003436175873503089,-0.0034200253430753946,-0.0034187184646725655,-0.003417173633351922,-0.0033949248027056456,-0.003358015790581703,-0.003357263281941414,-0.0033207046799361706,-0.003318207571282983,-0.003310717409476638,-0.003304817480966449,-0.003295409493148327,-0.003291895380243659,-0.0032871586736291647,-0.0032858788035809994,-0.003270040964707732,-0.003246401436626911,-0.0032071920577436686,-0.003193904645740986,-0.003162452019751072,-0.003161928616464138,-0.0031539485789835453,-0.003147068666294217,-0.003111122874543071,-0.003069506958127022,-0.0030653805006295443,-0.003058111760765314,-0.003052960615605116,-0.003043757053092122,-0.0030295830219984055,-0.0030120261944830418,-0.0029864609241485596,-0.002976186340674758,-0.002969434019178152,-0.002961368067190051,-0.0029391972348093987,-0.0029323261696845293,-0.002902377163991332,-0.002890427131205797,-0.0028735569212585688,-0.002870948286727071,-0.002868093317374587,-0.0028468447271734476,-0.0028466610237956047,-0.002807967597618699,-0.0028071331325918436,-0.002791666192933917,-0.0027809259481728077,-0.002773609943687916,-0.002766401506960392,-0.002719166222959757,-0.0027081549633294344,-0.00270337937399745,-0.002697895746678114,-0.0026906700804829597,-0.0026819007471203804,-0.002681734273210168,-0.002678625052794814,-0.002670712536200881,-0.0026567766908556223,-0.002647648798301816,-0.0026449740398675203,-0.0026327981613576412,-0.002627699403092265,-0.0026206739712506533,-0.0025999536737799644,-0.002594242338091135,-0.0025885466020554304,-0.0025823605246841908,-0.002578410552814603,-0.0025690498296171427,-0.00256733619607985,-0.0025455071590840816,-0.002543552778661251,-0.002543194917961955,-0.0025108151603490114,-0.002500157104805112,-0.002484892960637808,-0.0024661957286298275,-0.00246274471282959,-0.0024490647483617067,-0.0024379382375627756,-0.0024363696575164795,-0.0024334772024303675,-0.002427024533972144,-0.0023939041420817375,-0.0023830614518374205,-0.0023766288068145514,-0.0023750748950988054,-0.0023699623998254538,-0.0023420401848852634,-0.0023181948345154524,-0.002298275474458933,-0.002289138501510024,-0.002284430433064699,-0.002276864368468523,-0.002262403257191181,-0.0022592011373490095,-0.002255860948935151,-0.0022552362643182278,-0.002251066267490387,-0.0022484816145151854,-0.0022465609945356846,-0.002244400093331933,-0.002232002094388008,-0.0022093206644058228,-0.002209070138633251,-0.0021952521055936813,-0.0021915596444159746,-0.0021884401794523,-0.0021851721685379744,-0.0021797874942421913,-0.0021737865172326565,-0.0021714491304010153,-0.0021706614643335342,-0.0021692728623747826,-0.0021635014563798904,-0.002158939838409424,-0.0021421117708086967,-0.0021328956354409456,-0.0021324055269360542,-0.002112757647410035,-0.002108900109305978,-0.002104811603203416,-0.0020959184039384127,-0.0020718162413686514,-0.002069187816232443,-0.0020648872014135122,-0.002040604595094919,-0.002039868850260973,-0.0020365857053548098,-0.0020212081726640463,-0.0020054492633789778,-0.0020049535669386387,-0.002003365196287632,-0.0019982284866273403,-0.0019920000340789557,-0.0019800979644060135,-0.001965048722922802,-0.0019506380194798112,-0.0019504273077473044,-0.0019272076897323132,-0.0019057141616940498,-0.0019013232085853815,-0.001898018759675324,-0.0018840726697817445,-0.001878556446172297,-0.0018461893778294325,-0.0018443998415023088,-0.0018437521066516638,-0.0018287455895915627,-0.0018225363455712795,-0.0018176347948610783,-0.0017864495748654008,-0.0017768230754882097,-0.0017749537946656346,-0.0017412081360816956,-0.0017256418941542506,-0.0017122033750638366,-0.001699238084256649,-0.0016862067859619856,-0.0016766353510320187,-0.001676275976933539,-0.0016752913361415267,-0.0016628369921818376,-0.001653752289712429,-0.0016525669489055872,-0.0016475155716761947,-0.0016357398126274347,-0.0016189328161999583,-0.0016019305912777781,-0.0015912848757579923,-0.0015871557407081127,-0.0015659156488254666,-0.001558272517286241,-0.0015544025227427483,-0.0015440932475030422,-0.0015388033352792263,-0.001537160249426961,-0.0015328306471928954,-0.0015283861430361867,-0.0015187507960945368,-0.0015136093134060502,-0.0015017359983175993,-0.0015003990847617388,-0.001466287998482585,-0.0014622363960370421,-0.0014591105282306671,-0.0014503815909847617,-0.0014414781471714377,-0.0014344907831400633,-0.0014324134681373835,-0.001425896305590868,-0.0014170536305755377,-0.0014026927528902888,-0.0014010937884449959,-0.0013902053469792008,-0.00138786097522825,-0.00138463347684592,-0.0013812837423756719,-0.0013798171421512961,-0.001378709333948791,-0.0013745614560320973,-0.0013651864137500525,-0.0013598958030343056,-0.0013575158081948757,-0.0013423466589301825,-0.0013385863276198506,-0.0013375887647271156,-0.0013367822393774986,-0.0013350668596103787,-0.0013319122372195125,-0.0013267871690914035,-0.001318063703365624,-0.0013175964122638106,-0.0013048448599874973,-0.0013040617341175675,-0.0013032120186835527,-0.0012790152104571462,-0.0012757640797644854,-0.0012618325417861342,-0.0012569734826683998,-0.0012482514139264822,-0.0012478164862841368,-0.0012337141670286655,-0.0012264699907973409,-0.0012086614733561873,-0.001208196859806776,-0.0011707255616784096,-0.0011666066711768508,-0.0011584905441850424,-0.001158007769845426,-0.001157388323917985,-0.0011523584835231304,-0.0011504768626764417,-0.0011462642578408122,-0.0011361971264705062,-0.0011223104083910584,-0.0011211350793018937,-0.0011177209671586752,-0.0011158535489812493,-0.0011030767345800996,-0.0010923566296696663,-0.00107968517113477,-0.0010625526774674654,-0.0010564588010311127,-0.0010561246890574694,-0.0010202376870438457,-0.0010166919091716409,-0.0010154792107641697,-0.0010108472779393196,-0.00100624468177557,-0.001003510202281177,-0.001002397038973868,-0.0010021636262536049,-0.0010012059938162565,-0.0009997031884267926,-0.000989881926216185,-0.0009823314612731338,-0.0009775386424735188,-0.0009765664581209421,-0.0009699069196358323,-0.0009593747090548277,-0.0009416569955646992,-0.0009389244951307774,-0.0009372794884257019,-0.000935920572374016,-0.0009333264897577465,-0.0009304058621637523,-0.0009285241249017417,-0.0009283509571105242,-0.0009169472614303231,-0.0009116677683778107,-0.0009039351134561002,-0.0008963370928540826,-0.0008942201966419816,-0.0008920924155972898,-0.0008848166326060891,-0.000883251428604126,-0.000880471314303577,-0.0008685318171046674,-0.0008476211805827916,-0.0008442598627880216,-0.0008391509763896465,-0.0008340517524629831,-0.0008329603006131947,-0.0008327182149514556,-0.0008263762574642897,-0.0008197333663702011,-0.0008176953997462988,-0.0008066391455940902,-0.000803473754785955,-0.000798502063844353,-0.0007975460030138493,-0.0007922140648588538,-0.0007855095900595188,-0.0007779367151670158,-0.0007615866488777101,-0.0007533894968219101,-0.0007506904075853527,-0.0007478415500372648,-0.0007439727196469903,-0.000731482810806483,-0.0007258378900587559,-0.0007252985378727317,-0.000724976824130863,-0.0007225395529530942,-0.0007210884359665215,-0.0007194770150817931,-0.0007178578525781631,-0.0007120866794139147,-0.0007119327201507986,-0.0006992786657065153,-0.000696078292094171,-0.000685173028614372,-0.0006851323414593935,-0.0006843046867288649,-0.0006808792822994292,-0.0006803340511396527,-0.0006798087852075696,-0.0006737584481015801,-0.0006731378962285817,-0.0006717732758261263,-0.0006566785741597414,-0.0006487926584668458,-0.0006424595485441387,-0.0006317161023616791,-0.000629293848760426,-0.0006274224724620581,-0.0006160417106002569,-0.0006132316775619984,-0.0006116505246609449,-0.0006101804319769144,-0.0006022966117598116,-0.000598271144554019,-0.0005979219567961991,-0.0005978292902000248,-0.000596972880885005,-0.000593748758547008,-0.0005916224909015,-0.0005621503223665059,-0.0005532133509404957,-0.0005421841633506119,-0.0005389581783674657,-0.0005321566131897271,-0.0005270007532089949,-0.0005158372223377228,-0.0005124572198837996,-0.0005105045274831355,-0.0005067713791504502,-0.000501509930472821,-0.0004969306173734367,-0.0004960197838954628,-0.0004896440077573061,-0.0004885746748186648,-0.0004815872816834599,-0.00047869159607216716,-0.00047499590436927974,-0.000471470964839682,-0.000469042279291898,-0.00046137525350786746,-0.0004590736934915185,-0.0004553118487820029,-0.0004451786808203906,-0.00043963902862742543,-0.0004393084382172674,-0.00043727399315685034,-0.0004366650537122041,-0.00042982399463653564,-0.00042958930134773254,-0.00042517774272710085,-0.00042397298966534436,-0.00042085765744559467,-0.00041931605665013194,-0.00041307517676614225,-0.00041297427378594875,-0.0004078865749761462,-0.0004070052527822554,-0.0003993884602095932,-0.00039749883580952883,-0.00039668992394581437,-0.00039595880662091076,-0.000390713510569185,-0.0003906871424987912,-0.0003888414066750556,-0.00038839905755594373,-0.00038757771835662425,-0.00038014299934729934,-0.00037492267438210547,-0.00037337973481044173,-0.00036972598172724247,-0.00036468825419433415,-0.00036230526166036725,-0.0003606598766054958,-0.0003594011068344116,-0.00035346776712685823,-0.00035259537980891764,-0.0003497576108202338,-0.00034911700640805066,-0.00034718020469881594,-0.0003430901560932398,-0.0003429411444813013,-0.00033639854518696666,-0.00033584446646273136,-0.0003325024154037237,-0.00033237674506381154,-0.0003269094158895314,-0.0003268773143645376,-0.0003263657563365996,-0.00032628083135932684,-0.0003260084195062518,-0.0003173899604007602,-0.0003168281982652843,-0.00030772670288570225,-0.00030658385367132723,-0.00030345641425810754,-0.0002978441189043224,-0.00029162384453229606,-0.00029139703838154674,-0.00028897731681354344,-0.0002807432319968939,-0.00027776992646977305,-0.00027762018726207316,-0.0002639429294504225,-0.0002568767231423408,-0.00025617971550673246,-0.00024811073672026396,-0.00024409219622612,-0.00023591793433297426,-0.0002279697364429012,-0.00022646412253379822,-0.00022592992172576487,-0.00022358402202371508,-0.00022239438840188086,-0.00022125363466329873,-0.00022074431763030589,-0.0002195863489760086,-0.00021732992900069803,-0.00021143651974853128,-0.00021068780915811658,-0.00020958975073881447,-0.000209401550819166,-0.00020730025426018983,-0.00020699955348391086,-0.00020657852292060852,-0.00020452187163755298,-0.00020115509687457234,-0.0001950212608790025,-0.00019465095829218626,-0.00019300162966828793,-0.00019124425307381898,-0.0001887571852421388,-0.00018860660202335566,-0.00018548339721746743,-0.00018501505837775767,-0.00018344484851695597,-0.00018238567281514406,-0.00017814450256992131,-0.0001749367220327258,-0.00017237536667380482,-0.000171295105246827,-0.0001681786816334352,-0.00016661814879626036,-0.00015956841525621712,-0.00015868767513893545,-0.00015803218411747366,-0.00015406675811391324,-0.0001529258443042636,-0.0001526145642856136,-0.00015249632997438312,-0.00015177525347098708,-0.0001497673220001161,-0.0001475983881391585,-0.00014678880688734353,-0.0001463390071876347,-0.0001444201188860461,-0.00014396131155081093,-0.00014387525152415037,-0.00014330931298900396,-0.0001418251486029476,-0.00013990103616379201,-0.00013841241889167577,-0.00013621867401525378,-0.0001332380634266883,-0.0001305852783843875,-0.00012378409155644476,-0.00012210033310111612,-0.00011495799117255956,-0.00011485614231787622,-0.00011403798998799175,-0.00011331513815093786,-0.00011262640327913687,-0.00011107891623396426,-0.00010524920799070969,-0.0001026845711749047,-0.00010218397073913366,-0.00010149784066015854,-0.00010068230039905757,-0.0001003743673209101,-9.900071017909795e-05,-9.888552449410781e-05,-9.803704597288743e-05,-9.125508222496137e-05,-9.00810191524215e-05,-8.960641571320593e-05,-8.777193579589948e-05,-8.599363354733214e-05,-8.517294918419793e-05,-8.218489529099315e-05,-8.048609015531838e-05,-7.462926441803575e-05,-7.445767550962046e-05,-7.192909833975136e-05,-7.093116437317804e-05,-7.016092422418296e-05,-6.875738472444937e-05,-6.776146619813517e-05,-6.709560693707317e-05,-6.661705992883071e-05,-6.617166218347847e-05,-6.55797848594375e-05,-6.417654367396608e-05,-5.540996789932251e-05,-5.4261385230347514e-05,-5.308151230565272e-05,-5.034119021729566e-05,-5.0093382014892995e-05,-4.944987449562177e-05,-4.735775291919708e-05,-4.438780160853639e-05,-4.257254477124661e-05,-4.25209836976137e-05,-3.520086465869099e-05,-3.4484492061892524e-05,-3.317139999126084e-05,-3.196776015101932e-05,-3.0251369025791064e-05,-2.95592108159326e-05,-2.9356777304201387e-05,-2.9271543098730035e-05,-2.86187969322782e-05,-2.6957168302033097e-05,-2.5278181055909954e-05,-2.363122985116206e-05,-2.356052391405683e-05,-2.322316140634939e-05,-2.251170553790871e-05,-2.170011430280283e-05,-1.953773244167678e-05,-1.9332841475261375e-05,-1.7696171198622324e-05,-1.6646385120111518e-05,-1.648254692554474e-05,-1.6250611224677414e-05,-1.5903413441265002e-05,-1.439400057279272e-05,-9.517148100712802e-06,-8.89070361154154e-06,-7.008909960859455e-06,-6.077214948163601e-06,-6.017834039084846e-06,-4.577413164952304e-06,-4.261434241925599e-06,-2.3631007479707478e-06,-1.6628205230517779e-06,-5.288422357807576e-07,2.6226043559063328e-08,3.2454730103381735e-07,3.8459896245512937e-07,1.0775775081128813e-06,1.3005733308091294e-06,1.5290826240743627e-06,1.8526613985159202e-06,2.8440356345527107e-06,2.9593707040476147e-06,4.835203071706928e-06,6.953775937290629e-06,8.075460755208042e-06,8.091330528259277e-06,8.260011782112997e-06,8.266344593721442e-06,9.001046237244736e-06,9.031593435793184e-06,9.343549209006596e-06,1.110501580114942e-05,1.1402517884562258e-05,1.1584684216359165e-05,1.2195185263408348e-05,1.2320503628870938e-05,1.3382956240093336e-05,1.8129721865989268e-05,2.1466239559231326e-05,2.310596391907893e-05,2.6681424060370773e-05,2.74789326795144e-05,2.7928725103265606e-05,2.9652044759131968e-05,3.279529482824728e-05,3.333762288093567e-05,3.351107079652138e-05,3.359734910191037e-05,3.399543493287638e-05,3.4674703783821315e-05,3.6464931326918304e-05,3.6688892578240484e-05,3.6704688682220876e-05,3.998875763500109e-05,4.015311424154788e-05,4.063859523739666e-05,4.122830796404742e-05,4.13881971326191e-05,4.184939098195173e-05,4.436895324033685e-05,4.547528806142509e-05,4.564084156299941e-05,4.670776252169162e-05,4.6885685151210055e-05,4.705682295025326e-05,5.1744209486059844e-05,5.4345055104931816e-05,5.844443876412697e-05,6.011374352965504e-05,6.0189442592673004e-05,6.061114254407585e-05,6.114393181633204e-05,6.122059858171269e-05,6.171718268888071e-05,6.372295320034027e-05,6.646908877883106e-05,6.77099815220572e-05,6.923139153514057e-05,7.073238521115854e-05,7.145248673623428e-05,7.692404324188828e-05,7.776051643304527e-05,7.789954543113708e-05,7.970325532369316e-05,8.26425093691796e-05,8.291035919683054e-05,8.347391849383712e-05,8.385651017306373e-05,8.618891297373921e-05,9.075715934159234e-05,9.13783151190728e-05,9.366706217406318e-05,9.447850607102737e-05,9.781524568097666e-05,0.00010028809629147872,0.00010124310938408598,0.00010173402552027255,0.00010262720024911687,0.00010288260818924755,0.00010305263276677579,0.00010330676741432399,0.00010465688683325425,0.00010475404269527644,0.00010631136683514342,0.00010753482638392597,0.00010824859054991975,0.00010914631275227293,0.0001133705663960427,0.00011565245949896052,0.00011792220175266266,0.00011826679110527039,0.00012016073014819995,0.00012125454668421298,0.00012128278467571363,0.0001213700306834653,0.00012309617886785418,0.00013336360279936343,0.00013676747039426118,0.0001376497675664723,0.00013806938659399748,0.00013883263454772532,0.00014265954087022692,0.0001447435497539118,0.00014741018821951002,0.00014847554848529398,0.00014910459867678583,0.00014924511197023094,0.00015079654986038804,0.00015106797218322754,0.00015145070210564882,0.00015365816943813115,0.00015542663459200412,0.0001679861597949639,0.00017214551917277277,0.000172889675013721,0.0001806567597668618,0.00018119723245035857,0.00018126420036423951,0.00018134481797460467,0.00018217512115370482,0.00018587603699415922,0.00018612183339428157,0.0001864694058895111,0.00018659979104995728,0.0001866003149189055,0.00018749154696706682,0.00019242041162215173,0.00019613288168329746,0.00019651756156235933,0.000196634151507169,0.00020402431255206466,0.00021026238391641527,0.00021026864123996347,0.00021150746033526957,0.0002132200461346656,0.0002141540462616831,0.0002156140690203756,0.00021590694086626172,0.00022035621805116534,0.00022148773132357746,0.00022221996914595366,0.0002278289175592363,0.00022940538474358618,0.00023297571169678122,0.000233629863942042,0.0002370806032558903,0.00024001285783015192,0.00024112053506541997,0.000247657677391544,0.00024787738220766187,0.00025135918986052275,0.0002521071583032608,0.00025628870935179293,0.00026340133626945317,0.0002647309738676995,0.00026560440892353654,0.0002668866654857993,0.0002685082727111876,0.000271638942649588,0.0002777218178380281,0.000279175495961681,0.0002827495336532593,0.0002832207828760147,0.00028351499349810183,0.00028361575095914304,0.00028911055414937437,0.0002994310052599758,0.0003009671636391431,0.00030263871303759515,0.00030281886574812233,0.00030682532815262675,0.00031820297590456903,0.0003324642893858254,0.00033411054755561054,0.00033965439070016146,0.0003461787709966302,0.0003473145479802042,0.0003474517143331468,0.00034755945671349764,0.0003484589688014239,0.0003491272800602019,0.0003493315016385168,0.0003508484223857522,0.00035267247585579753,0.00035621144343167543,0.00036347671994008124,0.0003727237053681165,0.00037303700810298324,0.0003738377126865089,0.0003741304681170732,0.00037528134998865426,0.0003759762039408088,0.0003785206354223192,0.00038269825745373964,0.0003833626979030669,0.0003838519041892141,0.0003863063466269523,0.00038978466182015836,0.0003949645906686783,0.0003990097320638597,0.00039952844963409007,0.0004065733519382775,0.00040710970642976463,0.00041330463136546314,0.00041766479262150824,0.0004178901144769043,0.0004235519445501268,0.00043192983139306307,0.0004382318875286728,0.0004402315535116941,0.0004458673356566578,0.0004514387983363122,0.0004646051675081253,0.0004655099764931947,0.00046704799751751125,0.0004670956695917994,0.00047446676762774587,0.00047766900388523936,0.00047889427514746785,0.0004824130155611783,0.00048735871678218246,0.0004934971802867949,0.0004944881657138467,0.0004985218401998281,0.0005001756362617016,0.0005065466975793242,0.0005124983727000654,0.0005167919443920255,0.0005213911063037813,0.0005295012961141765,0.0005307567771524191,0.0005428431322798133,0.0005453967023640871,0.0005603775498457253,0.0005633177352137864,0.0005662072217091918,0.0005683621857315302,0.0005777105106972158,0.0005831181188113987,0.0005903705023229122,0.0005973693914711475,0.0005974634550511837,0.0006053543766029179,0.0006059966399334371,0.0006066074129194021,0.0006071971147321165,0.000615579541772604,0.0006329589523375034,0.0006415426032617688,0.0006522233597934246,0.0006594901788048446,0.0006661535007879138,0.0006665819673798978,0.0006699258228763938,0.0006742882542312145,0.0006749049643985927,0.0006769027095288038,0.0006782585987821221,0.0006803192081861198,0.0006805686862207949,0.0006821838906034827,0.0006922946777194738,0.0006971072871237993,0.0007006137166172266,0.0007038662442937493,0.0007041811477392912,0.0007061345386318862,0.0007090631406754255,0.0007121245726011693,0.000730685016606003,0.0007376300054602325,0.0007385869976133108,0.0007432508282363415,0.0007440043264068663,0.0007638805545866489,0.0007692194194532931,0.00077729212353006,0.0007779294392094016,0.0007834931602701545,0.0007877821335569024,0.0007955352193675935,0.0008026947034522891,0.0008028580341488123,0.0008110388880595565,0.0008129516500048339,0.0008134155068546534,0.0008166212355718017,0.0008194693946279585,0.0008207966457121074,0.0008303208160214126,0.0008379007922485471,0.0008395796176046133,0.0008467263542115688,0.0008531183120794594,0.0008576010004617274,0.0008632990648038685,0.000867112132254988,0.0008717937744222581,0.0008953611832112074,0.0008972595678642392,0.0009006511536426842,0.0009072409011423588,0.0009083920158445835,0.0009108856902457774,0.0009241442894563079,0.0009418964618816972,0.0009427821496501565,0.0009480764856562018,0.000956188712734729,0.000957445299718529,0.0009607489919289947,0.0009762011468410492,0.0009875126415863633,0.0009888268541544676,0.0009999580215662718,0.0010104195680469275,0.0010171675821766257,0.0010221615666523576,0.001028398983180523,0.0010310348588973284,0.0010402356274425983,0.0010455879382789135,0.001047842320986092,0.0010674027726054192,0.0010728967608883977,0.001086007454432547,0.0010881939670071006,0.0010888486867770553,0.0010899326298385859,0.0010904964292421937,0.0011067381128668785,0.0011070965556427836,0.0011079196119681,0.001111406832933426,0.0011152075603604317,0.0011225939961150289,0.0011319559998810291,0.00114535097964108,0.0011490153847262263,0.0011490951292216778,0.0011511140037328005,0.0011534627992659807,0.0011541842250153422,0.0011652751127257943,0.0011697282316163182,0.0011708429083228111,0.0011711171828210354,0.0011781762586906552,0.0011804912937805057,0.001180665218271315,0.0011815079487860203,0.001188356545753777,0.0011891777394339442,0.0011896375799551606,0.0011928391177207232,0.0012069280492141843,0.001210368238389492,0.0012161167105659842,0.001216714852489531,0.001227052416652441,0.0012310154270380735,0.001235035015270114,0.001245445921085775,0.0012637183535844088,0.0012671616859734058,0.0012748223962262273,0.0012890839716419578,0.0013055630261078477,0.0013195016654208302,0.0013302068691700697,0.0013312995433807373,0.001336932647973299,0.0013423210475593805,0.001342618721537292,0.0013435403816401958,0.0013531551230698824,0.0013559263898059726,0.0013601892860606313,0.0013694919180124998,0.001371721038594842,0.001397575717419386,0.0014065257273614407,0.0014084273716434836,0.0014477568911388516,0.0014502742560580373,0.0014541632262989879,0.0014632090460509062,0.001464841770939529,0.0014716153964400291,0.00147473334800452,0.0014765578089281917,0.0015042552258819342,0.0015050354413688183,0.0015134465647861362,0.0015181394992396235,0.0015263520181179047,0.0015331512549892068,0.001562608522363007,0.0015743300318717957,0.0015752631006762385,0.0015848878538236022,0.001585331279784441,0.0015883984742686152,0.001592738088220358,0.0015957849100232124,0.0016023695934563875,0.001616220804862678,0.0016256049275398254,0.0016428451053798199,0.0016466033412143588,0.0016508136177435517,0.0016534007154405117,0.001654391293413937,0.00165953254327178,0.001670059864409268,0.0016836359864100814,0.0016848454251885414,0.001695282175205648,0.0017019131919369102,0.0017076373333111405,0.001711224322207272,0.001720957923680544,0.0017381096258759499,0.0017436177004128695,0.001756975194439292,0.0017703312914818525,0.0017718476010486484,0.0017879903316497803,0.0018027573823928833,0.001809073961339891,0.001812475617043674,0.0018142502522096038,0.0018156847218051553,0.0018201625207439065,0.001832645502872765,0.0018499622819945216,0.0018703356618061662,0.001883273129351437,0.0018934953259304166,0.0018980992026627064,0.0019049898255616426,0.0019096203614026308,0.001916111446917057,0.0019219523528590798,0.001922469469718635,0.001940264250151813,0.0019426437793299556,0.001945158583112061,0.0019546998664736748,0.001964300638064742,0.0019655320793390274,0.0019688764587044716,0.001990783493965864,0.002004638547077775,0.002007625298574567,0.0020107831805944443,0.0020125149749219418,0.0020223536994308233,0.0020297705195844173,0.0020328443497419357,0.0020334292203187943,0.0020480237435549498,0.0020653752144426107,0.0020676031708717346,0.002073366893455386,0.0020822351798415184,0.002086939290165901,0.002091727452352643,0.002109625143930316,0.0021104298066347837,0.0021111024543642998,0.002119647338986397,0.002144900383427739,0.0021465057507157326,0.0021506547927856445,0.0021527735516428947,0.002156973583623767,0.0021708630956709385,0.0021917810663580894,0.0021924367174506187,0.002212522318586707,0.002215002430602908,0.002228470053523779,0.002228592988103628,0.0022421160247176886,0.002249326789751649,0.002289088908582926,0.0022948032710701227,0.002296564169228077,0.0022996068000793457,0.0023057861253619194,0.002318865619599819,0.0023205671459436417,0.0023214565590023994,0.002350335707888007,0.0023516472429037094,0.002351867500692606,0.0023519706446677446,0.0023762050550431013,0.0024077058769762516,0.0024148153606802225,0.002419124823063612,0.002421495271846652,0.002423705067485571,0.002436792477965355,0.002445060294121504,0.002447599545121193,0.00246250513009727,0.0024695389438420534,0.0024744849652051926,0.00247636204585433,0.0024820533581078053,0.0024829136673361063,0.002491601975634694,0.0024935391265898943,0.0025085718370974064,0.002509643090888858,0.00252424250356853,0.0025448172818869352,0.0025534802116453648,0.0025666654109954834,0.0025742752477526665,0.002604891313239932,0.0026167428586632013,0.002639367477968335,0.002687816508114338,0.002699502743780613,0.002713138470426202,0.0027282314840704203,0.002733454341068864,0.0027352548204362392,0.00274917995557189,0.002796154236420989,0.0028105878736823797,0.0028133608866482973,0.0028389827348291874,0.002843932481482625,0.002844712696969509,0.0028469860553741455,0.0028788046911358833,0.00292556663043797,0.0029384633526206017,0.0029487055726349354,0.002982543781399727,0.0029984689317643642,0.0030143694020807743,0.003029546234756708,0.003031013300642371,0.0030498586129397154,0.003052686806768179,0.0030577280558645725,0.0030659842304885387,0.003090810962021351,0.003142514731734991,0.003147884039208293,0.0031507848761975765,0.003167279064655304,0.0031728637404739857,0.0031919695902615786,0.0032262278255075216,0.0032434221357107162,0.0032529255840927362,0.0032706349156796932,0.003321899799630046,0.003356222528964281,0.003365278011187911,0.0033652945421636105,0.0033930616918951273,0.0034116851165890694,0.0034429184161126614,0.0034557816106826067,0.0034742143470793962,0.0034790479112416506,0.003503252752125263,0.0035182437859475613,0.0035367929376661777,0.003562365658581257,0.0035654432140290737,0.00358679355122149,0.003610466606914997,0.0036116645205765963,0.0036232275888323784,0.0036813789047300816,0.0036842855624854565,0.003692710306495428,0.0036938905250281096,0.0036981296725571156,0.003702397458255291,0.0038061905652284622,0.003832958871498704,0.003860841039568186,0.0038833508733659983,0.003889290615916252,0.003920293413102627,0.003938782960176468,0.004017619416117668,0.004034925252199173,0.004037695936858654,0.004093411844223738,0.004119060002267361,0.004138814751058817,0.004146758466959,0.004182639066129923,0.004186391364783049,0.004196589812636375,0.004270073026418686,0.004360485356301069,0.004367412067949772,0.004424678161740303,0.004450572654604912,0.004462019074708223,0.004462163429707289,0.004474971909075975,0.00449530640617013,0.004496491979807615,0.004506464581936598,0.004511713981628418,0.004578019957989454,0.004600220359861851,0.00463056517764926,0.004676565062254667,0.004732053726911545,0.004762250930070877,0.004789256490767002,0.004791766870766878,0.004848607815802097,0.004858991131186485,0.00487217353656888,0.004889213014394045,0.004922898020595312,0.004937445744872093,0.0049391938373446465,0.00494333216920495,0.004943981301039457,0.00496929744258523,0.004993440583348274,0.00500845443457365,0.005045260302722454,0.005107419565320015,0.0051119280979037285,0.005209624767303467,0.005234819836914539,0.005260491278022528,0.0052632177248597145,0.005277671851217747,0.005304034799337387,0.0053044469095766544,0.0053112744353711605,0.005354010965675116,0.005414290353655815,0.00544049683958292,0.005465060472488403,0.005479601677507162,0.005509662441909313,0.005511074792593718,0.0055239079520106316,0.0055597638711333275,0.005589218810200691,0.005592880304902792,0.005617301911115646,0.005672282539308071,0.005699324421584606,0.005705360788851976,0.005706774070858955,0.005755248013883829,0.005806867964565754,0.005825391970574856,0.005834924057126045,0.005850780755281448,0.005856023635715246,0.005863213911652565,0.005948907230049372,0.006068357732146978,0.006071730051189661,0.006147278007119894,0.006196880247443914,0.006198220886290073,0.006204713135957718,0.006270068231970072,0.0062883757054805756,0.0062963287346065044,0.006357464008033276,0.006359183229506016,0.006372088100761175,0.006376133766025305,0.006423935294151306,0.006444825325161219,0.006446541752666235,0.006459789350628853,0.0064912098459899426,0.0065140449441969395,0.006544310599565506,0.006556630600243807,0.006578771863132715,0.006591681391000748,0.006627440918236971,0.00668717548251152,0.006688314024358988,0.006698969751596451,0.006707577034831047,0.006807682570070028,0.006829972844570875,0.006857641972601414,0.00686821760609746,0.006902147084474564,0.006923861335963011,0.006930916104465723,0.006935368292033672,0.006944755092263222,0.006965419743210077,0.0069658756256103516,0.006969294976443052,0.006969373673200607,0.007016469724476337,0.007093784399330616,0.007119483780115843,0.007129264529794455,0.007133702747523785,0.007147935684770346,0.007212183438241482,0.007213805802166462,0.007241704035550356,0.007320445030927658,0.007383768446743488,0.007394688203930855,0.00741173978894949,0.007415310014039278,0.007433075923472643,0.00744160171598196,0.007445387076586485,0.007447027135640383,0.007497293874621391,0.007524228189140558,0.00762095395475626,0.007652769796550274,0.00767947593703866,0.00773262046277523,0.007735642604529858,0.00783488992601633,0.008030283264815807,0.008048318326473236,0.008058580569922924,0.008071178570389748,0.008113374002277851,0.008133998140692711,0.008137468248605728,0.008138589560985565,0.008277988992631435,0.00850600190460682,0.008535695262253284,0.00856040045619011,0.00860121101140976,0.008639171719551086,0.008647977374494076,0.008657602593302727,0.008671069517731667,0.008684414438903332,0.008790647611021996,0.008812024258077145,0.008843519724905491,0.009007261134684086,0.009101681411266327,0.009130745194852352,0.009150664322078228,0.00915087666362524,0.009157249704003334,0.009356842376291752,0.009367912076413631,0.009414372965693474,0.009432665072381496,0.009439622983336449,0.009454759769141674,0.00945716816931963,0.00949295423924923,0.0095276003703475,0.009633452631533146,0.009758023545145988,0.009928601793944836,0.010121016763150692,0.010148963890969753,0.010175283998250961,0.010246126912534237,0.010304272174835205,0.01031849067658186,0.010325081646442413,0.010357317514717579,0.010471041314303875,0.010514910332858562,0.010531666688621044,0.010545263066887856,0.010776038281619549,0.010808801278471947,0.010828638449311256,0.010833758860826492,0.010874295607209206,0.010900283232331276,0.01111413910984993,0.011155715212225914,0.011157417669892311,0.011205647140741348,0.011241582222282887,0.01124948263168335,0.011269314214587212,0.011275805532932281,0.011278724297881126,0.01129380613565445,0.011356215924024582,0.011401060037314892,0.011439588852226734,0.011476317420601845,0.011526847258210182,0.011601774953305721,0.01170351356267929,0.011829426512122154,0.011913713999092579,0.011957540176808834,0.012121009640395641,0.012140738777816296,0.012246573343873024,0.012403812259435654,0.012406609021127224,0.012523249723017216,0.012523273006081581,0.012535536661744118,0.012566144578158855,0.012714974582195282,0.012745792977511883,0.012772612273693085,0.012842731550335884,0.012883993797004223,0.013004451990127563,0.013018791563808918,0.013074617832899094,0.013130944222211838,0.013323342427611351,0.01336658000946045,0.013571660965681076,0.013631912879645824,0.013656003400683403,0.013680187053978443,0.013784630224108696,0.013922164216637611,0.014055474661290646,0.014062774367630482,0.014077918604016304,0.014231712557375431,0.014412501826882362,0.014512374065816402,0.014610333368182182,0.014984916895627975,0.015271587297320366,0.015344394370913506,0.01536617986857891,0.015402352437376976,0.015539738349616528,0.015588457696139812,0.015819644555449486,0.015873782336711884,0.015989840030670166,0.01612827740609646,0.016157446429133415,0.016465282067656517,0.016835717484354973,0.01724659651517868,0.01726817525923252,0.01731669157743454,0.01747257634997368,0.0174776129424572,0.017615346238017082,0.01766344904899597,0.01782909221947193,0.017887039110064507,0.01794685795903206,0.01795957051217556,0.018024737015366554,0.01803663745522499,0.018216224387288094,0.01822025701403618,0.01836693473160267,0.018510177731513977,0.018684839829802513,0.018710194155573845,0.01887449063360691,0.018944036215543747,0.019014038145542145,0.019866298884153366,0.019884616136550903,0.02000262960791588,0.020085301250219345,0.020213603973388672,0.020501503720879555,0.020661430433392525,0.02066281996667385,0.020983757451176643,0.02105092816054821,0.02145090512931347,0.021544132381677628,0.02159063145518303,0.021645892411470413,0.021697331219911575,0.021798552945256233,0.022234292700886726,0.022357916459441185,0.022365467622876167,0.02239278331398964,0.022525668144226074,0.02280573546886444,0.022854479029774666,0.022985048592090607,0.023400017991662025,0.023509884253144264,0.023852424696087837,0.023996345698833466,0.024198049679398537,0.024488305673003197,0.02469681017100811,0.024839157238602638,0.024956045672297478,0.02535083144903183,0.025411205366253853,0.025473225861787796,0.025597939267754555,0.025700785219669342,0.026140760630369186,0.02629972994327545,0.02727866731584072,0.027321936562657356,0.02760804258286953,0.02769928053021431,0.027872812002897263,0.027941523119807243,0.027953119948506355,0.028261823579669,0.02832576259970665,0.028679300099611282,0.029313933104276657,0.02963769994676113,0.030200909823179245,0.0304503683000803,0.030556805431842804,0.03059922717511654,0.031095461919903755,0.03190682455897331,0.032125502824783325,0.03228727728128433,0.03257112205028534,0.032585855573415756,0.032643191516399384,0.03306367248296738,0.03334463760256767,0.03342046961188316,0.03358710929751396,0.033901646733284,0.034317925572395325,0.034630462527275085,0.034642335027456284,0.0364728718996048,0.03648405149579048,0.03777635470032692,0.03792257606983185,0.037981532514095306,0.03837775066494942,0.03852139785885811,0.0393533818423748,0.039371736347675323,0.04134753346443176,0.04162691906094551,0.042234085500240326,0.042531490325927734,0.04263972118496895,0.04312586411833763,0.04356606304645538,0.04382184147834778,0.044045429676771164,0.044110722839832306,0.04461001977324486,0.04463579133152962,0.044842902570962906,0.04498865082859993,0.04547177627682686,0.04617096483707428,0.0475834384560585,0.04762258008122444,0.0479767806828022,0.04819054529070854,0.04955572634935379,0.049868203699588776,0.050224900245666504,0.051752377301454544,0.05211162939667702,0.05617262423038483,0.056518249213695526,0.05758466199040413,0.058714427053928375,0.059081487357616425,0.05910540744662285,0.06051434949040413,0.06077546253800392,0.061809200793504715,0.06297538429498672,0.06483963131904602,0.06502112746238708,0.06606698781251907,0.06623489409685135,0.06750744581222534,0.07035964727401733,0.070899598300457,0.0718594416975975,0.07265911251306534,0.07405905425548553,0.07541265338659286,0.07570872455835342,0.0763544887304306,0.07758665829896927,0.07867655158042908,0.0790141150355339,0.08244750648736954,0.08443108201026917,0.08653569966554642,0.08882593363523483,0.09091145545244217,0.09115160256624222,0.09260045737028122,0.09264032542705536,0.09486185759305954,0.10381665825843811,0.10392069816589355,0.10437168180942535,0.11052686721086502,0.11694537848234177,0.1187766045331955,0.1202428787946701,0.13064588606357574,0.1307137906551361,0.13922029733657837,0.14138789474964142,0.1490985006093979,0.1629728376865387,0.16437698900699615,0.16692611575126648,0.1685630977153778,0.20587806403636932,0.21104022860527039,0.21313557028770447],\"yaxis\":\"y\",\"type\":\"scattergl\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Sorted neurons\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Loss change\"}},\"legend\":{\"title\":{\"text\":\"variable\"},\"tracegroupgap\":0},\"title\":{\"text\":\"Loss change from ablating MLP5 neuron\"},\"width\":800,\"showlegend\":true},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('91c57798-7571-4209-8710-44df61105c34');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sorted_means, indices = torch.sort(diffs.mean(1))\n",
    "sorted_means = sorted_means.tolist()\n",
    "haystack_utils.line(sorted_means, xlabel=\"Sorted neurons\", ylabel=\"Loss change\", title=\"Loss change from ablating MLP5 neuron\") # xticks=indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>                            <div id=\"e0d07583-674f-4a76-9c66-b23cd7bc9dee\" class=\"plotly-graph-div\" style=\"height:525px; width:1000px;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"e0d07583-674f-4a76-9c66-b23cd7bc9dee\")) {                    Plotly.newPlot(                        \"e0d07583-674f-4a76-9c66-b23cd7bc9dee\",                        [{\"error_y\":{\"array\":[1.2252430265281449],\"type\":\"data\",\"visible\":true},\"name\":\"Original\",\"x\":[\"Original\"],\"y\":[1.4290467546321453],\"type\":\"bar\"},{\"error_y\":{\"array\":[1.919628490014],\"type\":\"data\",\"visible\":true},\"name\":\"Ablated\",\"x\":[\"Ablated\"],\"y\":[3.563321807384491],\"type\":\"bar\"},{\"error_y\":{\"array\":[1.3151057222295963],\"type\":\"data\",\"visible\":true},\"name\":\"MLP5 path patched\",\"x\":[\"MLP5 path patched\"],\"y\":[2.0997045168280604],\"type\":\"bar\"},{\"error_y\":{\"array\":[1.7762824310676582],\"type\":\"data\",\"visible\":true},\"name\":\"MLP5 path patched + Top 10 MLP5 neurons ablated\",\"x\":[\"Top MLP5 removed\"],\"y\":[4.073108800053596],\"type\":\"bar\"},{\"error_y\":{\"array\":[0.5853497418016615],\"type\":\"data\",\"visible\":true},\"name\":\"MLP5 path patched + Bottom 10 MLP5 neurons ablated\",\"x\":[\"Bottom MLP5 removed\"],\"y\":[0.5276507527055219],\"type\":\"bar\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"title\":{\"text\":\"Average last token loss when removing top \\u002f bottom neurons from path patching\"},\"xaxis\":{\"title\":{\"text\":\"\"}},\"yaxis\":{\"title\":{\"text\":\"Loss\"}},\"barmode\":\"group\",\"width\":1000,\"showlegend\":true},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('e0d07583-674f-4a76-9c66-b23cd7bc9dee');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check loss change when ablating top / bottom neurons\n",
    "top_neurons_count = 10\n",
    "top_neurons = indices[-top_neurons_count:]\n",
    "bottom_neurons = indices[:top_neurons_count]\n",
    "\n",
    "with model.hooks(deactivate_neurons_fwd_hooks):\n",
    "    ablated_loss, ablated_cache = model.run_with_cache(prompts, return_type=\"loss\")\n",
    "\n",
    "ablate_top_neurons_hook = get_ablate_neurons_hook(top_neurons, ablated_cache)\n",
    "ablate_bottom_neurons_hook = get_ablate_neurons_hook(bottom_neurons, ablated_cache)\n",
    "\n",
    "original_loss, ablated_loss, _, all_MLP5_loss = haystack_utils.get_direct_effect(prompts, model, pos=-1, context_ablation_hooks=deactivate_neurons_fwd_hooks, context_activation_hooks=activate_neurons_fwd_hooks)\n",
    "_, _, _, top_MLP5_ablated_loss = haystack_utils.get_direct_effect(prompts, model, pos=-1, context_ablation_hooks=deactivate_neurons_fwd_hooks, context_activation_hooks=activate_neurons_fwd_hooks+ablate_top_neurons_hook)\n",
    "_, _, _, bottom_MLP5_ablated_loss = haystack_utils.get_direct_effect(prompts, model, pos=-1, context_ablation_hooks=deactivate_neurons_fwd_hooks, context_activation_hooks=activate_neurons_fwd_hooks+ablate_bottom_neurons_hook)\n",
    "\n",
    "names = [\"Original\", \"Ablated\", \"MLP5 path patched\", f\"MLP5 path patched + Top {top_neurons_count} MLP5 neurons ablated\", f\"MLP5 path patched + Bottom {top_neurons_count} MLP5 neurons ablated\"]\n",
    "short_names = [\"Original\", \"Ablated\", \"MLP5 path patched\", f\"Top MLP5 removed\", f\"Bottom MLP5 removed\"]\n",
    "\n",
    "values = [original_loss.tolist(), ablated_loss.tolist(), all_MLP5_loss.tolist(), top_MLP5_ablated_loss.tolist(), bottom_MLP5_ablated_loss.tolist()]\n",
    "haystack_utils.plot_barplot(values, names, short_names=short_names, ylabel=\"Loss\", title=f\"Average last token loss when removing top / bottom neurons from path patching\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigate top / bottom neuron boosts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Patch in MLP5 neurons, ablated bottom neurons, compare with patched MLP5 neurons without ablating bottom neurons\n",
    "\n",
    "with model.hooks(deactivate_neurons_fwd_hooks):\n",
    "    ablated_logits, ablated_cache = model.run_with_cache(prompts)\n",
    "\n",
    "ablate_top_neurons_hook = get_ablate_neurons_hook(top_neurons, ablated_cache)\n",
    "ablate_bottom_neurons_hook = get_ablate_neurons_hook(bottom_neurons, ablated_cache)\n",
    "\n",
    "original_logits, ablated_logprobs, _, all_MLP5_logprobs = haystack_utils.get_direct_effect(prompts, model, pos=-2, context_ablation_hooks=deactivate_neurons_fwd_hooks, context_activation_hooks=activate_neurons_fwd_hooks, return_type='logprobs')\n",
    "_, _, _, top_MLP5_ablated_logprobs = haystack_utils.get_direct_effect(prompts, model, pos=-2, context_ablation_hooks=deactivate_neurons_fwd_hooks, context_activation_hooks=activate_neurons_fwd_hooks+ablate_top_neurons_hook, return_type='logprobs')\n",
    "_, _, _, bottom_MLP5_ablated_logprobs = haystack_utils.get_direct_effect(prompts, model, pos=-2, context_ablation_hooks=deactivate_neurons_fwd_hooks, context_activation_hooks=activate_neurons_fwd_hooks+ablate_bottom_neurons_hook, return_type='logprobs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_difference_neurons(baseline_logprobs, ablated_logprops, positive=True, logprob_threshold=-7, k=50):\n",
    "    neuron_logprob_difference = (baseline_logprobs - ablated_logprops).mean(0)\n",
    "    neuron_logprob_difference[baseline_logprobs.mean(0) < logprob_threshold] = 0\n",
    "    if positive:\n",
    "        non_zero_count = (neuron_logprob_difference > 0).sum()\n",
    "    else:\n",
    "        non_zero_count = (neuron_logprob_difference < 0).sum()\n",
    "    top_logprob_difference, top_neurons = haystack_utils.top_k_with_exclude(neuron_logprob_difference, min(non_zero_count, k), all_ignore, largest=positive)\n",
    "    return top_logprob_difference, top_neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boosted tokens\n",
    "bottom_neuron_pos_difference_logprobs, bottom_pos_indices = get_top_difference_neurons(all_MLP5_logprobs, bottom_MLP5_ablated_logprobs, positive=True)\n",
    "top_neuron_pos_difference_logprobs, top_pos_indices = get_top_difference_neurons(all_MLP5_logprobs, top_MLP5_ablated_logprobs, positive=True)\n",
    "# Deboosted tokens\n",
    "bottom_neuron_neg_difference_logprobs, bottom_neg_indices = get_top_difference_neurons(all_MLP5_logprobs, bottom_MLP5_ablated_logprobs, positive=False)\n",
    "top_neuron_neg_difference_logprobs, top_neg_indices = get_top_difference_neurons(all_MLP5_logprobs, top_MLP5_ablated_logprobs, positive=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "792fdd9be303459b9d9b0c6ae463e66a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='Tokens:', options=('Boosted', 'Deboosted'), value='Boosted'), Drop…"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def plot_data(boosted_deboosted, top_bottom):\n",
    "    if boosted_deboosted == 'Boosted' and top_bottom == 'Top':\n",
    "        logprobs = top_neuron_pos_difference_logprobs\n",
    "        indices = top_pos_indices\n",
    "        title_change = 'increase'\n",
    "    elif boosted_deboosted == 'Boosted' and top_bottom == 'Bottom':\n",
    "        logprobs = bottom_neuron_pos_difference_logprobs\n",
    "        indices = bottom_pos_indices\n",
    "        title_change = 'increase'\n",
    "    elif boosted_deboosted == 'Deboosted' and top_bottom == 'Top':\n",
    "        logprobs = top_neuron_neg_difference_logprobs\n",
    "        indices = top_neg_indices\n",
    "        title_change = 'decrease'\n",
    "    else:  # 'Deboosted' and 'Bottom'\n",
    "        logprobs = bottom_neuron_neg_difference_logprobs\n",
    "        indices = bottom_neg_indices\n",
    "        title_change = 'decrease'\n",
    "\n",
    "    xlabel = boosted_deboosted + \" tokens\"\n",
    "    ylabel = \"full_logprob - ablated_logprop\"\n",
    "    title = 'Logprob ' + title_change + ' from ' + top_bottom.lower() + ' neurons'\n",
    "    xticks = [model.to_str_tokens([i])[0] for i in indices]\n",
    "\n",
    "    haystack_utils.line(logprobs.cpu().numpy(), xlabel=xlabel, ylabel=ylabel, title=title, xticks=xticks, show_legend=False)\n",
    "\n",
    "\n",
    "boosted_deboosted = widgets.Dropdown(options=['Boosted', 'Deboosted'], value='Boosted', description='Tokens:')\n",
    "top_bottom = widgets.Dropdown(options=['Top', 'Bottom'], value='Top', description='Neurons:')\n",
    "widgets.interactive(plot_data, boosted_deboosted=boosted_deboosted, top_bottom=top_bottom)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decompose neuron effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get logprob change for individual neuron\n",
    "\n",
    "def get_individual_neuron_logprob_effect(sorted_indices, neuron_pos=0, top=True, positive=True, plot=True):\n",
    "    if top:\n",
    "        neurons = sorted_indices[-(neuron_pos+1)]\n",
    "    else:\n",
    "        neurons = sorted_indices[neuron_pos]\n",
    "    neurons_hook = get_ablate_neurons_hook([neurons], ablated_cache)\n",
    "\n",
    "    original_logits, ablated_logprobs, _, all_MLP5_logprobs = haystack_utils.get_direct_effect(prompts, model, pos=-2, context_ablation_hooks=deactivate_neurons_fwd_hooks, context_activation_hooks=activate_neurons_fwd_hooks, return_type='logprobs')\n",
    "    _, _, _, neuron_ablated_logprobs = haystack_utils.get_direct_effect(prompts, model, pos=-2, context_ablation_hooks=deactivate_neurons_fwd_hooks, context_activation_hooks=activate_neurons_fwd_hooks+neurons_hook, return_type='logprobs')\n",
    "    neuron_difference_logprobs, difference_indices = \\\n",
    "        get_top_difference_neurons(all_MLP5_logprobs, neuron_ablated_logprobs, positive=positive)\n",
    "    \n",
    "    if not plot:\n",
    "        return neuron_difference_logprobs, difference_indices\n",
    "    \n",
    "    if positive:\n",
    "        xlabel = 'Boosted tokens'\n",
    "        title = 'Logprob increase from neuron ' + str(neurons.item())\n",
    "    else: \n",
    "        xlabel = 'Deboosted tokens'\n",
    "        title = 'Logprob decrease from neuron ' + str(neurons.item())\n",
    "    if top:\n",
    "        title += f' (top {neuron_pos+1} neuron)'\n",
    "    else:\n",
    "        title += f' (bottom {neuron_pos+1} neuron)'\n",
    "\n",
    "    xticks = [model.to_str_tokens([i])[0] for i in difference_indices]\n",
    "    haystack_utils.line(neuron_difference_logprobs.cpu().numpy(), xlabel=xlabel, ylabel=\"full_logprob - ablated_logprop\", title=title, xticks=xticks, show_legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "805468658dee46b28afbbe331ab883bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='Neuron Pos:', max=20), Dropdown(description='Neurons:', …"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a function to call your function with widget inputs\n",
    "def plot_individual_neuron(neuron_pos, top_bottom, pos_neg):\n",
    "    top_bool = True if top_bottom == 'Top' else False\n",
    "    positive_bool = True if pos_neg == 'Positive' else False\n",
    "    get_individual_neuron_logprob_effect(indices, neuron_pos=neuron_pos, top=top_bool, positive=positive_bool)\n",
    "\n",
    "# Define widgets\n",
    "neuron_pos_slider = widgets.IntSlider(min=0, max=20, step=1, value=0, description='Neuron Pos:')\n",
    "top_bottom_dropdown = widgets.Dropdown(options=['Top', 'Bottom'], value='Top', description='Neurons:')\n",
    "pos_neg_dropdown = widgets.Dropdown(options=['Positive', 'Negative'], value='Positive', description='Pos/Neg:')\n",
    "\n",
    "# Use interactive function to bind the widgets to the plotting function\n",
    "widgets.interactive(plot_individual_neuron, neuron_pos=neuron_pos_slider, top_bottom=top_bottom_dropdown, pos_neg=pos_neg_dropdown)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum individual weighted positive / negative effects\n",
    "def summed_neuron_differences(num_neurons=10, top=True, positive=True, k=50):\n",
    "    summed_neuron_diffs = torch.zeros(model.cfg.d_vocab).cuda()\n",
    "    for neuron_pos in range(num_neurons):\n",
    "        # It would probably be more principle to do our filtering on the summed diffs instead of per neuron\n",
    "        neuron_diff, token_indices = get_individual_neuron_logprob_effect(indices, neuron_pos=neuron_pos, top=top, positive=positive, plot=False)\n",
    "        summed_neuron_diffs[token_indices] += neuron_diff\n",
    "    if positive:\n",
    "        non_zero_count = (summed_neuron_diffs > 0).sum()\n",
    "    else:\n",
    "        non_zero_count = (summed_neuron_diffs < 0).sum()\n",
    "    top_logprob_difference, top_tokens = haystack_utils.top_k_with_exclude(summed_neuron_diffs, min(non_zero_count, k), all_ignore, largest=positive)\n",
    "    return top_logprob_difference, top_tokens\n",
    "\n",
    "def plot_summed_neuron_differences(num_neurons=10, top=True, positive=True):\n",
    "    top_logprob_difference, top_logprob_tokens = summed_neuron_differences(num_neurons=num_neurons, top=top, positive=positive)\n",
    "    xticks = [model.to_str_tokens([i])[0] for i in top_logprob_tokens]\n",
    "    \n",
    "    if positive:\n",
    "        title = \"Summed individual boosts by\"\n",
    "        xlabel = \"Boosted tokens\"\n",
    "    else:\n",
    "        title = \"Summed individual deboosts by\"\n",
    "        xlabel = \"Deboosted tokens\"\n",
    "    if top:\n",
    "        title += f\" top {num_neurons} neurons\"\n",
    "    else:\n",
    "        title += f\" bottom {num_neurons} neurons\"\n",
    "    haystack_utils.line(top_logprob_difference.cpu().numpy(), xlabel=xlabel, ylabel=\"full_logprob - ablated_logprop\", title=title, xticks=xticks, show_legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7cd1478df3848ddaffdf7faef0be669",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=10, description='Num Neurons:', max=20, min=1), Dropdown(description='Ne…"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a function to call your function with widget inputs\n",
    "def plot_summed_neurons_widget(num_neurons, top_bottom, pos_neg):\n",
    "    top_bool = True if top_bottom == 'Top' else False\n",
    "    positive_bool = True if pos_neg == 'Positive' else False\n",
    "    plot_summed_neuron_differences(num_neurons=num_neurons, top=top_bool, positive=positive_bool)\n",
    "\n",
    "# Define widgets\n",
    "num_neuron_slider = widgets.IntSlider(min=1, max=20, step=1, value=10, description='Num Neurons:')\n",
    "top_bottom_dropdown = widgets.Dropdown(options=['Top', 'Bottom'], value='Top', description='Neurons:')\n",
    "pos_neg_dropdown = widgets.Dropdown(options=['Positive', 'Negative'], value='Positive', description='Pos/Neg:')\n",
    "\n",
    "# Use interactive function to bind the widgets to the plotting function\n",
    "widgets.interactive(plot_summed_neurons_widget, num_neurons=num_neuron_slider, top_bottom=top_bottom_dropdown, pos_neg=pos_neg_dropdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss is logprob(correct_token_logit) = logit(correct_token_logit) - LogSumExp(all_token_logits)\n",
    "Loss = -logprob\n",
    "\n",
    "LogSumExp approximates a maximum function. If the neuron engages in destructive interference of a high logit for a non-answer token, then the exp(logit) for the token will be lower and so the LogSumExp will be more similar to logit(correct_token_logit) so the loss will be lower. So a lower logsumexp(all vocab) is good.\n",
    "\n",
    "If the neuron engages in destructive interference of a low logit for a non-answer token, then the exp(logit) for the the token won't really change and so the logsumexp and the loss will both be the same.\n",
    "\n",
    "For a single neuron:\n",
    "1. For each \"gen\" prompt, zero centre the logits and record the logit of \"gen\". Calculate the mean \"gen\" logit.\n",
    "2. For each \"gen\" prompt, disable the neuron under test, zero centre the logits, edit the logit of \"gen\" to the mean with a hook, then get the logprob of \"gen\" and calculate logsumexp(all vocab) = logit(gen) - logprob(gen).\n",
    "2. For each \"gen\" prompt, enable the neuron, zero centre the logits, edit the logit of \"gen\" to the mean with a hook, then get the logprob of \"gen\" and calculate logsumexp(all vocab) = logit(gen) - logprob(gen).\n",
    "3. Take the difference in logsum exps. If it's positive, the neuron is reducing the loss via destructive interference by the difference.\n",
    "Can use the same procedure for sets of neurons, or for all neurons, to find high level effects of the context neuron\n",
    "\n",
    "Logprobs are logits with a constant subtracted and the constant is the same for every logit within a prompt.\n",
    "\n",
    "Taking the difference in terms with and without a neuron's effect via the context neuron:\n",
    "- If log sum exp increases, the neuron is boosting tokens on average. \n",
    "- If logit increases, the neuron is boosting the correct token\n",
    "\n",
    "\n",
    "Remove the neuron's effects on the gen logit. Take the mean on the prompt and position?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(11.4780, device='cuda:0')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Calculate the mean \"gen\" logit.\n",
    "gen_index = model.to_single_token('gen')\n",
    "gen_logits = []\n",
    "logits = model(prompts, return_type='logits') # batch pos vocab\n",
    "logits = logits - logits.mean(-1).unsqueeze(-1) # batch pos vocab, batch pos 1\n",
    "\n",
    "mean_gen_logit = logits[:, -1, gen_index].mean(0)\n",
    "mean_gen_logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import numpy as np\n",
    "# from transformer_lens import utils\n",
    "# \n",
    "# px.histogram(np.random.choice(logits[:, -1, gen_index].flatten().cpu().numpy(), 1000), nbins=100)\n",
    "# utils.test_prompt(\"\".join(model.to_str_tokens(prompts[0, :-1])), \"gen\", model, prepend_space_to_answer=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constructive interference diff, destructive interference diff\n",
      "0.125080 0.055533\n",
      "0.174759 -0.001215\n",
      "0.125720 0.063912\n",
      "0.156197 0.051667\n",
      "0.265738 -0.055894\n",
      "1.364952 0.882472\n"
     ]
    }
   ],
   "source": [
    "def decompose_interference_diff(ablated_cache, disabled_neurons=[top_neurons[0]], mean=True\n",
    "                                ) -> tuple[float, float] | tuple[Float[Tensor, \"n_prompts\"], Float[Tensor, \"n_prompts\"]]:\n",
    "    '''\n",
    "    Finds the effect of the German context neuron ablation via a given set of MLP5 neurons on the logit of each final \n",
    "    token. Decomposes it into constructive and destructive interference. A positive constructive interference difference\n",
    "    means the neurons boost the logit of the correct token, a positive destructive interference difference means the\n",
    "    LogSumExp is getting closer to the correct token logit because neurons deboost the logits of the higher probability \n",
    "    incorrect tokens.\n",
    "\n",
    "    Loss = -logprob(correct_token_logit)\n",
    "    logprob(correct_token_logit) = correct_token_logit - LogSumExp(all_token_logits)\n",
    "\n",
    "    LogSumExp is a smooth maximum function, so it approximates the max of all logits. A neuron that destructively \n",
    "    interfers with a non-answer token with a high logit lowers the LogSumExp(all_token_logits) and thus the loss.'''\n",
    "    ablate_top_neuron_hook = get_ablate_neurons_hook(disabled_neurons, ablated_cache)\n",
    "\n",
    "    _, _, _, mlp5_enabled_logits = haystack_utils.get_direct_effect(prompts, model, pos=-2, \n",
    "                                                                    context_ablation_hooks=deactivate_neurons_fwd_hooks, \n",
    "                                                                    context_activation_hooks=activate_neurons_fwd_hooks, return_type='logits')\n",
    "    _, _, _, mlp5_enabled_top_neuron_ablated_logits = haystack_utils.get_direct_effect(prompts, model, pos=-2, \n",
    "                                                                                       context_ablation_hooks=deactivate_neurons_fwd_hooks, \n",
    "                                                                                       context_activation_hooks=activate_neurons_fwd_hooks+ablate_top_neuron_hook, return_type='logits')\n",
    "    \n",
    "    # Mean center logits to avoid picking up on constant boosts / deboosts\n",
    "    mlp5_enabled_logits = mlp5_enabled_logits - mlp5_enabled_logits.mean(-1).unsqueeze(-1)\n",
    "    mlp5_enabled_top_neuron_ablated_logits = mlp5_enabled_top_neuron_ablated_logits - mlp5_enabled_top_neuron_ablated_logits.mean(-1).unsqueeze(-1)\n",
    "    \n",
    "    # 1. Constructive interference difference\n",
    "    # This is the change in the correct answer token logit from ablating the neuron, positive is good\n",
    "    constructive_interference_diffs = mlp5_enabled_logits[:, gen_index] - mlp5_enabled_top_neuron_ablated_logits[:, gen_index]\n",
    "\n",
    "    # 2. Destructive interference difference\n",
    "    # This is the change in the LogSumExp of all logits from not ablating the neuron positive/increase in LogSumExp is bad \n",
    "    # even though it means that all tokens get deboosted more by a constant amount because the LogSumExp is guaranteed to be\n",
    "    # >= the correct token logit\n",
    "\n",
    "    # Set the gen logit to its mean value so the neuron's constructive interference doesn't affect the LogSumExp difference\n",
    "    mlp5_enabled_logits[:, gen_index] = mean_gen_logit\n",
    "    mlp5_enabled_top_neuron_ablated_logits[:, gen_index] = mean_gen_logit\n",
    "    \n",
    "    # Compute logsumexp\n",
    "    mlp5_enabled_log_sum_exp = mlp5_enabled_logits.exp().sum(-1).log()\n",
    "    mlp5_enabled_top_neuron_ablated_log_sum_exp = mlp5_enabled_top_neuron_ablated_logits.exp().sum(-1).log()\n",
    "\n",
    "    # Check for errors\n",
    "    assert torch.allclose(mlp5_enabled_log_sum_exp, mlp5_enabled_logits[:, gen_index] - mlp5_enabled_logits.log_softmax(-1)[:, gen_index])\n",
    "    assert torch.allclose(mlp5_enabled_top_neuron_ablated_log_sum_exp, mlp5_enabled_top_neuron_ablated_logits[:, gen_index] - mlp5_enabled_top_neuron_ablated_logits.log_softmax(-1)[:, gen_index])\n",
    "\n",
    "    # Difference in logsumexp\n",
    "    # Logsumexp of enabled should be higher than ablated if the neuron does something good\n",
    "    # Negative results are good - they mean that all tokens are deboosted more when the neuron is active\n",
    "    destructive_interference_diffs = mlp5_enabled_log_sum_exp - mlp5_enabled_top_neuron_ablated_log_sum_exp\n",
    "    # Convert the results so positive is good\n",
    "    destructive_interference_diffs *= -1\n",
    "\n",
    "    if mean:\n",
    "        return constructive_interference_diffs.mean().item(), destructive_interference_diffs.mean().item(), \n",
    "    return constructive_interference_diffs, destructive_interference_diffs,\n",
    "\n",
    "\n",
    "# Calculate neuron-wise loss change\n",
    "print(\"constructive interference diff, destructive interference diff\")\n",
    "for i in range(5):\n",
    "    constructive_diff, destructive_diff = decompose_interference_diff(ablated_cache, disabled_neurons=[top_neurons[i]], mean=True)\n",
    "    print(f'{constructive_diff:2f}', f'{destructive_diff:2f}')\n",
    "\n",
    "constructive_diff, destructive_diff = decompose_interference_diff(ablated_cache, top_neurons)\n",
    "print(f'{constructive_diff:2f}', f'{destructive_diff:2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neuron -> Token Logit Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0237,  0.0346, -0.0347,  0.0397,  0.0595,  0.0501, -0.0274,  0.1405,\n",
      "         0.0417, -0.0392], device='cuda:0')\n",
      "tensor([0.1634, 0.1461, 0.1405, 0.1399, 0.1349, 0.1207, 0.1196, 0.1161, 0.1156,\n",
      "        0.1139, 0.1130, 0.1128, 0.1120, 0.1104, 0.1096, 0.1070, 0.1056, 0.1048,\n",
      "        0.1042, 0.1028], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Our top neurons are selected by the difference in their boost in gen based on the context neuron\n",
    "# Many other neurons boost gen more per unit of activation\n",
    "\n",
    "cosine_sim = torch.nn.CosineSimilarity(dim=1)\n",
    "answer_residual_direction = model.tokens_to_residual_directions(\"gen\")\n",
    "neuron_weights = model.state_dict()['blocks.5.mlp.W_out'][top_neurons]\n",
    "\n",
    "cosine_sims = cosine_sim(neuron_weights, answer_residual_direction.unsqueeze(0))\n",
    "print(cosine_sims)\n",
    "\n",
    "cosine_sim = torch.nn.CosineSimilarity(dim=1)\n",
    "answer_residual_direction = model.tokens_to_residual_directions(\"gen\")\n",
    "neuron_weights = model.state_dict()['blocks.5.mlp.W_out']\n",
    "\n",
    "cosine_sims = cosine_sim(neuron_weights, answer_residual_direction.unsqueeze(0))\n",
    "top, indices = torch.topk(cosine_sims, 20)\n",
    "print(top)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51de8ce11476477993ab1bfaa185f697",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7150857dc2924a3abf4df0d5a91f71ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92499cdb973e40b3bd86219e487733aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "haystack_utils.clean_cache()\n",
    "english_activations_l5 = {}\n",
    "german_activations_l5 = {}\n",
    "all_activations_l5 = {}\n",
    "for layer in range(5, 6):\n",
    "    english_activations_l5 = get_mlp_activations(english_data, layer, model, mean=True)\n",
    "    german_activations_l5 = get_mlp_activations(german_data, layer, model, mean=True)\n",
    "    all_activations_l5 = get_mlp_activations(german_data[:50] + english_data[:150], layer, model, mean=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2048])\n",
      "torch.Size([2048, 512]) torch.Size([512, 50304]) torch.Size([2048, 1])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (512) must match the size of tensor b (50304) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 31\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[39mprint\u001b[39m(all_activations_l5\u001b[39m.\u001b[39mshape)\n\u001b[1;32m     29\u001b[0m \u001b[39mprint\u001b[39m(W_out\u001b[39m.\u001b[39mshape, W_U\u001b[39m.\u001b[39mshape, all_activations_l5\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mshape)\n\u001b[0;32m---> 31\u001b[0m average_boost \u001b[39m=\u001b[39m W_out \u001b[39m*\u001b[39;49m all_activations_l5\u001b[39m.\u001b[39;49munsqueeze(\u001b[39m1\u001b[39;49m) \u001b[39m*\u001b[39;49m W_U\n\u001b[1;32m     32\u001b[0m \u001b[39mprint\u001b[39m(average_boost\u001b[39m.\u001b[39mshape) \u001b[39m# d_mlp d_vocab\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[39m# 'Average gen boost': [i zip(all_activations_l5,\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \n\u001b[1;32m     36\u001b[0m \u001b[39m# mean activation\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (512) must match the size of tensor b (50304) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "W_out = model.state_dict()['blocks.5.mlp.W_out']\n",
    "W_U = model.W_U\n",
    "print(all_activations_l5.shape)\n",
    "print(W_out.shape, W_U.shape, all_activations_l5.unsqueeze(1).shape)\n",
    "\n",
    "average_boost = W_out * all_activations_l5.unsqueeze(1) * W_U\n",
    "print(average_boost.shape) # d_mlp d_vocab\n",
    "\n",
    "# 'Average gen boost': [i zip(all_activations_l5,\n",
    "\n",
    "# mean activation\n",
    "data = {\n",
    "    'Neuron index': list(range(2048)),\n",
    "    'Cosine similarity with \\\"gen\\\"': cosine_sims.tolist(),\n",
    "    'Average act - context neuron enabled': german_activations_l5.tolist(),\n",
    "    'Average act - context neuron disabled': english_activations_l5.tolist(),\n",
    "    'Average act': all_activations_l5.tolist()\n",
    "    # 'Boost \\\"gen\\\" act': boost_gen_acts.tolist(),\n",
    "    # 'Loss change when ablated': \n",
    "}\n",
    "\n",
    "# Create the dataframe\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Print the dataframe\n",
    "print(df)\n",
    "\n",
    "# More complete picture of neurons that boost gen.\n",
    "# Line plot of each neuron's gen boost with dotted vertical lines where the German context neuron-boosted ones are"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brainstorm notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to decompose an MLP5 neuron's effect into its boost to the correct logit and its deboost of other logits. I want to discover how these two effects change the log prob.\n",
    "\n",
    "metric like loss reduction vs. token boost\n",
    "\n",
    "~~run direct effect, patch each neuron in top 10 individually, get overall loss reduction from neuron controlled by context neuron (equivalent to logprob increase for correct token)~~\n",
    "decompose loss reduction into two parts:\n",
    "run direct effect, patch the correct token logit boost from the neuron (removing the neuron's other effects), get overall loss reduction from neuron (equivalent to logprob increase for correct token)\n",
    "destructive interference loss reduction = overall loss reduction - loss reduction from correct token logit boost (component of the logprob increase for correct token due to it deboosting incorrect token)\n",
    "\n",
    "Patch the correct token logit boost from the neuron (removing the neuron's other effects).\n",
    "1. Get baseline logprobs for a prompt\n",
    "2. Get difference in logits from activating the neuron under test. Can use get_direct_effects with return_type='logits'.\n",
    "3. Run the model with return_type='logits' and the neuron under test zero ablated. \n",
    "4. Add the correct token logit from step 1 to a copy of the output logits. Convert to logprobs\n",
    "5. Add the incorrect token logits from step 1 to a copy of the output logits. Convert to logprobs\n",
    "6. Compare A. lobprobs with correct answer token logit increase, B. logprobs with incorrect answer token logit increases, and C. baseline logprobs\n",
    "\n",
    "~~If the context neuron gives each neuron a flat boost then if we decompose the resulting flat boost to one MLP5 neuron into a boost to one logit vs. boosts to all other logits it will change the logprobs (first will increase answer probability and second will reduce answer probability). \n",
    "\n",
    "the two resulting log probs at the correct answer token won't add up to the original log probs (?). \n",
    "\n",
    "If the boost is flat the correct percentage decomposition is 1/50000 and 49999/50000? In practice/all other factors being equal\n",
    "\n",
    "New plan:\n",
    "\n",
    "Difference between baseline log prob and neuron log prob?\n",
    "Classify individual neurons by percentage constructive vs destructive by looking at their log probs and summing the incorrect token log probs\n",
    "\n",
    "Correct log prob difference\n",
    "Incorrect log prob difference (summed over every plausible token?)\n",
    "\n",
    "Largest boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the log prob for an incorrect token is significantly lower then that's where the extra probability density on the correct answer is coming from \n",
    "# Constructive interference increases correct token log prob and uniformly decreases other log probs\n",
    "# Destructive interference decreases specific other log probs and uniformly increases other log probs\n",
    " \n",
    "# Most neurons are a mixture of the above\n",
    "# Decompose neurons into what % of their effect is each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original_logits, ablated_logprobs, _, all_MLP5_logprobs = haystack_utils.get_direct_effect(prompts, model, pos=-2, context_ablation_hooks=deactivate_neurons_fwd_hooks, context_activation_hooks=activate_neurons_fwd_hooks, return_type='logprobs')\n",
    "# _, _, _, top_MLP5_ablated_logprobs = haystack_utils.get_direct_effect(prompts, model, pos=-2, context_ablation_hooks=deactivate_neurons_fwd_hooks, context_activation_hooks=activate_neurons_fwd_hooks+ablate_top_neurons_hook, return_type='logprobs')\n",
    "# _, _, _, bottom_MLP5_ablated_logprobs = haystack_utils.get_direct_effect(prompts, model, pos=-2, context_ablation_hooks=deactivate_neurons_fwd_hooks, context_activation_hooks=activate_neurons_fwd_hooks+ablate_bottom_neurons_hook, return_type='logprobs')\n",
    "\n",
    "# bottom_neuron_high_difference_logprobs = (all_MLP5_logprobs - bottom_MLP5_ablated_logprobs)\n",
    "# bottom_neuron_high_difference_logprobs[bottom_neuron_high_difference_logprobs < 0] = 0\n",
    "# bottom_neuron_high_difference_logprobs = bottom_neuron_high_difference_logprobs.mean(0)\n",
    "# bottom_neuron_high_difference_logprobs[all_MLP5_logprobs.mean(0) < -7] = 0\n",
    "\n",
    "# bottom_non_zero_count = (bottom_neuron_high_difference_logprobs > 0).sum()\n",
    "# bottom_neuron_high_difference_logprobs, bottom_indices = haystack_utils.top_k_with_exclude(bottom_neuron_high_difference_logprobs, min(bottom_non_zero_count, 50), all_ignore)\n",
    "# haystack_utils.line(bottom_neuron_high_difference_logprobs.cpu().numpy(), title='Largest positive difference in log probs for tokens when bottom neurons are not ablated', xticks=[model.to_str_tokens([i])[0] for i in bottom_indices])\n",
    "\n",
    "\n",
    "# top_neuron_high_difference_logprobs = (all_MLP5_logprobs - top_MLP5_ablated_logprobs)\n",
    "# top_neuron_high_difference_logprobs[top_neuron_high_difference_logprobs < 0] = 0\n",
    "# top_neuron_high_difference_logprobs = top_neuron_high_difference_logprobs.mean(0)\n",
    "# top_neuron_high_difference_logprobs[all_MLP5_logprobs.mean(0) < -7] = 0\n",
    "\n",
    "# top_non_zero_count = (top_neuron_high_difference_logprobs > 0).sum()\n",
    "# top_neuron_high_difference_logprobs, top_indices = haystack_utils.top_k_with_exclude(top_neuron_high_difference_logprobs, min(top_non_zero_count, 50), all_ignore)\n",
    "# haystack_utils.line(top_neuron_high_difference_logprobs.cpu().numpy(), title='Largest positive difference in log probs for tokens when bottom neurons are not ablated', xticks=[model.to_str_tokens([i])[0] for i in top_indices])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
