{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from transformer_lens import HookedTransformer, ActivationCache, utils, patching\n",
    "from jaxtyping import Float, Int, Bool\n",
    "from torch import Tensor\n",
    "from tqdm.auto import tqdm\n",
    "import plotly.io as pio\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "pio.renderers.default = \"notebook_connected\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.autograd.set_grad_enabled(False)\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "from haystack_utils import get_mlp_activations\n",
    "import haystack_utils\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model EleutherAI/pythia-70m into HookedTransformer\n",
      "data/german_europarl.json: Loaded 2000 examples with 152 to 2000 characters each.\n",
      "data/english_europarl.json: Loaded 2000 examples with 165 to 2000 characters each.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2024678c026a4ffb871255c3fe1272b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78b6969ffed34642bb05a51f2cd1b63e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = HookedTransformer.from_pretrained(\"EleutherAI/pythia-70m\",\n",
    "    center_unembed=True,\n",
    "    center_writing_weights=True,\n",
    "    fold_ln=True,\n",
    "    device=device)\n",
    "\n",
    "german_data = haystack_utils.load_json_data(\"data/german_europarl.json\")[:200]\n",
    "english_data = haystack_utils.load_json_data(\"data/english_europarl.json\")[:200]\n",
    "\n",
    "\n",
    "english_activations = {}\n",
    "german_activations = {}\n",
    "for layer in range(3, 4):\n",
    "    english_activations[layer] = get_mlp_activations(english_data, layer, model, mean=False)\n",
    "    german_activations[layer] = get_mlp_activations(german_data, layer, model, mean=False)\n",
    "\n",
    "LAYER_TO_ABLATE = 3\n",
    "NEURONS_TO_ABLATE = [669]\n",
    "MEAN_ACTIVATION_ACTIVE = german_activations[LAYER_TO_ABLATE][:, NEURONS_TO_ABLATE].mean()\n",
    "MEAN_ACTIVATION_INACTIVE = english_activations[LAYER_TO_ABLATE][:, NEURONS_TO_ABLATE].mean()\n",
    "\n",
    "def deactivate_neurons_hook(value, hook):\n",
    "    value[:, :, NEURONS_TO_ABLATE] = MEAN_ACTIVATION_INACTIVE\n",
    "    return value\n",
    "deactivate_neurons_fwd_hooks=[(f'blocks.{LAYER_TO_ABLATE}.mlp.hook_post', deactivate_neurons_hook)]\n",
    "\n",
    "def activate_neurons_hook(value, hook):\n",
    "    value[:, :, NEURONS_TO_ABLATE] = MEAN_ACTIVATION_ACTIVE\n",
    "    return value\n",
    "activate_neurons_fwd_hooks=[(f'blocks.{LAYER_TO_ABLATE}.mlp.hook_post', activate_neurons_hook)]\n",
    "\n",
    "all_ignore, not_ignore = haystack_utils.get_weird_tokens(model, plot_norms=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find top common German tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67f8763b857f4c2e8fbfd029be46a54e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' der', 'en', ' die', ' und', 'ung', 'ä', ' in', ' den', ' des', ' zu', 'ch', 'n', 'st', 're', 'z', ' von', ' für', 'äsident', ' Pr', 'ischen', 't', 'ü', 'icht', 'gen', ' ist', ' auf', ' dass', 'ge', 'ig', ' im', 'in', ' über', 'g', ' das', 'te', ' er', 'men', ' w', 'es', ' an', 'ß', ' wir', ' eine', 'f', ' W', 'hen', 'w', ' Europ', ' ich', 'ungen', 'ren', 'le', ' dem', 'ten', ' ein', 'e', ' Z', ' Ver', 'der', ' B', ' mit', ' dies', 'h', ' nicht', 'ungs', 's', ' G', ' z', 'it', ' Herr', ' es', 'l', ' S', 'ich', 'lich', ' An', 'heit', 'ie', ' Er', ' zur', ' V', ' ver', 'u', 'hr', 'chaft', 'Der', ' Ich', ' Ab', ' haben', 'i', 'ant', 'chte', ' mö', 'er', ' K', 'igen', ' Ber', 'ür', ' Fra', 'em']\n"
     ]
    }
   ],
   "source": [
    "# Get top common german tokens excluding punctuation\n",
    "token_counts = torch.zeros(model.cfg.d_vocab).cuda()\n",
    "for example in tqdm(german_data):\n",
    "    tokens = model.to_tokens(example)\n",
    "    for token in tokens[0]:\n",
    "        token_counts[token.item()] += 1\n",
    "\n",
    "punctuation = [\"\\n\", \".\", \",\", \"!\", \"?\", \";\", \":\", \"-\", \"(\", \")\", \"[\", \"]\", \"{\", \"}\", \"<\", \">\", \"/\", \"\\\\\", \"\\\"\", \"'\"]\n",
    "leading_space_punctuation = [\" \" + char for char in punctuation]\n",
    "punctuation_tokens = model.to_tokens(punctuation + leading_space_punctuation + [' –', \" \", '  ', \"<|endoftext|>\"])[:, 1].flatten()\n",
    "token_counts[punctuation_tokens] = 0\n",
    "token_counts[all_ignore] = 0\n",
    "\n",
    "top_counts, top_tokens = torch.topk(token_counts, 100)\n",
    "print(model.to_str_tokens(top_tokens[:100]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of ngrams preceded by random prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_selection(tensor, n=12):\n",
    "    # Hacky replacement for np.random.choice\n",
    "    return tensor[torch.randperm(len(tensor))[:n]]\n",
    "\n",
    "def generate_random_prompts(end_string, n=50, length=12):\n",
    "    # Generate a batch of random prompts ending with a specific ngram\n",
    "    end_tokens = model.to_tokens(end_string).flatten()[1:]\n",
    "    prompts = []\n",
    "    for i in range(n):\n",
    "        prompt = get_random_selection(top_tokens[:max(50, length)], n=length).cuda()\n",
    "        prompt = torch.cat([prompt, end_tokens])\n",
    "        prompts.append(prompt)\n",
    "    prompts = torch.stack(prompts)\n",
    "    return prompts\n",
    "\n",
    "def replace_column(prompts: Int[Tensor, \"n_prompts n_tokens\"], token_index: int):\n",
    "    # Replaces a specific token position in a batch of prompts with random common German tokens\n",
    "    new_prompts = prompts.clone()\n",
    "    random_tokens = get_random_selection(top_tokens[:max(50, prompts.shape[0])], n=prompts.shape[0]).cuda()\n",
    "    new_prompts[:, token_index] = random_tokens\n",
    "    return new_prompts \n",
    "\n",
    "def loss_analysis(prompts: Tensor, title=\"\"):\n",
    "    # Loss plot for a batch of prompts\n",
    "    names = [\"Original\", \"Ablated\", \"MLP5 path patched\"]\n",
    "    original_loss, ablated_loss, _, only_activated_loss = \\\n",
    "        haystack_utils.get_direct_effect(prompts, model, pos=-1,\n",
    "                                        context_ablation_hooks=deactivate_neurons_fwd_hooks, \n",
    "                                        context_activation_hooks=activate_neurons_fwd_hooks, \n",
    "                                        )\n",
    "    haystack_utils.plot_barplot([original_loss.tolist(), ablated_loss.tolist(), only_activated_loss.tolist()], names, ylabel=\"Loss\", title=title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_analysis_random_prompts(end_string, n=50, length=12, replace_columns: list[int] | None = None):\n",
    "    # Loss plot for a batch of random prompts ending with a specific ngram and optionally replacing specific tokens\n",
    "    prompts = generate_random_prompts(end_string, n=n, length=length)\n",
    "    title=f\"Average last token loss on {length} random tokens ending in '{end_string}'\"\n",
    "    if replace_columns is not None:\n",
    "        replaced_tokens = model.to_str_tokens(prompts[0, replace_columns])\n",
    "        title += f\" replacing {replaced_tokens}\"\n",
    "        for column in replace_columns:\n",
    "            prompts = replace_column(prompts, column)\n",
    "    \n",
    "    loss_analysis(prompts, title=title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "# loss_analysis_random_prompts(\" Vorschlägen\", n=100, length=20)\n",
    "# loss_analysis_random_prompts(\" Vorschlägen\", n=100, length=20, replace_columns=[-2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e1ea624fb324db0af76a6e2dafe31c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Ngram: ', options=(' Vorschlägen', ' Vorschläge', ' häufig', ' schließt', ' beweglich'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8afd9f2d8e5c4e3e9cb55ee35fad20d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SelectMultiple(description='Replace Columns:', index=(3,), options=('-2', '-3', '-4', 'None'), value=('None',)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d72e9e24edad49e5b719c66d51159145",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a dropdown menu widget\n",
    "dropdown = widgets.Dropdown(\n",
    "    options = [\" Vorschlägen\", \" Vorschläge\", \" häufig\", \" schließt\", \" beweglich\"], \n",
    "    value = \" Vorschlägen\",\n",
    "    description = 'Ngram: ',\n",
    ")\n",
    "\n",
    "replace_columns_dropdown = widgets.SelectMultiple(\n",
    "    options = ['-2', '-3', '-4', 'None'],\n",
    "    value = ['None'],  # default selected value\n",
    "    description = 'Replace Columns:',\n",
    ")\n",
    "\n",
    "# Create an output widget to hold the plot\n",
    "output = widgets.Output()\n",
    "\n",
    "# Define a function to call when the widget's value changes\n",
    "def update_plot(*args):\n",
    "    # Clear the old plot from the output\n",
    "    with output:\n",
    "        clear_output(wait=True)\n",
    "\n",
    "        if 'None' in replace_columns_dropdown.value:\n",
    "            replace_columns = None\n",
    "        else:\n",
    "            # If 'None' not selected, convert the selected values to integers\n",
    "            replace_columns = [int(val) for val in replace_columns_dropdown.value]\n",
    "        \n",
    "        # Call your function with the values from the widgets\n",
    "        loss_analysis_random_prompts(dropdown.value, n=100, length=20, replace_columns=replace_columns)\n",
    "\n",
    "# Set the function to be called when the widget's value changes\n",
    "dropdown.observe(update_plot, 'value')\n",
    "replace_columns_dropdown.observe(update_plot, 'value')\n",
    "\n",
    "# Display the widget and the output\n",
    "display(dropdown, replace_columns_dropdown, output)\n",
    "\n",
    "# Run once at startup\n",
    "update_plot()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify relevant neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 24])\n"
     ]
    }
   ],
   "source": [
    "prompts = generate_random_prompts(\" Vorschlägen\", n=100, length=20)\n",
    "print(prompts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "715ee59aeac84a9496469523c829dd95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2048 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0007)\n"
     ]
    }
   ],
   "source": [
    "with model.hooks(deactivate_neurons_fwd_hooks):\n",
    "    _, ablated_cache = model.run_with_cache(prompts)\n",
    "\n",
    "def get_ablate_neurons_hook(neuron: int | list[int], ablated_cache, layer=5):\n",
    "    def ablate_neurons_hook(value, hook):\n",
    "        value[:, :, neuron] = ablated_cache[f'blocks.{layer}.mlp.hook_post'][:, :, neuron]\n",
    "        return value\n",
    "    return [(f'blocks.{layer}.mlp.hook_post', ablate_neurons_hook)]\n",
    "\n",
    "diffs = torch.zeros(2048, prompts.shape[0])\n",
    "# Loss with path patched MLP5 neurons\n",
    "_, _, _, baseline_loss = haystack_utils.get_direct_effect(prompts, model, pos=-1, context_ablation_hooks=deactivate_neurons_fwd_hooks, context_activation_hooks=activate_neurons_fwd_hooks)\n",
    "for neuron in tqdm(range(2048)):\n",
    "    ablate_single_neuron_hook = get_ablate_neurons_hook(neuron, ablated_cache)\n",
    "    # Loss with path patched MLP5 neurons but a single neuron changed back to original ablated value\n",
    "    _, _, _, only_activated_loss = haystack_utils.get_direct_effect(prompts, model, pos=-1, context_ablation_hooks=deactivate_neurons_fwd_hooks, context_activation_hooks=activate_neurons_fwd_hooks+ablate_single_neuron_hook)\n",
    "    diffs[neuron] = only_activated_loss - baseline_loss\n",
    "\n",
    "print(diffs.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>                            <div id=\"70f0ba39-eb0a-4ff6-9250-787f0a710437\" class=\"plotly-graph-div\" style=\"height:525px; width:800px;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"70f0ba39-eb0a-4ff6-9250-787f0a710437\")) {                    Plotly.newPlot(                        \"70f0ba39-eb0a-4ff6-9250-787f0a710437\",                        [{\"hovertemplate\":\"variable=0\\u003cbr\\u003eindex=%{x}\\u003cbr\\u003evalue=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"0\",\"line\":{\"color\":\"#636efa\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"0\",\"showlegend\":true,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542,543,544,545,546,547,548,549,550,551,552,553,554,555,556,557,558,559,560,561,562,563,564,565,566,567,568,569,570,571,572,573,574,575,576,577,578,579,580,581,582,583,584,585,586,587,588,589,590,591,592,593,594,595,596,597,598,599,600,601,602,603,604,605,606,607,608,609,610,611,612,613,614,615,616,617,618,619,620,621,622,623,624,625,626,627,628,629,630,631,632,633,634,635,636,637,638,639,640,641,642,643,644,645,646,647,648,649,650,651,652,653,654,655,656,657,658,659,660,661,662,663,664,665,666,667,668,669,670,671,672,673,674,675,676,677,678,679,680,681,682,683,684,685,686,687,688,689,690,691,692,693,694,695,696,697,698,699,700,701,702,703,704,705,706,707,708,709,710,711,712,713,714,715,716,717,718,719,720,721,722,723,724,725,726,727,728,729,730,731,732,733,734,735,736,737,738,739,740,741,742,743,744,745,746,747,748,749,750,751,752,753,754,755,756,757,758,759,760,761,762,763,764,765,766,767,768,769,770,771,772,773,774,775,776,777,778,779,780,781,782,783,784,785,786,787,788,789,790,791,792,793,794,795,796,797,798,799,800,801,802,803,804,805,806,807,808,809,810,811,812,813,814,815,816,817,818,819,820,821,822,823,824,825,826,827,828,829,830,831,832,833,834,835,836,837,838,839,840,841,842,843,844,845,846,847,848,849,850,851,852,853,854,855,856,857,858,859,860,861,862,863,864,865,866,867,868,869,870,871,872,873,874,875,876,877,878,879,880,881,882,883,884,885,886,887,888,889,890,891,892,893,894,895,896,897,898,899,900,901,902,903,904,905,906,907,908,909,910,911,912,913,914,915,916,917,918,919,920,921,922,923,924,925,926,927,928,929,930,931,932,933,934,935,936,937,938,939,940,941,942,943,944,945,946,947,948,949,950,951,952,953,954,955,956,957,958,959,960,961,962,963,964,965,966,967,968,969,970,971,972,973,974,975,976,977,978,979,980,981,982,983,984,985,986,987,988,989,990,991,992,993,994,995,996,997,998,999,1000,1001,1002,1003,1004,1005,1006,1007,1008,1009,1010,1011,1012,1013,1014,1015,1016,1017,1018,1019,1020,1021,1022,1023,1024,1025,1026,1027,1028,1029,1030,1031,1032,1033,1034,1035,1036,1037,1038,1039,1040,1041,1042,1043,1044,1045,1046,1047,1048,1049,1050,1051,1052,1053,1054,1055,1056,1057,1058,1059,1060,1061,1062,1063,1064,1065,1066,1067,1068,1069,1070,1071,1072,1073,1074,1075,1076,1077,1078,1079,1080,1081,1082,1083,1084,1085,1086,1087,1088,1089,1090,1091,1092,1093,1094,1095,1096,1097,1098,1099,1100,1101,1102,1103,1104,1105,1106,1107,1108,1109,1110,1111,1112,1113,1114,1115,1116,1117,1118,1119,1120,1121,1122,1123,1124,1125,1126,1127,1128,1129,1130,1131,1132,1133,1134,1135,1136,1137,1138,1139,1140,1141,1142,1143,1144,1145,1146,1147,1148,1149,1150,1151,1152,1153,1154,1155,1156,1157,1158,1159,1160,1161,1162,1163,1164,1165,1166,1167,1168,1169,1170,1171,1172,1173,1174,1175,1176,1177,1178,1179,1180,1181,1182,1183,1184,1185,1186,1187,1188,1189,1190,1191,1192,1193,1194,1195,1196,1197,1198,1199,1200,1201,1202,1203,1204,1205,1206,1207,1208,1209,1210,1211,1212,1213,1214,1215,1216,1217,1218,1219,1220,1221,1222,1223,1224,1225,1226,1227,1228,1229,1230,1231,1232,1233,1234,1235,1236,1237,1238,1239,1240,1241,1242,1243,1244,1245,1246,1247,1248,1249,1250,1251,1252,1253,1254,1255,1256,1257,1258,1259,1260,1261,1262,1263,1264,1265,1266,1267,1268,1269,1270,1271,1272,1273,1274,1275,1276,1277,1278,1279,1280,1281,1282,1283,1284,1285,1286,1287,1288,1289,1290,1291,1292,1293,1294,1295,1296,1297,1298,1299,1300,1301,1302,1303,1304,1305,1306,1307,1308,1309,1310,1311,1312,1313,1314,1315,1316,1317,1318,1319,1320,1321,1322,1323,1324,1325,1326,1327,1328,1329,1330,1331,1332,1333,1334,1335,1336,1337,1338,1339,1340,1341,1342,1343,1344,1345,1346,1347,1348,1349,1350,1351,1352,1353,1354,1355,1356,1357,1358,1359,1360,1361,1362,1363,1364,1365,1366,1367,1368,1369,1370,1371,1372,1373,1374,1375,1376,1377,1378,1379,1380,1381,1382,1383,1384,1385,1386,1387,1388,1389,1390,1391,1392,1393,1394,1395,1396,1397,1398,1399,1400,1401,1402,1403,1404,1405,1406,1407,1408,1409,1410,1411,1412,1413,1414,1415,1416,1417,1418,1419,1420,1421,1422,1423,1424,1425,1426,1427,1428,1429,1430,1431,1432,1433,1434,1435,1436,1437,1438,1439,1440,1441,1442,1443,1444,1445,1446,1447,1448,1449,1450,1451,1452,1453,1454,1455,1456,1457,1458,1459,1460,1461,1462,1463,1464,1465,1466,1467,1468,1469,1470,1471,1472,1473,1474,1475,1476,1477,1478,1479,1480,1481,1482,1483,1484,1485,1486,1487,1488,1489,1490,1491,1492,1493,1494,1495,1496,1497,1498,1499,1500,1501,1502,1503,1504,1505,1506,1507,1508,1509,1510,1511,1512,1513,1514,1515,1516,1517,1518,1519,1520,1521,1522,1523,1524,1525,1526,1527,1528,1529,1530,1531,1532,1533,1534,1535,1536,1537,1538,1539,1540,1541,1542,1543,1544,1545,1546,1547,1548,1549,1550,1551,1552,1553,1554,1555,1556,1557,1558,1559,1560,1561,1562,1563,1564,1565,1566,1567,1568,1569,1570,1571,1572,1573,1574,1575,1576,1577,1578,1579,1580,1581,1582,1583,1584,1585,1586,1587,1588,1589,1590,1591,1592,1593,1594,1595,1596,1597,1598,1599,1600,1601,1602,1603,1604,1605,1606,1607,1608,1609,1610,1611,1612,1613,1614,1615,1616,1617,1618,1619,1620,1621,1622,1623,1624,1625,1626,1627,1628,1629,1630,1631,1632,1633,1634,1635,1636,1637,1638,1639,1640,1641,1642,1643,1644,1645,1646,1647,1648,1649,1650,1651,1652,1653,1654,1655,1656,1657,1658,1659,1660,1661,1662,1663,1664,1665,1666,1667,1668,1669,1670,1671,1672,1673,1674,1675,1676,1677,1678,1679,1680,1681,1682,1683,1684,1685,1686,1687,1688,1689,1690,1691,1692,1693,1694,1695,1696,1697,1698,1699,1700,1701,1702,1703,1704,1705,1706,1707,1708,1709,1710,1711,1712,1713,1714,1715,1716,1717,1718,1719,1720,1721,1722,1723,1724,1725,1726,1727,1728,1729,1730,1731,1732,1733,1734,1735,1736,1737,1738,1739,1740,1741,1742,1743,1744,1745,1746,1747,1748,1749,1750,1751,1752,1753,1754,1755,1756,1757,1758,1759,1760,1761,1762,1763,1764,1765,1766,1767,1768,1769,1770,1771,1772,1773,1774,1775,1776,1777,1778,1779,1780,1781,1782,1783,1784,1785,1786,1787,1788,1789,1790,1791,1792,1793,1794,1795,1796,1797,1798,1799,1800,1801,1802,1803,1804,1805,1806,1807,1808,1809,1810,1811,1812,1813,1814,1815,1816,1817,1818,1819,1820,1821,1822,1823,1824,1825,1826,1827,1828,1829,1830,1831,1832,1833,1834,1835,1836,1837,1838,1839,1840,1841,1842,1843,1844,1845,1846,1847,1848,1849,1850,1851,1852,1853,1854,1855,1856,1857,1858,1859,1860,1861,1862,1863,1864,1865,1866,1867,1868,1869,1870,1871,1872,1873,1874,1875,1876,1877,1878,1879,1880,1881,1882,1883,1884,1885,1886,1887,1888,1889,1890,1891,1892,1893,1894,1895,1896,1897,1898,1899,1900,1901,1902,1903,1904,1905,1906,1907,1908,1909,1910,1911,1912,1913,1914,1915,1916,1917,1918,1919,1920,1921,1922,1923,1924,1925,1926,1927,1928,1929,1930,1931,1932,1933,1934,1935,1936,1937,1938,1939,1940,1941,1942,1943,1944,1945,1946,1947,1948,1949,1950,1951,1952,1953,1954,1955,1956,1957,1958,1959,1960,1961,1962,1963,1964,1965,1966,1967,1968,1969,1970,1971,1972,1973,1974,1975,1976,1977,1978,1979,1980,1981,1982,1983,1984,1985,1986,1987,1988,1989,1990,1991,1992,1993,1994,1995,1996,1997,1998,1999,2000,2001,2002,2003,2004,2005,2006,2007,2008,2009,2010,2011,2012,2013,2014,2015,2016,2017,2018,2019,2020,2021,2022,2023,2024,2025,2026,2027,2028,2029,2030,2031,2032,2033,2034,2035,2036,2037,2038,2039,2040,2041,2042,2043,2044,2045,2046,2047],\"xaxis\":\"x\",\"y\":[-0.6054291725158691,-0.315167635679245,-0.213931143283844,-0.16674861311912537,-0.16641965508460999,-0.16595062613487244,-0.14754614233970642,-0.12966491281986237,-0.12110500037670135,-0.11973629891872406,-0.1152930036187172,-0.11083187162876129,-0.09841438382863998,-0.0968722328543663,-0.09306725859642029,-0.08944074809551239,-0.08042851090431213,-0.08008259534835815,-0.07961190491914749,-0.07940065860748291,-0.07735755294561386,-0.07716724276542664,-0.07640518993139267,-0.075047068297863,-0.07359149307012558,-0.07251477986574173,-0.0718015506863594,-0.07096879184246063,-0.06994292140007019,-0.06910198926925659,-0.06782297790050507,-0.06780221313238144,-0.06509118527173996,-0.0626315101981163,-0.061831794679164886,-0.06181689724326134,-0.06166752427816391,-0.061472877860069275,-0.060668498277664185,-0.05946877598762512,-0.05925561860203743,-0.055819228291511536,-0.05530088022351265,-0.055078476667404175,-0.05491971969604492,-0.05437244102358818,-0.05318845808506012,-0.052935197949409485,-0.05156877636909485,-0.05122581496834755,-0.05021186172962189,-0.049250178039073944,-0.04786791279911995,-0.04753413051366806,-0.04708734154701233,-0.04466274753212929,-0.04375425726175308,-0.042403411120176315,-0.040324822068214417,-0.04028717055916786,-0.03979967534542084,-0.03955827280879021,-0.0384543240070343,-0.03803546354174614,-0.03791985288262367,-0.037803780287504196,-0.037522319704294205,-0.03704918920993805,-0.03683316707611084,-0.03662577271461487,-0.03627226874232292,-0.03621004894375801,-0.035163406282663345,-0.035114314407110214,-0.03481345996260643,-0.034641608595848083,-0.034205466508865356,-0.03367629274725914,-0.03354231268167496,-0.033176910132169724,-0.03251563012599945,-0.03235635161399841,-0.03221522644162178,-0.031581394374370575,-0.03137888014316559,-0.0307181216776371,-0.03040786273777485,-0.030263109132647514,-0.030118830502033234,-0.029971981421113014,-0.029818745329976082,-0.02981104701757431,-0.029637806117534637,-0.02952687442302704,-0.02946125902235508,-0.029405847191810608,-0.0293746180832386,-0.029059534892439842,-0.028930440545082092,-0.02836206741631031,-0.028322450816631317,-0.028270306065678596,-0.02799999713897705,-0.0275605209171772,-0.027490537613630295,-0.02710261382162571,-0.02706562541425228,-0.02682097628712654,-0.026621613651514053,-0.026620203629136086,-0.026294631883502007,-0.02629193477332592,-0.026290882378816605,-0.026086799800395966,-0.024617627263069153,-0.024570627138018608,-0.024252014234662056,-0.024199228733778,-0.024025117978453636,-0.02390514314174652,-0.02359696850180626,-0.023594044148921967,-0.023456113412976265,-0.023394135758280754,-0.02289818786084652,-0.02281351573765278,-0.02273552119731903,-0.022628551349043846,-0.022607039660215378,-0.021968884393572807,-0.021583253517746925,-0.02157304808497429,-0.021014250814914703,-0.020837003365159035,-0.02069634012877941,-0.020283006131649017,-0.020202098414301872,-0.020059576258063316,-0.02001427859067917,-0.019847862422466278,-0.019530892372131348,-0.01922404021024704,-0.01893758960068226,-0.018914178013801575,-0.018653294071555138,-0.01843057945370674,-0.018326960504055023,-0.01828552968800068,-0.01815728284418583,-0.01794656179845333,-0.01793401874601841,-0.01775975152850151,-0.017385726794600487,-0.01710003986954689,-0.017085161060094833,-0.01696886122226715,-0.016932150349020958,-0.016920559108257294,-0.016792159527540207,-0.01674976386129856,-0.016552742570638657,-0.01651281677186489,-0.016480980440974236,-0.016040753573179245,-0.015927892178297043,-0.01592050865292549,-0.015897229313850403,-0.015772098675370216,-0.015699738636612892,-0.015658825635910034,-0.015556114725768566,-0.015476113185286522,-0.015191401354968548,-0.01516771875321865,-0.01515083760023117,-0.01496852282434702,-0.014144391752779484,-0.01397152990102768,-0.013772492296993732,-0.013655870221555233,-0.013633535243570805,-0.013591817580163479,-0.013591598719358444,-0.013567510060966015,-0.013539671897888184,-0.013508710078895092,-0.013410471379756927,-0.01329864002764225,-0.013160197995603085,-0.013155446387827396,-0.013060249388217926,-0.012980875559151173,-0.012895672582089901,-0.012757204473018646,-0.012609818018972874,-0.012389102019369602,-0.012343940325081348,-0.012306058779358864,-0.01223018392920494,-0.011950530111789703,-0.011917799711227417,-0.011785414069890976,-0.011756452731788158,-0.011737857013940811,-0.011737738735973835,-0.011630215682089329,-0.011600039899349213,-0.011575622484087944,-0.011537086218595505,-0.011512530967593193,-0.011500321328639984,-0.011435729451477528,-0.01129758358001709,-0.011120013892650604,-0.011022866703569889,-0.010882934555411339,-0.01073639839887619,-0.010699586942791939,-0.0106891430914402,-0.010524284094572067,-0.010299635119736195,-0.010187778621912003,-0.010101783089339733,-0.01008429192006588,-0.009937289170920849,-0.00986598338931799,-0.009815172292292118,-0.009798447601497173,-0.00977581087499857,-0.00976948719471693,-0.009761042892932892,-0.009725077077746391,-0.00969190988689661,-0.009625107981264591,-0.009590771049261093,-0.009588870219886303,-0.00953780859708786,-0.00944542046636343,-0.009425905533134937,-0.009418864734470844,-0.009226808324456215,-0.009165456518530846,-0.009138435125350952,-0.008794081397354603,-0.00875899475067854,-0.008728502318263054,-0.008712662383913994,-0.008624549955129623,-0.008570410311222076,-0.008527485653758049,-0.008518815040588379,-0.008509404957294464,-0.008384477347135544,-0.008300305344164371,-0.008290836587548256,-0.008283158764243126,-0.008152369409799576,-0.00811920315027237,-0.008093263953924179,-0.008058438077569008,-0.00802074559032917,-0.007993432693183422,-0.007973461411893368,-0.007946173660457134,-0.007937608286738396,-0.007914640940725803,-0.00790537428110838,-0.007879444397985935,-0.007859595119953156,-0.007838033139705658,-0.007712900638580322,-0.007595396600663662,-0.007572855334728956,-0.0075598047114908695,-0.007434306666254997,-0.007414928637444973,-0.007396599743515253,-0.007373603526502848,-0.0073689366690814495,-0.007356762420386076,-0.0073560685850679874,-0.007320231292396784,-0.007316168397665024,-0.007288794033229351,-0.007266870699822903,-0.007238229736685753,-0.007143579889088869,-0.007126252632588148,-0.006991330999881029,-0.006988144014030695,-0.006904131732881069,-0.006869812496006489,-0.006861788220703602,-0.006838817615061998,-0.00678192637860775,-0.006754668429493904,-0.006704419385641813,-0.0066944812424480915,-0.006692848168313503,-0.006658976897597313,-0.0066438401117920876,-0.006604201626032591,-0.006578783970326185,-0.006573880091309547,-0.0064358217641711235,-0.006381279323250055,-0.0063614859245717525,-0.006359411403536797,-0.006350295152515173,-0.006339738145470619,-0.00633269315585494,-0.006297675427049398,-0.006255747750401497,-0.00624540401622653,-0.006188870873302221,-0.006170416250824928,-0.006125203799456358,-0.006110432092100382,-0.006096163764595985,-0.00608617952093482,-0.006018840242177248,-0.006016079802066088,-0.006002048030495644,-0.005977030377835035,-0.005953148473054171,-0.005936533212661743,-0.005919032730162144,-0.0058721499517560005,-0.005783676635473967,-0.005751617718487978,-0.005693414248526096,-0.005682960618287325,-0.005679027643054724,-0.00564779806882143,-0.005628929939121008,-0.005627987440675497,-0.0056177712976932526,-0.005603999365121126,-0.005599647760391235,-0.005598889198154211,-0.005541564431041479,-0.005528302397578955,-0.005473925266414881,-0.0054622190073132515,-0.005449249874800444,-0.005447191186249256,-0.005444674286991358,-0.005429619457572699,-0.005426847841590643,-0.00541454553604126,-0.005365264602005482,-0.005357154179364443,-0.005327311810106039,-0.0053038205951452255,-0.005244433414191008,-0.0052382610738277435,-0.005213524214923382,-0.005143578164279461,-0.005088644567877054,-0.0050385743379592896,-0.005036429967731237,-0.004986888263374567,-0.004968345630913973,-0.004921500571072102,-0.004918782040476799,-0.004918304737657309,-0.004913135431706905,-0.004896956030279398,-0.004896600730717182,-0.004870275035500526,-0.00486318115144968,-0.004828672390431166,-0.00481889583170414,-0.004760000854730606,-0.004736391361802816,-0.004712357651442289,-0.0046691601164639,-0.0046541644260287285,-0.0046018315479159355,-0.004538214299827814,-0.004515463020652533,-0.004503518342971802,-0.004428420215845108,-0.0044025154784321785,-0.004396377131342888,-0.004346736706793308,-0.004334175493568182,-0.004331702366471291,-0.004324583802372217,-0.004246458876878023,-0.004238605499267578,-0.004237111657857895,-0.004227636847645044,-0.004223579075187445,-0.0042215390130877495,-0.004203383810818195,-0.004196954425424337,-0.004187177401036024,-0.0041796183213591576,-0.004172542132437229,-0.004157918505370617,-0.004155488684773445,-0.004103369079530239,-0.004102460574358702,-0.004052833188325167,-0.00404759356752038,-0.004011849872767925,-0.003955058287829161,-0.003935626707971096,-0.003935565706342459,-0.003926789853721857,-0.003920530900359154,-0.003869223641231656,-0.0038619651459157467,-0.0037956079468131065,-0.0037846455816179514,-0.0037754701916128397,-0.003758379491046071,-0.0037364328745752573,-0.003730680327862501,-0.003682219423353672,-0.003660728922113776,-0.003631806466728449,-0.0036112284287810326,-0.003574046539142728,-0.0035602999851107597,-0.0035558855161070824,-0.0035484915133565664,-0.0035427140537649393,-0.00353446789085865,-0.003526825224980712,-0.0035035284236073494,-0.0034984308294951916,-0.00347832590341568,-0.00338885304518044,-0.003354982938617468,-0.0033471721690148115,-0.0033465633168816566,-0.003329312428832054,-0.0033257189206779003,-0.003308006562292576,-0.0033061783760786057,-0.003241168335080147,-0.00324074923992157,-0.0032319340389221907,-0.003224011277779937,-0.0032050032168626785,-0.0032006041146814823,-0.0031968357507139444,-0.003192309057340026,-0.0031890771351754665,-0.0031819718424230814,-0.003181224688887596,-0.0031759426929056644,-0.003156655700877309,-0.0030996801797300577,-0.003090648679062724,-0.0030805482529103756,-0.003041689284145832,-0.003037048038095236,-0.003010827349498868,-0.0030097300186753273,-0.0030029653571546078,-0.003001287579536438,-0.0029979264363646507,-0.002967344131320715,-0.0029593915678560734,-0.0029460338409990072,-0.002944611944258213,-0.002944214968010783,-0.002940972801297903,-0.0029338158201426268,-0.0029272036626935005,-0.0029225389007478952,-0.0028925896622240543,-0.0028918106108903885,-0.0028835020493716,-0.0028455588035285473,-0.0028399790171533823,-0.0028394863475114107,-0.0028302071150392294,-0.002827363321557641,-0.0028257230296730995,-0.002824314869940281,-0.0028038346208631992,-0.0028020860627293587,-0.002785817952826619,-0.00276769045740366,-0.0027664334047585726,-0.002752217696979642,-0.0027189659886062145,-0.0026696620043367147,-0.002645714208483696,-0.0026268402580171824,-0.002623047446832061,-0.0026025744155049324,-0.002583475783467293,-0.002575639868155122,-0.002573485719040036,-0.0025449946988373995,-0.002544774441048503,-0.002538766013458371,-0.002536695683375001,-0.002532885642722249,-0.002525506541132927,-0.0025195637717843056,-0.0025041403714567423,-0.0025019042659550905,-0.0024876517709344625,-0.0024759965017437935,-0.0024666236713528633,-0.002458896953612566,-0.0024463373702019453,-0.0024199914187192917,-0.002415077295154333,-0.0024072856176644564,-0.0023998438846319914,-0.00238687451928854,-0.0023853247985243797,-0.0023823666851967573,-0.0023729554377496243,-0.0023601704742759466,-0.0023454350885003805,-0.0023416937328875065,-0.0023388592526316643,-0.0023370797280222178,-0.0023336466401815414,-0.0023315243888646364,-0.002306522335857153,-0.0022945336531847715,-0.002287808572873473,-0.002285582013428211,-0.0022844523191452026,-0.0022829982917755842,-0.002281790366396308,-0.0022755220998078585,-0.002270015422254801,-0.0022653425112366676,-0.002261354587972164,-0.0022517365869134665,-0.0022496466990560293,-0.0022468678653240204,-0.002244641538709402,-0.0022361569572240114,-0.002231947146356106,-0.002228887053206563,-0.0022245764266699553,-0.0022017185110598803,-0.0021940062288194895,-0.0021605100482702255,-0.002145004691556096,-0.0021361438557505608,-0.002102202270179987,-0.002095651812851429,-0.002091851783916354,-0.0020827658008784056,-0.0020813983865082264,-0.002072448143735528,-0.0020671936217695475,-0.002063698135316372,-0.0020599651616066694,-0.002058015437796712,-0.002057667588815093,-0.0020519194658845663,-0.0020502705592662096,-0.002049750415608287,-0.00204732408747077,-0.002001166343688965,-0.0019870675168931484,-0.001961697591468692,-0.0019534751772880554,-0.0019502454670146108,-0.0019253561040386558,-0.0019082200014963746,-0.0018808364402502775,-0.001821832382120192,-0.0018152791308239102,-0.0018091733800247312,-0.0018080767476931214,-0.001790463924407959,-0.0017886324785649776,-0.0017857050988823175,-0.001767380628734827,-0.001764498301781714,-0.001762321568094194,-0.001733248122036457,-0.0017326984088867903,-0.0017127819592133164,-0.001706708804704249,-0.0017044720007106662,-0.0016982884844765067,-0.0016904693329706788,-0.0016820752061903477,-0.001670889207161963,-0.001656663604080677,-0.0016491818241775036,-0.0016459337202832103,-0.0016332146478816867,-0.001606357516720891,-0.0016011192928999662,-0.001593653578311205,-0.0015750373713672161,-0.0015703714452683926,-0.001562547287903726,-0.0015595024451613426,-0.0015588040696457028,-0.001551537774503231,-0.0015503003960475326,-0.001519311685115099,-0.0015101293101906776,-0.0014930807519704103,-0.0014924349961802363,-0.0014799752971157432,-0.0014625174226239324,-0.0014621710870414972,-0.0014560723211616278,-0.0014537012903019786,-0.0014382255030795932,-0.0014349088305607438,-0.001433156430721283,-0.0014184208121150732,-0.001414784463122487,-0.0014127737376838923,-0.0014112034114077687,-0.0013961876975372434,-0.0013960542855784297,-0.0013926243409514427,-0.0013904571533203125,-0.0013850692193955183,-0.0013841757318004966,-0.0013835568679496646,-0.001379108289256692,-0.0013762452872470021,-0.0013719823909923434,-0.001367578748613596,-0.001365188043564558,-0.0013609422603622079,-0.0013576671481132507,-0.001330220140516758,-0.0013284581946209073,-0.0013256819220259786,-0.0013249984476715326,-0.0013221831759437919,-0.001318918657489121,-0.001311848871409893,-0.0013115700567141175,-0.001310186693444848,-0.0013030758127570152,-0.0012999566970393062,-0.0012782105477526784,-0.0012755877105519176,-0.0012739726807922125,-0.0012723265681415796,-0.0012536161812022328,-0.0012497358256950974,-0.0012433743104338646,-0.0012396250385791063,-0.0012360448017716408,-0.001213346840813756,-0.0012099365703761578,-0.00118704279884696,-0.0011866108980029821,-0.0011836951598525047,-0.001180244144052267,-0.0011490161996334791,-0.0011446477146819234,-0.0011371574364602566,-0.0011313343420624733,-0.0011266361689195037,-0.0011258890153840184,-0.0011204708134755492,-0.001117134583182633,-0.0011144328163936734,-0.0010873869759961963,-0.0010802993783727288,-0.0010687434114515781,-0.0010658915853127837,-0.0010654734214767814,-0.0010633826022967696,-0.0010355898411944509,-0.0010307193733751774,-0.0010250702034682035,-0.0010245651938021183,-0.0010242585558444262,-0.001023852382786572,-0.00101445650216192,-0.001013160333968699,-0.0010120770893990993,-0.0010032288264483213,-0.0009925706544891,-0.000986929633654654,-0.0009786048904061317,-0.0009615804301574826,-0.0009600823395885527,-0.0009590508998371661,-0.0009516861173324287,-0.0009453975944779813,-0.0009318202501162887,-0.0009316620416939259,-0.0009248324204236269,-0.0009199399501085281,-0.0009088274091482162,-0.0009048821520991623,-0.0009025092585943639,-0.0008900375105440617,-0.0008872597827576101,-0.0008813613094389439,-0.000879370782058686,-0.0008606559713371098,-0.0008557003457099199,-0.0008547328179702163,-0.0008454798371531069,-0.0008400082588195801,-0.0008375971810892224,-0.0008242172189056873,-0.0008215127745643258,-0.0008183698519133031,-0.0008090894552879035,-0.0007952570449560881,-0.0007909737760201097,-0.0007841536425985396,-0.0007802366744726896,-0.0007775138365104795,-0.0007765809423290193,-0.0007761032902635634,-0.0007760666194371879,-0.0007640309631824493,-0.0007634303183294833,-0.0007627008017152548,-0.0007617493974976242,-0.0007605554419569671,-0.0007583407568745315,-0.0007544996333308518,-0.0007430453551933169,-0.0007416039588861167,-0.0007415867876261473,-0.0007403360214084387,-0.0007383411284536123,-0.0007374598062597215,-0.000733960943762213,-0.0007252697832882404,-0.0007211877964437008,-0.0007190598407760262,-0.0007150492747314274,-0.0007148603326641023,-0.0007095295004546642,-0.000702609249856323,-0.0007025167578831315,-0.0006830801721662283,-0.0006811719504185021,-0.0006753972847945988,-0.000674515264108777,-0.0006720750243403018,-0.0006652238080278039,-0.0006609537522308528,-0.000660049554426223,-0.0006579351611435413,-0.0006441375589929521,-0.0006404949817806482,-0.0006327145383693278,-0.0006289241719059646,-0.0006141822086647153,-0.0006069312221370637,-0.000606822082772851,-0.0006020446307957172,-0.000596742844209075,-0.0005918468814343214,-0.0005887054721824825,-0.0005823785322718322,-0.0005817993078380823,-0.0005701895570382476,-0.0005574753158725798,-0.0005566929467022419,-0.0005552813527174294,-0.000554234255105257,-0.0005531359347514808,-0.0005523419240489602,-0.0005495575023815036,-0.0005449588061310351,-0.0005446132272481918,-0.0005444950656965375,-0.0005402888637036085,-0.0005312704015523195,-0.0005230871611274779,-0.0005214856355451047,-0.0005214319098740816,-0.0005145046743564308,-0.0005142393638379872,-0.000513461185619235,-0.0005126366741023958,-0.0005103594157844782,-0.0005056177033111453,-0.0005048284074291587,-0.0005004379199817777,-0.000498166133183986,-0.0004973967443220317,-0.0004957641940563917,-0.000481425755424425,-0.0004772988613694906,-0.00046773464418947697,-0.0004673957882914692,-0.0004667493631131947,-0.00046511716209352016,-0.0004648365720640868,-0.0004562758549582213,-0.00044872143189422786,-0.000435200403444469,-0.00042992361704818904,-0.0004256136016920209,-0.00042493612272664905,-0.000420591386500746,-0.0004171541368123144,-0.0004077822086401284,-0.0004069049609825015,-0.0003979614994022995,-0.0003944820782635361,-0.0003938104200642556,-0.00039123825263231993,-0.00038883090019226074,-0.0003878459974657744,-0.0003871109220199287,-0.00038404465885832906,-0.00038106233114376664,-0.00038079454679973423,-0.0003806894237641245,-0.00037982501089572906,-0.0003782785788644105,-0.00037479677121154964,-0.0003737171064130962,-0.00037279375828802586,-0.0003723809204529971,-0.0003631029394455254,-0.00035676173865795135,-0.0003495404962450266,-0.0003462873282842338,-0.0003461620945017785,-0.00034410401713103056,-0.0003357809910085052,-0.00033470281050540507,-0.0003344409051351249,-0.0003330992767587304,-0.00033041471033357084,-0.0003237652708776295,-0.00031861261231824756,-0.0003145214868709445,-0.0003133293357677758,-0.00030347361462190747,-0.0002919475664384663,-0.00029095032368786633,-0.0002886079892050475,-0.00028362884768284857,-0.00028046645456925035,-0.0002781896910164505,-0.0002753467997536063,-0.0002751980791799724,-0.00027109013171866536,-0.0002670984831638634,-0.000261947134276852,-0.0002595621335785836,-0.0002580805739853531,-0.0002572158118709922,-0.00025254301726818085,-0.0002487628080416471,-0.0002466779260430485,-0.00023612081713508815,-0.00023324668291024864,-0.0002302203356521204,-0.0002295534359291196,-0.00022914953297004104,-0.00022795818222220987,-0.00022589840227738023,-0.00021908283815719187,-0.00021688229753635824,-0.0002115506649715826,-0.00020850881992373616,-0.00020705469069071114,-0.00019874624558724463,-0.0001953114551724866,-0.00019122562662232667,-0.00019102200167253613,-0.00018775687203742564,-0.00018589876708574593,-0.0001848723040893674,-0.0001848660467658192,-0.00018381222616881132,-0.00017638989083934575,-0.00017562269931659102,-0.00016984008834697306,-0.000165796052897349,-0.00016476958990097046,-0.0001642541610635817,-0.00016318290727213025,-0.00016192659677471966,-0.0001607220619916916,-0.0001591859763721004,-0.0001576445938553661,-0.00015758269000798464,-0.00015346684085670859,-0.0001529426808701828,-0.00015214183076750487,-0.00015039653226267546,-0.0001496358891017735,-0.00014867506979499012,-0.00014603785530198365,-0.00014006071432959288,-0.0001390063698636368,-0.00013879165635444224,-0.0001385226787533611,-0.0001369597011944279,-0.00013501122884918004,-0.0001332322572125122,-0.0001327999634668231,-0.00013211622717790306,-0.00012761980178765953,-0.0001218496254296042,-0.00012134402641095221,-0.00012047432392137125,-0.00011728294339263812,-0.00011530600750120357,-0.00011481583351269364,-0.00011093958892161027,-0.00011089019244536757,-0.0001102426613215357,-0.00010877333261305466,-0.00010577522334642708,-0.00010559395013842732,-0.00010457530879648402,-0.00010453037975821644,-9.532101103104651e-05,-9.347483864985406e-05,-9.331628825748339e-05,-9.329713793704286e-05,-9.192094148602337e-05,-9.183965448755771e-05,-8.748598338570446e-05,-8.65983238327317e-05,-8.580661960877478e-05,-8.535467350156978e-05,-7.94421139289625e-05,-7.862433994887397e-05,-7.78279427322559e-05,-7.61366609367542e-05,-7.497847400372848e-05,-7.448099495377392e-05,-7.150076271500438e-05,-6.841115828137845e-05,-6.777696398785338e-05,-6.484158075181767e-05,-6.121851765783504e-05,-6.105795182520524e-05,-5.826540291309357e-05,-5.553409573622048e-05,-5.4632575483992696e-05,-5.301110286382027e-05,-5.035631329519674e-05,-4.9841775762615725e-05,-4.8633366532158107e-05,-4.657335739466362e-05,-4.48290265921969e-05,-4.396624717628583e-05,-4.395224095787853e-05,-4.3421314330771565e-05,-4.337661084719002e-05,-4.079848440596834e-05,-4.048742266604677e-05,-4.035361052956432e-05,-3.922328323824331e-05,-3.9089471101760864e-05,-3.605224264902063e-05,-3.597915201680735e-05,-3.37268429575488e-05,-3.2860934879863635e-05,-3.271505192969926e-05,-3.188401387888007e-05,-2.8345584723865613e-05,-2.8307660613791086e-05,-2.623364343889989e-05,-2.5500208721496165e-05,-2.3140386474551633e-05,-2.1436362658278085e-05,-2.0503550331341103e-05,-1.9603519831434824e-05,-1.835122748161666e-05,-1.7271637261728756e-05,-1.6213953131227754e-05,-1.4797970834479202e-05,-1.443639393983176e-05,-1.3379230949794874e-05,-1.2700632396445144e-05,-1.2401342246448621e-05,-1.0260790986649226e-05,-6.6459924710216e-06,-6.242841664061416e-06,-6.091073373681866e-06,-5.894303285458591e-06,-5.667358436767245e-06,-4.179477855359437e-06,-3.319755251141032e-06,-2.8999895675951848e-06,-2.6641785098036053e-06,-1.9025058008992346e-06,-1.8514692783355713e-06,-6.486475285782944e-07,0.0,1.3150274469353462e-07,2.785772039715084e-07,7.970631372700154e-07,3.1616539217793616e-06,3.1732022307551233e-06,3.542676495271735e-06,3.7137419894861523e-06,4.028379862575093e-06,5.331486590876011e-06,7.638931492692791e-06,8.619502295914572e-06,9.238124221155886e-06,9.531825526210014e-06,1.2225583304825705e-05,1.2863203664892353e-05,1.2946576134709176e-05,1.4091953744355123e-05,1.4278292837843765e-05,1.4432892385229934e-05,1.537926436867565e-05,1.636311390029732e-05,1.745417648635339e-05,1.8579959942144342e-05,1.9334256649017334e-05,2.0539537217700854e-05,2.3588090698467568e-05,2.3666545530431904e-05,2.468086859153118e-05,2.5690496840979904e-05,2.7454569135443307e-05,2.8582662707776763e-05,2.973303162434604e-05,2.99096845992608e-05,3.012843444594182e-05,3.0267983675003052e-05,3.079876478295773e-05,3.1640156521461904e-05,3.166675742249936e-05,3.250516965636052e-05,3.332197593408637e-05,3.507495057419874e-05,3.5759210732067004e-05,3.592155917431228e-05,3.721565008163452e-05,3.737919178092852e-05,3.745019421330653e-05,4.2550487705739215e-05,4.3771193304564804e-05,4.648566391551867e-05,4.776149944518693e-05,4.979878576705232e-05,5.028977830079384e-05,5.0603674026206136e-05,5.170293297851458e-05,5.221374158281833e-05,5.295246955938637e-05,5.392193634179421e-05,5.441218672785908e-05,5.832858369103633e-05,5.927376332692802e-05,5.9713347582146525e-05,6.205983663676307e-05,6.406798638636246e-05,6.499774463009089e-05,6.828561163274571e-05,6.987273809500039e-05,6.993055285420269e-05,7.21739197615534e-05,7.330492371693254e-05,7.35063076717779e-05,7.628552702954039e-05,7.696486864006147e-05,7.719799759797752e-05,7.969028956722468e-05,8.799209899734706e-05,8.84398104972206e-05,8.929416799219325e-05,8.94334152690135e-05,9.097866859519854e-05,9.288624278269708e-05,9.358667011838406e-05,9.372688509756699e-05,9.904742182698101e-05,0.00010006516822613776,0.0001035062232404016,0.00010659992403816432,0.00010717004624893889,0.00010765045590233058,0.00010835163266165182,0.00011082157288910821,0.00011269472452113405,0.00011298678873572499,0.00012094221892766654,0.00012151703413110226,0.00012690901348832995,0.00012791842164006084,0.00013023421342950314,0.00013030074478592724,0.0001348377700196579,0.00013765781477559358,0.00013789742661174387,0.00013984180986881256,0.0001398881577188149,0.00014270178508013487,0.00014618426212109625,0.00014708988601341844,0.00014740772894583642,0.00014870964514557272,0.00015225872630253434,0.0001542013924336061,0.0001551349414512515,0.00015598423487972468,0.0001563768892083317,0.00015930234803818166,0.00016140796651598066,0.00016774550022091717,0.00017053134797606617,0.00017822034715209156,0.00018751501920633018,0.0001914057065732777,0.00019178040383849293,0.00019713565416168422,0.00019853055709972978,0.00020010762091260403,0.00020264476188458502,0.00020542465790640563,0.00020740367472171783,0.0002121280849678442,0.0002122381265508011,0.00021910094073973596,0.00022140539658721536,0.00022342502779792994,0.00022353872191160917,0.0002268158714286983,0.0002280191401951015,0.00022988274577073753,0.00023260920715983957,0.00023287668591365218,0.0002405671839369461,0.00024462162400595844,0.00024692690931260586,0.00024904421297833323,0.0002523747971281409,0.00025564595125615597,0.0002561669098213315,0.0002599330910015851,0.00026732683181762695,0.00026912204339168966,0.0002704019716475159,0.0002713593712542206,0.00027700126520358026,0.000279789644991979,0.0002827999123837799,0.00028307101456448436,0.00028411103994585574,0.0002857206854969263,0.00028585398104041815,0.00028864399064332247,0.0002924936998169869,0.00029291518148966134,0.00030783406691625714,0.00031040870817378163,0.0003117343003395945,0.0003147069364786148,0.0003168802650179714,0.0003191758587490767,0.00031923799542710185,0.00032500678207725286,0.0003302868572063744,0.00033235191949643195,0.0003337051603011787,0.0003343592688906938,0.000335072138113901,0.00033591032843105495,0.0003363615251146257,0.0003390745841898024,0.00033954731770791113,0.00034289195900782943,0.00034501866321079433,0.00035141207627020776,0.00035143710556440055,0.0003517841687425971,0.0003554997674655169,0.0003556614392437041,0.00036256969906389713,0.000364026433089748,0.0003670525620691478,0.0003685786505229771,0.00037022450123913586,0.0003722930559888482,0.0003761531552299857,0.0003767564194276929,0.0003800831036642194,0.0003810668713413179,0.00038115851930342615,0.00038367934757843614,0.00038975357892923057,0.00039263605140149593,0.00039527370245195925,0.0003983588539995253,0.00040191769949160516,0.00040964968502521515,0.00042134785326197743,0.00042366221896372736,0.000428388942964375,0.00043077394366264343,0.0004450664564501494,0.00045252099516801536,0.0004557038191705942,0.00046001255395822227,0.0004614171339198947,0.0004635646182578057,0.0004721946897916496,0.0004761037998832762,0.00048398703802376986,0.0004940645303577185,0.0004956733318977058,0.000496937776915729,0.0004980314406566322,0.0005018917727284133,0.0005039206007495522,0.0005216900026425719,0.0005226488574407995,0.0005274068680591881,0.0005306412349455059,0.0005367659032344818,0.0005428188014775515,0.0005432188627310097,0.000545727030839771,0.0005505425506271422,0.0005569696659222245,0.0005594444228336215,0.0005690839025191963,0.0005733107682317495,0.0005798074416816235,0.0005855102790519595,0.000596952042542398,0.0005970842903479934,0.0005990315694361925,0.0006106109940446913,0.0006214343593455851,0.0006231740699149668,0.0006238583591766655,0.0006277819629758596,0.0006297279614955187,0.0006308614974841475,0.0006356015219353139,0.0006490602972917259,0.0006492717075161636,0.0006647799164056778,0.0006746192811988294,0.0006888102507218719,0.0006920177256688476,0.0007012135465629399,0.0007053596782498062,0.0007081407238729298,0.0007092446321621537,0.0007117972709238529,0.0007203652057796717,0.0007203996065072715,0.000723783450666815,0.0007369219674728811,0.0007565650157630444,0.0007597248186357319,0.0007692180806770921,0.0007764779729768634,0.0007795976707711816,0.0007873903377912939,0.0007882760255597532,0.0007919056806713343,0.0007935732719488442,0.0007939586066640913,0.0007984284893609583,0.0007987371063791215,0.0008053718484006822,0.000809539866168052,0.0008108233450911939,0.00081483373651281,0.0008203663746826351,0.0008208751678466797,0.0008241518517024815,0.0008246528450399637,0.0008253673440776765,0.0008450502064079046,0.0008452587062492967,0.0008514279616065323,0.0008582283626310527,0.0008598428685218096,0.000870975200086832,0.0008711162954568863,0.0008712874841876328,0.0008915596990846097,0.0008925885194912553,0.0008960228879004717,0.0009013339295051992,0.0009090278763324022,0.0009127717930823565,0.0009178214240819216,0.000924380321521312,0.0009310372406616807,0.0009323068661615252,0.000934891402721405,0.0009459365392103791,0.0009475437691435218,0.0009561767219565809,0.0009642174118198454,0.0009706493001431227,0.0009770046453922987,0.0009775864891707897,0.0009779492393136024,0.000988126965239644,0.000990076456218958,0.0010018927277997136,0.0010032859863713384,0.0010299375280737877,0.0010352545650675893,0.0010368351358920336,0.0010567905846983194,0.0010581633541733027,0.0010660175466910005,0.0010681268759071827,0.0010974822798743844,0.0011011913884431124,0.0011035915231332183,0.001107779797166586,0.0011096685193479061,0.0011251041432842612,0.0011363241355866194,0.001138272462412715,0.0011404601391404867,0.0011424855329096317,0.0011440330417826772,0.0011483154958114028,0.0011483336566016078,0.0011560114799067378,0.0011593260569497943,0.0011609570356085896,0.0011645217891782522,0.0011688974918797612,0.0011970576597377658,0.0012134044663980603,0.0012139980681240559,0.0012145980726927519,0.00121645862236619,0.001219248864799738,0.0012277159839868546,0.0012320788810029626,0.001233388902619481,0.0012359869433566928,0.0012379862600937486,0.0012388553004711866,0.001254093600437045,0.0012655407190322876,0.0012662681983783841,0.001277429866604507,0.0012803087010979652,0.0012931766686961055,0.001301518059335649,0.0013027051463723183,0.0013084614183753729,0.0013220049440860748,0.001333908410742879,0.001347798970527947,0.0013496556784957647,0.0013717968249693513,0.0013742010341957211,0.0013798862928524613,0.001388146891258657,0.0013952755834907293,0.0014080856926739216,0.0014125130837783217,0.0014153877273201942,0.0014263594057410955,0.0014635564293712378,0.0014716159785166383,0.0014732491690665483,0.0014761919155716896,0.0014867280842736363,0.0014869545120745897,0.0014963049907237291,0.0015124781057238579,0.0015200084308162332,0.001524800667539239,0.0015251132426783442,0.0015305468114092946,0.0015508118085563183,0.001556019182316959,0.0015833356883376837,0.0015859794802963734,0.0015889772912487388,0.0016002244083210826,0.0016025524819269776,0.0016182651743292809,0.0016241219127550721,0.001624430064111948,0.0016385293565690517,0.001646392047405243,0.0016538065392524004,0.001665437244810164,0.001673723803833127,0.0016792875248938799,0.001686098868958652,0.001686565112322569,0.0016914677107706666,0.0017070438480004668,0.0017129055922850966,0.0017243550391867757,0.0017399820499122143,0.0017472736071795225,0.0017531930934637785,0.0017558586550876498,0.0017579856794327497,0.0017675734125077724,0.0017704316414892673,0.0017772432183846831,0.0017844104440882802,0.001793745206668973,0.0017986288294196129,0.0018164983484894037,0.0018707714043557644,0.0018714341567829251,0.0018783101113513112,0.001881891512311995,0.001884379074908793,0.0018847695318982005,0.0018949370132759213,0.0018953908002004027,0.001897817593999207,0.0018981784814968705,0.001900402712635696,0.0019081091741099954,0.0019110849825665355,0.0019166843267157674,0.0019232556223869324,0.0019233232596889138,0.0019364685285836458,0.0019404164049774408,0.001942555303685367,0.001942599075846374,0.0019531329162418842,0.0019697356037795544,0.0019752979278564453,0.0019792465027421713,0.0019795370753854513,0.0019877178128808737,0.001993916928768158,0.001994819613173604,0.0020019218791276217,0.002004201989620924,0.00200890121050179,0.0020146681927144527,0.0020172710064798594,0.0020315598230808973,0.0020463145337998867,0.0020484670531004667,0.002049948088824749,0.0020550061017274857,0.0020574775990098715,0.0020590287167578936,0.002065870212391019,0.002069947775453329,0.00207601604051888,0.0020772700663655996,0.0020802668295800686,0.0020873176399618387,0.0021116810385137796,0.002113162772729993,0.002123862272128463,0.002129584550857544,0.0021561994217336178,0.002171131083741784,0.002183290431275964,0.0021987557411193848,0.0022055988665670156,0.0022179242223501205,0.002220652997493744,0.002269214019179344,0.0022771088406443596,0.0022904977668076754,0.002294500358402729,0.0022957297042012215,0.0023139147087931633,0.0023157624527812004,0.0023231154773384333,0.002329822164028883,0.002346029970794916,0.0023592067882418633,0.002375594340264797,0.0024147755466401577,0.002434545662254095,0.002436284441500902,0.0024389110039919615,0.002450040075927973,0.002450500149279833,0.002450904343277216,0.0024715950712561607,0.002471985761076212,0.002479791874065995,0.0024842959828674793,0.002500426722690463,0.002514506923034787,0.00252733682282269,0.002528558485209942,0.0025313913356512785,0.00253858114592731,0.002546823350712657,0.0025484799407422543,0.002554739825427532,0.002592351520434022,0.002623553853482008,0.0026320877950638533,0.0026361984200775623,0.0026371979620307684,0.002668066881597042,0.0026719397865235806,0.002673589624464512,0.002697089221328497,0.00270357308909297,0.0027083789464086294,0.002721679164096713,0.0027382408734411,0.002743565710261464,0.0027596428990364075,0.0027659398037940264,0.0027700478676706553,0.002798832254484296,0.002799126785248518,0.0028302958235144615,0.0028333687223494053,0.002845350420102477,0.002846917137503624,0.002858598018065095,0.0028855446726083755,0.002886916045099497,0.002894930774345994,0.0029032647144049406,0.0029281771276146173,0.002934184158220887,0.002976624295115471,0.003010790329426527,0.0030345371924340725,0.0030595860444009304,0.003099345602095127,0.003125268267467618,0.00315611413680017,0.003160064108669758,0.0031725389417260885,0.0031825131736695766,0.0031881644390523434,0.0031948385294526815,0.0032077536452561617,0.0032144635915756226,0.0032187181059271097,0.003221739549189806,0.0032672828529030085,0.003280361881479621,0.0032920045778155327,0.0032937689684331417,0.0033081898000091314,0.003310570027679205,0.0033202224876731634,0.0033271792344748974,0.003338353708386421,0.0033510252833366394,0.0033683860674500465,0.003369207726791501,0.003377101616933942,0.0034449463710188866,0.0034497324377298355,0.0034565948881208897,0.0034632377792149782,0.003473710035905242,0.0035001712385565042,0.0035062634851783514,0.0035152542404830456,0.003538896329700947,0.0035520577803254128,0.0035635835956782103,0.0035713734105229378,0.003587773535400629,0.0035970367025583982,0.003641256131231785,0.003693661652505398,0.003739592619240284,0.0037612193264067173,0.0037691788747906685,0.0037736480589956045,0.003818824887275696,0.0038229364436119795,0.0038299134466797113,0.0038358683232218027,0.00384052493609488,0.0038892074953764677,0.0039050853811204433,0.003914241679012775,0.0039566014893352985,0.00397463096305728,0.003992567770183086,0.004124002065509558,0.004144391976296902,0.004171795677393675,0.004194134846329689,0.004258664790540934,0.004270878620445728,0.004273512866348028,0.0043026418425142765,0.0043233162723481655,0.004366830922663212,0.004398784134536982,0.004401935264468193,0.0044315773993730545,0.004434137139469385,0.004484640434384346,0.0045902300626039505,0.00460194144397974,0.004648321308195591,0.004676529206335545,0.004680689889937639,0.0047262669540941715,0.004730702843517065,0.00474408408626914,0.00474411528557539,0.004747116006910801,0.004770150873810053,0.00477236695587635,0.004805757664144039,0.004813110921531916,0.004820855334401131,0.0049203564412891865,0.00495142163708806,0.004968008957803249,0.004971051122993231,0.004983816295862198,0.005003189668059349,0.005053243599832058,0.005083166994154453,0.0050849271938204765,0.005090453661978245,0.005102942697703838,0.005122787319123745,0.005169081501662731,0.005197473336011171,0.005212514195591211,0.005234912503510714,0.005264595150947571,0.0052899266593158245,0.005322945304214954,0.005346274934709072,0.0053514051251113415,0.005364121403545141,0.00539751909673214,0.005398993380367756,0.005399786867201328,0.005431063007563353,0.00546529283747077,0.005470041185617447,0.005504471715539694,0.005543959327042103,0.0055671208538115025,0.005617368035018444,0.005621850024908781,0.005622706841677427,0.005654032342135906,0.0057053049094974995,0.0057078516110777855,0.005710906349122524,0.005787555128335953,0.005806407891213894,0.005833489820361137,0.005859206430613995,0.005883717443794012,0.005947509780526161,0.005997626110911369,0.006001781206578016,0.006051409058272839,0.0060750264674425125,0.006155393086373806,0.006196769420057535,0.006271408870816231,0.006297433748841286,0.006304377689957619,0.006305517628788948,0.006306771654635668,0.0063078622333705425,0.006349287461489439,0.006358996499329805,0.006370370276272297,0.00642034225165844,0.006426427513360977,0.00653021689504385,0.0065602464601397514,0.006570449098944664,0.006610861048102379,0.006622727494686842,0.006658407859504223,0.006706486456096172,0.00672279903665185,0.00673654954880476,0.0067682331427931786,0.006772343534976244,0.006790312007069588,0.006808910984545946,0.006812744773924351,0.006816435605287552,0.006821509450674057,0.0068426881916821,0.00687599228695035,0.006917338352650404,0.006953672971576452,0.006954194977879524,0.006990686058998108,0.006993800401687622,0.00701467040926218,0.007067954633384943,0.007080241572111845,0.007103037089109421,0.00711618410423398,0.007193362340331078,0.007196268066763878,0.0072280592285096645,0.007289053872227669,0.007294264622032642,0.007388505153357983,0.007411669474095106,0.007432537619024515,0.007465557660907507,0.007563597522675991,0.007617774419486523,0.007622379809617996,0.007642725482583046,0.007746870629489422,0.007770410273224115,0.007813416421413422,0.007839766331017017,0.007844610139727592,0.007850456982851028,0.007982033304870129,0.007991625927388668,0.008003484457731247,0.008014791645109653,0.008069011382758617,0.008115716278553009,0.008130163885653019,0.008144352585077286,0.008238732814788818,0.008282741531729698,0.008294865489006042,0.008345200680196285,0.008373145014047623,0.00845241267234087,0.00847533531486988,0.008589578792452812,0.008598856627941132,0.00869353860616684,0.00872811395674944,0.008737429976463318,0.008799390867352486,0.008817373774945736,0.008843284100294113,0.008930178359150887,0.009073606692254543,0.009228670969605446,0.009230838157236576,0.009258422069251537,0.009337687864899635,0.009357009083032608,0.009385671466588974,0.009403747506439686,0.009406337514519691,0.009525856003165245,0.009559721685945988,0.009569472633302212,0.00957427080720663,0.009605394676327705,0.00974422786384821,0.009824005886912346,0.009877363219857216,0.009918015450239182,0.010062310844659805,0.010096260346472263,0.01014291774481535,0.010157573036849499,0.010219675488770008,0.010233731009066105,0.010253074578940868,0.010470150969922543,0.010523333214223385,0.010769104585051537,0.010854542255401611,0.010862882249057293,0.010926498100161552,0.010934046469628811,0.010954057797789574,0.011061220429837704,0.011179229244589806,0.011209569871425629,0.01123218983411789,0.011264876462519169,0.011307379230856895,0.011324683204293251,0.011500844731926918,0.011530226096510887,0.011639170348644257,0.011640471406280994,0.011714298278093338,0.011799566447734833,0.0118008553981781,0.011805098503828049,0.011816339567303658,0.011827295646071434,0.011850791051983833,0.011898801662027836,0.012018630281090736,0.012102714739739895,0.012108358554542065,0.012248746119439602,0.012319295667111874,0.012381001375615597,0.012401469051837921,0.012422110885381699,0.012439117766916752,0.012452490627765656,0.012519282288849354,0.012549216859042645,0.01261934731155634,0.012640058062970638,0.01266070082783699,0.012695524841547012,0.0127190463244915,0.013016968034207821,0.013137873262166977,0.013278203085064888,0.013378331437706947,0.013539881445467472,0.01368365902453661,0.013766450807452202,0.013769660145044327,0.013864478096365929,0.013877306133508682,0.014144296757876873,0.014160123653709888,0.014192857779562473,0.014408251270651817,0.014465246349573135,0.014605593867599964,0.01461095456033945,0.014786944724619389,0.01495335903018713,0.01501149870455265,0.01530901063233614,0.015542860142886639,0.01562647894024849,0.01571667194366455,0.015774047002196312,0.015810050070285797,0.016016174107789993,0.016514429822564125,0.016662588343024254,0.016700174659490585,0.016897257417440414,0.01723497547209263,0.01728234253823757,0.01757538504898548,0.01762036606669426,0.017659036442637444,0.017669014632701874,0.01783369481563568,0.01807607337832451,0.018134936690330505,0.01822829619050026,0.018246563151478767,0.018270136788487434,0.018309811130166054,0.018439926207065582,0.01870257966220379,0.018893716856837273,0.019046813249588013,0.019092369824647903,0.019532546401023865,0.0195710938423872,0.0196150541305542,0.019676445052027702,0.0197511725127697,0.01977039873600006,0.0197738204151392,0.019823050126433372,0.02050102688372135,0.020596276968717575,0.020750196650624275,0.021062836050987244,0.021110182628035545,0.02120688185095787,0.021246323361992836,0.02127763070166111,0.021327003836631775,0.02133328840136528,0.021402066573500633,0.02144203893840313,0.021455174311995506,0.021555662155151367,0.02160017192363739,0.021606523543596268,0.02161226235330105,0.0217953659594059,0.02211008593440056,0.022167528048157692,0.02261328510940075,0.022692374885082245,0.0229277852922678,0.022944802418351173,0.022956544533371925,0.023050451651215553,0.023609589785337448,0.023862432688474655,0.02413414977490902,0.024195728823542595,0.02436666004359722,0.02477412484586239,0.02509484440088272,0.025457926094532013,0.025791656225919724,0.02600463479757309,0.02643263153731823,0.026439907029271126,0.026485735550522804,0.02649623714387417,0.026868877932429314,0.026883497834205627,0.026911979541182518,0.027353601530194283,0.027554914355278015,0.027591200545430183,0.027989113703370094,0.02831149287521839,0.02856723964214325,0.029847120866179466,0.030448932200670242,0.030527884140610695,0.03098604641854763,0.031204063445329666,0.03135791793465614,0.03141391649842262,0.03178573027253151,0.032126013189554214,0.03269578516483307,0.03281770274043083,0.033321481198072433,0.03367667645215988,0.03375060111284256,0.03406047821044922,0.034158624708652496,0.0344618558883667,0.03484273701906204,0.03530057892203331,0.035776689648628235,0.03584940731525421,0.03607849031686783,0.036924734711647034,0.03760259971022606,0.03787410259246826,0.03864620625972748,0.039264388382434845,0.03934868425130844,0.03967173770070076,0.04074211046099663,0.04107700288295746,0.04191021993756294,0.041991785168647766,0.04207497462630272,0.04340491443872452,0.043421678245067596,0.04344823211431503,0.04372302070260048,0.045106932520866394,0.045422572642564774,0.04560821130871773,0.045737043023109436,0.04681403189897537,0.047180723398923874,0.04733354225754738,0.04785040020942688,0.04790615662932396,0.04820559546351433,0.050067804753780365,0.050944145768880844,0.051558125764131546,0.05190879851579666,0.052039984613657,0.052511945366859436,0.05442036688327789,0.0565425381064415,0.0573539175093174,0.0574955977499485,0.057848475873470306,0.059250880032777786,0.05974660441279411,0.06051820144057274,0.06256339699029922,0.06302616745233536,0.06326635181903839,0.06631691753864288,0.06672531366348267,0.0675651952624321,0.06776813417673111,0.0714309811592102,0.07151812314987183,0.07244668900966644,0.0749790295958519,0.07600625604391098,0.07893864810466766,0.07998315989971161,0.08104971051216125,0.08179030567407608,0.08380582928657532,0.08408216387033463,0.08425327390432358,0.08554403483867645,0.0888512060046196,0.09042833000421524,0.09072739630937576,0.0917469710111618,0.09233933687210083,0.09416231513023376,0.09482625871896744,0.09488615393638611,0.10021363943815231,0.10026653110980988,0.11117485910654068,0.11327535659074783,0.1195472925901413,0.12370643764734268,0.12378283590078354,0.12726764380931854,0.1339966207742691,0.13568846881389618,0.14960415661334991,0.15129826962947845,0.16254079341888428,0.16652429103851318,0.17182722687721252,0.1763855516910553,0.20483805239200592,0.21330226957798004,0.22966457903385162],\"yaxis\":\"y\",\"type\":\"scattergl\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Sorted neurons\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Loss change\"}},\"legend\":{\"title\":{\"text\":\"variable\"},\"tracegroupgap\":0},\"title\":{\"text\":\"Loss change from ablating MLP5 neuron\"},\"width\":800},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('70f0ba39-eb0a-4ff6-9250-787f0a710437');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sorted_means, indices = torch.sort(diffs.mean(1))\n",
    "sorted_means = sorted_means.tolist()\n",
    "haystack_utils.line(sorted_means, xlabel=\"Sorted neurons\", ylabel=\"Loss change\", title=\"Loss change from ablating MLP5 neuron\") # xticks=indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>                            <div id=\"46e0e785-f40a-4047-9be4-53c998633e44\" class=\"plotly-graph-div\" style=\"height:525px; width:1000px;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"46e0e785-f40a-4047-9be4-53c998633e44\")) {                    Plotly.newPlot(                        \"46e0e785-f40a-4047-9be4-53c998633e44\",                        [{\"error_y\":{\"array\":[1.407925896564596],\"type\":\"data\",\"visible\":true},\"name\":\"Original\",\"x\":[\"Original\"],\"y\":[1.6733419652003796],\"type\":\"bar\"},{\"error_y\":{\"array\":[2.15244760946657],\"type\":\"data\",\"visible\":true},\"name\":\"Ablated\",\"x\":[\"Ablated\"],\"y\":[3.835807584524155],\"type\":\"bar\"},{\"error_y\":{\"array\":[1.4618049571707405],\"type\":\"data\",\"visible\":true},\"name\":\"MLP5 path patched\",\"x\":[\"MLP5 path patched\"],\"y\":[2.316885909885168],\"type\":\"bar\"},{\"error_y\":{\"array\":[1.9111105778524375],\"type\":\"data\",\"visible\":true},\"name\":\"MLP5 path patched + Top 10 MLP5 neurons ablated\",\"x\":[\"Top MLP5 removed\"],\"y\":[4.318096536397934],\"type\":\"bar\"},{\"error_y\":{\"array\":[0.73745520811858],\"type\":\"data\",\"visible\":true},\"name\":\"MLP5 path patched + Bottom 10 MLP5 neurons ablated\",\"x\":[\"Bottom MLP5 removed\"],\"y\":[0.673909611478448],\"type\":\"bar\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"title\":{\"text\":\"Average last token loss when removing top \\u002f bottom neurons from path patching\"},\"xaxis\":{\"title\":{\"text\":\"\"}},\"yaxis\":{\"title\":{\"text\":\"Loss\"}},\"barmode\":\"group\",\"width\":1000},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('46e0e785-f40a-4047-9be4-53c998633e44');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check loss change when ablating top / bottom neurons\n",
    "top_neurons_count = 10\n",
    "top_neurons = indices[-top_neurons_count:]\n",
    "bottom_neurons = indices[:top_neurons_count]\n",
    "\n",
    "with model.hooks(deactivate_neurons_fwd_hooks):\n",
    "    ablated_loss, ablated_cache = model.run_with_cache(prompts, return_type=\"loss\")\n",
    "\n",
    "ablate_top_neurons_hook = get_ablate_neurons_hook(top_neurons, ablated_cache)\n",
    "ablate_bottom_neurons_hook = get_ablate_neurons_hook(bottom_neurons, ablated_cache)\n",
    "\n",
    "original_loss, ablated_loss, _, all_MLP5_loss = haystack_utils.get_direct_effect(prompts, model, pos=-1, context_ablation_hooks=deactivate_neurons_fwd_hooks, context_activation_hooks=activate_neurons_fwd_hooks)\n",
    "_, _, _, top_MLP5_ablated_loss = haystack_utils.get_direct_effect(prompts, model, pos=-1, context_ablation_hooks=deactivate_neurons_fwd_hooks, context_activation_hooks=activate_neurons_fwd_hooks+ablate_top_neurons_hook)\n",
    "_, _, _, bottom_MLP5_ablated_loss = haystack_utils.get_direct_effect(prompts, model, pos=-1, context_ablation_hooks=deactivate_neurons_fwd_hooks, context_activation_hooks=activate_neurons_fwd_hooks+ablate_bottom_neurons_hook)\n",
    "\n",
    "names = [\"Original\", \"Ablated\", \"MLP5 path patched\", f\"MLP5 path patched + Top {top_neurons_count} MLP5 neurons ablated\", f\"MLP5 path patched + Bottom {top_neurons_count} MLP5 neurons ablated\"]\n",
    "short_names = [\"Original\", \"Ablated\", \"MLP5 path patched\", f\"Top MLP5 removed\", f\"Bottom MLP5 removed\"]\n",
    "\n",
    "values = [original_loss.tolist(), ablated_loss.tolist(), all_MLP5_loss.tolist(), top_MLP5_ablated_loss.tolist(), bottom_MLP5_ablated_loss.tolist()]\n",
    "haystack_utils.plot_barplot(values, names, short_names=short_names, ylabel=\"Loss\", title=f\"Average last token loss when removing top / bottom neurons from path patching\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigate top / bottom neuron boosts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Patch in MLP5 neurons, ablated bottom neurons, compare with patched MLP5 neurons without ablating bottom neurons\n",
    "\n",
    "with model.hooks(deactivate_neurons_fwd_hooks):\n",
    "    ablated_logits, ablated_cache = model.run_with_cache(prompts)\n",
    "\n",
    "ablate_top_neurons_hook = get_ablate_neurons_hook(top_neurons, ablated_cache)\n",
    "ablate_bottom_neurons_hook = get_ablate_neurons_hook(bottom_neurons, ablated_cache)\n",
    "\n",
    "original_logits, ablated_logprobs, _, all_MLP5_logprobs = haystack_utils.get_direct_effect(prompts, model, pos=-2, context_ablation_hooks=deactivate_neurons_fwd_hooks, context_activation_hooks=activate_neurons_fwd_hooks, return_type='logprobs')\n",
    "_, _, _, top_MLP5_ablated_logprobs = haystack_utils.get_direct_effect(prompts, model, pos=-2, context_ablation_hooks=deactivate_neurons_fwd_hooks, context_activation_hooks=activate_neurons_fwd_hooks+ablate_top_neurons_hook, return_type='logprobs')\n",
    "_, _, _, bottom_MLP5_ablated_logprobs = haystack_utils.get_direct_effect(prompts, model, pos=-2, context_ablation_hooks=deactivate_neurons_fwd_hooks, context_activation_hooks=activate_neurons_fwd_hooks+ablate_bottom_neurons_hook, return_type='logprobs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_difference_neurons(baseline_logprobs, ablated_logprops, positive=True, logprob_threshold=-7, k=50):\n",
    "    neuron_logprob_difference = (baseline_logprobs - ablated_logprops).mean(0)\n",
    "    neuron_logprob_difference[baseline_logprobs.mean(0) < logprob_threshold] = 0\n",
    "    if positive:\n",
    "        non_zero_count = (neuron_logprob_difference > 0).sum()\n",
    "    else:\n",
    "        non_zero_count = (neuron_logprob_difference < 0).sum()\n",
    "    top_logprob_difference, top_neurons = haystack_utils.top_k_with_exclude(neuron_logprob_difference, min(non_zero_count, k), all_ignore, largest=positive)\n",
    "    return top_logprob_difference, top_neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boosted tokens\n",
    "bottom_neuron_pos_difference_logprobs, bottom_pos_indices = get_top_difference_neurons(all_MLP5_logprobs, bottom_MLP5_ablated_logprobs, positive=True)\n",
    "top_neuron_pos_difference_logprobs, top_pos_indices = get_top_difference_neurons(all_MLP5_logprobs, top_MLP5_ablated_logprobs, positive=True)\n",
    "# Deboosted tokens\n",
    "bottom_neuron_neg_difference_logprobs, bottom_neg_indices = get_top_difference_neurons(all_MLP5_logprobs, bottom_MLP5_ablated_logprobs, positive=False)\n",
    "top_neuron_neg_difference_logprobs, top_neg_indices = get_top_difference_neurons(all_MLP5_logprobs, top_MLP5_ablated_logprobs, positive=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1134022e8379458aab1c86ba1353cdc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='Tokens:', options=('Boosted', 'Deboosted'), value='Boosted'), Drop…"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def plot_data(boosted_deboosted, top_bottom):\n",
    "    if boosted_deboosted == 'Boosted' and top_bottom == 'Top':\n",
    "        logprobs = top_neuron_pos_difference_logprobs\n",
    "        indices = top_pos_indices\n",
    "        title_change = 'increase'\n",
    "    elif boosted_deboosted == 'Boosted' and top_bottom == 'Bottom':\n",
    "        logprobs = bottom_neuron_pos_difference_logprobs\n",
    "        indices = bottom_pos_indices\n",
    "        title_change = 'increase'\n",
    "    elif boosted_deboosted == 'Deboosted' and top_bottom == 'Top':\n",
    "        logprobs = top_neuron_neg_difference_logprobs\n",
    "        indices = top_neg_indices\n",
    "        title_change = 'decrease'\n",
    "    else:  # 'Deboosted' and 'Bottom'\n",
    "        logprobs = bottom_neuron_neg_difference_logprobs\n",
    "        indices = bottom_neg_indices\n",
    "        title_change = 'decrease'\n",
    "\n",
    "    xlabel = boosted_deboosted + \" tokens\"\n",
    "    ylabel = \"full_logprob - ablated_logprop\"\n",
    "    title = 'Logprob ' + title_change + ' from ' + top_bottom.lower() + ' neurons'\n",
    "    xticks = [model.to_str_tokens([i])[0] for i in indices]\n",
    "\n",
    "    haystack_utils.line(logprobs.cpu().numpy(), xlabel=xlabel, ylabel=ylabel, title=title, xticks=xticks, show_legend=False)\n",
    "\n",
    "\n",
    "boosted_deboosted = widgets.Dropdown(options=['Boosted', 'Deboosted'], value='Boosted', description='Tokens:')\n",
    "top_bottom = widgets.Dropdown(options=['Top', 'Bottom'], value='Top', description='Neurons:')\n",
    "widgets.interactive(plot_data, boosted_deboosted=boosted_deboosted, top_bottom=top_bottom)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decompose neuron effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get logprob change for individual neuron\n",
    "\n",
    "def get_individual_neuron_logprob_effect(sorted_indices, neuron_pos=0, top=True, positive=True, plot=True):\n",
    "    if top:\n",
    "        neurons = sorted_indices[-(neuron_pos+1)]\n",
    "    else:\n",
    "        neurons = sorted_indices[neuron_pos]\n",
    "    neurons_hook = get_ablate_neurons_hook([neurons], ablated_cache)\n",
    "\n",
    "    original_logits, ablated_logprobs, _, all_MLP5_logprobs = haystack_utils.get_direct_effect(prompts, model, pos=-2, context_ablation_hooks=deactivate_neurons_fwd_hooks, context_activation_hooks=activate_neurons_fwd_hooks, return_type='logprobs')\n",
    "    _, _, _, neuron_ablated_logprobs = haystack_utils.get_direct_effect(prompts, model, pos=-2, context_ablation_hooks=deactivate_neurons_fwd_hooks, context_activation_hooks=activate_neurons_fwd_hooks+neurons_hook, return_type='logprobs')\n",
    "    neuron_difference_logprobs, difference_indices = \\\n",
    "        get_top_difference_neurons(all_MLP5_logprobs, neuron_ablated_logprobs, positive=positive)\n",
    "    \n",
    "    if not plot:\n",
    "        return neuron_difference_logprobs, difference_indices\n",
    "    \n",
    "    if positive:\n",
    "        xlabel = 'Boosted tokens'\n",
    "        title = 'Logprob increase from neuron ' + str(neurons.item())\n",
    "    else: \n",
    "        xlabel = 'Deboosted tokens'\n",
    "        title = 'Logprob decrease from neuron ' + str(neurons.item())\n",
    "    if top:\n",
    "        title += f' (top {neuron_pos+1} neuron)'\n",
    "    else:\n",
    "        title += f' (bottom {neuron_pos+1} neuron)'\n",
    "\n",
    "    xticks = [model.to_str_tokens([i])[0] for i in difference_indices]\n",
    "    haystack_utils.line(neuron_difference_logprobs.cpu().numpy(), xlabel=xlabel, ylabel=\"full_logprob - ablated_logprop\", title=title, xticks=xticks, show_legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "952c7c5b5bf84f8b9b77f6e9232a22aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='Neuron Pos:', max=20), Dropdown(description='Neurons:', …"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a function to call your function with widget inputs\n",
    "def plot_individual_neuron(neuron_pos, top_bottom, pos_neg):\n",
    "    top_bool = True if top_bottom == 'Top' else False\n",
    "    positive_bool = True if pos_neg == 'Positive' else False\n",
    "    get_individual_neuron_logprob_effect(indices, neuron_pos=neuron_pos, top=top_bool, positive=positive_bool)\n",
    "\n",
    "# Define widgets\n",
    "neuron_pos_slider = widgets.IntSlider(min=0, max=20, step=1, value=0, description='Neuron Pos:')\n",
    "top_bottom_dropdown = widgets.Dropdown(options=['Top', 'Bottom'], value='Top', description='Neurons:')\n",
    "pos_neg_dropdown = widgets.Dropdown(options=['Positive', 'Negative'], value='Positive', description='Pos/Neg:')\n",
    "\n",
    "# Use interactive function to bind the widgets to the plotting function\n",
    "widgets.interactive(plot_individual_neuron, neuron_pos=neuron_pos_slider, top_bottom=top_bottom_dropdown, pos_neg=pos_neg_dropdown)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum individual weighted positive / negative effects\n",
    "def summed_neuron_differences(num_neurons=10, top=True, positive=True, k=50):\n",
    "    summed_neuron_diffs = torch.zeros(model.cfg.d_vocab).cuda()\n",
    "    for neuron_pos in range(num_neurons):\n",
    "        # It would probably be more principle to do our filtering on the summed diffs instead of per neuron\n",
    "        neuron_diff, token_indices = get_individual_neuron_logprob_effect(indices, neuron_pos=neuron_pos, top=top, positive=positive, plot=False)\n",
    "        summed_neuron_diffs[token_indices] += neuron_diff\n",
    "    if positive:\n",
    "        non_zero_count = (summed_neuron_diffs > 0).sum()\n",
    "    else:\n",
    "        non_zero_count = (summed_neuron_diffs < 0).sum()\n",
    "    top_logprob_difference, top_tokens = haystack_utils.top_k_with_exclude(summed_neuron_diffs, min(non_zero_count, k), all_ignore, largest=positive)\n",
    "    return top_logprob_difference, top_tokens\n",
    "\n",
    "def plot_summed_neuron_differences(num_neurons=10, top=True, positive=True):\n",
    "    top_logprob_difference, top_logprob_tokens = summed_neuron_differences(num_neurons=num_neurons, top=top, positive=positive)\n",
    "    xticks = [model.to_str_tokens([i])[0] for i in top_logprob_tokens]\n",
    "    \n",
    "    if positive:\n",
    "        title = \"Summed individual boosts by\"\n",
    "        xlabel = \"Boosted tokens\"\n",
    "    else:\n",
    "        title = \"Summed individual deboosts by\"\n",
    "        xlabel = \"Deboosted tokens\"\n",
    "    if top:\n",
    "        title += f\" top {num_neurons} neurons\"\n",
    "    else:\n",
    "        title += f\" bottom {num_neurons} neurons\"\n",
    "    haystack_utils.line(top_logprob_difference.cpu().numpy(), xlabel=xlabel, ylabel=\"full_logprob - ablated_logprop\", title=title, xticks=xticks, show_legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7062f70efb424c8fa600639a4d8c395a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=10, description='Num Neurons:', max=20, min=1), Dropdown(description='Ne…"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a function to call your function with widget inputs\n",
    "def plot_summed_neurons_widget(num_neurons, top_bottom, pos_neg):\n",
    "    top_bool = True if top_bottom == 'Top' else False\n",
    "    positive_bool = True if pos_neg == 'Positive' else False\n",
    "    plot_summed_neuron_differences(num_neurons=num_neurons, top=top_bool, positive=positive_bool)\n",
    "\n",
    "# Define widgets\n",
    "num_neuron_slider = widgets.IntSlider(min=1, max=20, step=1, value=10, description='Num Neurons:')\n",
    "top_bottom_dropdown = widgets.Dropdown(options=['Top', 'Bottom'], value='Top', description='Neurons:')\n",
    "pos_neg_dropdown = widgets.Dropdown(options=['Positive', 'Negative'], value='Positive', description='Pos/Neg:')\n",
    "\n",
    "# Use interactive function to bind the widgets to the plotting function\n",
    "widgets.interactive(plot_summed_neurons_widget, num_neurons=num_neuron_slider, top_bottom=top_bottom_dropdown, pos_neg=pos_neg_dropdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss is logprob(correct_token_logit) = logit(correct_token_logit) - LogSumExp(all_token_logits)\n",
    "Loss = -logprob\n",
    "\n",
    "LogSumExp approximates a maximum function. If the neuron engages in destructive interference of a high logit for a non-answer token, then the exp(logit) for the token will be lower and so the LogSumExp will be more similar to logit(correct_token_logit) so the loss will be lower. So a lower logsumexp(all vocab) is good.\n",
    "\n",
    "If the neuron engages in destructive interference of a low logit for a non-answer token, then the exp(logit) for the the token won't really change and so the logsumexp and the loss will both be the same.\n",
    "\n",
    "For a single neuron:\n",
    "1. For each \"gen\" prompt, zero centre the logits and record the logit of \"gen\". Calculate the mean \"gen\" logit.\n",
    "2. For each \"gen\" prompt, disable the neuron under test, zero centre the logits, edit the logit of \"gen\" to the mean with a hook, then get the logprob of \"gen\" and calculate logsumexp(all vocab) = logit(gen) - logprob(gen).\n",
    "2. For each \"gen\" prompt, enable the neuron, zero centre the logits, edit the logit of \"gen\" to the mean with a hook, then get the logprob of \"gen\" and calculate logsumexp(all vocab) = logit(gen) - logprob(gen).\n",
    "3. Take the difference in logsum exps. If it's positive, the neuron is reducing the loss via destructive interference by the difference.\n",
    "Can use the same procedure for sets of neurons, or for all neurons, to find high level effects of the context neuron\n",
    "\n",
    "Logprobs are logits with a constant subtracted and the constant is the same for every logit within a prompt.\n",
    "\n",
    "Taking the difference in terms with and without a neuron's effect via the context neuron:\n",
    "- If log sum exp increases, the neuron is boosting tokens on average. \n",
    "- If logit increases, the neuron is boosting the correct token\n",
    "\n",
    "\n",
    "Remove the neuron's effects on the gen logit. Take the mean on the prompt and position?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(11.2327, device='cuda:0')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Calculate the mean \"gen\" logit.\n",
    "gen_index = model.to_single_token('gen')\n",
    "gen_logits = []\n",
    "logits = model(prompts, return_type='logits') # batch pos vocab\n",
    "logits = logits - logits.mean(-1).unsqueeze(-1) # batch pos vocab, batch pos 1\n",
    "\n",
    "mean_gen_logit = logits[:, -1, gen_index].mean(0)\n",
    "mean_gen_logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import numpy as np\n",
    "# from transformer_lens import utils\n",
    "# \n",
    "# px.histogram(np.random.choice(logits[:, -1, gen_index].flatten().cpu().numpy(), 1000), nbins=100)\n",
    "# utils.test_prompt(\"\".join(model.to_str_tokens(prompts[0, :-1])), \"gen\", model, prepend_space_to_answer=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(-0.25178220868110657, -0.400439590215683)\n",
      "(0.04646552354097366, -0.4279060661792755)\n"
     ]
    }
   ],
   "source": [
    "def decompose_interference_diff(ablated_cache, disabled_neurons=[top_neurons[0]], mean=True\n",
    "                                ) -> tuple[float, float] | tuple[Float[Tensor, \"n_prompts\"], Float[Tensor, \"n_prompts\"]]:\n",
    "    '''\n",
    "    Finds the effect of the German context neuron ablation via a given set of MLP5 neurons on the logit of each final \n",
    "    token. Decomposes it into constructive and destructive interference. A positive constructive interference difference\n",
    "    means the neurons boost the logit of the correct token, a positive destructive interference difference means the\n",
    "    neurons deboost the logits of the incorrect tokens.\n",
    "\n",
    "    Loss = -logprob(correct_token_logit)\n",
    "    logprob(correct_token_logit) = correct_token_logit - LogSumExp(all_token_logits)\n",
    "\n",
    "    LogSumExp is a smooth maximum function, so it approximates the max of all logits. A neuron that destructively \n",
    "    interfers with a non-answer token with a high logit lowers the LogSumExp(all_token_logits) and thus the loss.'''\n",
    "    ablate_top_neuron_hook = get_ablate_neurons_hook(disabled_neurons, ablated_cache)\n",
    "\n",
    "    _, ablated_logits, _, top_neuron_ablated_logits = haystack_utils.get_direct_effect(prompts, model, pos=-1, context_ablation_hooks=deactivate_neurons_fwd_hooks, context_activation_hooks=activate_neurons_fwd_hooks+ablate_top_neuron_hook, return_type='logits')\n",
    "    ablated_logits = ablated_logits - ablated_logits.mean(-1).unsqueeze(-1)\n",
    "    top_neuron_ablated_logits = top_neuron_ablated_logits - top_neuron_ablated_logits.mean(-1).unsqueeze(-1)\n",
    "    \n",
    "    # 1. Constructive interference difference\n",
    "    # This is the change in the correct answer token logit from not ablating the neuron, positive is good\n",
    "    constructive_interference_diffs = ablated_logits[:, gen_index] - top_neuron_ablated_logits[:, gen_index]\n",
    "\n",
    "    # 2. Destructive interference difference\n",
    "    # This is the change in the LogSumExp of all logits from not ablating the neuron, positive is bad\n",
    "    ablated_logits[:, gen_index] = mean_gen_logit\n",
    "    top_neuron_ablated_logits[:, gen_index] = mean_gen_logit\n",
    "    \n",
    "    ablated_destructive_interference = ablated_logits.exp().sum(-1).log()\n",
    "    top_neuron_ablated_destructive_interference = top_neuron_ablated_logits.exp().sum(-1).log()\n",
    "    \n",
    "    # 3. Flip the diff's sign to ease interpretation, now positive is good for both constructive and destructive interference\n",
    "    destructive_interference_diffs = (ablated_destructive_interference - top_neuron_ablated_destructive_interference) * -1\n",
    "\n",
    "    if mean:\n",
    "        return constructive_interference_diffs.mean().item(), destructive_interference_diffs.mean().item(), \n",
    "    return constructive_interference_diffs, destructive_interference_diffs,\n",
    "\n",
    "print(decompose_interference_diff(ablated_cache, [top_neurons[0]]))\n",
    "print(decompose_interference_diff(ablated_cache, top_neurons))\n",
    "\n",
    "# Calculate neuron-wise loss change"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neuron -> Token Logit Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.0346, -0.0237, -0.0347,  0.0397,  0.0595,  0.0501, -0.0274,  0.1405,\n",
      "         0.0417, -0.0392], device='cuda:0')\n",
      "tensor([0.1634, 0.1461, 0.1405, 0.1399, 0.1349, 0.1207, 0.1196, 0.1161, 0.1156,\n",
      "        0.1139, 0.1130, 0.1128, 0.1120, 0.1104, 0.1096, 0.1070, 0.1056, 0.1048,\n",
      "        0.1042, 0.1028], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Our top neurons are selected by the difference in their boost in gen based on the context neuron\n",
    "# Many other neurons boost gen more per unit of activation\n",
    "\n",
    "cosine_sim = torch.nn.CosineSimilarity(dim=1)\n",
    "answer_residual_direction = model.tokens_to_residual_directions(\"gen\")\n",
    "neuron_weights = model.state_dict()['blocks.5.mlp.W_out'][top_neurons]\n",
    "\n",
    "cosine_sims = cosine_sim(neuron_weights, answer_residual_direction.unsqueeze(0))\n",
    "print(cosine_sims)\n",
    "\n",
    "cosine_sim = torch.nn.CosineSimilarity(dim=1)\n",
    "answer_residual_direction = model.tokens_to_residual_directions(\"gen\")\n",
    "neuron_weights = model.state_dict()['blocks.5.mlp.W_out']\n",
    "\n",
    "cosine_sims = cosine_sim(neuron_weights, answer_residual_direction.unsqueeze(0))\n",
    "top, indices = torch.topk(cosine_sims, 20)\n",
    "print(top)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b91c760c76ca443d91131d04b5309c8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8d91f081d6147a79ba6d0e55ca07974",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26dc6e4aa61545f3aa42f2f8f2d5ee6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "haystack_utils.clean_cache()\n",
    "english_activations_l5 = {}\n",
    "german_activations_l5 = {}\n",
    "all_activations_l5 = {}\n",
    "for layer in range(5, 6):\n",
    "    english_activations_l5[layer] = get_mlp_activations(english_data, layer, model, mean=True)\n",
    "    german_activations_l5[layer] = get_mlp_activations(german_data, layer, model, mean=True)\n",
    "    all_activations_l5[layer] = get_mlp_activations(german_data[:50] + english_data[:150], layer, model, mean=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'unsqueeze'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 28\u001b[0m\n\u001b[1;32m     26\u001b[0m W_out \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mstate_dict()[\u001b[39m'\u001b[39m\u001b[39mblocks.5.mlp.W_out\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     27\u001b[0m W_U \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mW_U\n\u001b[0;32m---> 28\u001b[0m \u001b[39mprint\u001b[39m(W_out\u001b[39m.\u001b[39mshape, W_U\u001b[39m.\u001b[39mshape, all_activations_l5\u001b[39m.\u001b[39;49munsqueeze(\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mshape)\n\u001b[1;32m     30\u001b[0m average_boost \u001b[39m=\u001b[39m W_out \u001b[39m*\u001b[39m all_activations_l5\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m) \u001b[39m*\u001b[39m W_U\n\u001b[1;32m     31\u001b[0m \u001b[39mprint\u001b[39m(average_boost\u001b[39m.\u001b[39mshape) \u001b[39m# d_mlp d_vocab\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'unsqueeze'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# token based measures:\n",
    "# index\n",
    "# next index\n",
    "\n",
    "# neuron based measures:\n",
    "# index\n",
    "# cosine sim with gen\n",
    "# average activation when context neuron enabled\n",
    "# average activation when context neuron disabled\n",
    "# 5 if cosine sim > 0 else -5\n",
    "# loss change when neuron ablated\n",
    "\n",
    "# boost_gen_acts = cosine_sims\n",
    "# boost_gen_acts[cosine_sims > 0] = 5.0\n",
    "# boost_gen_acts[cosine_sims <= 0] = -5.0\n",
    "\n",
    "# def get_deactivate_neuron_hook(neuron):\n",
    "#     def deactivate_neurons_hook(value, hook):\n",
    "#         value[:, :, neuron] = MEAN_ACTIVATION_INACTIVE\n",
    "#         return value\n",
    "# deactivate_neurons_fwd_hooks=[(f'blocks.{LAYER_TO_ABLATE}.mlp.hook_post', deactivate_neurons_hook)]\n",
    "# loss_diffs = []\n",
    "# for i in range(2048):\n",
    "\n",
    "W_out = model.state_dict()['blocks.5.mlp.W_out']\n",
    "W_U = model.W_U\n",
    "print(W_out.shape, W_U.shape, all_activations_l5.unsqueeze(1).shape)\n",
    "\n",
    "average_boost = W_out * all_activations_l5.unsqueeze(1) * W_U\n",
    "print(average_boost.shape) # d_mlp d_vocab\n",
    "\n",
    "# 'Average gen boost': [i zip(all_activations_l5,\n",
    "\n",
    "# mean activation\n",
    "data = {\n",
    "    'Neuron index': list(range(2048)),\n",
    "    'Cosine similarity with \\\"gen\\\"': cosine_sims.tolist(),\n",
    "    'Average act - context neuron enabled': german_activations_l5.tolist(),\n",
    "    'Average act - context neuron disabled': english_activations_l5.tolist(),\n",
    "    'Average act': all_activations_l5.tolist()\n",
    "    # 'Boost \\\"gen\\\" act': boost_gen_acts.tolist(),\n",
    "    # 'Loss change when ablated': \n",
    "}\n",
    "\n",
    "# Create the dataframe\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Print the dataframe\n",
    "print(df)\n",
    "\n",
    "# More complete picture of neurons that boost gen.\n",
    "# Line plot of each neuron's gen boost with dotted vertical lines where the German context neuron-boosted ones are"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brainstorm notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to decompose an MLP5 neuron's effect into its boost to the correct logit and its deboost of other logits. I want to discover how these two effects change the log prob.\n",
    "\n",
    "metric like loss reduction vs. token boost\n",
    "\n",
    "~~run direct effect, patch each neuron in top 10 individually, get overall loss reduction from neuron controlled by context neuron (equivalent to logprob increase for correct token)~~\n",
    "decompose loss reduction into two parts:\n",
    "run direct effect, patch the correct token logit boost from the neuron (removing the neuron's other effects), get overall loss reduction from neuron (equivalent to logprob increase for correct token)\n",
    "destructive interference loss reduction = overall loss reduction - loss reduction from correct token logit boost (component of the logprob increase for correct token due to it deboosting incorrect token)\n",
    "\n",
    "Patch the correct token logit boost from the neuron (removing the neuron's other effects).\n",
    "1. Get baseline logprobs for a prompt\n",
    "2. Get difference in logits from activating the neuron under test. Can use get_direct_effects with return_type='logits'.\n",
    "3. Run the model with return_type='logits' and the neuron under test zero ablated. \n",
    "4. Add the correct token logit from step 1 to a copy of the output logits. Convert to logprobs\n",
    "5. Add the incorrect token logits from step 1 to a copy of the output logits. Convert to logprobs\n",
    "6. Compare A. lobprobs with correct answer token logit increase, B. logprobs with incorrect answer token logit increases, and C. baseline logprobs\n",
    "\n",
    "~~If the context neuron gives each neuron a flat boost then if we decompose the resulting flat boost to one MLP5 neuron into a boost to one logit vs. boosts to all other logits it will change the logprobs (first will increase answer probability and second will reduce answer probability). \n",
    "\n",
    "the two resulting log probs at the correct answer token won't add up to the original log probs (?). \n",
    "\n",
    "If the boost is flat the correct percentage decomposition is 1/50000 and 49999/50000? In practice/all other factors being equal\n",
    "\n",
    "New plan:\n",
    "\n",
    "Difference between baseline log prob and neuron log prob?\n",
    "Classify individual neurons by percentage constructive vs destructive by looking at their log probs and summing the incorrect token log probs\n",
    "\n",
    "Correct log prob difference\n",
    "Incorrect log prob difference (summed over every plausible token?)\n",
    "\n",
    "Largest boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the log prob for an incorrect token is significantly lower then that's where the extra probability density on the correct answer is coming from \n",
    "# Constructive interference increases correct token log prob and uniformly decreases other log probs\n",
    "# Destructive interference decreases specific other log probs and uniformly increases other log probs\n",
    " \n",
    "# Most neurons are a mixture of the above\n",
    "# Decompose neurons into what % of their effect is each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original_logits, ablated_logprobs, _, all_MLP5_logprobs = haystack_utils.get_direct_effect(prompts, model, pos=-2, context_ablation_hooks=deactivate_neurons_fwd_hooks, context_activation_hooks=activate_neurons_fwd_hooks, return_type='logprobs')\n",
    "# _, _, _, top_MLP5_ablated_logprobs = haystack_utils.get_direct_effect(prompts, model, pos=-2, context_ablation_hooks=deactivate_neurons_fwd_hooks, context_activation_hooks=activate_neurons_fwd_hooks+ablate_top_neurons_hook, return_type='logprobs')\n",
    "# _, _, _, bottom_MLP5_ablated_logprobs = haystack_utils.get_direct_effect(prompts, model, pos=-2, context_ablation_hooks=deactivate_neurons_fwd_hooks, context_activation_hooks=activate_neurons_fwd_hooks+ablate_bottom_neurons_hook, return_type='logprobs')\n",
    "\n",
    "# bottom_neuron_high_difference_logprobs = (all_MLP5_logprobs - bottom_MLP5_ablated_logprobs)\n",
    "# bottom_neuron_high_difference_logprobs[bottom_neuron_high_difference_logprobs < 0] = 0\n",
    "# bottom_neuron_high_difference_logprobs = bottom_neuron_high_difference_logprobs.mean(0)\n",
    "# bottom_neuron_high_difference_logprobs[all_MLP5_logprobs.mean(0) < -7] = 0\n",
    "\n",
    "# bottom_non_zero_count = (bottom_neuron_high_difference_logprobs > 0).sum()\n",
    "# bottom_neuron_high_difference_logprobs, bottom_indices = haystack_utils.top_k_with_exclude(bottom_neuron_high_difference_logprobs, min(bottom_non_zero_count, 50), all_ignore)\n",
    "# haystack_utils.line(bottom_neuron_high_difference_logprobs.cpu().numpy(), title='Largest positive difference in log probs for tokens when bottom neurons are not ablated', xticks=[model.to_str_tokens([i])[0] for i in bottom_indices])\n",
    "\n",
    "\n",
    "# top_neuron_high_difference_logprobs = (all_MLP5_logprobs - top_MLP5_ablated_logprobs)\n",
    "# top_neuron_high_difference_logprobs[top_neuron_high_difference_logprobs < 0] = 0\n",
    "# top_neuron_high_difference_logprobs = top_neuron_high_difference_logprobs.mean(0)\n",
    "# top_neuron_high_difference_logprobs[all_MLP5_logprobs.mean(0) < -7] = 0\n",
    "\n",
    "# top_non_zero_count = (top_neuron_high_difference_logprobs > 0).sum()\n",
    "# top_neuron_high_difference_logprobs, top_indices = haystack_utils.top_k_with_exclude(top_neuron_high_difference_logprobs, min(top_non_zero_count, 50), all_ignore)\n",
    "# haystack_utils.line(top_neuron_high_difference_logprobs.cpu().numpy(), title='Largest positive difference in log probs for tokens when bottom neurons are not ablated', xticks=[model.to_str_tokens([i])[0] for i in top_indices])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
