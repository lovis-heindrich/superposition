{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from transformer_lens import HookedTransformer, ActivationCache, utils, patching\n",
    "from jaxtyping import Float, Int, Bool\n",
    "from torch import Tensor\n",
    "from tqdm.auto import tqdm\n",
    "import plotly.io as pio\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "pio.renderers.default = \"notebook_connected\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.autograd.set_grad_enabled(False)\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "from haystack_utils import get_mlp_activations\n",
    "import haystack_utils\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model EleutherAI/pythia-70m into HookedTransformer\n",
      "data/german_europarl.json: Loaded 2000 examples with 152 to 2000 characters each.\n",
      "data/english_europarl.json: Loaded 2000 examples with 165 to 2000 characters each.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "caab3dbe6ec4421e9e51a46169f2e8be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06d1fbb1280348d99b43c3bc296893cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = HookedTransformer.from_pretrained(\"EleutherAI/pythia-70m\",\n",
    "    center_unembed=True,\n",
    "    center_writing_weights=True,\n",
    "    fold_ln=True,\n",
    "    device=device)\n",
    "\n",
    "german_data = haystack_utils.load_json_data(\"data/german_europarl.json\")[:200]\n",
    "english_data = haystack_utils.load_json_data(\"data/english_europarl.json\")[:200]\n",
    "\n",
    "\n",
    "english_activations = {}\n",
    "german_activations = {}\n",
    "for layer in range(3, 4):\n",
    "    english_activations[layer] = get_mlp_activations(english_data, layer, model, mean=False)\n",
    "    german_activations[layer] = get_mlp_activations(german_data, layer, model, mean=False)\n",
    "\n",
    "LAYER_TO_ABLATE = 3\n",
    "NEURONS_TO_ABLATE = [669]\n",
    "MEAN_ACTIVATION_ACTIVE = german_activations[LAYER_TO_ABLATE][:, NEURONS_TO_ABLATE].mean()\n",
    "MEAN_ACTIVATION_INACTIVE = english_activations[LAYER_TO_ABLATE][:, NEURONS_TO_ABLATE].mean()\n",
    "\n",
    "def deactivate_neurons_hook(value, hook):\n",
    "    value[:, :, NEURONS_TO_ABLATE] = MEAN_ACTIVATION_INACTIVE\n",
    "    return value\n",
    "deactivate_neurons_fwd_hooks=[(f'blocks.{LAYER_TO_ABLATE}.mlp.hook_post', deactivate_neurons_hook)]\n",
    "\n",
    "def activate_neurons_hook(value, hook):\n",
    "    value[:, :, NEURONS_TO_ABLATE] = MEAN_ACTIVATION_ACTIVE\n",
    "    return value\n",
    "activate_neurons_fwd_hooks=[(f'blocks.{LAYER_TO_ABLATE}.mlp.hook_post', activate_neurons_hook)]\n",
    "\n",
    "all_ignore, not_ignore = haystack_utils.get_weird_tokens(model, plot_norms=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find top common German tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a170aead403944e192b8cd4bd67c4f33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' der', 'en', ' die', ' und', 'ung', 'ä', ' in', ' den', ' des', ' zu', 'ch', 'n', 'st', 're', 'z', ' von', ' für', 'äsident', ' Pr', 'ischen', 't', 'ü', 'icht', 'gen', ' ist', ' auf', ' dass', 'ge', 'ig', ' im', 'in', ' über', 'g', ' das', 'te', ' er', 'men', ' w', 'es', ' an', 'ß', ' wir', ' eine', 'f', ' W', 'hen', 'w', ' Europ', ' ich', 'ungen', 'ren', 'le', ' dem', 'ten', ' ein', 'e', ' Z', ' Ver', 'der', ' B', ' mit', ' dies', 'h', ' nicht', 'ungs', 's', ' G', ' z', 'it', ' Herr', ' es', 'l', ' S', 'ich', 'lich', ' An', 'heit', 'ie', ' Er', ' zur', ' V', ' ver', 'u', 'hr', 'chaft', 'Der', ' Ich', ' Ab', ' haben', 'i', 'ant', 'chte', ' mö', 'er', ' K', 'igen', ' Ber', 'ür', ' Fra', 'em']\n"
     ]
    }
   ],
   "source": [
    "# Get top common german tokens excluding punctuation\n",
    "token_counts = torch.zeros(model.cfg.d_vocab).cuda()\n",
    "for example in tqdm(german_data):\n",
    "    tokens = model.to_tokens(example)\n",
    "    for token in tokens[0]:\n",
    "        token_counts[token.item()] += 1\n",
    "\n",
    "punctuation = [\"\\n\", \".\", \",\", \"!\", \"?\", \";\", \":\", \"-\", \"(\", \")\", \"[\", \"]\", \"{\", \"}\", \"<\", \">\", \"/\", \"\\\\\", \"\\\"\", \"'\"]\n",
    "leading_space_punctuation = [\" \" + char for char in punctuation]\n",
    "punctuation_tokens = model.to_tokens(punctuation + leading_space_punctuation + [' –', \" \", '  ', \"<|endoftext|>\"])[:, 1].flatten()\n",
    "token_counts[punctuation_tokens] = 0\n",
    "token_counts[all_ignore] = 0\n",
    "\n",
    "top_counts, top_tokens = torch.topk(token_counts, 100)\n",
    "print(model.to_str_tokens(top_tokens[:100]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of ngrams preceded by random prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_selection(tensor, n=12):\n",
    "    # Hacky replacement for np.random.choice\n",
    "    return tensor[torch.randperm(len(tensor))[:n]]\n",
    "\n",
    "def generate_random_prompts(end_string, n=50, length=12):\n",
    "    # Generate a batch of random prompts ending with a specific ngram\n",
    "    end_tokens = model.to_tokens(end_string).flatten()[1:]\n",
    "    prompts = []\n",
    "    for i in range(n):\n",
    "        prompt = get_random_selection(top_tokens[:max(50, length)], n=length).cuda()\n",
    "        prompt = torch.cat([prompt, end_tokens])\n",
    "        prompts.append(prompt)\n",
    "    prompts = torch.stack(prompts)\n",
    "    return prompts\n",
    "\n",
    "def replace_column(prompts: Int[Tensor, \"n_prompts n_tokens\"], token_index: int):\n",
    "    # Replaces a specific token position in a batch of prompts with random common German tokens\n",
    "    new_prompts = prompts.clone()\n",
    "    random_tokens = get_random_selection(top_tokens[:max(50, prompts.shape[0])], n=prompts.shape[0]).cuda()\n",
    "    new_prompts[:, token_index] = random_tokens\n",
    "    return new_prompts \n",
    "\n",
    "def loss_analysis(prompts: Tensor, title=\"\"):\n",
    "    # Loss plot for a batch of prompts\n",
    "    names = [\"Original\", \"Ablated\", \"MLP5 path patched\"]\n",
    "    original_loss, ablated_loss, _, only_activated_loss = \\\n",
    "        haystack_utils.get_direct_effect(prompts, model, pos=-1,\n",
    "                                        context_ablation_hooks=deactivate_neurons_fwd_hooks, \n",
    "                                        context_activation_hooks=activate_neurons_fwd_hooks, \n",
    "                                        )\n",
    "    haystack_utils.plot_barplot([original_loss.tolist(), ablated_loss.tolist(), only_activated_loss.tolist()], names, ylabel=\"Loss\", title=title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_analysis_random_prompts(end_string, n=50, length=12, replace_columns: list[int] | None = None):\n",
    "    # Loss plot for a batch of random prompts ending with a specific ngram and optionally replacing specific tokens\n",
    "    prompts = generate_random_prompts(end_string, n=n, length=length)\n",
    "    title=f\"Average last token loss on {length} random tokens ending in '{end_string}'\"\n",
    "    if replace_columns is not None:\n",
    "        replaced_tokens = model.to_str_tokens(prompts[0, replace_columns])\n",
    "        title += f\" replacing {replaced_tokens}\"\n",
    "        for column in replace_columns:\n",
    "            prompts = replace_column(prompts, column)\n",
    "    \n",
    "    loss_analysis(prompts, title=title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "# loss_analysis_random_prompts(\" Vorschlägen\", n=100, length=20)\n",
    "# loss_analysis_random_prompts(\" Vorschlägen\", n=100, length=20, replace_columns=[-2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7932e1ec68a442f9e59f074031058ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Ngram: ', options=(' Vorschlägen', ' Vorschläge', ' häufig', ' schließt', ' beweglich'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28092ef3432a4aca926636606592bac0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SelectMultiple(description='Replace Columns:', index=(3,), options=('-2', '-3', '-4', 'None'), value=('None',)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "743258bf387d4be4a59aa4ea49af228a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a dropdown menu widget\n",
    "dropdown = widgets.Dropdown(\n",
    "    options = [\" Vorschlägen\", \" Vorschläge\", \" häufig\", \" schließt\", \" beweglich\"], \n",
    "    value = \" Vorschlägen\",\n",
    "    description = 'Ngram: ',\n",
    ")\n",
    "\n",
    "replace_columns_dropdown = widgets.SelectMultiple(\n",
    "    options = ['-2', '-3', '-4', 'None'],\n",
    "    value = ['None'],  # default selected value\n",
    "    description = 'Replace Columns:',\n",
    ")\n",
    "\n",
    "# Create an output widget to hold the plot\n",
    "output = widgets.Output()\n",
    "\n",
    "# Define a function to call when the widget's value changes\n",
    "def update_plot(*args):\n",
    "    # Clear the old plot from the output\n",
    "    with output:\n",
    "        clear_output(wait=True)\n",
    "\n",
    "        if 'None' in replace_columns_dropdown.value:\n",
    "            replace_columns = None\n",
    "        else:\n",
    "            # If 'None' not selected, convert the selected values to integers\n",
    "            replace_columns = [int(val) for val in replace_columns_dropdown.value]\n",
    "        \n",
    "        # Call your function with the values from the widgets\n",
    "        loss_analysis_random_prompts(dropdown.value, n=100, length=20, replace_columns=replace_columns)\n",
    "\n",
    "# Set the function to be called when the widget's value changes\n",
    "dropdown.observe(update_plot, 'value')\n",
    "replace_columns_dropdown.observe(update_plot, 'value')\n",
    "\n",
    "# Display the widget and the output\n",
    "display(dropdown, replace_columns_dropdown, output)\n",
    "\n",
    "# Run once at startup\n",
    "update_plot()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify relevant neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 24])\n"
     ]
    }
   ],
   "source": [
    "prompts = generate_random_prompts(\" Vorschlägen\", n=100, length=20)\n",
    "print(prompts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e310608888c48358315b29a4db9fe27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2048 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0007)\n"
     ]
    }
   ],
   "source": [
    "with model.hooks(deactivate_neurons_fwd_hooks):\n",
    "    _, ablated_cache = model.run_with_cache(prompts)\n",
    "\n",
    "def get_ablate_neurons_hook(neuron: int | list[int], ablated_cache, layer=5):\n",
    "    def ablate_neurons_hook(value, hook):\n",
    "        value[:, :, neuron] = ablated_cache[f'blocks.{layer}.mlp.hook_post'][:, :, neuron]\n",
    "        return value\n",
    "    return [(f'blocks.{layer}.mlp.hook_post', ablate_neurons_hook)]\n",
    "\n",
    "diffs = torch.zeros(2048, prompts.shape[0])\n",
    "# Loss with path patched MLP5 neurons\n",
    "_, _, _, baseline_loss = haystack_utils.get_direct_effect(prompts, model, pos=-1, context_ablation_hooks=deactivate_neurons_fwd_hooks, context_activation_hooks=activate_neurons_fwd_hooks)\n",
    "for neuron in tqdm(range(2048)):\n",
    "    ablate_single_neuron_hook = get_ablate_neurons_hook(neuron, ablated_cache)\n",
    "    # Loss with path patched MLP5 neurons but a single neuron changed back to original ablated value\n",
    "    _, _, _, only_deactivated_loss = haystack_utils.get_direct_effect(prompts, model, pos=-1, context_ablation_hooks=deactivate_neurons_fwd_hooks, context_activation_hooks=activate_neurons_fwd_hooks+ablate_single_neuron_hook)\n",
    "    diffs[neuron] = only_deactivated_loss - baseline_loss\n",
    "\n",
    "print(diffs.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>                            <div id=\"bb840bff-3ff5-44c1-bab4-cc151887bab6\" class=\"plotly-graph-div\" style=\"height:525px; width:800px;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"bb840bff-3ff5-44c1-bab4-cc151887bab6\")) {                    Plotly.newPlot(                        \"bb840bff-3ff5-44c1-bab4-cc151887bab6\",                        [{\"hovertemplate\":\"variable=0\\u003cbr\\u003eindex=%{x}\\u003cbr\\u003evalue=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"0\",\"line\":{\"color\":\"#636efa\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"0\",\"showlegend\":true,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542,543,544,545,546,547,548,549,550,551,552,553,554,555,556,557,558,559,560,561,562,563,564,565,566,567,568,569,570,571,572,573,574,575,576,577,578,579,580,581,582,583,584,585,586,587,588,589,590,591,592,593,594,595,596,597,598,599,600,601,602,603,604,605,606,607,608,609,610,611,612,613,614,615,616,617,618,619,620,621,622,623,624,625,626,627,628,629,630,631,632,633,634,635,636,637,638,639,640,641,642,643,644,645,646,647,648,649,650,651,652,653,654,655,656,657,658,659,660,661,662,663,664,665,666,667,668,669,670,671,672,673,674,675,676,677,678,679,680,681,682,683,684,685,686,687,688,689,690,691,692,693,694,695,696,697,698,699,700,701,702,703,704,705,706,707,708,709,710,711,712,713,714,715,716,717,718,719,720,721,722,723,724,725,726,727,728,729,730,731,732,733,734,735,736,737,738,739,740,741,742,743,744,745,746,747,748,749,750,751,752,753,754,755,756,757,758,759,760,761,762,763,764,765,766,767,768,769,770,771,772,773,774,775,776,777,778,779,780,781,782,783,784,785,786,787,788,789,790,791,792,793,794,795,796,797,798,799,800,801,802,803,804,805,806,807,808,809,810,811,812,813,814,815,816,817,818,819,820,821,822,823,824,825,826,827,828,829,830,831,832,833,834,835,836,837,838,839,840,841,842,843,844,845,846,847,848,849,850,851,852,853,854,855,856,857,858,859,860,861,862,863,864,865,866,867,868,869,870,871,872,873,874,875,876,877,878,879,880,881,882,883,884,885,886,887,888,889,890,891,892,893,894,895,896,897,898,899,900,901,902,903,904,905,906,907,908,909,910,911,912,913,914,915,916,917,918,919,920,921,922,923,924,925,926,927,928,929,930,931,932,933,934,935,936,937,938,939,940,941,942,943,944,945,946,947,948,949,950,951,952,953,954,955,956,957,958,959,960,961,962,963,964,965,966,967,968,969,970,971,972,973,974,975,976,977,978,979,980,981,982,983,984,985,986,987,988,989,990,991,992,993,994,995,996,997,998,999,1000,1001,1002,1003,1004,1005,1006,1007,1008,1009,1010,1011,1012,1013,1014,1015,1016,1017,1018,1019,1020,1021,1022,1023,1024,1025,1026,1027,1028,1029,1030,1031,1032,1033,1034,1035,1036,1037,1038,1039,1040,1041,1042,1043,1044,1045,1046,1047,1048,1049,1050,1051,1052,1053,1054,1055,1056,1057,1058,1059,1060,1061,1062,1063,1064,1065,1066,1067,1068,1069,1070,1071,1072,1073,1074,1075,1076,1077,1078,1079,1080,1081,1082,1083,1084,1085,1086,1087,1088,1089,1090,1091,1092,1093,1094,1095,1096,1097,1098,1099,1100,1101,1102,1103,1104,1105,1106,1107,1108,1109,1110,1111,1112,1113,1114,1115,1116,1117,1118,1119,1120,1121,1122,1123,1124,1125,1126,1127,1128,1129,1130,1131,1132,1133,1134,1135,1136,1137,1138,1139,1140,1141,1142,1143,1144,1145,1146,1147,1148,1149,1150,1151,1152,1153,1154,1155,1156,1157,1158,1159,1160,1161,1162,1163,1164,1165,1166,1167,1168,1169,1170,1171,1172,1173,1174,1175,1176,1177,1178,1179,1180,1181,1182,1183,1184,1185,1186,1187,1188,1189,1190,1191,1192,1193,1194,1195,1196,1197,1198,1199,1200,1201,1202,1203,1204,1205,1206,1207,1208,1209,1210,1211,1212,1213,1214,1215,1216,1217,1218,1219,1220,1221,1222,1223,1224,1225,1226,1227,1228,1229,1230,1231,1232,1233,1234,1235,1236,1237,1238,1239,1240,1241,1242,1243,1244,1245,1246,1247,1248,1249,1250,1251,1252,1253,1254,1255,1256,1257,1258,1259,1260,1261,1262,1263,1264,1265,1266,1267,1268,1269,1270,1271,1272,1273,1274,1275,1276,1277,1278,1279,1280,1281,1282,1283,1284,1285,1286,1287,1288,1289,1290,1291,1292,1293,1294,1295,1296,1297,1298,1299,1300,1301,1302,1303,1304,1305,1306,1307,1308,1309,1310,1311,1312,1313,1314,1315,1316,1317,1318,1319,1320,1321,1322,1323,1324,1325,1326,1327,1328,1329,1330,1331,1332,1333,1334,1335,1336,1337,1338,1339,1340,1341,1342,1343,1344,1345,1346,1347,1348,1349,1350,1351,1352,1353,1354,1355,1356,1357,1358,1359,1360,1361,1362,1363,1364,1365,1366,1367,1368,1369,1370,1371,1372,1373,1374,1375,1376,1377,1378,1379,1380,1381,1382,1383,1384,1385,1386,1387,1388,1389,1390,1391,1392,1393,1394,1395,1396,1397,1398,1399,1400,1401,1402,1403,1404,1405,1406,1407,1408,1409,1410,1411,1412,1413,1414,1415,1416,1417,1418,1419,1420,1421,1422,1423,1424,1425,1426,1427,1428,1429,1430,1431,1432,1433,1434,1435,1436,1437,1438,1439,1440,1441,1442,1443,1444,1445,1446,1447,1448,1449,1450,1451,1452,1453,1454,1455,1456,1457,1458,1459,1460,1461,1462,1463,1464,1465,1466,1467,1468,1469,1470,1471,1472,1473,1474,1475,1476,1477,1478,1479,1480,1481,1482,1483,1484,1485,1486,1487,1488,1489,1490,1491,1492,1493,1494,1495,1496,1497,1498,1499,1500,1501,1502,1503,1504,1505,1506,1507,1508,1509,1510,1511,1512,1513,1514,1515,1516,1517,1518,1519,1520,1521,1522,1523,1524,1525,1526,1527,1528,1529,1530,1531,1532,1533,1534,1535,1536,1537,1538,1539,1540,1541,1542,1543,1544,1545,1546,1547,1548,1549,1550,1551,1552,1553,1554,1555,1556,1557,1558,1559,1560,1561,1562,1563,1564,1565,1566,1567,1568,1569,1570,1571,1572,1573,1574,1575,1576,1577,1578,1579,1580,1581,1582,1583,1584,1585,1586,1587,1588,1589,1590,1591,1592,1593,1594,1595,1596,1597,1598,1599,1600,1601,1602,1603,1604,1605,1606,1607,1608,1609,1610,1611,1612,1613,1614,1615,1616,1617,1618,1619,1620,1621,1622,1623,1624,1625,1626,1627,1628,1629,1630,1631,1632,1633,1634,1635,1636,1637,1638,1639,1640,1641,1642,1643,1644,1645,1646,1647,1648,1649,1650,1651,1652,1653,1654,1655,1656,1657,1658,1659,1660,1661,1662,1663,1664,1665,1666,1667,1668,1669,1670,1671,1672,1673,1674,1675,1676,1677,1678,1679,1680,1681,1682,1683,1684,1685,1686,1687,1688,1689,1690,1691,1692,1693,1694,1695,1696,1697,1698,1699,1700,1701,1702,1703,1704,1705,1706,1707,1708,1709,1710,1711,1712,1713,1714,1715,1716,1717,1718,1719,1720,1721,1722,1723,1724,1725,1726,1727,1728,1729,1730,1731,1732,1733,1734,1735,1736,1737,1738,1739,1740,1741,1742,1743,1744,1745,1746,1747,1748,1749,1750,1751,1752,1753,1754,1755,1756,1757,1758,1759,1760,1761,1762,1763,1764,1765,1766,1767,1768,1769,1770,1771,1772,1773,1774,1775,1776,1777,1778,1779,1780,1781,1782,1783,1784,1785,1786,1787,1788,1789,1790,1791,1792,1793,1794,1795,1796,1797,1798,1799,1800,1801,1802,1803,1804,1805,1806,1807,1808,1809,1810,1811,1812,1813,1814,1815,1816,1817,1818,1819,1820,1821,1822,1823,1824,1825,1826,1827,1828,1829,1830,1831,1832,1833,1834,1835,1836,1837,1838,1839,1840,1841,1842,1843,1844,1845,1846,1847,1848,1849,1850,1851,1852,1853,1854,1855,1856,1857,1858,1859,1860,1861,1862,1863,1864,1865,1866,1867,1868,1869,1870,1871,1872,1873,1874,1875,1876,1877,1878,1879,1880,1881,1882,1883,1884,1885,1886,1887,1888,1889,1890,1891,1892,1893,1894,1895,1896,1897,1898,1899,1900,1901,1902,1903,1904,1905,1906,1907,1908,1909,1910,1911,1912,1913,1914,1915,1916,1917,1918,1919,1920,1921,1922,1923,1924,1925,1926,1927,1928,1929,1930,1931,1932,1933,1934,1935,1936,1937,1938,1939,1940,1941,1942,1943,1944,1945,1946,1947,1948,1949,1950,1951,1952,1953,1954,1955,1956,1957,1958,1959,1960,1961,1962,1963,1964,1965,1966,1967,1968,1969,1970,1971,1972,1973,1974,1975,1976,1977,1978,1979,1980,1981,1982,1983,1984,1985,1986,1987,1988,1989,1990,1991,1992,1993,1994,1995,1996,1997,1998,1999,2000,2001,2002,2003,2004,2005,2006,2007,2008,2009,2010,2011,2012,2013,2014,2015,2016,2017,2018,2019,2020,2021,2022,2023,2024,2025,2026,2027,2028,2029,2030,2031,2032,2033,2034,2035,2036,2037,2038,2039,2040,2041,2042,2043,2044,2045,2046,2047],\"xaxis\":\"x\",\"y\":[-0.626203715801239,-0.32223811745643616,-0.21439656615257263,-0.16937896609306335,-0.16847142577171326,-0.1676722764968872,-0.14869967103004456,-0.12485181540250778,-0.11949136853218079,-0.11882081627845764,-0.11721665412187576,-0.11400332301855087,-0.09752312302589417,-0.0970047190785408,-0.09498725831508636,-0.08953308314085007,-0.08096150308847427,-0.0797683373093605,-0.07825647294521332,-0.07760561257600784,-0.07662937045097351,-0.07552552968263626,-0.07552388310432434,-0.07465876638889313,-0.07354392111301422,-0.07290498912334442,-0.07110336422920227,-0.07108300924301147,-0.07084714621305466,-0.07060644030570984,-0.07020005583763123,-0.06902012228965759,-0.0681864321231842,-0.06763505190610886,-0.0628969743847847,-0.06225412338972092,-0.06158764660358429,-0.061077915132045746,-0.05922193452715874,-0.05898747593164444,-0.058405350893735886,-0.058328114449977875,-0.05821117013692856,-0.05682684853672981,-0.0559924840927124,-0.05544129014015198,-0.054374340921640396,-0.05277816951274872,-0.05116628482937813,-0.050702232867479324,-0.04949411377310753,-0.04845385625958443,-0.048241421580314636,-0.04726717621088028,-0.04692952334880829,-0.04554326832294464,-0.044299930334091187,-0.04199613630771637,-0.0412939116358757,-0.04119740054011345,-0.040975168347358704,-0.04059096798300743,-0.04015011340379715,-0.04003256931900978,-0.03943949565291405,-0.03916759416460991,-0.03909296914935112,-0.03791627287864685,-0.03740980476140976,-0.03718116134405136,-0.03700238838791847,-0.03699636831879616,-0.03670203685760498,-0.03650559112429619,-0.036334339529275894,-0.03523574769496918,-0.03521973267197609,-0.0344710648059845,-0.03381243348121643,-0.033687297254800797,-0.03352930396795273,-0.03309888765215874,-0.03288090601563454,-0.03261932358145714,-0.03204677999019623,-0.03199195861816406,-0.03187493234872818,-0.03162365034222603,-0.03151228278875351,-0.03120000846683979,-0.03118891641497612,-0.030981270596385002,-0.030728967860341072,-0.03044292889535427,-0.03038800321519375,-0.03037436679005623,-0.030174221843481064,-0.029920442029833794,-0.029915301129221916,-0.02945195510983467,-0.02938028797507286,-0.029297422617673874,-0.0292922742664814,-0.029055876657366753,-0.02833973802626133,-0.02808474376797676,-0.027979163452982903,-0.027652669697999954,-0.02735980972647667,-0.027318811044096947,-0.027313388884067535,-0.027134045958518982,-0.026666827499866486,-0.026569053530693054,-0.026440074667334557,-0.026305412873625755,-0.026227159425616264,-0.026199428364634514,-0.025337375700473785,-0.02506818063557148,-0.02506658062338829,-0.024280423298478127,-0.02388385497033596,-0.023784277960658073,-0.02360238879919052,-0.023543348535895348,-0.02289302833378315,-0.02274099737405777,-0.022595930844545364,-0.022525129839777946,-0.021700425073504448,-0.021417679265141487,-0.021299628540873528,-0.02092956379055977,-0.020849311724305153,-0.02031635120511055,-0.020122598856687546,-0.01937681995332241,-0.01925116963684559,-0.01912306249141693,-0.018894897773861885,-0.018703466281294823,-0.018685124814510345,-0.0183832086622715,-0.018229572102427483,-0.018004028126597404,-0.017895398661494255,-0.01787755638360977,-0.017862465232610703,-0.017667457461357117,-0.01763588935136795,-0.017590539529919624,-0.017535017803311348,-0.017516836524009705,-0.017161371186375618,-0.017143039032816887,-0.017053984105587006,-0.01694200374186039,-0.016936294734477997,-0.016797613352537155,-0.016711212694644928,-0.016363758593797684,-0.016004664823412895,-0.0159006267786026,-0.015746820718050003,-0.015608765184879303,-0.01555120013654232,-0.015396774746477604,-0.015325757674872875,-0.015127611346542835,-0.015072598122060299,-0.01470744889229536,-0.01469345297664404,-0.014664760790765285,-0.014653574675321579,-0.014649511314928532,-0.014353853650391102,-0.014191202819347382,-0.014160486869513988,-0.01405937597155571,-0.013972853310406208,-0.01394357904791832,-0.013551401905715466,-0.013415280729532242,-0.013302158564329147,-0.013082008808851242,-0.013048219494521618,-0.0129934623837471,-0.0129682756960392,-0.012934727594256401,-0.012874034233391285,-0.012840049341320992,-0.012812712229788303,-0.012809707783162594,-0.01256675273180008,-0.012500165961682796,-0.012434394098818302,-0.012228027917444706,-0.01214064285159111,-0.01202801987528801,-0.011991016566753387,-0.011924009770154953,-0.011894755065441132,-0.011630672961473465,-0.011618628166615963,-0.01151075679808855,-0.01149632129818201,-0.011410918086767197,-0.011382796801626682,-0.011301812715828419,-0.011206594295799732,-0.011104345321655273,-0.01101994514465332,-0.010867642238736153,-0.010781939141452312,-0.010747608728706837,-0.010724843479692936,-0.010703363455832005,-0.010659378953278065,-0.010633853264153004,-0.010441819205880165,-0.010407133027911186,-0.010382653214037418,-0.010269221849739552,-0.01022164337337017,-0.010154856368899345,-0.010001334361732006,-0.009939913637936115,-0.009885961189866066,-0.009841490536928177,-0.009805607609450817,-0.009703012183308601,-0.009652300737798214,-0.00957215204834938,-0.00956781581044197,-0.009505018591880798,-0.009460709057748318,-0.009454911574721336,-0.009434032253921032,-0.009421239607036114,-0.009396713227033615,-0.009338464587926865,-0.009299563243985176,-0.009237002581357956,-0.009165942668914795,-0.008968742564320564,-0.008948281407356262,-0.008893122896552086,-0.008811724372208118,-0.00863765086978674,-0.008632747456431389,-0.00859780889004469,-0.00854470208287239,-0.008521793410182,-0.008399249985814095,-0.008354119025170803,-0.00829929206520319,-0.00822526216506958,-0.00820726715028286,-0.008190695196390152,-0.008103722706437111,-0.008045869879424572,-0.007993269711732864,-0.007901649922132492,-0.007874896749854088,-0.007861685007810593,-0.007851403206586838,-0.007810982409864664,-0.007752389181405306,-0.007701255846768618,-0.007607442792505026,-0.007592067588120699,-0.0075567676685750484,-0.007503824308514595,-0.00737770926207304,-0.007362631149590015,-0.007339580450206995,-0.007329395040869713,-0.007304737344384193,-0.0072999559342861176,-0.007297098636627197,-0.007284717634320259,-0.007238779682666063,-0.007233100477606058,-0.007232749834656715,-0.007229049690067768,-0.007219741120934486,-0.007218729704618454,-0.007134641520678997,-0.007118275389075279,-0.007084913086146116,-0.007082447875291109,-0.007051301654428244,-0.007011975161731243,-0.006987371016293764,-0.006955590099096298,-0.006955197546631098,-0.006931767333298922,-0.006902282126247883,-0.006885239388793707,-0.006875400431454182,-0.006784745957702398,-0.006724534090608358,-0.0067067532800138,-0.006683139596134424,-0.006654500495642424,-0.006650886964052916,-0.006647729780524969,-0.006577577441930771,-0.006493526045233011,-0.006409715861082077,-0.006338598672300577,-0.006318572908639908,-0.006299980450421572,-0.006209170911461115,-0.006208023987710476,-0.006205188110470772,-0.0061659179627895355,-0.006132174748927355,-0.006087596528232098,-0.006065287161618471,-0.006036627106368542,-0.006026155315339565,-0.006010971497744322,-0.005973901599645615,-0.005940927192568779,-0.00594055512920022,-0.005901726894080639,-0.005860436707735062,-0.005846438929438591,-0.005821594037115574,-0.005818849895149469,-0.005817130673676729,-0.005810034926980734,-0.005803872365504503,-0.005782978609204292,-0.005772599019110203,-0.005771636962890625,-0.005739853251725435,-0.005739822518080473,-0.005726822651922703,-0.005706848576664925,-0.005686349235475063,-0.00565290218219161,-0.005650355014950037,-0.005641154013574123,-0.00556965172290802,-0.0055390517227351665,-0.005491348914802074,-0.005487262737005949,-0.005453951191157103,-0.005451440345495939,-0.00540571566671133,-0.0053630005568265915,-0.005351560655981302,-0.005332679953426123,-0.005316612310707569,-0.005298697389662266,-0.005251309834420681,-0.005148341879248619,-0.0051382421515882015,-0.00512334518134594,-0.005084353033453226,-0.00508294440805912,-0.005067998077720404,-0.004981145262718201,-0.004974357783794403,-0.004970815498381853,-0.0049346755258738995,-0.0049171061255037785,-0.004901613108813763,-0.0048681218177080154,-0.004786976147443056,-0.004755035042762756,-0.004748104140162468,-0.004713558591902256,-0.004703473765403032,-0.004699604120105505,-0.004687045700848103,-0.004687001928687096,-0.004670140333473682,-0.004648812115192413,-0.004602203145623207,-0.004568162839859724,-0.0045237732119858265,-0.004470731597393751,-0.004468221683055162,-0.00444341916590929,-0.004433083347976208,-0.004413330927491188,-0.004393303766846657,-0.004365867003798485,-0.004354469943791628,-0.004228312522172928,-0.004210255574434996,-0.004183893091976643,-0.004176896996796131,-0.004175978247076273,-0.004172667860984802,-0.004167640116065741,-0.004124540835618973,-0.0041073099710047245,-0.004038942977786064,-0.003978871274739504,-0.003968998324126005,-0.003967659082263708,-0.0039625586941838264,-0.003922055475413799,-0.003913551103323698,-0.003890441032126546,-0.0038820868358016014,-0.0038787852972745895,-0.0038780260365456343,-0.0038508379366248846,-0.003849201602861285,-0.003824070794507861,-0.0038195610977709293,-0.0037994026206433773,-0.0037726492155343294,-0.003758232342079282,-0.0037356442771852016,-0.0037152280565351248,-0.0037097972817718983,-0.0037019802257418633,-0.003690106328576803,-0.003676017513498664,-0.0036637545563280582,-0.003656960790976882,-0.003648113925009966,-0.0036014311481267214,-0.0035981356631964445,-0.0035614233929663897,-0.0035600634291768074,-0.0035585195291787386,-0.003534936811774969,-0.003533177077770233,-0.0034808176569640636,-0.0034766332246363163,-0.003464974695816636,-0.003457004437223077,-0.003454413963481784,-0.003452001139521599,-0.0034358345437794924,-0.003434910671785474,-0.0034184628166258335,-0.0034072028938680887,-0.0033924097660928965,-0.0033921163994818926,-0.00339014595374465,-0.0033636160660535097,-0.0033627310767769814,-0.0033421546686440706,-0.0033386508002877235,-0.0032769483514130116,-0.0032513225451111794,-0.003232535207644105,-0.0032291116658598185,-0.0032267773058265448,-0.003222639672458172,-0.0032026381231844425,-0.003201031591743231,-0.0031850060913711786,-0.003100177040323615,-0.003065353725105524,-0.003064969088882208,-0.0030192090198397636,-0.003016661386936903,-0.0029974791686981916,-0.0029515090864151716,-0.002948225475847721,-0.002947997534647584,-0.0029037713538855314,-0.0028449324890971184,-0.002836599014699459,-0.002835677471011877,-0.0028274774085730314,-0.002793064806610346,-0.00278833438642323,-0.002780602313578129,-0.0027693226002156734,-0.00275108078494668,-0.0027441061101853848,-0.0027364527340978384,-0.002728032413870096,-0.0027231816202402115,-0.002712238347157836,-0.0026934680063277483,-0.0026921259704977274,-0.0026859634090214968,-0.002680013421922922,-0.0026721120811998844,-0.0026684501208364964,-0.002629123395308852,-0.0026252197567373514,-0.002619777340441942,-0.0026018512435257435,-0.0026018493808805943,-0.002570347860455513,-0.002568395109847188,-0.0025664602871984243,-0.0025508843827992678,-0.00255077937617898,-0.002543131820857525,-0.0025398870930075645,-0.002530941041186452,-0.0025203656405210495,-0.0024899288546293974,-0.002488324884325266,-0.002480094088241458,-0.0024667587131261826,-0.002451019361615181,-0.0024309984873980284,-0.0024216107558459044,-0.0024171224795281887,-0.00241278950124979,-0.0024046709295362234,-0.002397518837824464,-0.0023972494527697563,-0.0023819790221750736,-0.0023768292739987373,-0.0023744041100144386,-0.0023630082141608,-0.002357295248657465,-0.002347639761865139,-0.0023388087283819914,-0.0023362133651971817,-0.0023273529950529337,-0.002321322215721011,-0.0023165615275502205,-0.0023024275433272123,-0.002301906468346715,-0.0022963674273341894,-0.0022901601623743773,-0.0022806189954280853,-0.002271466888487339,-0.0022467183880507946,-0.002224002731963992,-0.0022215908393263817,-0.0022204688284546137,-0.0022147453855723143,-0.002213072497397661,-0.0022014526184648275,-0.0021981163881719112,-0.00219385395757854,-0.002190243685618043,-0.002171939704567194,-0.002158714924007654,-0.0021504571195691824,-0.0021350758615881205,-0.0021349529270082712,-0.0021244909148663282,-0.0021220168564468622,-0.00212082383222878,-0.002115233102813363,-0.002102011814713478,-0.0020962818525731564,-0.0020868515130132437,-0.0020698574371635914,-0.002068368950858712,-0.002067947294563055,-0.0020583956502377987,-0.002046598820015788,-0.0020346911624073982,-0.0020298620220273733,-0.002010284224525094,-0.0020093468483537436,-0.0020077391527593136,-0.002006411086767912,-0.001999689731746912,-0.001999220345169306,-0.0019935176242142916,-0.0019763715099543333,-0.0019725142046809196,-0.0019596097990870476,-0.0019562658853828907,-0.0019546053372323513,-0.0019490579143166542,-0.0019469719845801592,-0.0019461828051134944,-0.0019379716832190752,-0.0019035087898373604,-0.0018988944357261062,-0.001893865060992539,-0.0018883789889514446,-0.001884282217361033,-0.0018642301438376307,-0.0018614616710692644,-0.0018591778352856636,-0.0018291996093466878,-0.0018282491946592927,-0.0018069621874019504,-0.001796786324121058,-0.0017913255142048001,-0.0017813480226323009,-0.00177292269654572,-0.0017679753946140409,-0.0017602998996153474,-0.001740962266921997,-0.0017233293037861586,-0.0017109285108745098,-0.0016807744977995753,-0.0016793485265225172,-0.0016662213020026684,-0.0016647969605401158,-0.0016645306022837758,-0.0016374990809708834,-0.0016267186729237437,-0.001625305158086121,-0.0016107659321278334,-0.0016054826555773616,-0.0016025373479351401,-0.0015981676988303661,-0.0015924129402264953,-0.0015906551852822304,-0.0015867699403315783,-0.0015816171653568745,-0.0015792663907632232,-0.0015626683598384261,-0.0015443334123119712,-0.0015413900837302208,-0.001537343137897551,-0.0015324100386351347,-0.001531154615804553,-0.0015212995931506157,-0.0015179961919784546,-0.0015022194711491466,-0.0015009226044639945,-0.0014947265153750777,-0.001492584589868784,-0.0014891853788867593,-0.0014845328405499458,-0.0014810888096690178,-0.0014779414050281048,-0.001472526346333325,-0.001471891300752759,-0.0014571180799975991,-0.001451626536436379,-0.001448323018848896,-0.0014478308148682117,-0.001445071306079626,-0.0014352783327922225,-0.0014060467947274446,-0.0014046883443370461,-0.0013899649493396282,-0.0013851559488102794,-0.0013330786023288965,-0.0013107312843203545,-0.0013093742309138179,-0.0012520721647888422,-0.0012513445690274239,-0.0012183813378214836,-0.0012144966749474406,-0.0012012445367872715,-0.0012000746792182326,-0.0011962286662310362,-0.001191550400108099,-0.0011885683052241802,-0.00118302705232054,-0.001179114100523293,-0.001174364355392754,-0.001170928357169032,-0.0011681871255859733,-0.001156760728918016,-0.0011566898319870234,-0.0011551706120371819,-0.001153552788309753,-0.0011386253172531724,-0.0011276559671387076,-0.0011275969445705414,-0.0011274509597569704,-0.0011057028314098716,-0.0010995946358889341,-0.0010922716464847326,-0.0010906251845881343,-0.0010875810403376818,-0.0010770027292892337,-0.0010733249364420772,-0.0010732458904385567,-0.0010661729611456394,-0.0010514712193980813,-0.001048788777552545,-0.0010467584943398833,-0.0010352146346122026,-0.0010310534853488207,-0.0010280364658683538,-0.0010233818320557475,-0.0010117152705788612,-0.0010097172344103456,-0.001006180769763887,-0.0010033395374193788,-0.0009994646534323692,-0.0009932512184605002,-0.0009906591149047017,-0.0009882982121780515,-0.0009807082824409008,-0.0009783980203792453,-0.0009631653083488345,-0.0009557330631650984,-0.0009516240679658949,-0.0009482518071308732,-0.0009436808759346604,-0.0009390749037265778,-0.0009382723364979029,-0.0009348338935524225,-0.0009193342993967235,-0.0009086528443731368,-0.0009052300592884421,-0.0009032954112626612,-0.0008874700870364904,-0.0008747450774535537,-0.0008683140622451901,-0.0008666166686452925,-0.0008611644734628499,-0.0008576224790886045,-0.0008499753312207758,-0.0008391218143515289,-0.00083313969662413,-0.0008313541184179485,-0.0008267425000667572,-0.000825650233309716,-0.0008238317095674574,-0.0008171722292900085,-0.0007992683094926178,-0.0007978071225807071,-0.0007969178259372711,-0.0007928974810056388,-0.0007874401053413749,-0.0007869234541431069,-0.0007715558749623597,-0.0007708009798079729,-0.0007635149522684515,-0.0007482327637262642,-0.0007430858677253127,-0.0007392713450826705,-0.0007315746042877436,-0.0007279530400410295,-0.0007258137920871377,-0.0007252942305058241,-0.0007236720412038267,-0.0007230456103570759,-0.0007167605799622834,-0.0006899440195411444,-0.0006865491159260273,-0.0006780929979868233,-0.0006778237293474376,-0.0006772059132345021,-0.0006654025637544692,-0.0006646214169450104,-0.000663727056235075,-0.0006636937032453716,-0.0006584060029126704,-0.000655951676890254,-0.00065067526884377,-0.0006453426321968436,-0.000641750986687839,-0.0006201402866281569,-0.0006199717754498124,-0.0006197121692821383,-0.0006018911371938884,-0.0005997033440507948,-0.0005963078001514077,-0.0005882099503651261,-0.000582769513130188,-0.0005813144380226731,-0.0005785418907180429,-0.0005746691022068262,-0.0005702680209651589,-0.000565705297049135,-0.0005570584326051176,-0.0005497780512087047,-0.0005485974252223969,-0.000544370268471539,-0.0005396586493588984,-0.0005313238361850381,-0.0005304175429046154,-0.0005253127310425043,-0.0005247805966064334,-0.0005209764931350946,-0.0005181400338187814,-0.0005179356085136533,-0.0005156348925083876,-0.0005107244942337275,-0.0005081585259176791,-0.0005056209629401565,-0.000504920317325741,-0.0005030971951782703,-0.000502931943628937,-0.000501814647577703,-0.0004981708480045199,-0.0004956285702064633,-0.0004921763902530074,-0.000489767815452069,-0.0004881690547335893,-0.00048176408745348454,-0.0004749679646920413,-0.0004690241767093539,-0.00046216085320338607,-0.0004526038537733257,-0.0004457208560779691,-0.00044358655577525496,-0.0004264900053385645,-0.0004221186100039631,-0.00042115748510695994,-0.0004205989826004952,-0.000415325746871531,-0.00041529894224368036,-0.0004132357134949416,-0.0004107354616280645,-0.0004096353077329695,-0.0004046534013468772,-0.0004040069761686027,-0.0004038336919620633,-0.00040143384831026196,-0.00040010720840655267,-0.00039964212919585407,-0.00039638503221794963,-0.0003947167133446783,-0.0003861184522975236,-0.00038393601425923407,-0.0003769528993871063,-0.0003752154007088393,-0.00037416041595861316,-0.0003699036024045199,-0.0003652283630799502,-0.00036508217453956604,-0.0003603656659834087,-0.00035782321356236935,-0.00035267099156044424,-0.00034820527071133256,-0.00034185335971415043,-0.0003405395254958421,-0.00034016877179965377,-0.00033943011658266187,-0.000332964351400733,-0.00033293067826889455,-0.00033109576907008886,-0.00032552762422710657,-0.00032391457352787256,-0.00032270848168991506,-0.00032179191475734115,-0.0003214772150386125,-0.0003196564211975783,-0.0003174486628267914,-0.00030365452403202653,-0.00030223920475691557,-0.0003014311078004539,-0.0003004042664542794,-0.0002903151616919786,-0.00028189405566081405,-0.00027429431793279946,-0.00027274683816358447,-0.0002701063349377364,-0.0002689521061256528,-0.0002681899059098214,-0.00026505126152187586,-0.0002572260855231434,-0.00025633780751377344,-0.0002561229339335114,-0.0002555440296418965,-0.00025023682974278927,-0.00024874790688045323,-0.0002476929221302271,-0.00024638487957417965,-0.00024570644018240273,-0.0002454315253999084,-0.00024538158322684467,-0.00024272740120068192,-0.0002424488920951262,-0.00024212762946262956,-0.00024076491536106914,-0.00023731037799734622,-0.0002359902864554897,-0.00022858097509015352,-0.00022557482589036226,-0.00022417963191401213,-0.0002221895701950416,-0.0002188040380133316,-0.00021862030553165823,-0.0002186174679081887,-0.00021387085143942386,-0.0002127292682416737,-0.00020846635743509978,-0.00020696863066405058,-0.00020590708300005645,-0.00020501136896200478,-0.0001977339416043833,-0.00019304633315186948,-0.0001930037105921656,-0.00019055500160902739,-0.00018724665278568864,-0.00018636166350916028,-0.0001837645540945232,-0.00018337488290853798,-0.00018309906590729952,-0.0001818022137740627,-0.00018008769256994128,-0.00017971679335460067,-0.00017900019884109497,-0.00017410516738891602,-0.0001691642391961068,-0.0001689042110228911,-0.00016468182730022818,-0.00016396239516325295,-0.00016118318308144808,-0.0001603145938133821,-0.00015716507914476097,-0.00015607595560140908,-0.0001554544287500903,-0.00015363558486569673,-0.0001511360751464963,-0.00015034734678920358,-0.00014707937953062356,-0.00014599844871554524,-0.00014172062219586223,-0.00014015585475135595,-0.00013547584239859134,-0.00013324469910003245,-0.0001323145697824657,-0.0001315693516517058,-0.0001288391649723053,-0.0001286499173147604,-0.00012794673966709524,-0.00011905729479622096,-0.00011638864816632122,-0.00011324197112116963,-0.00011301413178443909,-0.00010859251051442698,-0.00010583818220766261,-0.0001031532883644104,-0.00010266706522088498,-0.00010180175013374537,-0.00010179057426284999,-9.92839050013572e-05,-9.87252569757402e-05,-9.824558947002515e-05,-9.635672176955268e-05,-9.146660886472091e-05,-9.047120693139732e-05,-8.942186832427979e-05,-8.603229798609391e-05,-8.5994899563957e-05,-8.516430534655228e-05,-7.656768138986081e-05,-7.62219715397805e-05,-7.601782999699935e-05,-7.372036634478718e-05,-7.173240010160953e-05,-6.791278428863734e-05,-6.490379746537656e-05,-6.397620018105954e-05,-6.35747637716122e-05,-6.158828909974545e-05,-5.8827848988585174e-05,-5.112662984174676e-05,-4.8548430640948936e-05,-4.6474338887492195e-05,-4.6067983930697665e-05,-4.52850763394963e-05,-4.515722321229987e-05,-4.465907841222361e-05,-4.398286182549782e-05,-4.038944825879298e-05,-3.7950576370349154e-05,-3.782436397159472e-05,-3.756254955078475e-05,-3.6976336559746414e-05,-3.485411434667185e-05,-3.4818054700735956e-05,-3.1922609196044505e-05,-3.084257332375273e-05,-2.919524922617711e-05,-2.885699359467253e-05,-2.8025955543853343e-05,-2.7032643629354425e-05,-2.5816709239734337e-05,-2.4831295377225615e-05,-2.341657818760723e-05,-2.2158175852382556e-05,-2.1072775780339725e-05,-1.9315331883262843e-05,-1.834407521528192e-05,-1.6852021872182377e-05,-1.6379059161408804e-05,-1.4442503015743569e-05,-1.4396309779840522e-05,-1.4199614270182792e-05,-1.340165727015119e-05,-1.1196583727723919e-05,-1.0917633517237846e-05,-7.459372227458516e-06,-7.29486328054918e-06,-6.871521691209637e-06,-6.364583896356635e-06,-4.961043487128336e-06,-4.800558144779643e-06,-3.5378336633584695e-06,-2.4011731056816643e-06,-2.2403896764444653e-06,-2.0329653125372715e-06,-1.0901690075115766e-06,-2.090632875706433e-07,9.53674295089968e-09,4.214048487938271e-07,8.422136374974798e-07,1.3661384627994266e-06,1.8289684931005468e-06,2.0587444851116743e-06,3.779381586355157e-06,4.307031758798985e-06,6.592124918824993e-06,7.803439984854776e-06,9.364187462779228e-06,9.975433385989163e-06,1.0757446034403984e-05,1.1322945283609442e-05,1.2085586604371201e-05,1.3072491128696129e-05,1.376420277665602e-05,1.387357679050183e-05,1.420483022229746e-05,1.4207661479304079e-05,1.6148685972439125e-05,1.644104668230284e-05,1.813516064430587e-05,1.8174052456743084e-05,2.007096918532625e-05,2.0198523998260498e-05,2.15503569052089e-05,2.2889078536536545e-05,2.359181598876603e-05,2.3721457182546146e-05,2.960801066365093e-05,3.0109285944490694e-05,3.098323941230774e-05,3.417879270273261e-05,3.6167650250718e-05,3.631725849118084e-05,3.658801142591983e-05,3.786235902225599e-05,3.829851630143821e-05,3.833025766653009e-05,4.2606890929164365e-05,4.270121280569583e-05,4.386037471704185e-05,4.412710768519901e-05,4.586473005474545e-05,4.625230940291658e-05,5.033761408412829e-05,5.053520362707786e-05,5.248218803899363e-05,5.422905087471008e-05,5.833774775965139e-05,6.09819580859039e-05,6.385266897268593e-05,6.52818416710943e-05,6.567075615748763e-05,6.668195419479162e-05,6.717353971907869e-05,6.74268594593741e-05,6.810501508880407e-05,6.847292388556525e-05,6.88241416355595e-05,7.07924336893484e-05,7.47168087400496e-05,7.614716741954908e-05,7.684915908612311e-05,7.783561886753887e-05,7.917106267996132e-05,8.077502570813522e-05,8.398533100262284e-05,8.604124013800174e-05,9.040847362484783e-05,9.097188740270212e-05,9.127378143602982e-05,9.314819908468053e-05,9.347632294520736e-05,9.69009124673903e-05,9.733990009408444e-05,9.793773642741144e-05,0.00010045796807389706,0.00010474711598362774,0.00010537877824390307,0.00010578215005807579,0.0001084092291421257,0.00010885387746384367,0.0001126289353123866,0.00011473507038317621,0.00011675998393911868,0.00011831075244117528,0.00012278780923224986,0.00012611330021172762,0.00012937665451318026,0.00013106003461871296,0.00013112723536323756,0.0001319247530773282,0.00013637542724609375,0.00013662576384376734,0.00013819083687849343,0.0001385635114274919,0.00014068916789256036,0.00014182105951476842,0.0001423052017344162,0.00014480501704383641,0.00014743894280400127,0.00015042870654724538,0.00015329450252465904,0.00015830114716663957,0.00016021967167034745,0.00016808421059977263,0.00016853630950208753,0.00016861155745573342,0.00017586186004336923,0.00018149807874578983,0.000187744342838414,0.0001883658696897328,0.00018848404579330236,0.00019088566477876157,0.00019319594139233232,0.00019470721599645913,0.00019627735309768468,0.0001971766323549673,0.00019873723795171827,0.00019884706125594676,0.0001999711967073381,0.00020124077855143696,0.0002020962565438822,0.00020286679500713944,0.00020469381706789136,0.00020620733266696334,0.00020676464191637933,0.00020699128799606115,0.00020774826407432556,0.0002078150282613933,0.00020835355098824948,0.00020911828323733062,0.00021084173931740224,0.0002138312120223418,0.00021526530326809734,0.00021743401885032654,0.00022397398424800485,0.00022421151516027749,0.00022527128749061376,0.00022725075541529804,0.00022842451289761811,0.00022974908642936498,0.00023334160505328327,0.00024121195019688457,0.00024328425934072584,0.00024375990324188024,0.0002491211926098913,0.000250032840995118,0.00025134682073257864,0.00025284706498496234,0.0002549995551817119,0.00025765865575522184,0.00026470079319551587,0.00026654647081159055,0.0002678458404261619,0.00027930468786507845,0.0002864466514438391,0.00028976501198485494,0.00029292554245330393,0.00029603272560052574,0.0002995236136484891,0.0002997228584717959,0.0002999626158270985,0.0003038799623027444,0.0003074222768191248,0.0003087489167228341,0.0003184959350619465,0.00032162709976546466,0.00032307193032465875,0.00032793209538795054,0.0003286613500677049,0.00033003941643983126,0.00033361464738845825,0.000340415834216401,0.00034829601645469666,0.0003527668013703078,0.0003535710275173187,0.00036517216358333826,0.0003705020353663713,0.0003710490418598056,0.0003720113600138575,0.00037473722477443516,0.0003756274236366153,0.00037790878559462726,0.00037960574263706803,0.00038384823710657656,0.00038935631164349616,0.0003897590795531869,0.0003902821335941553,0.00039071738137863576,0.00039487273897975683,0.0003993170103058219,0.0004020740161649883,0.0004058995982632041,0.00040638743666931987,0.0004076006880495697,0.0004082416126038879,0.00041101023089140654,0.0004124276456423104,0.0004136209317948669,0.0004164965357631445,0.00041970997699536383,0.00042589710210449994,0.00042882858542725444,0.0004307381750550121,0.0004318776773288846,0.00043258577352389693,0.00044055760372430086,0.00044132559560239315,0.00044288189383223653,0.00044295101542957127,0.0004503455711528659,0.0004555150808300823,0.0004574610211420804,0.00045862511615268886,0.0004718525742646307,0.000472319865366444,0.00047841190826147795,0.00047970429295673966,0.0004948072019033134,0.0005070684710517526,0.0005090028280392289,0.0005148000782355666,0.0005152660887688398,0.000527863041497767,0.0005300657358020544,0.0005301311612129211,0.0005376397166401148,0.0005408709985204041,0.0005435626371763647,0.0005557640106417239,0.0005588374915532768,0.000559308216907084,0.0005596342962235212,0.0005608283099718392,0.0005706404335796833,0.0005749908159486949,0.0005777113256044686,0.0005882201949134469,0.00059408915694803,0.0006051752134226263,0.0006079655722714961,0.0006090798997320235,0.0006164131918922067,0.0006208608974702656,0.0006261107046157122,0.0006284001283347607,0.0006311830948106945,0.0006342327687889338,0.0006398728583008051,0.0006428414490073919,0.0006441862788051367,0.0006847076001577079,0.0006882819579914212,0.0006924945046193898,0.0006933046970516443,0.0006944282213225961,0.0006960439495742321,0.0007002699421718717,0.0007087801350280643,0.0007097197812981904,0.0007101481896825135,0.000716034322977066,0.0007442556088790298,0.0007446396630257368,0.0007469683769159019,0.0007490976131521165,0.0007689617341384292,0.0007708652410656214,0.0007805466884747148,0.0007862299680709839,0.0007894744048826396,0.0007906702230684459,0.0007920770440250635,0.0007954576867632568,0.0008128082845360041,0.0008273156126961112,0.0008312797290273011,0.0008322675712406635,0.0008326774695888162,0.0008414565236307681,0.0008473183261230588,0.0008476290386170149,0.0008481526165269315,0.0008499297546222806,0.0008504425059072673,0.0008532290230505168,0.0008600421133451164,0.0008633187389932573,0.000863956636749208,0.000864252622704953,0.0008682043990120292,0.0008910347241908312,0.0008944690343923867,0.000896537909284234,0.0008974935044534504,0.0009021158330142498,0.0009054414695128798,0.0009076542919501662,0.0009096716530621052,0.0009113599080592394,0.0009227190748788416,0.0009290631278418005,0.0009413170628249645,0.0009433415252715349,0.0009451521909795702,0.0009635809110477567,0.0009650880238041282,0.0009695729822851717,0.0009756405488587916,0.0009797331877052784,0.00098208908457309,0.0009849455673247576,0.0009851986542344093,0.0009971816325560212,0.001004914054647088,0.0010428997920826077,0.001046623452566564,0.0010496570030227304,0.0010704564629122615,0.0010717781260609627,0.0010855401633307338,0.0010871206177398562,0.0010926109971478581,0.001103908522054553,0.0011046278523281217,0.001107333111576736,0.0011184002505615354,0.0011360683711245656,0.001139024505391717,0.0011459417873993516,0.0011476698564365506,0.0011508676689118147,0.0011565051972866058,0.0011599153513088822,0.00116331921890378,0.0011677442817017436,0.0011677780421450734,0.001171350129880011,0.0011721696937456727,0.0011746924137696624,0.0012044570175930858,0.0012076363200321794,0.0012169204419478774,0.0012210604036226869,0.0012230828870087862,0.001236950047314167,0.0012376290978863835,0.0012428376358002424,0.0012473997194319963,0.001253597903996706,0.0012567371595650911,0.0012636329047381878,0.0012674742611125112,0.0012679861392825842,0.0012708381982520223,0.0012754453346133232,0.0012847835896536708,0.0012856931425631046,0.0012934888945892453,0.001306665362790227,0.0013155089691281319,0.0013367817737162113,0.001354181207716465,0.0013703794684261084,0.0013743287418037653,0.0013756788102909923,0.0013774980325251818,0.001378726097755134,0.0013824972556903958,0.001395442639477551,0.0013999453512951732,0.001410310622304678,0.0014136485988274217,0.0014154756208881736,0.0014199013821780682,0.001429581898264587,0.0014306787634268403,0.0014406575355678797,0.001441184082068503,0.0014534721849486232,0.0014640851877629757,0.0014825824182480574,0.0014977551763877273,0.001501430757343769,0.0015248529380187392,0.0015361675759777427,0.0015363326529040933,0.0015418987022712827,0.0015638842014595866,0.0015660709468647838,0.0015671714209020138,0.0015765853459015489,0.0015771581092849374,0.001581590506248176,0.0015871630748733878,0.0016021054470911622,0.0016042748466134071,0.0016178112709894776,0.0016183204716071486,0.0016229754546657205,0.0016365733463317156,0.0016401137690991163,0.0016405306523665786,0.0016432530246675014,0.0016528010601177812,0.001655904925428331,0.0016570419538766146,0.0016637352528050542,0.0016673202626407146,0.0016758613055571914,0.0016762004233896732,0.0017022379906848073,0.0017053009942173958,0.001715158112347126,0.0017198242712765932,0.0017306015361100435,0.0017392303561791778,0.0017533173086121678,0.0017539752880111337,0.0017565034795552492,0.0017608945490792394,0.0017797840991988778,0.0017868832219392061,0.001809965819120407,0.0018138695741072297,0.001825515879318118,0.0018260537181049585,0.0018311907770112157,0.0018502563470974565,0.0018508441280573606,0.0018754644552245736,0.001878295443020761,0.0018813501810654998,0.0018837526440620422,0.00188953906763345,0.0019049597904086113,0.001915750908665359,0.0019462122581899166,0.0019470625557005405,0.0019510951824486256,0.0019662287086248398,0.001971208257600665,0.001988950651139021,0.0020028806757181883,0.002007113303989172,0.002012254437431693,0.0020155382808297873,0.0020217366982251406,0.0020231378730386496,0.002030265750363469,0.0020342045463621616,0.002048269845545292,0.002058704849332571,0.0020604052115231752,0.0020610997453331947,0.002070473041385412,0.0020806314423680305,0.002087712986394763,0.002096437616273761,0.0020998758263885975,0.0021016558166593313,0.0021147560328245163,0.0021229544654488564,0.0021297894418239594,0.0021696698386222124,0.0021774042397737503,0.0021812424529343843,0.002200648421421647,0.002204115968197584,0.0022199552040547132,0.002231148537248373,0.0022624731063842773,0.0022629520390182734,0.0022668943274766207,0.0022672016639262438,0.002270192140713334,0.002272057579830289,0.0022759386338293552,0.0022905291989445686,0.0023152227513492107,0.002321418607607484,0.002343621803447604,0.002357856137678027,0.0023601395078003407,0.002374454401433468,0.0023811301216483116,0.0024095915723592043,0.00241155456751585,0.0024323936086148024,0.0024330986198037863,0.0024342418182641268,0.0024363736156374216,0.002443097298964858,0.0024772225879132748,0.002498253248631954,0.0024996844585984945,0.0025004534982144833,0.002501086099073291,0.0025019950699061155,0.0025025522336363792,0.0025084512308239937,0.002549521392211318,0.002554331673309207,0.0025578374043107033,0.002558455802500248,0.00256728520616889,0.0025694752112030983,0.0025755208916962147,0.002592637436464429,0.0026079309172928333,0.0026103269774466753,0.0026181049179285765,0.002632756717503071,0.0026381337083876133,0.0026390182320028543,0.0026893543545156717,0.0027164549101144075,0.002720156917348504,0.0027247921098023653,0.002737985225394368,0.0027475168462842703,0.0027528125792741776,0.0027618086896836758,0.002771862084046006,0.0027784116100519896,0.002778900321573019,0.0028041673358529806,0.0028224277775734663,0.0028259591199457645,0.002828825730830431,0.002839562948793173,0.002844502916559577,0.0028632772155106068,0.0028720777481794357,0.0028820913285017014,0.002883926033973694,0.0028938036412000656,0.002900254214182496,0.0029112615156918764,0.0029189158231019974,0.002928741043433547,0.002983847865834832,0.0029955036006867886,0.002995683578774333,0.0030539953149855137,0.003065689466893673,0.003066239645704627,0.0030764061957597733,0.003094755345955491,0.003126146737486124,0.0031538724433630705,0.0031598813366144896,0.0031607700511813164,0.0031974625308066607,0.0032623750157654285,0.0032680409494787455,0.003295689122751355,0.0032967785373330116,0.0033026086166501045,0.003305742982774973,0.003321238560602069,0.0033798986114561558,0.0033873366191983223,0.003391633043065667,0.0034172695595771074,0.0034435344859957695,0.003512364812195301,0.0035360553301870823,0.003539934055879712,0.0035891635343432426,0.003608708968386054,0.0036198454909026623,0.003663318231701851,0.003667059587314725,0.0036761879455298185,0.0036767988931387663,0.003690021112561226,0.0037111076526343822,0.003732322948053479,0.0037360715214163065,0.0037510162219405174,0.003767319256439805,0.0037690894678235054,0.003775965655222535,0.003796340199187398,0.003805415239185095,0.003827858017757535,0.003835726296529174,0.003896842012181878,0.003968326840549707,0.003988862503319979,0.00399486068636179,0.0040200864896178246,0.004046446178108454,0.004054518416523933,0.004069854039698839,0.004130551591515541,0.00415149936452508,0.004163829609751701,0.004218199755996466,0.004228674340993166,0.0042405324056744576,0.00426810048520565,0.004274510312825441,0.004322925116866827,0.004344253335148096,0.004368983209133148,0.004372172988951206,0.004379493184387684,0.004415047820657492,0.0044891731813549995,0.004496240522712469,0.004498607479035854,0.0045722355134785175,0.0046528964303433895,0.0047216168604791164,0.004742596764117479,0.00474363099783659,0.0047593628987669945,0.004774957895278931,0.0048149446956813335,0.0048181745223701,0.0048197973519563675,0.004831172060221434,0.004861212335526943,0.004870743956416845,0.00492489617317915,0.004943576641380787,0.004949058871716261,0.004989281762391329,0.005018311087042093,0.0050579700618982315,0.00506233936175704,0.005077484995126724,0.005116896238178015,0.005131985526531935,0.005138823296874762,0.005178668536245823,0.005218409467488527,0.005224261432886124,0.005261480342596769,0.005279654636979103,0.005304553546011448,0.005305694416165352,0.00534904794767499,0.005368627142161131,0.005390734877437353,0.0054553295485675335,0.005472899880260229,0.0054967389442026615,0.005507082212716341,0.005566426087170839,0.005637196358293295,0.005667678080499172,0.005726224277168512,0.005823805462568998,0.005826975684612989,0.005853296723216772,0.0058884755708277225,0.005898602772504091,0.005915054120123386,0.00592045858502388,0.005942962598055601,0.0059549687430262566,0.0059846616350114346,0.0060544987209141254,0.006094872951507568,0.006163675803691149,0.006195500493049622,0.006197760812938213,0.006205801852047443,0.006215651053935289,0.006226543337106705,0.00625157356262207,0.006309864576905966,0.006322115659713745,0.006329547613859177,0.006333814933896065,0.006354204379022121,0.006575033068656921,0.006576050538569689,0.00658037792891264,0.0065850079990923405,0.006627932190895081,0.006656348705291748,0.006657833233475685,0.006658900994807482,0.006659486796706915,0.006679347716271877,0.006685793399810791,0.006731174420565367,0.006734306924045086,0.006742628291249275,0.006767945364117622,0.006798410322517157,0.006893971934914589,0.0069134128279984,0.0069166747853159904,0.006921726278960705,0.006961699575185776,0.006987177301198244,0.007017024327069521,0.007030332460999489,0.007038669195026159,0.0070478832349181175,0.007070877589285374,0.007099348120391369,0.0071345907635986805,0.007147533819079399,0.007202937733381987,0.007246843539178371,0.007270763628184795,0.007340981159359217,0.007372674997895956,0.007483704015612602,0.007589810993522406,0.007669500075280666,0.007687882054597139,0.007730531040579081,0.007760114502161741,0.007767891976982355,0.0078035201877355576,0.007818999700248241,0.007852721959352493,0.00789431482553482,0.00789689738303423,0.007898267358541489,0.007907025516033173,0.00793710257858038,0.007957938127219677,0.007963285781443119,0.007966245524585247,0.008068456314504147,0.008106057532131672,0.008107174187898636,0.008117660880088806,0.008177588693797588,0.00818108394742012,0.008195758797228336,0.008213055320084095,0.008258294314146042,0.00836222991347313,0.0083707794547081,0.008407706394791603,0.00841966737061739,0.008485358208417892,0.008497144095599651,0.008505343459546566,0.008516710251569748,0.008569239638745785,0.008693020790815353,0.008798588067293167,0.008962958119809628,0.008987278677523136,0.00901447981595993,0.009022869169712067,0.009028409607708454,0.009187460877001286,0.009264842607080936,0.009306305088102818,0.009363812394440174,0.009433445520699024,0.009595418348908424,0.009604199789464474,0.00962921418249607,0.009684603661298752,0.009766577742993832,0.009771237149834633,0.009878605604171753,0.00988735817372799,0.009920865297317505,0.009926791302859783,0.009933676570653915,0.009968371130526066,0.01011411752551794,0.010179859586060047,0.010188068263232708,0.010216537863016129,0.01023041270673275,0.01024468895047903,0.010292858816683292,0.01031024381518364,0.010337207466363907,0.010341479443013668,0.010347550734877586,0.010532965883612633,0.010574886575341225,0.010600385256111622,0.010744839906692505,0.01082198042422533,0.010845904238522053,0.010846150107681751,0.010898340493440628,0.010903648100793362,0.010906238108873367,0.010987685061991215,0.011038873344659805,0.011089649051427841,0.011104150675237179,0.011165862902998924,0.011263600550591946,0.01130745280534029,0.0114882942289114,0.011489992029964924,0.011500036343932152,0.011501908302307129,0.011525378562510014,0.011570790782570839,0.011790814809501171,0.011833805590867996,0.011850100941956043,0.011856676079332829,0.011917391791939735,0.01191953755915165,0.012011878192424774,0.012086846865713596,0.012167461216449738,0.01220640167593956,0.012231244705617428,0.012560168281197548,0.01267693005502224,0.012821724638342857,0.012833543121814728,0.012839755043387413,0.01287278812378645,0.012941384688019753,0.013007099740207195,0.013024645857512951,0.013077928684651852,0.013110406696796417,0.013175548054277897,0.01321880891919136,0.013243939727544785,0.013255205936729908,0.013302674517035484,0.013326823711395264,0.01345094945281744,0.013462568633258343,0.013492576777935028,0.013600430451333523,0.013756555505096912,0.013865470886230469,0.013885480351746082,0.014240601100027561,0.014446772634983063,0.014755751006305218,0.014921841211616993,0.01500216405838728,0.015030499547719955,0.015071552246809006,0.015158253721892834,0.015303723514080048,0.015402312390506268,0.01547892577946186,0.015525780618190765,0.015713870525360107,0.016046661883592606,0.016076982021331787,0.01610587164759636,0.01618638075888157,0.0162984486669302,0.016413170844316483,0.016429901123046875,0.016691284254193306,0.016957923769950867,0.017684346064925194,0.017813019454479218,0.017957322299480438,0.018406614661216736,0.018411055207252502,0.018549518659710884,0.018583426252007484,0.018662728369235992,0.0186750665307045,0.019088899716734886,0.019246598705649376,0.01929287239909172,0.019303886219859123,0.01931191235780716,0.019351443275809288,0.019693056121468544,0.019845401868224144,0.01987801305949688,0.020064977928996086,0.020177705213427544,0.02052501030266285,0.02057492919266224,0.02064892277121544,0.020681099966168404,0.020737629383802414,0.021077122539281845,0.02110319770872593,0.021123962476849556,0.021191179752349854,0.021262312307953835,0.021765511482954025,0.02177366428077221,0.022118404507637024,0.022699246183037758,0.023012392222881317,0.023258907720446587,0.023543663322925568,0.02360432595014572,0.023753583431243896,0.023787731304764748,0.023792430758476257,0.023807186633348465,0.023810971528291702,0.023930197581648827,0.024005986750125885,0.02401319518685341,0.024337710812687874,0.02434469945728779,0.02435482107102871,0.024742238223552704,0.025166289880871773,0.025269968435168266,0.025765683501958847,0.025801129639148712,0.025961142033338547,0.026173770427703857,0.026349198073148727,0.026523267850279808,0.026712412014603615,0.026837298646569252,0.027062801644206047,0.0271753016859293,0.0275480467826128,0.027805591002106667,0.027915023267269135,0.028290623798966408,0.028453491628170013,0.02859278954565525,0.028799541294574738,0.029023220762610435,0.029034288600087166,0.02966996468603611,0.02993103675544262,0.03027299791574478,0.030358942225575447,0.031221644952893257,0.03152081370353699,0.03157920762896538,0.03169146552681923,0.032798849046230316,0.033312469720840454,0.03346164524555206,0.033545516431331635,0.033782124519348145,0.03427308425307274,0.03474542498588562,0.03481180593371391,0.03522508591413498,0.036037199199199677,0.03605131804943085,0.03614361584186554,0.036873187869787216,0.03746384009718895,0.03761880472302437,0.03800918534398079,0.03869731351733208,0.040158793330192566,0.04052244499325752,0.04134383797645569,0.04161272197961807,0.04178952798247337,0.04256181791424751,0.04256822541356087,0.04289652407169342,0.04308193176984787,0.04352830722928047,0.04391433671116829,0.04444187507033348,0.04507899284362793,0.04553744196891785,0.04596684128046036,0.046502113342285156,0.047021493315696716,0.04741837456822395,0.04771098494529724,0.04782387614250183,0.04833180457353592,0.04881439357995987,0.05041002109646797,0.05045398697257042,0.05135195329785347,0.05186130478978157,0.052628327161073685,0.05296141281723976,0.05459490418434143,0.05714021623134613,0.05745215341448784,0.05884841829538345,0.059260301291942596,0.05945352092385292,0.05945912376046181,0.06029261276125908,0.06170274689793587,0.06514265388250351,0.06624427437782288,0.06655309349298477,0.06734179705381393,0.0676211342215538,0.0684695914387703,0.06912961602210999,0.07139246165752411,0.07281796634197235,0.07343385368585587,0.07369670271873474,0.07510344684123993,0.0764039158821106,0.07753913104534149,0.07786780595779419,0.07928187400102615,0.08030975610017776,0.08258280158042908,0.08411238342523575,0.0863582044839859,0.08708435297012329,0.08876854181289673,0.0915321335196495,0.09154899418354034,0.09623252600431442,0.09899803251028061,0.10040155053138733,0.10751891881227493,0.10846693068742752,0.11212414503097534,0.11227703094482422,0.11608804017305374,0.12018456310033798,0.12393809109926224,0.1269037276506424,0.1317369043827057,0.1436857134103775,0.14501258730888367,0.1485622525215149,0.1662718653678894,0.16976921260356903,0.17244523763656616,0.17949314415454865,0.21061863005161285,0.21094514429569244,0.22563445568084717],\"yaxis\":\"y\",\"type\":\"scattergl\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Sorted neurons\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Loss change\"}},\"legend\":{\"title\":{\"text\":\"variable\"},\"tracegroupgap\":0},\"title\":{\"text\":\"Loss change from ablating MLP5 neuron\"},\"width\":800,\"showlegend\":true},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('bb840bff-3ff5-44c1-bab4-cc151887bab6');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sorted_means, indices = torch.sort(diffs.mean(1))\n",
    "sorted_means = sorted_means.tolist()\n",
    "haystack_utils.line(sorted_means, xlabel=\"Sorted neurons\", ylabel=\"Loss change\", title=\"Loss change from ablating MLP5 neuron\") # xticks=indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>                            <div id=\"df67543a-07d5-4a9d-9063-8ce3d5f05289\" class=\"plotly-graph-div\" style=\"height:525px; width:1000px;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"df67543a-07d5-4a9d-9063-8ce3d5f05289\")) {                    Plotly.newPlot(                        \"df67543a-07d5-4a9d-9063-8ce3d5f05289\",                        [{\"error_y\":{\"array\":[1.30427956780808],\"type\":\"data\",\"visible\":true},\"name\":\"Original\",\"x\":[\"Original\"],\"y\":[1.5710028438037262],\"type\":\"bar\"},{\"error_y\":{\"array\":[1.948098644842592],\"type\":\"data\",\"visible\":true},\"name\":\"Ablated\",\"x\":[\"Ablated\"],\"y\":[3.7324311143159865],\"type\":\"bar\"},{\"error_y\":{\"array\":[1.294354692921908],\"type\":\"data\",\"visible\":true},\"name\":\"MLP5 path patched\",\"x\":[\"MLP5 path patched\"],\"y\":[2.187495462447405],\"type\":\"bar\"},{\"error_y\":{\"array\":[1.754108923041463],\"type\":\"data\",\"visible\":true},\"name\":\"MLP5 path patched + Top 10 MLP5 neurons ablated\",\"x\":[\"Top MLP5 removed\"],\"y\":[4.198886030316353],\"type\":\"bar\"},{\"error_y\":{\"array\":[0.5961360344239801],\"type\":\"data\",\"visible\":true},\"name\":\"MLP5 path patched + Bottom 10 MLP5 neurons ablated\",\"x\":[\"Bottom MLP5 removed\"],\"y\":[0.5500866486504674],\"type\":\"bar\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"title\":{\"text\":\"Average last token loss when removing top \\u002f bottom neurons from path patching\"},\"xaxis\":{\"title\":{\"text\":\"\"}},\"yaxis\":{\"title\":{\"text\":\"Loss\"}},\"barmode\":\"group\",\"width\":1000,\"showlegend\":true},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('df67543a-07d5-4a9d-9063-8ce3d5f05289');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check loss change when ablating top / bottom neurons\n",
    "top_neurons_count = 10\n",
    "top_neurons = indices[-top_neurons_count:]\n",
    "bottom_neurons = indices[:top_neurons_count]\n",
    "\n",
    "with model.hooks(deactivate_neurons_fwd_hooks):\n",
    "    ablated_loss, ablated_cache = model.run_with_cache(prompts, return_type=\"loss\")\n",
    "\n",
    "ablate_top_neurons_hook = get_ablate_neurons_hook(top_neurons, ablated_cache)\n",
    "ablate_bottom_neurons_hook = get_ablate_neurons_hook(bottom_neurons, ablated_cache)\n",
    "\n",
    "original_loss, ablated_loss, _, all_MLP5_loss = haystack_utils.get_direct_effect(prompts, model, pos=-1, context_ablation_hooks=deactivate_neurons_fwd_hooks, context_activation_hooks=activate_neurons_fwd_hooks)\n",
    "_, _, _, top_MLP5_ablated_loss = haystack_utils.get_direct_effect(prompts, model, pos=-1, context_ablation_hooks=deactivate_neurons_fwd_hooks, context_activation_hooks=activate_neurons_fwd_hooks+ablate_top_neurons_hook)\n",
    "_, _, _, bottom_MLP5_ablated_loss = haystack_utils.get_direct_effect(prompts, model, pos=-1, context_ablation_hooks=deactivate_neurons_fwd_hooks, context_activation_hooks=activate_neurons_fwd_hooks+ablate_bottom_neurons_hook)\n",
    "\n",
    "names = [\"Original\", \"Ablated\", \"MLP5 path patched\", f\"MLP5 path patched + Top {top_neurons_count} MLP5 neurons ablated\", f\"MLP5 path patched + Bottom {top_neurons_count} MLP5 neurons ablated\"]\n",
    "short_names = [\"Original\", \"Ablated\", \"MLP5 path patched\", f\"Top MLP5 removed\", f\"Bottom MLP5 removed\"]\n",
    "\n",
    "values = [original_loss.tolist(), ablated_loss.tolist(), all_MLP5_loss.tolist(), top_MLP5_ablated_loss.tolist(), bottom_MLP5_ablated_loss.tolist()]\n",
    "haystack_utils.plot_barplot(values, names, short_names=short_names, ylabel=\"Loss\", title=f\"Average last token loss when removing top / bottom neurons from path patching\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigate top / bottom neuron boosts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Patch in MLP5 neurons, ablated bottom neurons, compare with patched MLP5 neurons without ablating bottom neurons\n",
    "\n",
    "with model.hooks(deactivate_neurons_fwd_hooks):\n",
    "    ablated_logits, ablated_cache = model.run_with_cache(prompts)\n",
    "\n",
    "ablate_top_neurons_hook = get_ablate_neurons_hook(top_neurons, ablated_cache)\n",
    "ablate_bottom_neurons_hook = get_ablate_neurons_hook(bottom_neurons, ablated_cache)\n",
    "\n",
    "original_logits, ablated_logprobs, _, all_MLP5_logprobs = haystack_utils.get_direct_effect(prompts, model, pos=-2, context_ablation_hooks=deactivate_neurons_fwd_hooks, context_activation_hooks=activate_neurons_fwd_hooks, return_type='logprobs')\n",
    "_, _, _, top_MLP5_ablated_logprobs = haystack_utils.get_direct_effect(prompts, model, pos=-2, context_ablation_hooks=deactivate_neurons_fwd_hooks, context_activation_hooks=activate_neurons_fwd_hooks+ablate_top_neurons_hook, return_type='logprobs')\n",
    "_, _, _, bottom_MLP5_ablated_logprobs = haystack_utils.get_direct_effect(prompts, model, pos=-2, context_ablation_hooks=deactivate_neurons_fwd_hooks, context_activation_hooks=activate_neurons_fwd_hooks+ablate_bottom_neurons_hook, return_type='logprobs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_difference_neurons(baseline_logprobs, ablated_logprops, positive=True, logprob_threshold=-7, k=50):\n",
    "    neuron_logprob_difference = (baseline_logprobs - ablated_logprops).mean(0)\n",
    "    neuron_logprob_difference[baseline_logprobs.mean(0) < logprob_threshold] = 0\n",
    "    if positive:\n",
    "        non_zero_count = (neuron_logprob_difference > 0).sum()\n",
    "    else:\n",
    "        non_zero_count = (neuron_logprob_difference < 0).sum()\n",
    "    top_logprob_difference, top_neurons = haystack_utils.top_k_with_exclude(neuron_logprob_difference, min(non_zero_count, k), all_ignore, largest=positive)\n",
    "    return top_logprob_difference, top_neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boosted tokens\n",
    "bottom_neuron_pos_difference_logprobs, bottom_pos_indices = get_top_difference_neurons(all_MLP5_logprobs, bottom_MLP5_ablated_logprobs, positive=True)\n",
    "top_neuron_pos_difference_logprobs, top_pos_indices = get_top_difference_neurons(all_MLP5_logprobs, top_MLP5_ablated_logprobs, positive=True)\n",
    "# Deboosted tokens\n",
    "bottom_neuron_neg_difference_logprobs, bottom_neg_indices = get_top_difference_neurons(all_MLP5_logprobs, bottom_MLP5_ablated_logprobs, positive=False)\n",
    "top_neuron_neg_difference_logprobs, top_neg_indices = get_top_difference_neurons(all_MLP5_logprobs, top_MLP5_ablated_logprobs, positive=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63a9ab8010334b6a80ecf4c748c49bf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='Tokens:', options=('Boosted', 'Deboosted'), value='Boosted'), Drop…"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def plot_data(boosted_deboosted, top_bottom):\n",
    "    if boosted_deboosted == 'Boosted' and top_bottom == 'Top':\n",
    "        logprobs = top_neuron_pos_difference_logprobs\n",
    "        indices = top_pos_indices\n",
    "        title_change = 'increase'\n",
    "    elif boosted_deboosted == 'Boosted' and top_bottom == 'Bottom':\n",
    "        logprobs = bottom_neuron_pos_difference_logprobs\n",
    "        indices = bottom_pos_indices\n",
    "        title_change = 'increase'\n",
    "    elif boosted_deboosted == 'Deboosted' and top_bottom == 'Top':\n",
    "        logprobs = top_neuron_neg_difference_logprobs\n",
    "        indices = top_neg_indices\n",
    "        title_change = 'decrease'\n",
    "    else:  # 'Deboosted' and 'Bottom'\n",
    "        logprobs = bottom_neuron_neg_difference_logprobs\n",
    "        indices = bottom_neg_indices\n",
    "        title_change = 'decrease'\n",
    "\n",
    "    xlabel = boosted_deboosted + \" tokens\"\n",
    "    ylabel = \"full_logprob - ablated_logprop\"\n",
    "    title = 'Logprob ' + title_change + ' from ' + top_bottom.lower() + ' neurons'\n",
    "    xticks = [model.to_str_tokens([i])[0] for i in indices]\n",
    "\n",
    "    haystack_utils.line(logprobs.cpu().numpy(), xlabel=xlabel, ylabel=ylabel, title=title, xticks=xticks, show_legend=False)\n",
    "\n",
    "\n",
    "boosted_deboosted = widgets.Dropdown(options=['Boosted', 'Deboosted'], value='Boosted', description='Tokens:')\n",
    "top_bottom = widgets.Dropdown(options=['Top', 'Bottom'], value='Top', description='Neurons:')\n",
    "widgets.interactive(plot_data, boosted_deboosted=boosted_deboosted, top_bottom=top_bottom)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decompose neuron effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get logprob change for individual neuron\n",
    "\n",
    "def get_individual_neuron_logprob_effect(sorted_indices, neuron_pos=0, top=True, positive=True, plot=True):\n",
    "    if top:\n",
    "        neurons = sorted_indices[-(neuron_pos+1)]\n",
    "    else:\n",
    "        neurons = sorted_indices[neuron_pos]\n",
    "    neurons_hook = get_ablate_neurons_hook([neurons], ablated_cache)\n",
    "\n",
    "    original_logits, ablated_logprobs, _, all_MLP5_logprobs = haystack_utils.get_direct_effect(prompts, model, pos=-2, context_ablation_hooks=deactivate_neurons_fwd_hooks, context_activation_hooks=activate_neurons_fwd_hooks, return_type='logprobs')\n",
    "    _, _, _, neuron_ablated_logprobs = haystack_utils.get_direct_effect(prompts, model, pos=-2, context_ablation_hooks=deactivate_neurons_fwd_hooks, context_activation_hooks=activate_neurons_fwd_hooks+neurons_hook, return_type='logprobs')\n",
    "    neuron_difference_logprobs, difference_indices = \\\n",
    "        get_top_difference_neurons(all_MLP5_logprobs, neuron_ablated_logprobs, positive=positive)\n",
    "    \n",
    "    if not plot:\n",
    "        return neuron_difference_logprobs, difference_indices\n",
    "    \n",
    "    if positive:\n",
    "        xlabel = 'Boosted tokens'\n",
    "        title = 'Logprob increase from neuron ' + str(neurons.item())\n",
    "    else: \n",
    "        xlabel = 'Deboosted tokens'\n",
    "        title = 'Logprob decrease from neuron ' + str(neurons.item())\n",
    "    if top:\n",
    "        title += f' (top {neuron_pos+1} neuron)'\n",
    "    else:\n",
    "        title += f' (bottom {neuron_pos+1} neuron)'\n",
    "\n",
    "    xticks = [model.to_str_tokens([i])[0] for i in difference_indices]\n",
    "    haystack_utils.line(neuron_difference_logprobs.cpu().numpy(), xlabel=xlabel, ylabel=\"full_logprob - ablated_logprop\", title=title, xticks=xticks, show_legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5606e9083ae4721a6866340d8c078fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='Neuron Pos:', max=20), Dropdown(description='Neurons:', …"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a function to call your function with widget inputs\n",
    "def plot_individual_neuron(neuron_pos, top_bottom, pos_neg):\n",
    "    top_bool = True if top_bottom == 'Top' else False\n",
    "    positive_bool = True if pos_neg == 'Positive' else False\n",
    "    get_individual_neuron_logprob_effect(indices, neuron_pos=neuron_pos, top=top_bool, positive=positive_bool)\n",
    "\n",
    "# Define widgets\n",
    "neuron_pos_slider = widgets.IntSlider(min=0, max=20, step=1, value=0, description='Neuron Pos:')\n",
    "top_bottom_dropdown = widgets.Dropdown(options=['Top', 'Bottom'], value='Top', description='Neurons:')\n",
    "pos_neg_dropdown = widgets.Dropdown(options=['Positive', 'Negative'], value='Positive', description='Pos/Neg:')\n",
    "\n",
    "# Use interactive function to bind the widgets to the plotting function\n",
    "widgets.interactive(plot_individual_neuron, neuron_pos=neuron_pos_slider, top_bottom=top_bottom_dropdown, pos_neg=pos_neg_dropdown)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum individual weighted positive / negative effects\n",
    "def summed_neuron_differences(num_neurons=10, top=True, positive=True, k=50):\n",
    "    summed_neuron_diffs = torch.zeros(model.cfg.d_vocab).cuda()\n",
    "    for neuron_pos in range(num_neurons):\n",
    "        # It would probably be more principle to do our filtering on the summed diffs instead of per neuron\n",
    "        neuron_diff, token_indices = get_individual_neuron_logprob_effect(indices, neuron_pos=neuron_pos, top=top, positive=positive, plot=False)\n",
    "        summed_neuron_diffs[token_indices] += neuron_diff\n",
    "    if positive:\n",
    "        non_zero_count = (summed_neuron_diffs > 0).sum()\n",
    "    else:\n",
    "        non_zero_count = (summed_neuron_diffs < 0).sum()\n",
    "    top_logprob_difference, top_tokens = haystack_utils.top_k_with_exclude(summed_neuron_diffs, min(non_zero_count, k), all_ignore, largest=positive)\n",
    "    return top_logprob_difference, top_tokens\n",
    "\n",
    "def plot_summed_neuron_differences(num_neurons=10, top=True, positive=True):\n",
    "    top_logprob_difference, top_logprob_tokens = summed_neuron_differences(num_neurons=num_neurons, top=top, positive=positive)\n",
    "    xticks = [model.to_str_tokens([i])[0] for i in top_logprob_tokens]\n",
    "    \n",
    "    if positive:\n",
    "        title = \"Summed individual boosts by\"\n",
    "        xlabel = \"Boosted tokens\"\n",
    "    else:\n",
    "        title = \"Summed individual deboosts by\"\n",
    "        xlabel = \"Deboosted tokens\"\n",
    "    if top:\n",
    "        title += f\" top {num_neurons} neurons\"\n",
    "    else:\n",
    "        title += f\" bottom {num_neurons} neurons\"\n",
    "    haystack_utils.line(top_logprob_difference.cpu().numpy(), xlabel=xlabel, ylabel=\"full_logprob - ablated_logprop\", title=title, xticks=xticks, show_legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b87fee8fad4e47a787280708e732b2cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=10, description='Num Neurons:', max=20, min=1), Dropdown(description='Ne…"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a function to call your function with widget inputs\n",
    "def plot_summed_neurons_widget(num_neurons, top_bottom, pos_neg):\n",
    "    top_bool = True if top_bottom == 'Top' else False\n",
    "    positive_bool = True if pos_neg == 'Positive' else False\n",
    "    plot_summed_neuron_differences(num_neurons=num_neurons, top=top_bool, positive=positive_bool)\n",
    "\n",
    "# Define widgets\n",
    "num_neuron_slider = widgets.IntSlider(min=1, max=20, step=1, value=10, description='Num Neurons:')\n",
    "top_bottom_dropdown = widgets.Dropdown(options=['Top', 'Bottom'], value='Top', description='Neurons:')\n",
    "pos_neg_dropdown = widgets.Dropdown(options=['Positive', 'Negative'], value='Positive', description='Pos/Neg:')\n",
    "\n",
    "# Use interactive function to bind the widgets to the plotting function\n",
    "widgets.interactive(plot_summed_neurons_widget, num_neurons=num_neuron_slider, top_bottom=top_bottom_dropdown, pos_neg=pos_neg_dropdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss is logprob(correct_token_logit) = logit(correct_token_logit) - LogSumExp(all_token_logits)\n",
    "Loss = -logprob\n",
    "\n",
    "LogSumExp approximates a maximum function. If the neuron engages in destructive interference of a high logit for a non-answer token, then the exp(logit) for the token will be lower and so the LogSumExp will be more similar to logit(correct_token_logit) so the loss will be lower. So a lower logsumexp(all vocab) is good.\n",
    "\n",
    "If the neuron engages in destructive interference of a low logit for a non-answer token, then the exp(logit) for the the token won't really change and so the logsumexp and the loss will both be the same.\n",
    "\n",
    "For a single neuron:\n",
    "1. For each \"gen\" prompt, zero centre the logits and record the logit of \"gen\". Calculate the mean \"gen\" logit.\n",
    "2. For each \"gen\" prompt, disable the neuron under test, zero centre the logits, edit the logit of \"gen\" to the mean with a hook, then get the logprob of \"gen\" and calculate logsumexp(all vocab) = logit(gen) - logprob(gen).\n",
    "2. For each \"gen\" prompt, enable the neuron, zero centre the logits, edit the logit of \"gen\" to the mean with a hook, then get the logprob of \"gen\" and calculate logsumexp(all vocab) = logit(gen) - logprob(gen).\n",
    "3. Take the difference in logsum exps. If it's positive, the neuron is reducing the loss via destructive interference by the difference.\n",
    "Can use the same procedure for sets of neurons, or for all neurons, to find high level effects of the context neuron\n",
    "\n",
    "Logprobs are logits with a constant subtracted and the constant is the same for every logit within a prompt.\n",
    "\n",
    "Taking the difference in terms with and without a neuron's effect via the context neuron:\n",
    "- If log sum exp increases, the neuron is boosting tokens on average. \n",
    "- If logit increases, the neuron is boosting the correct token\n",
    "\n",
    "\n",
    "Remove the neuron's effects on the gen logit. Take the mean on the prompt and position?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(40.1799, device='cuda:0')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Calculate the mean \"gen\" logit.\n",
    "gen_index = model.to_single_token('gen')\n",
    "gen_logits = []\n",
    "logits = model(prompts, return_type='logits') # batch pos vocab\n",
    "logits = logits - logits.mean(-1).unsqueeze(-1) # batch pos vocab, batch pos 1\n",
    "\n",
    "mean_gen_logit = logits[:, -2, gen_index].mean(0)\n",
    "mean_gen_logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import numpy as np\n",
    "# from transformer_lens import utils\n",
    "# \n",
    "# px.histogram(np.random.choice(logits[:, -1, gen_index].flatten().cpu().numpy(), 1000), nbins=100)\n",
    "# utils.test_prompt(\"\".join(model.to_str_tokens(prompts[0, :-1])), \"gen\", model, prepend_space_to_answer=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constructive interference diff, destructive interference diff\n",
      "0.170594 0.002825\n",
      "0.121188 0.002024\n",
      "0.124797 0.004656\n",
      "0.156059 0.004893\n",
      "0.261433 -0.002853\n",
      "0.245677 -0.000237\n",
      "-0.402446 0.052646\n",
      "0.213606 0.005037\n",
      "0.358533 -0.007410\n",
      "0.043210 0.018672\n",
      "1.319245 0.103190\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>                            <div id=\"f6153b65-3878-411e-a379-77f10ff37f8d\" class=\"plotly-graph-div\" style=\"height:525px; width:1000px;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"f6153b65-3878-411e-a379-77f10ff37f8d\")) {                    Plotly.newPlot(                        \"f6153b65-3878-411e-a379-77f10ff37f8d\",                        [{\"error_y\":{\"array\":[0.0],\"type\":\"data\",\"visible\":true},\"name\":\"Neuron 0\",\"x\":[\"Neuron 0\"],\"y\":[0.0028245162684470415],\"type\":\"bar\"},{\"error_y\":{\"array\":[0.0],\"type\":\"data\",\"visible\":true},\"name\":\"Neuron 1\",\"x\":[\"Neuron 1\"],\"y\":[0.0020238112192600965],\"type\":\"bar\"},{\"error_y\":{\"array\":[0.0],\"type\":\"data\",\"visible\":true},\"name\":\"Neuron 2\",\"x\":[\"Neuron 2\"],\"y\":[0.004656410310417414],\"type\":\"bar\"},{\"error_y\":{\"array\":[0.0],\"type\":\"data\",\"visible\":true},\"name\":\"Neuron 3\",\"x\":[\"Neuron 3\"],\"y\":[0.004893341101706028],\"type\":\"bar\"},{\"error_y\":{\"array\":[0.0],\"type\":\"data\",\"visible\":true},\"name\":\"Neuron 4\",\"x\":[\"Neuron 4\"],\"y\":[-0.0028527448885142803],\"type\":\"bar\"},{\"error_y\":{\"array\":[0.0],\"type\":\"data\",\"visible\":true},\"name\":\"Neuron 5\",\"x\":[\"Neuron 5\"],\"y\":[-0.00023685455380473286],\"type\":\"bar\"},{\"error_y\":{\"array\":[0.0],\"type\":\"data\",\"visible\":true},\"name\":\"Neuron 6\",\"x\":[\"Neuron 6\"],\"y\":[0.05264587327837944],\"type\":\"bar\"},{\"error_y\":{\"array\":[0.0],\"type\":\"data\",\"visible\":true},\"name\":\"Neuron 7\",\"x\":[\"Neuron 7\"],\"y\":[0.00503681180998683],\"type\":\"bar\"},{\"error_y\":{\"array\":[0.0],\"type\":\"data\",\"visible\":true},\"name\":\"Neuron 8\",\"x\":[\"Neuron 8\"],\"y\":[-0.00741031626239419],\"type\":\"bar\"},{\"error_y\":{\"array\":[0.0],\"type\":\"data\",\"visible\":true},\"name\":\"Neuron 9\",\"x\":[\"Neuron 9\"],\"y\":[0.018671797588467598],\"type\":\"bar\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"title\":{\"text\":\"LogSumExp reduction from ablating top neurons\"},\"xaxis\":{\"title\":{\"text\":\"\"}},\"yaxis\":{\"title\":{\"text\":\"LogSumExp reduction\"}},\"barmode\":\"group\",\"width\":1000,\"showlegend\":true},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('f6153b65-3878-411e-a379-77f10ff37f8d');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def decompose_interference_diff(ablated_cache, disabled_neurons=[top_neurons[0]], mean=True\n",
    "                                ) -> tuple[float, float] | tuple[Float[Tensor, \"n_prompts\"], Float[Tensor, \"n_prompts\"]]:\n",
    "    '''\n",
    "    Finds the effect of the German context neuron ablation via a given set of MLP5 neurons on the logit of each final \n",
    "    token. Decomposes it into constructive and destructive interference. A positive constructive interference difference\n",
    "    means the neurons boost the logit of the correct token, a positive destructive interference difference means the\n",
    "    LogSumExp is getting closer to the correct token logit because neurons deboost the logits of the higher probability \n",
    "    incorrect tokens.\n",
    "\n",
    "    Loss = -logprob(correct_token_logit)\n",
    "    logprob(correct_token_logit) = correct_token_logit - LogSumExp(all_token_logits)\n",
    "\n",
    "    LogSumExp is a smooth maximum function, so it approximates the max of all logits. A neuron that destructively \n",
    "    interfers with a non-answer token with a high logit lowers the LogSumExp(all_token_logits) and thus the loss.'''\n",
    "    ablate_top_neuron_hook = get_ablate_neurons_hook(disabled_neurons, ablated_cache)\n",
    "\n",
    "    _, _, _, mlp5_enabled_logits = haystack_utils.get_direct_effect(prompts, model, pos=-2, \n",
    "                                                                    context_ablation_hooks=deactivate_neurons_fwd_hooks, \n",
    "                                                                    context_activation_hooks=activate_neurons_fwd_hooks, return_type='logits')\n",
    "    _, _, _, mlp5_enabled_top_neuron_ablated_logits = haystack_utils.get_direct_effect(prompts, model, pos=-2, \n",
    "                                                                                       context_ablation_hooks=deactivate_neurons_fwd_hooks, \n",
    "                                                                                       context_activation_hooks=activate_neurons_fwd_hooks+ablate_top_neuron_hook, return_type='logits')\n",
    "    \n",
    "    # Mean center logits to avoid picking up on constant boosts / deboosts\n",
    "    mlp5_enabled_logits = mlp5_enabled_logits - mlp5_enabled_logits.mean(-1).unsqueeze(-1)\n",
    "    mlp5_enabled_top_neuron_ablated_logits = mlp5_enabled_top_neuron_ablated_logits - mlp5_enabled_top_neuron_ablated_logits.mean(-1).unsqueeze(-1)\n",
    "    \n",
    "    # 1. Constructive interference difference\n",
    "    # This is the change in the correct answer token logit from ablating the neuron, positive is good\n",
    "    constructive_interference_diffs = mlp5_enabled_logits[:, gen_index] - mlp5_enabled_top_neuron_ablated_logits[:, gen_index]\n",
    "\n",
    "    # 2. Destructive interference difference\n",
    "    # This is the change in the LogSumExp of all logits from not ablating the neuron positive/increase in LogSumExp is bad \n",
    "    # even though it means that all tokens get deboosted more by a constant amount because the LogSumExp is guaranteed to be\n",
    "    # >= the correct token logit\n",
    "\n",
    "    # Set the gen logit to its mean value so the neuron's constructive interference doesn't affect the LogSumExp difference\n",
    "    mlp5_enabled_logits[:, gen_index] = mean_gen_logit\n",
    "    mlp5_enabled_top_neuron_ablated_logits[:, gen_index] = mean_gen_logit\n",
    "    \n",
    "    # Compute logsumexp\n",
    "    mlp5_enabled_log_sum_exp = mlp5_enabled_logits.exp().sum(-1).log()\n",
    "    mlp5_enabled_top_neuron_ablated_log_sum_exp = mlp5_enabled_top_neuron_ablated_logits.exp().sum(-1).log()\n",
    "\n",
    "    # Check for errors\n",
    "    assert torch.allclose(mlp5_enabled_log_sum_exp, mlp5_enabled_logits[:, gen_index] - mlp5_enabled_logits.log_softmax(-1)[:, gen_index])\n",
    "    assert torch.allclose(mlp5_enabled_top_neuron_ablated_log_sum_exp, mlp5_enabled_top_neuron_ablated_logits[:, gen_index] - mlp5_enabled_top_neuron_ablated_logits.log_softmax(-1)[:, gen_index])\n",
    "\n",
    "    # Difference in logsumexp\n",
    "    # Logsumexp of enabled should be higher than ablated if the neuron does something good\n",
    "    # Negative results are good - they mean that all tokens are deboosted more when the neuron is active\n",
    "    destructive_interference_diffs = mlp5_enabled_log_sum_exp - mlp5_enabled_top_neuron_ablated_log_sum_exp\n",
    "    # Convert the results so positive is good\n",
    "    destructive_interference_diffs *= -1\n",
    "\n",
    "    if mean:\n",
    "        return constructive_interference_diffs.mean().item(), destructive_interference_diffs.mean().item(), \n",
    "    return constructive_interference_diffs, destructive_interference_diffs,\n",
    "\n",
    "\n",
    "destructive_diffs = []\n",
    "# Calculate neuron-wise loss change\n",
    "print(\"constructive interference diff, destructive interference diff\")\n",
    "for i in range(10):\n",
    "    constructive_diff, destructive_diff = decompose_interference_diff(ablated_cache, disabled_neurons=[top_neurons[i]], mean=True)\n",
    "    print(f'{constructive_diff:2f}', f'{destructive_diff:2f}')\n",
    "    destructive_diffs.append(destructive_diff)\n",
    "\n",
    "constructive_diff, destructive_diff = decompose_interference_diff(ablated_cache, top_neurons)\n",
    "print(f'{constructive_diff:2f}', f'{destructive_diff:2f}')\n",
    "\n",
    "# line plot of logsumexp\n",
    "haystack_utils.plot_barplot([[item] for item in destructive_diffs], names=[f\"Neuron {i}\" for i in range(10)], ylabel=\"LogSumExp reduction\", title=\"LogSumExp reduction from ablating top neurons\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neuron -> Token Logit Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.0346, -0.0237, -0.0347,  0.0397,  0.0595,  0.0501, -0.0274,  0.0417,\n",
      "         0.1405, -0.0392], device='cuda:0')\n",
      "tensor([0.1634, 0.1461, 0.1405, 0.1399, 0.1349, 0.1207, 0.1196, 0.1161, 0.1156,\n",
      "        0.1139, 0.1130, 0.1128, 0.1120, 0.1104, 0.1096, 0.1070, 0.1056, 0.1048,\n",
      "        0.1042, 0.1028], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Our top neurons are selected by the difference in their boost in gen based on the context neuron\n",
    "# Many other neurons boost gen more per unit of activation\n",
    "\n",
    "cosine_sim = torch.nn.CosineSimilarity(dim=1)\n",
    "answer_residual_direction = model.tokens_to_residual_directions(\"gen\")\n",
    "neuron_weights = model.state_dict()['blocks.5.mlp.W_out'][top_neurons]\n",
    "\n",
    "cosine_sims = cosine_sim(neuron_weights, answer_residual_direction.unsqueeze(0))\n",
    "print(cosine_sims)\n",
    "\n",
    "cosine_sim = torch.nn.CosineSimilarity(dim=1)\n",
    "answer_residual_direction = model.tokens_to_residual_directions(\"gen\")\n",
    "neuron_weights = model.state_dict()['blocks.5.mlp.W_out']\n",
    "\n",
    "cosine_sims = cosine_sim(neuron_weights, answer_residual_direction.unsqueeze(0))\n",
    "top, indices = torch.topk(cosine_sims, 20)\n",
    "print(top)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6768e40dff844f18fef196e4afc0977",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a306651d1dd9449dbc49cd34f80280b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "532cda5e8c7048b38b9066b7067e6be5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "haystack_utils.clean_cache()\n",
    "english_activations_l5 = {}\n",
    "german_activations_l5 = {}\n",
    "all_activations_l5 = {}\n",
    "for layer in range(5, 6):\n",
    "    english_activations_l5 = get_mlp_activations(english_data, layer, model, mean=True)\n",
    "    german_activations_l5 = get_mlp_activations(german_data, layer, model, mean=True)\n",
    "    all_activations_l5 = get_mlp_activations(german_data[:50] + english_data[:150], layer, model, mean=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "060cc53864894323852c246e325abed2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9e7b808c1124db194a327b1a00ebe75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06801d6eb80e4cb99e1342a154e5037a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2483876f638048e1ae61911b636c4809",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8083e1a6fe44492b30ba3b8bd64db80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import einops\n",
    "\n",
    "W_out = model.state_dict()['blocks.5.mlp.W_out']\n",
    "W_U = model.W_U\n",
    "avg_W_out_all = einops.einsum(all_activations_l5, W_out, 'd_mlp, d_mlp d_model -> d_mlp d_model')\n",
    "avg_mlp5_boosts_all = einops.einsum(avg_W_out_all, W_U, 'd_mlp d_model, d_model d_vocab -> d_mlp d_vocab')\n",
    "avg_W_out_enabled = einops.einsum(german_activations_l5, W_out, 'd_mlp, d_mlp d_model -> d_mlp d_model')\n",
    "avg_mlp5_boosts_enabled = einops.einsum(avg_W_out_enabled, W_U, 'd_mlp d_model, d_model d_vocab -> d_mlp d_vocab')\n",
    "avg_W_out_disabled = einops.einsum(english_activations_l5, W_out, 'd_mlp, d_mlp d_model -> d_mlp d_model')\n",
    "avg_mlp5_boosts_disabled = einops.einsum(avg_W_out_disabled, W_U, 'd_mlp d_model, d_model d_vocab -> d_mlp d_vocab')\n",
    "\n",
    "prompt_strs = [model.tokenizer.decode(prompts[i].tolist()) for i in range(prompts.shape[0])]\n",
    "\n",
    "gen_acts_l5_all = get_mlp_activations(prompt_strs, 5, model, mean=True, pos=-2)\n",
    "with model.hooks(deactivate_neurons_fwd_hooks):\n",
    "    gen_acts_l5_disabled = get_mlp_activations(prompt_strs, 5, model, mean=True, pos=-2)\n",
    "with model.hooks(activate_neurons_fwd_hooks):\n",
    "    gen_acts_l5_enabled = get_mlp_activations(prompt_strs, 5, model, mean=True, pos=-2)\n",
    "\n",
    "with model.hooks(deactivate_neurons_fwd_hooks):\n",
    "    all_gen_acts_l5_disabled = get_mlp_activations(prompt_strs, 5, model, mean=False, pos=-2)\n",
    "with model.hooks(activate_neurons_fwd_hooks):\n",
    "    all_gen_acts_l5_enabled = get_mlp_activations(prompt_strs, 5, model, mean=False, pos=-2)\n",
    "\n",
    "data = {\n",
    "    'firing count diff': ((all_gen_acts_l5_enabled > 0).sum(0) - (all_gen_acts_l5_disabled > 0).sum(0)).tolist(),\n",
    "    # 'neuron index': list(range(2048)),\n",
    "    'cos sim gen': cosine_sims.tolist(),\n",
    "    # 'avg act (enabled)': german_activations_l5.tolist(),\n",
    "    # 'avg act (disabled)': english_activations_l5.tolist(),\n",
    "    'avg act increase enabled': (german_activations_l5 - english_activations_l5).tolist(),\n",
    "    'avg act': all_activations_l5.tolist(),\n",
    "    'avg gen act': gen_acts_l5_all.tolist(),\n",
    "    'avg gen act enabled': gen_acts_l5_enabled.tolist(),\n",
    "    'avg gen act disabled': gen_acts_l5_disabled.tolist(),\n",
    "    # 'avg boost gen': avg_mlp5_boosts_all[:, gen_index].tolist(),\n",
    "    # 'avg boost enabled gen': avg_mlp5_boosts_enabled[:, gen_index].tolist(),\n",
    "    # 'avg boost disabled gen': avg_mlp5_boosts_disabled[:, gen_index].tolist(),\n",
    "    # 'enabled firing count': (all_gen_acts_l5_enabled > 0).sum(0).tolist(),\n",
    "    # 'disabled firing count': (all_gen_acts_l5_disabled > 0).sum(0).tolist(),\n",
    "    'firing count': (all_gen_acts_l5_disabled.bool().sum(0).tolist())\n",
    "    \n",
    "    # 'Loss change when ablated': \n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "# Line plot of each neuron's gen boost with dotted vertical lines where the German context neuron-boosted ones are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' and', ' creating', ' the', ' Sc', 'hen', 'gen', ' area', '.', ' From', ' 2012']\n",
      "[' Application', ' of', ' the', ' Sc', 'hen', 'gen', ' acquis', ' relating', ' to', ' the']\n",
      "[' Bulgaria', ' to', ' the', ' Sc', 'hen', 'gen', ' Area', ',', ' nor', ' the']\n",
      "[' can', ' enter', ' the', ' Sc', 'hen', 'gen', ' area', ' without', ' having', ' to']\n",
      "[' to', ' restrict', ' the', ' Sc', 'hen', 'gen', ' area', ',', ' compatible', ' with']\n"
     ]
    }
   ],
   "source": [
    "for prompt in english_data:\n",
    "    prompt_tokens = model.to_tokens(prompt)\n",
    "    if gen_index in prompt_tokens:\n",
    "        index = prompt_tokens[0].tolist().index(gen_index)\n",
    "        print(model.to_str_tokens(prompt_tokens[0, index-5:index+5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1107/2048 MLP5 neurons generically increase \"gen\", 54.05%\n",
      "461 of these fire more when context neuron enabled, 22.51%\n",
      "646 of these fire less when context neuron enabled, 31.54%\n",
      "\n",
      "277 gen neurons fire on average on gen token with context neuron enabled, 13.53%\n",
      "250 gen neurons fire on average on gen token with context neuron disabled, 12.21%\n",
      "      firing count diff  cos sim gen  avg act increase enabled   avg act  \\\n",
      "13                   92     0.057594                  0.739129  0.160572   \n",
      "1927                 90     0.025486                  0.347759  0.020567   \n",
      "1877                 89     0.030004                  0.049302  0.190596   \n",
      "84                   87     0.041687                  0.345945 -0.001036   \n",
      "1667                 86     0.038450                  0.321733  0.052155   \n",
      "...                 ...          ...                       ...       ...   \n",
      "370                 -75     0.085419                 -0.001659 -0.056421   \n",
      "610                 -85     0.002886                  0.005306 -0.070067   \n",
      "1928                -85     0.035947                 -0.173583  0.088994   \n",
      "1723                -87     0.049513                 -0.083405 -0.040209   \n",
      "593                 -88     0.014631                  0.006157 -0.101475   \n",
      "\n",
      "      avg gen act  avg gen act enabled  avg gen act disabled  firing count  \n",
      "13       0.246213             0.148275             -0.128106           100  \n",
      "1927     0.193920             0.132682             -0.083573           100  \n",
      "1877     0.417323             0.315993             -0.069937           100  \n",
      "84       0.640189             0.457385             -0.058182           100  \n",
      "1667     0.157651             0.080240             -0.128277           100  \n",
      "...           ...                  ...                   ...           ...  \n",
      "370     -0.119488            -0.094941              0.100665           100  \n",
      "610     -0.150869            -0.124368              0.111482           100  \n",
      "1928    -0.083386            -0.049552              0.118440           100  \n",
      "1723    -0.157139            -0.143780              0.185857           100  \n",
      "593     -0.146381            -0.115487              0.072901           100  \n",
      "\n",
      "[1107 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "# Weight analysis\n",
    "gen_df = df[(df['cos sim gen'] > 0)]\n",
    "print(f\"{len(gen_df)}/{len(df)} MLP5 neurons generically increase \\\"gen\\\", {len(gen_df)/len(df)*100:.2f}%\")\n",
    "\n",
    "# Activation analysis\n",
    "# All on German prompts\n",
    "filtered_df = gen_df[(gen_df['avg act increase enabled'] > 0)]\n",
    "print(f\"{len(filtered_df)} of these fire more when context neuron enabled, {len(filtered_df)/len(df)*100:.2f}%\")\n",
    "filtered_df = gen_df[(gen_df['avg act increase enabled'] < 0)]\n",
    "print(f\"{len(filtered_df)} of these fire less when context neuron enabled, {len(filtered_df)/len(df)*100:.2f}%\")\n",
    "\n",
    "filtered_df = gen_df[(gen_df['avg gen act enabled'] >= 0)]\n",
    "print(f\"\\n{len(filtered_df)} gen neurons fire on average on gen token with context neuron enabled, {len(filtered_df)/len(df)*100:.2f}%\")\n",
    "filtered_df = gen_df[(gen_df['avg gen act disabled'] >= 0)]\n",
    "print(len(filtered_df), f\"gen neurons fire on average on gen token with context neuron disabled, {len(filtered_df)/len(df)*100:.2f}%\")\n",
    "\n",
    "pd.set_option('display.max_rows', 20)\n",
    "sorted = gen_df.sort_values(by=['firing count diff'], ascending=False)\n",
    "print(sorted)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interested in how often the context neuron makes the difference between firing and not, rather than firing on average.\n",
    "Neurons with top difference between number of fires with neuron enabled vs. disabled\n",
    "x% more likely to fire\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brainstorm notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to decompose an MLP5 neuron's effect into its boost to the correct logit and its deboost of other logits. I want to discover how these two effects change the log prob.\n",
    "\n",
    "metric like loss reduction vs. token boost\n",
    "\n",
    "~~run direct effect, patch each neuron in top 10 individually, get overall loss reduction from neuron controlled by context neuron (equivalent to logprob increase for correct token)~~\n",
    "decompose loss reduction into two parts:\n",
    "run direct effect, patch the correct token logit boost from the neuron (removing the neuron's other effects), get overall loss reduction from neuron (equivalent to logprob increase for correct token)\n",
    "destructive interference loss reduction = overall loss reduction - loss reduction from correct token logit boost (component of the logprob increase for correct token due to it deboosting incorrect token)\n",
    "\n",
    "Patch the correct token logit boost from the neuron (removing the neuron's other effects).\n",
    "1. Get baseline logprobs for a prompt\n",
    "2. Get difference in logits from activating the neuron under test. Can use get_direct_effects with return_type='logits'.\n",
    "3. Run the model with return_type='logits' and the neuron under test zero ablated. \n",
    "4. Add the correct token logit from step 1 to a copy of the output logits. Convert to logprobs\n",
    "5. Add the incorrect token logits from step 1 to a copy of the output logits. Convert to logprobs\n",
    "6. Compare A. lobprobs with correct answer token logit increase, B. logprobs with incorrect answer token logit increases, and C. baseline logprobs\n",
    "\n",
    "~~If the context neuron gives each neuron a flat boost then if we decompose the resulting flat boost to one MLP5 neuron into a boost to one logit vs. boosts to all other logits it will change the logprobs (first will increase answer probability and second will reduce answer probability). \n",
    "\n",
    "the two resulting log probs at the correct answer token won't add up to the original log probs (?). \n",
    "\n",
    "If the boost is flat the correct percentage decomposition is 1/50000 and 49999/50000? In practice/all other factors being equal\n",
    "\n",
    "New plan:\n",
    "\n",
    "Difference between baseline log prob and neuron log prob?\n",
    "Classify individual neurons by percentage constructive vs destructive by looking at their log probs and summing the incorrect token log probs\n",
    "\n",
    "Correct log prob difference\n",
    "Incorrect log prob difference (summed over every plausible token?)\n",
    "\n",
    "Largest boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'base' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n base ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# If the log prob for an incorrect token is significantly lower then that's where the extra probability density on the correct answer is coming from \n",
    "# Constructive interference increases correct token log prob and uniformly decreases other log probs\n",
    "# Destructive interference decreases specific other log probs and uniformly increases other log probs\n",
    " \n",
    "# Most neurons are a mixture of the above\n",
    "# Decompose neurons into what % of their effect is each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'base' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n base ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# original_logits, ablated_logprobs, _, all_MLP5_logprobs = haystack_utils.get_direct_effect(prompts, model, pos=-2, context_ablation_hooks=deactivate_neurons_fwd_hooks, context_activation_hooks=activate_neurons_fwd_hooks, return_type='logprobs')\n",
    "# _, _, _, top_MLP5_ablated_logprobs = haystack_utils.get_direct_effect(prompts, model, pos=-2, context_ablation_hooks=deactivate_neurons_fwd_hooks, context_activation_hooks=activate_neurons_fwd_hooks+ablate_top_neurons_hook, return_type='logprobs')\n",
    "# _, _, _, bottom_MLP5_ablated_logprobs = haystack_utils.get_direct_effect(prompts, model, pos=-2, context_ablation_hooks=deactivate_neurons_fwd_hooks, context_activation_hooks=activate_neurons_fwd_hooks+ablate_bottom_neurons_hook, return_type='logprobs')\n",
    "\n",
    "# bottom_neuron_high_difference_logprobs = (all_MLP5_logprobs - bottom_MLP5_ablated_logprobs)\n",
    "# bottom_neuron_high_difference_logprobs[bottom_neuron_high_difference_logprobs < 0] = 0\n",
    "# bottom_neuron_high_difference_logprobs = bottom_neuron_high_difference_logprobs.mean(0)\n",
    "# bottom_neuron_high_difference_logprobs[all_MLP5_logprobs.mean(0) < -7] = 0\n",
    "\n",
    "# bottom_non_zero_count = (bottom_neuron_high_difference_logprobs > 0).sum()\n",
    "# bottom_neuron_high_difference_logprobs, bottom_indices = haystack_utils.top_k_with_exclude(bottom_neuron_high_difference_logprobs, min(bottom_non_zero_count, 50), all_ignore)\n",
    "# haystack_utils.line(bottom_neuron_high_difference_logprobs.cpu().numpy(), title='Largest positive difference in log probs for tokens when bottom neurons are not ablated', xticks=[model.to_str_tokens([i])[0] for i in bottom_indices])\n",
    "\n",
    "\n",
    "# top_neuron_high_difference_logprobs = (all_MLP5_logprobs - top_MLP5_ablated_logprobs)\n",
    "# top_neuron_high_difference_logprobs[top_neuron_high_difference_logprobs < 0] = 0\n",
    "# top_neuron_high_difference_logprobs = top_neuron_high_difference_logprobs.mean(0)\n",
    "# top_neuron_high_difference_logprobs[all_MLP5_logprobs.mean(0) < -7] = 0\n",
    "\n",
    "# top_non_zero_count = (top_neuron_high_difference_logprobs > 0).sum()\n",
    "# top_neuron_high_difference_logprobs, top_indices = haystack_utils.top_k_with_exclude(top_neuron_high_difference_logprobs, min(top_non_zero_count, 50), all_ignore)\n",
    "# haystack_utils.line(top_neuron_high_difference_logprobs.cpu().numpy(), title='Largest positive difference in log probs for tokens when bottom neurons are not ablated', xticks=[model.to_str_tokens([i])[0] for i in top_indices])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
