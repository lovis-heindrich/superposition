{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from transformer_lens import HookedTransformer, ActivationCache, utils, patching\n",
    "from jaxtyping import Float, Int, Bool\n",
    "from torch import Tensor\n",
    "from tqdm.auto import tqdm\n",
    "import plotly.io as pio\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "pio.renderers.default = \"notebook_connected\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.autograd.set_grad_enabled(False)\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "from haystack_utils import get_mlp_activations\n",
    "import haystack_utils\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model EleutherAI/pythia-70m into HookedTransformer\n",
      "data/german_europarl.json: Loaded 2000 examples with 152 to 2000 characters each.\n",
      "data/english_europarl.json: Loaded 2000 examples with 165 to 2000 characters each.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8576e56e714d4ad5b3ad5db5f1b35259",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9139e4e440fb488d93128b43f24f818a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = HookedTransformer.from_pretrained(\"EleutherAI/pythia-70m\",\n",
    "    center_unembed=True,\n",
    "    center_writing_weights=True,\n",
    "    fold_ln=True,\n",
    "    device=device)\n",
    "\n",
    "german_data = haystack_utils.load_json_data(\"data/german_europarl.json\")[:200]\n",
    "english_data = haystack_utils.load_json_data(\"data/english_europarl.json\")[:200]\n",
    "\n",
    "\n",
    "english_activations = {}\n",
    "german_activations = {}\n",
    "for layer in range(3, 4):\n",
    "    english_activations[layer] = get_mlp_activations(english_data, layer, model, mean=False)\n",
    "    german_activations[layer] = get_mlp_activations(german_data, layer, model, mean=False)\n",
    "\n",
    "LAYER_TO_ABLATE = 3\n",
    "NEURONS_TO_ABLATE = [669]\n",
    "MEAN_ACTIVATION_ACTIVE = german_activations[LAYER_TO_ABLATE][:, NEURONS_TO_ABLATE].mean()\n",
    "MEAN_ACTIVATION_INACTIVE = english_activations[LAYER_TO_ABLATE][:, NEURONS_TO_ABLATE].mean()\n",
    "\n",
    "def deactivate_neurons_hook(value, hook):\n",
    "    value[:, :, NEURONS_TO_ABLATE] = MEAN_ACTIVATION_INACTIVE\n",
    "    return value\n",
    "deactivate_neurons_fwd_hooks=[(f'blocks.{LAYER_TO_ABLATE}.mlp.hook_post', deactivate_neurons_hook)]\n",
    "\n",
    "def activate_neurons_hook(value, hook):\n",
    "    value[:, :, NEURONS_TO_ABLATE] = MEAN_ACTIVATION_ACTIVE\n",
    "    return value\n",
    "activate_neurons_fwd_hooks=[(f'blocks.{LAYER_TO_ABLATE}.mlp.hook_post', activate_neurons_hook)]\n",
    "\n",
    "all_ignore, not_ignore = haystack_utils.get_weird_tokens(model, plot_norms=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find top common German tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e5252a4034f4d52b5a83f7c20597b74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' der', 'en', ' die', ' und', 'ung', 'ä', ' in', ' den', ' des', ' zu', 'ch', 'n', 'st', 're', 'z', ' von', ' für', 'äsident', ' Pr', 'ischen', 't', 'ü', 'icht', 'gen', ' ist', ' auf', ' dass', 'ge', 'ig', ' im', 'in', ' über', 'g', ' das', 'te', ' er', 'men', ' w', 'es', ' an', 'ß', ' wir', ' eine', 'f', ' W', 'hen', 'w', ' Europ', ' ich', 'ungen', 'ren', 'le', ' dem', 'ten', ' ein', 'e', ' Z', ' Ver', 'der', ' B', ' mit', ' dies', 'h', ' nicht', 'ungs', 's', ' G', ' z', 'it', ' Herr', ' es', 'l', ' S', 'ich', 'lich', ' An', 'heit', 'ie', ' Er', ' zur', ' V', ' ver', 'u', 'hr', 'chaft', 'Der', ' Ich', ' Ab', ' haben', 'i', 'ant', 'chte', ' mö', 'er', ' K', 'igen', ' Ber', 'ür', ' Fra', 'em']\n"
     ]
    }
   ],
   "source": [
    "# Get top common german tokens excluding punctuation\n",
    "token_counts = torch.zeros(model.cfg.d_vocab).cuda()\n",
    "for example in tqdm(german_data):\n",
    "    tokens = model.to_tokens(example)\n",
    "    for token in tokens[0]:\n",
    "        token_counts[token.item()] += 1\n",
    "\n",
    "punctuation = [\"\\n\", \".\", \",\", \"!\", \"?\", \";\", \":\", \"-\", \"(\", \")\", \"[\", \"]\", \"{\", \"}\", \"<\", \">\", \"/\", \"\\\\\", \"\\\"\", \"'\"]\n",
    "leading_space_punctuation = [\" \" + char for char in punctuation]\n",
    "punctuation_tokens = model.to_tokens(punctuation + leading_space_punctuation + [' –', \" \", '  ', \"<|endoftext|>\"])[:, 1].flatten()\n",
    "token_counts[punctuation_tokens] = 0\n",
    "token_counts[all_ignore] = 0\n",
    "\n",
    "top_counts, top_tokens = torch.topk(token_counts, 100)\n",
    "print(model.to_str_tokens(top_tokens[:100]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of ngrams preceded by random prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_selection(tensor, n=12):\n",
    "    # Hacky replacement for np.random.choice\n",
    "    return tensor[torch.randperm(len(tensor))[:n]]\n",
    "\n",
    "def generate_random_prompts(end_string, n=50, length=12):\n",
    "    # Generate a batch of random prompts ending with a specific ngram\n",
    "    end_tokens = model.to_tokens(end_string).flatten()[1:]\n",
    "    prompts = []\n",
    "    for i in range(n):\n",
    "        prompt = get_random_selection(top_tokens[:max(50, length)], n=length).cuda()\n",
    "        prompt = torch.cat([prompt, end_tokens])\n",
    "        prompts.append(prompt)\n",
    "    prompts = torch.stack(prompts)\n",
    "    return prompts\n",
    "\n",
    "def replace_column(prompts: Int[Tensor, \"n_prompts n_tokens\"], token_index: int):\n",
    "    # Replaces a specific token position in a batch of prompts with random common German tokens\n",
    "    new_prompts = prompts.clone()\n",
    "    random_tokens = get_random_selection(top_tokens[:max(50, prompts.shape[0])], n=prompts.shape[0]).cuda()\n",
    "    new_prompts[:, token_index] = random_tokens\n",
    "    return new_prompts \n",
    "\n",
    "def loss_analysis(prompts: Tensor, title=\"\"):\n",
    "    # Loss plot for a batch of prompts\n",
    "    names = [\"Original\", \"Ablated\", \"MLP5 path patched\"]\n",
    "    original_loss, ablated_loss, _, only_activated_loss = \\\n",
    "        haystack_utils.get_direct_effect(prompts, model, pos=-1,\n",
    "                                        context_ablation_hooks=deactivate_neurons_fwd_hooks, \n",
    "                                        context_activation_hooks=activate_neurons_fwd_hooks, \n",
    "                                        )\n",
    "    haystack_utils.plot_barplot([original_loss.tolist(), ablated_loss.tolist(), only_activated_loss.tolist()], names, ylabel=\"Loss\", title=title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_analysis_random_prompts(end_string, n=50, length=12, replace_columns: list[int] | None = None):\n",
    "    # Loss plot for a batch of random prompts ending with a specific ngram and optionally replacing specific tokens\n",
    "    prompts = generate_random_prompts(end_string, n=n, length=length)\n",
    "    title=f\"Average last token loss on {length} random tokens ending in '{end_string}'\"\n",
    "    if replace_columns is not None:\n",
    "        replaced_tokens = model.to_str_tokens(prompts[0, replace_columns])\n",
    "        title += f\" replacing {replaced_tokens}\"\n",
    "        for column in replace_columns:\n",
    "            prompts = replace_column(prompts, column)\n",
    "    \n",
    "    loss_analysis(prompts, title=title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "# loss_analysis_random_prompts(\" Vorschlägen\", n=100, length=20)\n",
    "# loss_analysis_random_prompts(\" Vorschlägen\", n=100, length=20, replace_columns=[-2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e28a22da6de4a87a12dfe05ec5673ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Ngram: ', options=(' Vorschlägen', ' Vorschläge', ' häufig', ' schließt', ' beweglich'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a08b868649e945da8b513f7bf1a9f278",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SelectMultiple(description='Replace Columns:', index=(3,), options=('-2', '-3', '-4', 'None'), value=('None',)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1aaf7400407f46fca201efe9ae192ac0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a dropdown menu widget\n",
    "dropdown = widgets.Dropdown(\n",
    "    options = [\" Vorschlägen\", \" Vorschläge\", \" häufig\", \" schließt\", \" beweglich\"], \n",
    "    value = \" Vorschlägen\",\n",
    "    description = 'Ngram: ',\n",
    ")\n",
    "\n",
    "replace_columns_dropdown = widgets.SelectMultiple(\n",
    "    options = ['-2', '-3', '-4', 'None'],\n",
    "    value = ['None'],  # default selected value\n",
    "    description = 'Replace Columns:',\n",
    ")\n",
    "\n",
    "# Create an output widget to hold the plot\n",
    "output = widgets.Output()\n",
    "\n",
    "# Define a function to call when the widget's value changes\n",
    "def update_plot(*args):\n",
    "    # Clear the old plot from the output\n",
    "    with output:\n",
    "        clear_output(wait=True)\n",
    "\n",
    "        if 'None' in replace_columns_dropdown.value:\n",
    "            replace_columns = None\n",
    "        else:\n",
    "            # If 'None' not selected, convert the selected values to integers\n",
    "            replace_columns = [int(val) for val in replace_columns_dropdown.value]\n",
    "        \n",
    "        # Call your function with the values from the widgets\n",
    "        loss_analysis_random_prompts(dropdown.value, n=100, length=20, replace_columns=replace_columns)\n",
    "\n",
    "# Set the function to be called when the widget's value changes\n",
    "dropdown.observe(update_plot, 'value')\n",
    "replace_columns_dropdown.observe(update_plot, 'value')\n",
    "\n",
    "# Display the widget and the output\n",
    "display(dropdown, replace_columns_dropdown, output)\n",
    "\n",
    "# Run once at startup\n",
    "update_plot()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify relevant neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 24])\n"
     ]
    }
   ],
   "source": [
    "prompts = generate_random_prompts(\" Vorschlägen\", n=100, length=20)\n",
    "print(prompts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b232a505d5a4bb89b703351334da829",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2048 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0007)\n"
     ]
    }
   ],
   "source": [
    "haystack_utils.clean_cache\n",
    "# Calculate neuron-wise loss change\n",
    "with model.hooks(deactivate_neurons_fwd_hooks):\n",
    "    _, ablated_cache = model.run_with_cache(prompts)\n",
    "\n",
    "def get_ablate_neurons_hook(neuron: int | list[int], ablated_cache):\n",
    "    def ablate_neurons_hook(value, hook):\n",
    "        value[:, :, neuron] = ablated_cache['blocks.5.mlp.hook_post'][:, :, neuron]\n",
    "        return value\n",
    "    return [('blocks.5.mlp.hook_post', ablate_neurons_hook)]\n",
    "\n",
    "diffs = torch.zeros(2048, prompts.shape[0])\n",
    "# Loss with path patched MLP5 neurons\n",
    "_, _, _, baseline_loss = haystack_utils.get_direct_effect(prompts, model, pos=-1, context_ablation_hooks=deactivate_neurons_fwd_hooks, context_activation_hooks=activate_neurons_fwd_hooks)\n",
    "for neuron in tqdm(range(2048)):\n",
    "    ablate_single_neuron_hook = get_ablate_neurons_hook(neuron, ablated_cache)\n",
    "    # Loss with path patched MLP5 neurons but a single neuron changed back to original ablated value\n",
    "    _, _, _, only_activated_loss = haystack_utils.get_direct_effect(prompts, model, pos=-1, context_ablation_hooks=deactivate_neurons_fwd_hooks, context_activation_hooks=activate_neurons_fwd_hooks+ablate_single_neuron_hook)\n",
    "    diffs[neuron] = only_activated_loss - baseline_loss\n",
    "\n",
    "print(diffs.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>                            <div id=\"91acc16a-d4ad-4f7b-9135-e46dd0c49549\" class=\"plotly-graph-div\" style=\"height:525px; width:800px;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"91acc16a-d4ad-4f7b-9135-e46dd0c49549\")) {                    Plotly.newPlot(                        \"91acc16a-d4ad-4f7b-9135-e46dd0c49549\",                        [{\"hovertemplate\":\"variable=0\\u003cbr\\u003eindex=%{x}\\u003cbr\\u003evalue=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"0\",\"line\":{\"color\":\"#636efa\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"0\",\"showlegend\":true,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542,543,544,545,546,547,548,549,550,551,552,553,554,555,556,557,558,559,560,561,562,563,564,565,566,567,568,569,570,571,572,573,574,575,576,577,578,579,580,581,582,583,584,585,586,587,588,589,590,591,592,593,594,595,596,597,598,599,600,601,602,603,604,605,606,607,608,609,610,611,612,613,614,615,616,617,618,619,620,621,622,623,624,625,626,627,628,629,630,631,632,633,634,635,636,637,638,639,640,641,642,643,644,645,646,647,648,649,650,651,652,653,654,655,656,657,658,659,660,661,662,663,664,665,666,667,668,669,670,671,672,673,674,675,676,677,678,679,680,681,682,683,684,685,686,687,688,689,690,691,692,693,694,695,696,697,698,699,700,701,702,703,704,705,706,707,708,709,710,711,712,713,714,715,716,717,718,719,720,721,722,723,724,725,726,727,728,729,730,731,732,733,734,735,736,737,738,739,740,741,742,743,744,745,746,747,748,749,750,751,752,753,754,755,756,757,758,759,760,761,762,763,764,765,766,767,768,769,770,771,772,773,774,775,776,777,778,779,780,781,782,783,784,785,786,787,788,789,790,791,792,793,794,795,796,797,798,799,800,801,802,803,804,805,806,807,808,809,810,811,812,813,814,815,816,817,818,819,820,821,822,823,824,825,826,827,828,829,830,831,832,833,834,835,836,837,838,839,840,841,842,843,844,845,846,847,848,849,850,851,852,853,854,855,856,857,858,859,860,861,862,863,864,865,866,867,868,869,870,871,872,873,874,875,876,877,878,879,880,881,882,883,884,885,886,887,888,889,890,891,892,893,894,895,896,897,898,899,900,901,902,903,904,905,906,907,908,909,910,911,912,913,914,915,916,917,918,919,920,921,922,923,924,925,926,927,928,929,930,931,932,933,934,935,936,937,938,939,940,941,942,943,944,945,946,947,948,949,950,951,952,953,954,955,956,957,958,959,960,961,962,963,964,965,966,967,968,969,970,971,972,973,974,975,976,977,978,979,980,981,982,983,984,985,986,987,988,989,990,991,992,993,994,995,996,997,998,999,1000,1001,1002,1003,1004,1005,1006,1007,1008,1009,1010,1011,1012,1013,1014,1015,1016,1017,1018,1019,1020,1021,1022,1023,1024,1025,1026,1027,1028,1029,1030,1031,1032,1033,1034,1035,1036,1037,1038,1039,1040,1041,1042,1043,1044,1045,1046,1047,1048,1049,1050,1051,1052,1053,1054,1055,1056,1057,1058,1059,1060,1061,1062,1063,1064,1065,1066,1067,1068,1069,1070,1071,1072,1073,1074,1075,1076,1077,1078,1079,1080,1081,1082,1083,1084,1085,1086,1087,1088,1089,1090,1091,1092,1093,1094,1095,1096,1097,1098,1099,1100,1101,1102,1103,1104,1105,1106,1107,1108,1109,1110,1111,1112,1113,1114,1115,1116,1117,1118,1119,1120,1121,1122,1123,1124,1125,1126,1127,1128,1129,1130,1131,1132,1133,1134,1135,1136,1137,1138,1139,1140,1141,1142,1143,1144,1145,1146,1147,1148,1149,1150,1151,1152,1153,1154,1155,1156,1157,1158,1159,1160,1161,1162,1163,1164,1165,1166,1167,1168,1169,1170,1171,1172,1173,1174,1175,1176,1177,1178,1179,1180,1181,1182,1183,1184,1185,1186,1187,1188,1189,1190,1191,1192,1193,1194,1195,1196,1197,1198,1199,1200,1201,1202,1203,1204,1205,1206,1207,1208,1209,1210,1211,1212,1213,1214,1215,1216,1217,1218,1219,1220,1221,1222,1223,1224,1225,1226,1227,1228,1229,1230,1231,1232,1233,1234,1235,1236,1237,1238,1239,1240,1241,1242,1243,1244,1245,1246,1247,1248,1249,1250,1251,1252,1253,1254,1255,1256,1257,1258,1259,1260,1261,1262,1263,1264,1265,1266,1267,1268,1269,1270,1271,1272,1273,1274,1275,1276,1277,1278,1279,1280,1281,1282,1283,1284,1285,1286,1287,1288,1289,1290,1291,1292,1293,1294,1295,1296,1297,1298,1299,1300,1301,1302,1303,1304,1305,1306,1307,1308,1309,1310,1311,1312,1313,1314,1315,1316,1317,1318,1319,1320,1321,1322,1323,1324,1325,1326,1327,1328,1329,1330,1331,1332,1333,1334,1335,1336,1337,1338,1339,1340,1341,1342,1343,1344,1345,1346,1347,1348,1349,1350,1351,1352,1353,1354,1355,1356,1357,1358,1359,1360,1361,1362,1363,1364,1365,1366,1367,1368,1369,1370,1371,1372,1373,1374,1375,1376,1377,1378,1379,1380,1381,1382,1383,1384,1385,1386,1387,1388,1389,1390,1391,1392,1393,1394,1395,1396,1397,1398,1399,1400,1401,1402,1403,1404,1405,1406,1407,1408,1409,1410,1411,1412,1413,1414,1415,1416,1417,1418,1419,1420,1421,1422,1423,1424,1425,1426,1427,1428,1429,1430,1431,1432,1433,1434,1435,1436,1437,1438,1439,1440,1441,1442,1443,1444,1445,1446,1447,1448,1449,1450,1451,1452,1453,1454,1455,1456,1457,1458,1459,1460,1461,1462,1463,1464,1465,1466,1467,1468,1469,1470,1471,1472,1473,1474,1475,1476,1477,1478,1479,1480,1481,1482,1483,1484,1485,1486,1487,1488,1489,1490,1491,1492,1493,1494,1495,1496,1497,1498,1499,1500,1501,1502,1503,1504,1505,1506,1507,1508,1509,1510,1511,1512,1513,1514,1515,1516,1517,1518,1519,1520,1521,1522,1523,1524,1525,1526,1527,1528,1529,1530,1531,1532,1533,1534,1535,1536,1537,1538,1539,1540,1541,1542,1543,1544,1545,1546,1547,1548,1549,1550,1551,1552,1553,1554,1555,1556,1557,1558,1559,1560,1561,1562,1563,1564,1565,1566,1567,1568,1569,1570,1571,1572,1573,1574,1575,1576,1577,1578,1579,1580,1581,1582,1583,1584,1585,1586,1587,1588,1589,1590,1591,1592,1593,1594,1595,1596,1597,1598,1599,1600,1601,1602,1603,1604,1605,1606,1607,1608,1609,1610,1611,1612,1613,1614,1615,1616,1617,1618,1619,1620,1621,1622,1623,1624,1625,1626,1627,1628,1629,1630,1631,1632,1633,1634,1635,1636,1637,1638,1639,1640,1641,1642,1643,1644,1645,1646,1647,1648,1649,1650,1651,1652,1653,1654,1655,1656,1657,1658,1659,1660,1661,1662,1663,1664,1665,1666,1667,1668,1669,1670,1671,1672,1673,1674,1675,1676,1677,1678,1679,1680,1681,1682,1683,1684,1685,1686,1687,1688,1689,1690,1691,1692,1693,1694,1695,1696,1697,1698,1699,1700,1701,1702,1703,1704,1705,1706,1707,1708,1709,1710,1711,1712,1713,1714,1715,1716,1717,1718,1719,1720,1721,1722,1723,1724,1725,1726,1727,1728,1729,1730,1731,1732,1733,1734,1735,1736,1737,1738,1739,1740,1741,1742,1743,1744,1745,1746,1747,1748,1749,1750,1751,1752,1753,1754,1755,1756,1757,1758,1759,1760,1761,1762,1763,1764,1765,1766,1767,1768,1769,1770,1771,1772,1773,1774,1775,1776,1777,1778,1779,1780,1781,1782,1783,1784,1785,1786,1787,1788,1789,1790,1791,1792,1793,1794,1795,1796,1797,1798,1799,1800,1801,1802,1803,1804,1805,1806,1807,1808,1809,1810,1811,1812,1813,1814,1815,1816,1817,1818,1819,1820,1821,1822,1823,1824,1825,1826,1827,1828,1829,1830,1831,1832,1833,1834,1835,1836,1837,1838,1839,1840,1841,1842,1843,1844,1845,1846,1847,1848,1849,1850,1851,1852,1853,1854,1855,1856,1857,1858,1859,1860,1861,1862,1863,1864,1865,1866,1867,1868,1869,1870,1871,1872,1873,1874,1875,1876,1877,1878,1879,1880,1881,1882,1883,1884,1885,1886,1887,1888,1889,1890,1891,1892,1893,1894,1895,1896,1897,1898,1899,1900,1901,1902,1903,1904,1905,1906,1907,1908,1909,1910,1911,1912,1913,1914,1915,1916,1917,1918,1919,1920,1921,1922,1923,1924,1925,1926,1927,1928,1929,1930,1931,1932,1933,1934,1935,1936,1937,1938,1939,1940,1941,1942,1943,1944,1945,1946,1947,1948,1949,1950,1951,1952,1953,1954,1955,1956,1957,1958,1959,1960,1961,1962,1963,1964,1965,1966,1967,1968,1969,1970,1971,1972,1973,1974,1975,1976,1977,1978,1979,1980,1981,1982,1983,1984,1985,1986,1987,1988,1989,1990,1991,1992,1993,1994,1995,1996,1997,1998,1999,2000,2001,2002,2003,2004,2005,2006,2007,2008,2009,2010,2011,2012,2013,2014,2015,2016,2017,2018,2019,2020,2021,2022,2023,2024,2025,2026,2027,2028,2029,2030,2031,2032,2033,2034,2035,2036,2037,2038,2039,2040,2041,2042,2043,2044,2045,2046,2047],\"xaxis\":\"x\",\"y\":[-0.5762313604354858,-0.2957223653793335,-0.20513702929019928,-0.15846304595470428,-0.15841549634933472,-0.15798956155776978,-0.13945604860782623,-0.12050101906061172,-0.11517885327339172,-0.11344992369413376,-0.11028328537940979,-0.1053919568657875,-0.09210870414972305,-0.09102358669042587,-0.09025219827890396,-0.08143201470375061,-0.07752401381731033,-0.0763145238161087,-0.0756172463297844,-0.07514733076095581,-0.07366489619016647,-0.07309628278017044,-0.07234916090965271,-0.07182016223669052,-0.06951797753572464,-0.06899390369653702,-0.06894917786121368,-0.06785713136196136,-0.06717880070209503,-0.06648552417755127,-0.06418806314468384,-0.06287472695112228,-0.06262120604515076,-0.06138167530298233,-0.06045228987932205,-0.05931324139237404,-0.059231024235486984,-0.0588667094707489,-0.05754638835787773,-0.05736919492483139,-0.056517213582992554,-0.053613629192113876,-0.05265921726822853,-0.052508316934108734,-0.05234590545296669,-0.05181083083152771,-0.05048193037509918,-0.05019112676382065,-0.0497065968811512,-0.04916132986545563,-0.048159800469875336,-0.04800358787178993,-0.04545311629772186,-0.04531143605709076,-0.04477837681770325,-0.04196052625775337,-0.04175573214888573,-0.03903625160455704,-0.03883754834532738,-0.0386078767478466,-0.037999268621206284,-0.036855507642030716,-0.03657137230038643,-0.036201391369104385,-0.03619525581598282,-0.036131925880908966,-0.03596009314060211,-0.035398054867982864,-0.03533488139510155,-0.03530783951282501,-0.03506928309798241,-0.03470839560031891,-0.03427702561020851,-0.03404847905039787,-0.03336929902434349,-0.033332183957099915,-0.033121172338724136,-0.03299246355891228,-0.03252022713422775,-0.03223744034767151,-0.03194217383861542,-0.03121168538928032,-0.03112575225532055,-0.030512962490320206,-0.02983923628926277,-0.02972879447042942,-0.029600663110613823,-0.029596198350191116,-0.029000496491789818,-0.028943490236997604,-0.02886579930782318,-0.028648434206843376,-0.02852783165872097,-0.028280198574066162,-0.02809634618461132,-0.02802671492099762,-0.027811381965875626,-0.027756864205002785,-0.027751201763749123,-0.027684638276696205,-0.02766352705657482,-0.027470530942082405,-0.027347631752490997,-0.026772012934088707,-0.026564819738268852,-0.026286037638783455,-0.026239214465022087,-0.02619992010295391,-0.026170575991272926,-0.02507602609694004,-0.02488386072218418,-0.024652371183037758,-0.02447013184428215,-0.02425532042980194,-0.024107100442051888,-0.02409939467906952,-0.023958493024110794,-0.02386537380516529,-0.023731505498290062,-0.023272329941391945,-0.02274508774280548,-0.022529754787683487,-0.022096805274486542,-0.02192561700940132,-0.0218422319740057,-0.021826904267072678,-0.02146429941058159,-0.021165989339351654,-0.020930595695972443,-0.020863186568021774,-0.020674491301178932,-0.020601043477654457,-0.020397108048200607,-0.02039603888988495,-0.020380310714244843,-0.019889846444129944,-0.01960822194814682,-0.01956111192703247,-0.019191592931747437,-0.019024498760700226,-0.01897328533232212,-0.01887376420199871,-0.01886049285531044,-0.01817954331636429,-0.01786765456199646,-0.017770521342754364,-0.01738249696791172,-0.017132412642240524,-0.017067626118659973,-0.0170099139213562,-0.016969606280326843,-0.01695164293050766,-0.01690533198416233,-0.016793718561530113,-0.016238518059253693,-0.015911485999822617,-0.015888625755906105,-0.015863610431551933,-0.015684355050325394,-0.015529329888522625,-0.015507404692471027,-0.01549807284027338,-0.01545655820518732,-0.015367861837148666,-0.015325878746807575,-0.015164465643465519,-0.015126899816095829,-0.015061062760651112,-0.014868587255477905,-0.01468969788402319,-0.014597666449844837,-0.01453277375549078,-0.01433483324944973,-0.014313722029328346,-0.014202266000211239,-0.014192530885338783,-0.014104485511779785,-0.013910145498812199,-0.013887706212699413,-0.01365327276289463,-0.0135458679869771,-0.013525953516364098,-0.013351056724786758,-0.013015229254961014,-0.012860200367867947,-0.012758203782141209,-0.012430315837264061,-0.012376295402646065,-0.012357103638350964,-0.012273447588086128,-0.012265891768038273,-0.012178976088762283,-0.012051885016262531,-0.011993694119155407,-0.011978927068412304,-0.011890619061887264,-0.011767628602683544,-0.011597110889852047,-0.011512411758303642,-0.011407993733882904,-0.011391948908567429,-0.011391300708055496,-0.011221271008253098,-0.011187386699020863,-0.011176353320479393,-0.011167912743985653,-0.011066529899835587,-0.011019275523722172,-0.010973802767693996,-0.01091190055012703,-0.010791834443807602,-0.010779538191854954,-0.01064921636134386,-0.010522924363613129,-0.010492307133972645,-0.010382970795035362,-0.010157432407140732,-0.010100305080413818,-0.009952987544238567,-0.009935719892382622,-0.009757496416568756,-0.009672761894762516,-0.009671522304415703,-0.009660639800131321,-0.009634685702621937,-0.009609555825591087,-0.009454513899981976,-0.009438442066311836,-0.00942761730402708,-0.009426907636225224,-0.009398721158504486,-0.009346600621938705,-0.009302647784352303,-0.009288865141570568,-0.009276708588004112,-0.009078321047127247,-0.00893581286072731,-0.008907144889235497,-0.008877113461494446,-0.00886852853000164,-0.00881347805261612,-0.008753576315939426,-0.008710638619959354,-0.0085263317450881,-0.008467831648886204,-0.008310463279485703,-0.008217467926442623,-0.008205194026231766,-0.008204561658203602,-0.008146807551383972,-0.008077990263700485,-0.00804898701608181,-0.00796625204384327,-0.007937407121062279,-0.007906891405582428,-0.00785905309021473,-0.007825419306755066,-0.007812858559191227,-0.007736357860267162,-0.007633538451045752,-0.007622491102665663,-0.007612063083797693,-0.007603573612868786,-0.007597265765070915,-0.0075313616544008255,-0.007501769810914993,-0.007381639908999205,-0.007381458766758442,-0.007310537155717611,-0.007301654201000929,-0.007235429715365171,-0.007197962608188391,-0.007166815921664238,-0.007098247297108173,-0.007091113831847906,-0.007048864848911762,-0.00701629463583231,-0.007008659187704325,-0.007005415391176939,-0.006996983662247658,-0.006961314007639885,-0.006950326729565859,-0.006940916180610657,-0.006933522876352072,-0.006917329505085945,-0.006894062738865614,-0.006858461070805788,-0.006809391546994448,-0.006470543798059225,-0.006452323403209448,-0.006451908964663744,-0.0064473748207092285,-0.006421285681426525,-0.006417437922209501,-0.006381675601005554,-0.006372378673404455,-0.006334556266665459,-0.006321310997009277,-0.006302548106759787,-0.00629060622304678,-0.006290363147854805,-0.0061904918402433395,-0.006165921222418547,-0.0061658574268221855,-0.006134133320301771,-0.006132419221103191,-0.006107687950134277,-0.006094740703701973,-0.0060864840634167194,-0.006083821877837181,-0.006080787628889084,-0.006065302528440952,-0.006053721532225609,-0.006045714020729065,-0.006018171086907387,-0.00600632606074214,-0.005997374188154936,-0.005953290034085512,-0.005921392235904932,-0.005915310233831406,-0.0059079499915242195,-0.005859840661287308,-0.005857252050191164,-0.005812549963593483,-0.0057409945875406265,-0.005732970777899027,-0.005731329787522554,-0.005702590569853783,-0.0056623611599206924,-0.005658363923430443,-0.005571836140006781,-0.005559384357184172,-0.005530904978513718,-0.005513761192560196,-0.00550390500575304,-0.005493738688528538,-0.005489027593284845,-0.005464036483317614,-0.0054468195885419846,-0.005421621724963188,-0.005373192951083183,-0.005364309065043926,-0.005353014450520277,-0.0053512537851929665,-0.0052068959921598434,-0.005184803158044815,-0.005164653528481722,-0.005163431633263826,-0.005156225524842739,-0.0051493216305971146,-0.005148295313119888,-0.005130991339683533,-0.005066630896180868,-0.005064133554697037,-0.005002593621611595,-0.004950770176947117,-0.004905936773866415,-0.0048862118273973465,-0.004856886807829142,-0.004814242944121361,-0.004811285529285669,-0.004803359974175692,-0.004684937186539173,-0.004681606311351061,-0.004671528935432434,-0.00466673681512475,-0.004661606624722481,-0.00464250985532999,-0.0045914542861282825,-0.004583556670695543,-0.004514902364462614,-0.004482697229832411,-0.004480865318328142,-0.004472702741622925,-0.004471063148230314,-0.0044312067329883575,-0.004392217844724655,-0.004392113536596298,-0.004392049740999937,-0.00432794401422143,-0.0042898328974843025,-0.004248030949383974,-0.0042092688381671906,-0.004165283869951963,-0.004121965728700161,-0.004105576779693365,-0.00409657321870327,-0.0040628258138895035,-0.004055403638631105,-0.004039581399410963,-0.004039007239043713,-0.004032016731798649,-0.004026930779218674,-0.00401703454554081,-0.004015900660306215,-0.004001891706138849,-0.003929076716303825,-0.003912684507668018,-0.003889263840392232,-0.0038829806726425886,-0.0038744837511330843,-0.0038692904636263847,-0.0038187671452760696,-0.0038130718749016523,-0.0038047402631491423,-0.0037975155282765627,-0.0037898689042776823,-0.0037760273553431034,-0.003771055955439806,-0.0037531275302171707,-0.0037364524323493242,-0.003691994585096836,-0.0036722528748214245,-0.0036576581187546253,-0.003615261986851692,-0.003579860320314765,-0.003571608569473028,-0.0035671540535986423,-0.0035105966962873936,-0.0034764485899358988,-0.0034526956733316183,-0.0034381174482405186,-0.003429149277508259,-0.003416025545448065,-0.0033982456661760807,-0.0033948253840208054,-0.003362378804013133,-0.003344668773934245,-0.0033236253075301647,-0.0032447203993797302,-0.0032386290840804577,-0.0032152654603123665,-0.003200923791155219,-0.00320064602419734,-0.0031867630314081907,-0.0031821071170270443,-0.003167490940541029,-0.0031674036290496588,-0.0031543683726340532,-0.003142501227557659,-0.0031328725162893534,-0.003132488811388612,-0.0031292587518692017,-0.0031284200958907604,-0.003099624766036868,-0.003076614113524556,-0.003075710032135248,-0.003072564024478197,-0.0030600978061556816,-0.0030517654959112406,-0.0030457312241196632,-0.0030419041868299246,-0.0030301488004624844,-0.003024243051186204,-0.003023684024810791,-0.00301882391795516,-0.0030012226197868586,-0.0029718203004449606,-0.002920096507295966,-0.0028832328971475363,-0.0028663992416113615,-0.002866296097636223,-0.002857874147593975,-0.0028383261524140835,-0.0028343701269477606,-0.002830049954354763,-0.0028254904318600893,-0.0028207493014633656,-0.0027920000720769167,-0.0027871597558259964,-0.0027772807516157627,-0.0027664920780807734,-0.002764376113191247,-0.002762595424428582,-0.0027272317092865705,-0.00272544682957232,-0.002717561786994338,-0.0027009081095457077,-0.002698054537177086,-0.0026928025763481855,-0.0026891857851296663,-0.0026682550087571144,-0.002660476602613926,-0.0026523899286985397,-0.0026460408698767424,-0.0026312328409403563,-0.0025985355023294687,-0.0025876439176499844,-0.002583929803222418,-0.0025324744638055563,-0.002504727803170681,-0.0024936639238148928,-0.002488937461748719,-0.0024673203006386757,-0.002448493614792824,-0.002448430983349681,-0.0024406258016824722,-0.0024354243651032448,-0.0024323193356394768,-0.002431698376312852,-0.0024185162037611008,-0.0024084211327135563,-0.0024036262184381485,-0.0024006858002394438,-0.002398180076852441,-0.0023875392507761717,-0.002383478684350848,-0.0023820737842470407,-0.0023814074229449034,-0.002348781330510974,-0.0023332855198532343,-0.0023312775883823633,-0.002325264969840646,-0.0023172441869974136,-0.0023141340352594852,-0.002311859978362918,-0.00229916675016284,-0.0022958170156925917,-0.002289262367412448,-0.0022834211122244596,-0.0022682291455566883,-0.0022623662371188402,-0.002260196255519986,-0.002257219748571515,-0.002247947035357356,-0.002239017281681299,-0.0022370312362909317,-0.0022348472848534584,-0.0022036966402083635,-0.002199803479015827,-0.0021930045913904905,-0.00219170399941504,-0.0021843435242772102,-0.0021773134358227253,-0.0021700712386518717,-0.00216787401586771,-0.0021555230487138033,-0.002151935361325741,-0.0021449383348226547,-0.0021418596152216196,-0.002135688904672861,-0.002118248026818037,-0.0021152456756681204,-0.0021096665877848864,-0.0021094861440360546,-0.00210078083910048,-0.0020865765400230885,-0.0020841946825385094,-0.002079552970826626,-0.0020673885010182858,-0.0020538719836622477,-0.0020437531638890505,-0.0020216675475239754,-0.0020186419133096933,-0.0020171874202787876,-0.002009986899793148,-0.0020066224969923496,-0.0020035156048834324,-0.0019870572723448277,-0.001986025832593441,-0.00198410009033978,-0.001973906299099326,-0.0019725009333342314,-0.0019599602092057467,-0.0019188072765246034,-0.0019127640407532454,-0.0018981813918799162,-0.0018875566311180592,-0.001884029945358634,-0.0018812312046065927,-0.00185893673915416,-0.0018538731383159757,-0.001852027722634375,-0.0018394191283732653,-0.0018263566307723522,-0.001822679303586483,-0.001821294892579317,-0.001807916909456253,-0.0018065449548885226,-0.0017787024844437838,-0.0017678161384537816,-0.0017667412757873535,-0.00176225823815912,-0.0017435523914173245,-0.0017249351367354393,-0.0017211416270583868,-0.001716962899081409,-0.0017056433716788888,-0.0016975548351183534,-0.001697509316727519,-0.0016971436562016606,-0.001668642507866025,-0.0016611393075436354,-0.0016538383206352592,-0.0016070031560957432,-0.0015997630544006824,-0.0015933971153572202,-0.0015798648819327354,-0.0015722110401839018,-0.0015642998041585088,-0.0015497717540711164,-0.0015412010252475739,-0.0015291967429220676,-0.0015165181830525398,-0.0015000530984252691,-0.0014999459963291883,-0.0014983239816501737,-0.0014964127913117409,-0.001491397270001471,-0.001471413648687303,-0.0014711132971569896,-0.0014659148873761296,-0.0014616373227909207,-0.0014479326782748103,-0.0014160961145535111,-0.0014005157863721251,-0.0013962584780529141,-0.0013937189942225814,-0.001374248880892992,-0.0013742485316470265,-0.0013680733973160386,-0.0013666569720953703,-0.0013652786146849394,-0.0013608026783913374,-0.0013578154612332582,-0.0013543512905016541,-0.0013529604766517878,-0.0013418486341834068,-0.0013409345410764217,-0.001339810318313539,-0.0013255217345431447,-0.0013220086693763733,-0.0013199120294302702,-0.0013139125658199191,-0.0013136306079104543,-0.0013042120262980461,-0.0013026692904531956,-0.0012975105782970786,-0.0012949130032211542,-0.0012948805233463645,-0.0012893673265352845,-0.0012864029267802835,-0.0012802177807316184,-0.0012695565819740295,-0.0012694359757006168,-0.0012505543418228626,-0.0012501663295552135,-0.0012400188716128469,-0.001225982210598886,-0.0012131350813433528,-0.0012087262002751231,-0.001197500852867961,-0.0011928973253816366,-0.0011743990471586585,-0.0011726138181984425,-0.0011693112319335341,-0.001164848916232586,-0.0011426863493397832,-0.0011409258004277945,-0.001122444518841803,-0.001119523891247809,-0.001118711312301457,-0.001098561566323042,-0.0010963331442326307,-0.0010940026259049773,-0.001090857433155179,-0.0010851899860426784,-0.0010846625082194805,-0.0010773439425975084,-0.0010733865201473236,-0.0010641403496265411,-0.0010621355613693595,-0.0010420535691082478,-0.0010401750914752483,-0.0010394258424639702,-0.0010200716787949204,-0.0010164629202336073,-0.0010151277529075742,-0.0010097390040755272,-0.0009992639534175396,-0.0009969203965738416,-0.0009968209778890014,-0.000996640883386135,-0.0009732645121403039,-0.0009682430536486208,-0.000958736811298877,-0.0009528862428851426,-0.0009443312883377075,-0.0009437934495508671,-0.0009365366422571242,-0.0009361215634271502,-0.0009275775519199669,-0.0009249672293663025,-0.000917556113563478,-0.0009106865618377924,-0.0009084308403544128,-0.0009045241167768836,-0.0009021043078973889,-0.0009018738055601716,-0.000881024228874594,-0.0008668449590913951,-0.0008649348746985197,-0.000861447595525533,-0.0008469606982544065,-0.0008453888585790992,-0.0008448732551187277,-0.0008386809495277703,-0.0008292842539958656,-0.0008268718956969678,-0.0008186075137928128,-0.0008174463291652501,-0.000816420535556972,-0.0008151977672241628,-0.0008107235771603882,-0.0008010601159185171,-0.0007988422876223922,-0.0007944763638079166,-0.000789230631198734,-0.0007835889118723571,-0.000782875285949558,-0.0007811295799911022,-0.0007807853980921209,-0.0007621584227308631,-0.0007599717355333269,-0.0007555771735496819,-0.0007550149457529187,-0.0007513387245126069,-0.000748219492379576,-0.0007388932863250375,-0.0007349087973125279,-0.0007317298441193998,-0.0007272895891219378,-0.000723106088116765,-0.0007169946329668164,-0.0007144228438846767,-0.0007142738904803991,-0.0007127586286514997,-0.0007127221906557679,-0.0007109686266630888,-0.000710067804902792,-0.000708308769389987,-0.0007050431449897587,-0.0007027837564237416,-0.0007002332713454962,-0.0006986545631662011,-0.0006959819002076983,-0.0006889564683660865,-0.0006886898190714419,-0.0006881478475406766,-0.00068560556974262,-0.0006755589856766164,-0.0006752543849870563,-0.0006722788675688207,-0.0006694005569443107,-0.0006685589905828238,-0.0006662089144811034,-0.0006655108882114291,-0.0006625662208534777,-0.0006618391489610076,-0.0006590797565877438,-0.0006490905652754009,-0.0006420385907404125,-0.0006371379713527858,-0.0006335716461762786,-0.0006078356527723372,-0.0006010608631186187,-0.0005998483975417912,-0.0005980161367915571,-0.0005962943541817367,-0.0005946962628513575,-0.0005902854609303176,-0.0005876033683307469,-0.0005850482266396284,-0.0005824182298965752,-0.0005623770412057638,-0.0005620213341899216,-0.0005520385457202792,-0.000547794799786061,-0.0005470804171636701,-0.000540498411282897,-0.0005404547555372119,-0.0005329650593921542,-0.0005315656308084726,-0.0005296692834235728,-0.0005053793429397047,-0.0005045607103966177,-0.0005033950437791646,-0.0005013035261072218,-0.000499070156365633,-0.0004894019803032279,-0.0004759158182423562,-0.0004689279303420335,-0.00046550811384804547,-0.0004639150865841657,-0.00045956968097016215,-0.0004588421434164047,-0.00045860320096835494,-0.00045744457747787237,-0.00044608392636291683,-0.0004440174961928278,-0.0004361726460047066,-0.00043304628343321383,-0.00042550996295176446,-0.0004245572490617633,-0.0004228235920891166,-0.000419190910179168,-0.00041798330494202673,-0.00041282951133325696,-0.00039584882324561477,-0.0003958208253607154,-0.00039255915908142924,-0.00038798421155661345,-0.0003869984357152134,-0.00038484096876345575,-0.000380261626560241,-0.00037846690975129604,-0.0003780725528486073,-0.00037653109757229686,-0.00037469452945515513,-0.0003697770880535245,-0.0003622584918048233,-0.0003546554653439671,-0.0003467255737632513,-0.0003464370092842728,-0.0003452638629823923,-0.0003449917712714523,-0.0003403442387934774,-0.000332330702804029,-0.00033054203959181905,-0.00033039270783774555,-0.0003282433026470244,-0.00032237908453680575,-0.00032200090936385095,-0.00031737805693410337,-0.000316298013785854,-0.0003156934690196067,-0.00031472311820834875,-0.0003085696080233902,-0.00030443869763985276,-0.0002999688731506467,-0.00029866761178709567,-0.0002935673401225358,-0.0002919688122346997,-0.0002903114364016801,-0.00028975435998290777,-0.0002878148225136101,-0.00028059951728209853,-0.0002791353326756507,-0.00027727216365747154,-0.0002724648220464587,-0.00027087493799626827,-0.00026767945382744074,-0.0002665139618329704,-0.0002646022185217589,-0.00026436313055455685,-0.0002590271760709584,-0.00025033659767359495,-0.0002490721526555717,-0.00024708674754947424,-0.0002377583150519058,-0.00023743577185086906,-0.0002319298655493185,-0.0002308674156665802,-0.00022336773690767586,-0.0002193159598391503,-0.00021750933956354856,-0.00021467290935106575,-0.00021242529328446835,-0.00020898260117974132,-0.0002087386674247682,-0.0002084759616991505,-0.00020616769324988127,-0.00020275019051041454,-0.0001982420653803274,-0.0001973022590391338,-0.0001963396352948621,-0.00019630319729913026,-0.00019492991850711405,-0.00019355013500899076,-0.00019000686006620526,-0.0001857674797065556,-0.0001799273450160399,-0.00017988398030865937,-0.00017658114666119218,-0.00017025858687702566,-0.00016067444812506437,-0.0001605585275683552,-0.00016049653640948236,-0.00015960134624037892,-0.0001586727739777416,-0.00015678077761549503,-0.0001548534637549892,-0.00015313825861085206,-0.0001529357541585341,-0.00015216485189739615,-0.0001515888434369117,-0.00015150003309827298,-0.00014980278501752764,-0.00014793648733757436,-0.00014776669559068978,-0.00014437391655519605,-0.0001412254641763866,-0.00014104627189226449,-0.00013436071458272636,-0.00013148211291991174,-0.0001306156045757234,-0.0001292043161811307,-0.00012762397818733007,-0.00012654803867917508,-0.00012513682304415852,-0.00012231245636940002,-0.0001172792908619158,-0.00011633679241640493,-0.0001161786203738302,-0.00011517703387653455,-0.00011500946857267991,-0.00011454768537078053,-0.00011399857612559572,-0.00011125594028271735,-0.00010794267291203141,-0.00010790400119731203,-0.00010501407086849213,-0.00010472319991094992,-0.00010445952648296952,-0.00010291382932337001,-0.00010239995754091069,-0.00010089307761518285,-9.98659452307038e-05,-9.469956421526149e-05,-8.996501856017858e-05,-8.729875116841868e-05,-8.638076542411e-05,-8.49676871439442e-05,-8.449554297840223e-05,-8.378155325772241e-05,-8.266150689451024e-05,-8.03731381893158e-05,-7.849738176446408e-05,-7.817976438673213e-05,-7.775433186907321e-05,-7.413447019644082e-05,-7.333100074902177e-05,-7.039971387712285e-05,-6.948634836589918e-05,-6.797484820708632e-05,-6.751537148375064e-05,-6.711945025017485e-05,-6.70263179927133e-05,-6.650060095125809e-05,-6.479590956587344e-05,-6.435275281546637e-05,-6.232172017917037e-05,-5.417399006546475e-05,-4.677765173255466e-05,-4.599586100084707e-05,-4.568830263451673e-05,-4.432872083270922e-05,-4.340007944847457e-05,-4.2657778976717964e-05,-4.154831185587682e-05,-4.146680294070393e-05,-3.988802563981153e-05,-3.731399920070544e-05,-3.235637996112928e-05,-3.116406514891423e-05,-2.7106478228233755e-05,-2.6248917492921464e-05,-2.5958717742469162e-05,-2.5501773052383214e-05,-2.487987330823671e-05,-2.1889507479500026e-05,-1.9949675333919004e-05,-1.8392949641565792e-05,-1.783832885848824e-05,-1.7601772924535908e-05,-1.742780295899138e-05,-1.712135963316541e-05,-1.669436642259825e-05,-1.665622039581649e-05,-1.5493407772737555e-05,-1.4774575902265497e-05,-1.3360753655433655e-05,-1.2986660294700414e-05,-1.2872219485871028e-05,-1.1712982995959464e-05,-1.0832324733200949e-05,-1.0639429092407227e-05,-1.0420829312351998e-05,-9.864643288892694e-06,-7.684975571464747e-06,-6.404071882570861e-06,-6.08161099080462e-06,-4.5965612116560806e-06,-4.284307578927837e-06,-2.3536383650935022e-06,-2.341717390663689e-06,-1.743212351357215e-06,-2.2694467816108954e-07,-4.2915345943583816e-08,-6.1839817711017986e-09,2.8312206268310547e-07,1.4309584912552964e-06,2.2032857032172615e-06,2.4601818040537182e-06,2.576187171143829e-06,3.311932005090057e-06,5.202442480367608e-06,5.362182946555549e-06,7.148459644668037e-06,8.763372534303926e-06,9.128749297815375e-06,9.356662303616758e-06,9.610876077204011e-06,1.0886192285397556e-05,1.1312588867440354e-05,1.2054443686793093e-05,1.2355595572444145e-05,1.4707371519762091e-05,1.5981644537532702e-05,1.62523992912611e-05,1.652590981393587e-05,1.7856507838587277e-05,1.888476253952831e-05,1.998022162297275e-05,2.2090300262789242e-05,2.2145211914903484e-05,2.3515074644819833e-05,2.4193823264795355e-05,2.420812779746484e-05,2.5237352019757964e-05,2.5501474738121033e-05,2.6406794859212823e-05,2.822607712005265e-05,3.078028385061771e-05,3.0979143048170954e-05,3.136880695819855e-05,3.174483936163597e-05,3.285534694441594e-05,3.307379665784538e-05,3.549873872543685e-05,3.9883030694909394e-05,4.651263225241564e-05,4.88016739836894e-05,5.320191485225223e-05,5.5339336540782824e-05,5.5640339269302785e-05,5.766652611782774e-05,5.796827463200316e-05,5.925558434682898e-05,5.931086707278155e-05,5.950890408712439e-05,6.0051828768337145e-05,6.287589349085465e-05,6.39830541331321e-05,6.563819624716416e-05,7.01682292856276e-05,7.03722980688326e-05,7.13332774466835e-05,7.151164027163759e-05,7.471427670679986e-05,7.572017784696072e-05,7.611892942804843e-05,7.668934995308518e-05,8.065447036642581e-05,8.074306242633611e-05,8.197799616027623e-05,8.348681149072945e-05,8.470587636111304e-05,8.491068729199469e-05,8.900761895347387e-05,9.016119292937219e-05,9.44966814131476e-05,9.479656728217378e-05,9.5738323580008e-05,9.585067891748622e-05,9.595759183866903e-05,9.69625252764672e-05,9.742721886141226e-05,9.790204785531387e-05,9.954311099136248e-05,0.00010095797188114375,0.00010145120177185163,0.00010150969319511205,0.00010215789370704442,0.00010605066927382722,0.00010687947360565886,0.00011021599493687972,0.00011524811270646751,0.00011763795919250697,0.00011881068348884583,0.00012069992953911424,0.0001226241874974221,0.0001248084008693695,0.00012532681284938008,0.00012565850920509547,0.00012642599176615477,0.00012842350406572223,0.00012885205796919763,0.0001308572245761752,0.00013137131463736296,0.0001349101949017495,0.00013744465832132846,0.0001405331422574818,0.00014228619693312794,0.0001457238249713555,0.00014704004570376128,0.00014801420911680907,0.00015039801655802876,0.0001522435195511207,0.00015389159671030939,0.00015440821880474687,0.00015973247354850173,0.00016277939721476287,0.00016560553922317922,0.00016620740643702447,0.0001663637231104076,0.00016672782658133656,0.00017126508464571089,0.00017772003775462508,0.00017793364531826228,0.0001837281888583675,0.0001837290037656203,0.00019201904069632292,0.00019271523342467844,0.0001962052338058129,0.00019890158728230745,0.00020049110753461719,0.0002039797545876354,0.0002042913401965052,0.00020908325677737594,0.0002101441496051848,0.00021696224575862288,0.00021959946025162935,0.00022027865634299815,0.00022403948241844773,0.000224182236706838,0.00022792078380007297,0.00022885396901983768,0.00023035184131003916,0.00023161344870459288,0.00023421540390700102,0.00023433298338204622,0.0002367920387769118,0.0002439991367282346,0.0002447043370921165,0.00024690382997505367,0.0002482026757206768,0.0002492681087460369,0.00025663396809250116,0.00025957354228012264,0.0002628832298796624,0.0002673029957804829,0.0002683402562979609,0.0002697989984881133,0.00028170578298158944,0.00028471171390265226,0.0002850042947102338,0.0002870875468943268,0.0002878130180761218,0.0002885413123294711,0.0002915389777626842,0.0003003155579790473,0.0003015839320141822,0.0003059830632992089,0.00030835269717499614,0.0003108703240286559,0.0003131341945845634,0.00031673707417212427,0.00031790538923814893,0.0003198186168447137,0.00032612046925351024,0.0003304939018562436,0.00033202371560037136,0.00033473505754955113,0.00034037395380437374,0.00034060238976962864,0.00034713363857008517,0.0003487756766844541,0.0003490913659334183,0.00035152456257492304,0.00035322821349836886,0.00035413503064773977,0.0003550270339474082,0.00035944179398939013,0.00035986199509352446,0.0003664365503937006,0.00036651297705248,0.0003670686564873904,0.00036774479667656124,0.000368258886737749,0.00036826246650889516,0.00037106149829924107,0.00037115244776941836,0.0003818254917860031,0.00038867906550876796,0.0003933383559342474,0.0003965491196140647,0.0003979061439167708,0.0004113497561775148,0.0004126272979192436,0.0004148569714743644,0.0004177372029516846,0.00042289571138098836,0.000427478167694062,0.00042903819121420383,0.00043736674706451595,0.0004425386432558298,0.0004465965903364122,0.00045510558993555605,0.00046016843407414854,0.0004622657725121826,0.0004639156104531139,0.0004677475953940302,0.00048006942961364985,0.0004822938935831189,0.00048383616376668215,0.0004853287246078253,0.00048570765648037195,0.0004864860966335982,0.00048772088484838605,0.0004901195061393082,0.0004909940762445331,0.0004932323936372995,0.0004941693041473627,0.0004984081606380641,0.0005018439842388034,0.0005065739387646317,0.0005066312151029706,0.0005155776161700487,0.0005190520896576345,0.000519371242262423,0.000524256145581603,0.0005312266875989735,0.0005315382150001824,0.0005406680866144598,0.0005411786842159927,0.0005481024272739887,0.0005484462599270046,0.0005499754333868623,0.0005531233036890626,0.0005576022085733712,0.0005579449934884906,0.0005608764477074146,0.0005669269594363868,0.0005689272074960172,0.0005882846307940781,0.0005900693940930068,0.0005922137061133981,0.0005951167549937963,0.0005960579728707671,0.0006063736509531736,0.0006093161646276712,0.0006183702498674393,0.0006249143043532968,0.0006342084961943328,0.0006372544448822737,0.0006401310674846172,0.0006533463019877672,0.0006595627055503428,0.0006648069829680026,0.0006674901233054698,0.000677791831549257,0.0006843509036116302,0.0006948671652935445,0.0007041040807962418,0.0007064826204441488,0.0007121525704860687,0.0007133998442441225,0.0007343140314333141,0.0007404281059280038,0.0007404597708955407,0.0007404711213894188,0.0007425700314342976,0.0007471420685760677,0.0007479783380404115,0.000762858078815043,0.0007645604782737792,0.0007650328916497529,0.0007755253463983536,0.0007755588740110397,0.0007803738117218018,0.0007833876297809184,0.0007868835818953812,0.0007889063563197851,0.0007908164989203215,0.0008014124468900263,0.0008021563407965004,0.0008063411223702133,0.0008098089601844549,0.0008224837947636843,0.0008238201844505966,0.0008258529123850167,0.000837277271784842,0.0008384592365473509,0.0008390088914893568,0.0008395619806833565,0.0008500287658534944,0.0008533734362572432,0.0008572033839300275,0.0008597637643106282,0.0008627402712590992,0.0008760418859310448,0.0008793563465587795,0.0008806238183751702,0.0008839957299642265,0.0008843920659273863,0.0008857380598783493,0.0008871768950484693,0.0008890117169357836,0.0009015537216328084,0.0009070017840713263,0.0009238403872586787,0.000940050755161792,0.0009419710841029882,0.0009610297274775803,0.0009920734446495771,0.000998609815724194,0.001005525584332645,0.0010072229197248816,0.001009167404845357,0.0010097887134179473,0.001010311534628272,0.0010142283281311393,0.0010230691405013204,0.001027531223371625,0.0010282737202942371,0.0010294165695086122,0.001044846256263554,0.0010500620119273663,0.0010522084776312113,0.0010579554364085197,0.001059207133948803,0.0010615077335387468,0.00106511649210006,0.0010745511390268803,0.0010748034110292792,0.001080766087397933,0.0010926241520792246,0.001094335806556046,0.0010955287143588066,0.0011075204238295555,0.0011238167062401772,0.0011276031145825982,0.001135191647335887,0.0011352627770975232,0.0011420175433158875,0.0011425482807680964,0.001147035975009203,0.001152056036517024,0.0011544262524694204,0.0011616549454629421,0.0011622337624430656,0.0011666989885270596,0.0011928154854103923,0.0011946770828217268,0.0011996596585959196,0.001218487392179668,0.0012198894983157516,0.0012202630750834942,0.0012335372157394886,0.0012342632981017232,0.0012355867074802518,0.0012494437396526337,0.001250917324796319,0.0012530749663710594,0.0012624385999515653,0.001263078418560326,0.0012665486428886652,0.0012758910888805985,0.001299439463764429,0.0013014876749366522,0.0013033986324444413,0.0013039560290053487,0.0013600758975371718,0.001362674287520349,0.001384125673212111,0.001404959592036903,0.0014068895252421498,0.0014078376116231084,0.001416490413248539,0.0014237528666853905,0.0014276888687163591,0.0014284361386671662,0.0014345936942845583,0.0014364987146109343,0.0014398362254723907,0.0014407970011234283,0.0014458696823567152,0.0014548583421856165,0.0014639000874012709,0.0014648609794676304,0.0014689188683405519,0.0014833445893600583,0.001484654494561255,0.0014911899343132973,0.0014971941709518433,0.0015122152399271727,0.001538580167107284,0.0015411102212965488,0.0015438040718436241,0.0015496553387492895,0.0015507322968915105,0.0015569531824439764,0.0015671530272811651,0.0015893550589680672,0.0015936827985569835,0.0015966672217473388,0.0016009450191631913,0.0016044575022533536,0.001628831960260868,0.001636118278838694,0.0016405907226726413,0.0016407775692641735,0.001640809583477676,0.0016417837468907237,0.0016531860455870628,0.001658931840211153,0.0016700169071555138,0.0016753579257056117,0.0016777687706053257,0.0016913223080337048,0.0016961279325187206,0.0016968321288004518,0.001701383269391954,0.0017015330959111452,0.0017139731207862496,0.0017158580012619495,0.0017183775780722499,0.0017223325558006763,0.001724062254652381,0.0017385412938892841,0.0017403578385710716,0.0017516853986307979,0.001758789410814643,0.0017653763061389327,0.0017712776316329837,0.0017793572042137384,0.0017817444168031216,0.0017906352877616882,0.0017923972336575389,0.001803482649847865,0.0018086291383951902,0.0018128068186342716,0.0018355813808739185,0.0018420409178361297,0.0018516601994633675,0.0018600501352921128,0.001876441529020667,0.0018863242585211992,0.0018864428857341409,0.0018879830604419112,0.0018954735714942217,0.0019030696712434292,0.001906431745737791,0.0019127881387248635,0.0019322739681228995,0.001961678499355912,0.0019685153383761644,0.001970637822523713,0.0019922645296901464,0.0019981423392891884,0.002005286980420351,0.0020063489209860563,0.002007655566558242,0.0020080094691365957,0.0020091934129595757,0.0020141289569437504,0.0020148390904068947,0.0020200484432280064,0.0020493543706834316,0.002062269253656268,0.0020642997696995735,0.0020644732285290956,0.00207472313195467,0.0020798915065824986,0.0020812125876545906,0.0020902701653540134,0.002094109309837222,0.0021028912160545588,0.002118391916155815,0.0021185835357755423,0.002122666221112013,0.0021305035334080458,0.002134159905835986,0.0021357405930757523,0.0021888259798288345,0.0022035420406609774,0.002219981048256159,0.002228855388239026,0.00223425030708313,0.0022376645356416702,0.002246587071567774,0.002247296739369631,0.00227105850353837,0.002276328159496188,0.0022773239761590958,0.002280269283801317,0.0022851922549307346,0.0022869252134114504,0.0022891294211149216,0.002299015875905752,0.0023021551314741373,0.002304431516677141,0.0023166353348642588,0.0023266756907105446,0.0023452769964933395,0.0023455952759832144,0.0023551660124212503,0.0023610317148268223,0.0023752469569444656,0.002377080265432596,0.0023917495273053646,0.002396180760115385,0.0024097170680761337,0.002427751664072275,0.0024432556238025427,0.002446067286655307,0.002451003063470125,0.002464314689859748,0.002468607621267438,0.002494034357368946,0.0025046628434211016,0.002521274611353874,0.002530703553929925,0.002535404171794653,0.0025525952223688364,0.002563926624134183,0.0025950020644813776,0.0026263254694640636,0.002634369069710374,0.002648866269737482,0.002659211400896311,0.002669679932296276,0.002680335659533739,0.0027084797620773315,0.0027270084246993065,0.0027558086439967155,0.0027629851829260588,0.002769770100712776,0.002781145041808486,0.0027812886983156204,0.0027853739447891712,0.002792651066556573,0.002795241540297866,0.002806109143421054,0.0028346972540020943,0.0028357666451483965,0.0028570734430104494,0.002888698363676667,0.002947261556982994,0.002953840419650078,0.0029576134402304888,0.0029731637332588434,0.0029790159314870834,0.002980358200147748,0.0029932863544672728,0.0030129575170576572,0.00301490630954504,0.0030762087553739548,0.003081743372604251,0.0030927935149520636,0.0031112348660826683,0.003125229384750128,0.003133724443614483,0.0031526421662420034,0.003172866068780422,0.0031730991322547197,0.003182868706062436,0.0032100945245474577,0.003216888289898634,0.0032345070503652096,0.0032353466376662254,0.0032435113098472357,0.003294385736808181,0.0033061683643609285,0.0033440678380429745,0.003344736760482192,0.0034036848228424788,0.0034159375354647636,0.0034237001091241837,0.0034406278282403946,0.0034639001823961735,0.0034771254286170006,0.003527481807395816,0.0035479252692312002,0.003548112465068698,0.003551457542926073,0.0035518293734639883,0.0035697310231626034,0.003612547880038619,0.003624711651355028,0.003649597056210041,0.0036975375842303038,0.0037079041358083487,0.0037167267873883247,0.0037386275362223387,0.003766692942008376,0.0038051314186304808,0.0038372052367776632,0.0038410001434385777,0.0038559180684387684,0.0038812789134681225,0.0039376141503453255,0.003951034974306822,0.003968516364693642,0.003979180008172989,0.00399747584015131,0.004004286136478186,0.004067194648087025,0.00413721427321434,0.004188147373497486,0.0042105852626264095,0.00422496534883976,0.004365070257335901,0.00438872491940856,0.004400445614010096,0.004440832883119583,0.00445720786228776,0.00447935052216053,0.004504484124481678,0.00453442707657814,0.004545813892036676,0.0045532588846981525,0.004587189760059118,0.004600128158926964,0.004606494214385748,0.004612734541296959,0.004648531787097454,0.004662558902055025,0.004674870520830154,0.004697722382843494,0.004706076346337795,0.004720853175967932,0.004742409102618694,0.004742590244859457,0.0047569056041538715,0.004758491646498442,0.004830439109355211,0.0048563797026872635,0.0048578898422420025,0.004877116531133652,0.004880947060883045,0.004884033463895321,0.004889033734798431,0.004922464955598116,0.004934522323310375,0.004944846499711275,0.00495119160041213,0.005021481309086084,0.005053885746747255,0.005141516216099262,0.005146739538758993,0.005173338111490011,0.005205673631280661,0.005213229451328516,0.005220280960202217,0.0052259499207139015,0.005276854149997234,0.005326703656464815,0.0053611742332577705,0.005374969448894262,0.005451515316963196,0.005483880639076233,0.005499604158103466,0.005544736981391907,0.00555382901802659,0.005582868587225676,0.005603710189461708,0.005603853613138199,0.005624137353152037,0.005645576864480972,0.005652517545968294,0.005670017562806606,0.005677966866642237,0.005696093663573265,0.005722446832805872,0.0057229455560445786,0.005733971484005451,0.00574082275852561,0.005795123055577278,0.0058061229065060616,0.005869891028851271,0.005961266811937094,0.0059772515669465065,0.0060067735612392426,0.0060425265692174435,0.006087145768105984,0.0060976650565862656,0.00611932622268796,0.006123350467532873,0.00616997666656971,0.006218897178769112,0.006235312204807997,0.006235974375158548,0.006238092668354511,0.00624091038480401,0.006258498411625624,0.006263939663767815,0.0062888567335903645,0.006329651921987534,0.006342269945889711,0.006355238147079945,0.006370772607624531,0.0063719237223267555,0.006375149358063936,0.006376537494361401,0.006386500783264637,0.006412046030163765,0.006412889808416367,0.006470815744251013,0.006541022099554539,0.006581962574273348,0.006594586186110973,0.006633288227021694,0.006663089618086815,0.006675838027149439,0.006677701603621244,0.006679164245724678,0.006692560855299234,0.0066999043338000774,0.006704829167574644,0.0067083812318742275,0.006762655917555094,0.006886216811835766,0.006986293010413647,0.0069888923317193985,0.006992171984165907,0.007076808251440525,0.007103262469172478,0.007122275419533253,0.007159858476370573,0.007208108901977539,0.00721606845036149,0.007318216376006603,0.007374053820967674,0.007415322586894035,0.007464347407221794,0.007492046803236008,0.007539490237832069,0.007579067721962929,0.007613074034452438,0.007640373893082142,0.007660592906177044,0.00767834298312664,0.007678959518671036,0.0076901973225176334,0.007701234892010689,0.007715052925050259,0.007732210215181112,0.007747637573629618,0.007769959047436714,0.00797250121831894,0.007974926382303238,0.008004896342754364,0.008012416772544384,0.008120033890008926,0.008208803832530975,0.008223898708820343,0.008280996233224869,0.008285093121230602,0.008341312408447266,0.00842740386724472,0.008583778515458107,0.008761722594499588,0.008920368738472462,0.008988777175545692,0.009075508452951908,0.009130815975368023,0.009137693792581558,0.009188633412122726,0.009203429333865643,0.009203550405800343,0.009208152070641518,0.009234374389052391,0.00924111157655716,0.009249323979020119,0.009399917908012867,0.009510865435004234,0.009572918526828289,0.009591630659997463,0.009609737433493137,0.009704475291073322,0.009709768928587437,0.009716682136058807,0.009788937866687775,0.009851044043898582,0.00985350739210844,0.009860473684966564,0.00992086622864008,0.010121772065758705,0.01015649363398552,0.010162727907299995,0.010169696062803268,0.010267894715070724,0.010298719629645348,0.01033647172152996,0.010386555455625057,0.010430126450955868,0.010442840866744518,0.010509579442441463,0.010707132518291473,0.010788335464894772,0.01091012917459011,0.010924527421593666,0.01094122789800167,0.010985661298036575,0.011028986424207687,0.011034305207431316,0.011036340147256851,0.011038882657885551,0.011039185337722301,0.011058359406888485,0.011093582957983017,0.011137125082314014,0.011152653954923153,0.011179800145328045,0.01131044514477253,0.011406617239117622,0.01142969261854887,0.011467909440398216,0.011494972743093967,0.011760836467146873,0.01180775836110115,0.01185219269245863,0.012095167301595211,0.012243116274476051,0.012246173806488514,0.012281682342290878,0.012318935245275497,0.012353078462183475,0.01236750278621912,0.01254402194172144,0.012554638087749481,0.012600373476743698,0.012606451287865639,0.012622133828699589,0.012927941977977753,0.012969989329576492,0.012994940392673016,0.013051033951342106,0.013117123395204544,0.013122971169650555,0.01317183393985033,0.013217903673648834,0.013222008012235165,0.013448302634060383,0.013520295731723309,0.01354250404983759,0.013557076454162598,0.013568167574703693,0.01374535821378231,0.013777288608253002,0.014090681448578835,0.014223754405975342,0.014245306141674519,0.014383899979293346,0.014461275190114975,0.015078016556799412,0.015187489800155163,0.015214682556688786,0.01570156402885914,0.015727167949080467,0.015757465735077858,0.015944484621286392,0.01604718528687954,0.01609216257929802,0.016118528321385384,0.016474122181534767,0.01652919501066208,0.016736704856157303,0.01685701310634613,0.016888028010725975,0.016909418627619743,0.017013300210237503,0.017029309645295143,0.017207302153110504,0.017737990245223045,0.017806053161621094,0.01789429970085621,0.01796363666653633,0.01812116429209709,0.01822488009929657,0.018246565014123917,0.01831243559718132,0.01859263889491558,0.018654225394129753,0.018812811002135277,0.01882590539753437,0.019822489470243454,0.019865043461322784,0.01989801973104477,0.020045867189764977,0.020047305151820183,0.020166311413049698,0.020341716706752777,0.020374828949570656,0.020378556102514267,0.02041498012840748,0.020524783059954643,0.02082056738436222,0.02087101712822914,0.020967047661542892,0.021176114678382874,0.02122669294476509,0.021318620070815086,0.021331988275051117,0.02144288457930088,0.021707387641072273,0.02175971493124962,0.021816672757267952,0.021882886067032814,0.022100573405623436,0.022500984370708466,0.022730952128767967,0.022868480533361435,0.02322157472372055,0.023268690332770348,0.023309264332056046,0.023347029462456703,0.024177268147468567,0.02422640472650528,0.02445988915860653,0.024576691910624504,0.024608859792351723,0.024923905730247498,0.02504787966609001,0.02530285343527794,0.025380076840519905,0.02547202818095684,0.025512229651212692,0.02564028464257717,0.025913001969456673,0.026332324370741844,0.026349158957600594,0.026498565450310707,0.026583833619952202,0.027956143021583557,0.028300534933805466,0.028613518923521042,0.029311956837773323,0.0294084120541811,0.029422922059893608,0.029431989416480064,0.029755137860774994,0.02984245866537094,0.03029693104326725,0.03038068115711212,0.030387401580810547,0.030426951125264168,0.03085048869252205,0.0310850627720356,0.03132224082946777,0.032072409987449646,0.032544005662202835,0.03258161246776581,0.03306293487548828,0.033469174057245255,0.03350135311484337,0.03427250310778618,0.034627728164196014,0.03525895997881889,0.03647946938872337,0.03683827072381973,0.03744146227836609,0.037770748138427734,0.03881658986210823,0.03885207325220108,0.039270225912332535,0.039731062948703766,0.040463510900735855,0.04054292291402817,0.040943242609500885,0.04118622466921806,0.04174002632498741,0.041896771639585495,0.042975831776857376,0.043388091027736664,0.0434134304523468,0.043514080345630646,0.04372256621718407,0.043830327689647675,0.04566694796085358,0.045748405158519745,0.04600383713841438,0.0460103303194046,0.04642457887530327,0.04692193120718002,0.04693906381726265,0.04824198782444,0.0482836477458477,0.048894863575696945,0.050216928124427795,0.051059383898973465,0.05185224115848541,0.052279625087976456,0.05327261611819267,0.054286859929561615,0.05607416480779648,0.05627235397696495,0.05640009418129921,0.05679692327976227,0.061197806149721146,0.06158849224448204,0.06290370225906372,0.06292146444320679,0.06354333460330963,0.06443168967962265,0.06710829585790634,0.06755495816469193,0.06821295619010925,0.07031872123479843,0.07090672850608826,0.07278143614530563,0.07384202629327774,0.07425091415643692,0.07523239403963089,0.07583292573690414,0.07765226066112518,0.07955625653266907,0.07966016978025436,0.08055544644594193,0.08365228772163391,0.08535828441381454,0.08631231635808945,0.08817508816719055,0.09024421870708466,0.09053413569927216,0.0922926738858223,0.09262149780988693,0.09934057295322418,0.10146328061819077,0.10251696407794952,0.11270331591367722,0.11271901428699493,0.11801978945732117,0.11970219016075134,0.12720954418182373,0.12825995683670044,0.13080687820911407,0.14127780497074127,0.14652171730995178,0.15743781626224518,0.1578630656003952,0.1595725268125534,0.16441009938716888,0.19474878907203674,0.20369964838027954,0.21209895610809326],\"yaxis\":\"y\",\"type\":\"scattergl\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Sorted neurons\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Loss change\"}},\"legend\":{\"title\":{\"text\":\"variable\"},\"tracegroupgap\":0},\"title\":{\"text\":\"Loss change from ablating MLP5 neuron\"},\"width\":800},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('91acc16a-d4ad-4f7b-9135-e46dd0c49549');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sorted_means, indices = torch.sort(diffs.mean(1))\n",
    "sorted_means = sorted_means.tolist()\n",
    "haystack_utils.line(sorted_means, xlabel=\"Sorted neurons\", ylabel=\"Loss change\", title=\"Loss change from ablating MLP5 neuron\") # xticks=indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>                            <div id=\"569485e0-bfce-438d-8b13-2e1b3166aceb\" class=\"plotly-graph-div\" style=\"height:525px; width:1000px;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"569485e0-bfce-438d-8b13-2e1b3166aceb\")) {                    Plotly.newPlot(                        \"569485e0-bfce-438d-8b13-2e1b3166aceb\",                        [{\"error_y\":{\"array\":[1.300034398855698],\"type\":\"data\",\"visible\":true},\"name\":\"Original\",\"x\":[\"Original\"],\"y\":[1.4640032747248188],\"type\":\"bar\"},{\"error_y\":{\"array\":[2.0677192060806893],\"type\":\"data\",\"visible\":true},\"name\":\"Ablated\",\"x\":[\"Ablated\"],\"y\":[3.5792385311424733],\"type\":\"bar\"},{\"error_y\":{\"array\":[1.4077217522076093],\"type\":\"data\",\"visible\":true},\"name\":\"MLP5 path patched\",\"x\":[\"MLP5 path patched\"],\"y\":[2.066986262202263],\"type\":\"bar\"},{\"error_y\":{\"array\":[1.8487889363203198],\"type\":\"data\",\"visible\":true},\"name\":\"MLP5 path patched + Top 10 MLP5 neurons ablated\",\"x\":[\"Top MLP5 removed\"],\"y\":[3.991814288198948],\"type\":\"bar\"},{\"error_y\":{\"array\":[0.7551657350013382],\"type\":\"data\",\"visible\":true},\"name\":\"MLP5 path patched + Bottom 10 MLP5 neurons ablated\",\"x\":[\"Bottom MLP5 removed\"],\"y\":[0.5595124128367752],\"type\":\"bar\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"title\":{\"text\":\"Average last token loss when removing top \\u002f bottom neurons from path patching\"},\"xaxis\":{\"title\":{\"text\":\"\"}},\"yaxis\":{\"title\":{\"text\":\"Loss\"}},\"barmode\":\"group\",\"width\":1000},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('569485e0-bfce-438d-8b13-2e1b3166aceb');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check loss change when ablating top / bottom neurons\n",
    "\n",
    "top_neurons_count = 10\n",
    "top_neurons = indices[-top_neurons_count:]\n",
    "bottom_neurons = indices[:top_neurons_count]\n",
    "\n",
    "with model.hooks(deactivate_neurons_fwd_hooks):\n",
    "    ablated_loss, ablated_cache = model.run_with_cache(prompts, return_type=\"loss\")\n",
    "\n",
    "ablate_top_neurons_hook = get_ablate_neurons_hook(top_neurons, ablated_cache)\n",
    "ablate_bottom_neurons_hook = get_ablate_neurons_hook(bottom_neurons, ablated_cache)\n",
    "\n",
    "original_loss, ablated_loss, _, all_MLP5_loss = haystack_utils.get_direct_effect(prompts, model, pos=-1, context_ablation_hooks=deactivate_neurons_fwd_hooks, context_activation_hooks=activate_neurons_fwd_hooks)\n",
    "_, _, _, top_MLP5_ablated_loss = haystack_utils.get_direct_effect(prompts, model, pos=-1, context_ablation_hooks=deactivate_neurons_fwd_hooks, context_activation_hooks=activate_neurons_fwd_hooks+ablate_top_neurons_hook)\n",
    "_, _, _, bottom_MLP5_ablated_loss = haystack_utils.get_direct_effect(prompts, model, pos=-1, context_ablation_hooks=deactivate_neurons_fwd_hooks, context_activation_hooks=activate_neurons_fwd_hooks+ablate_bottom_neurons_hook)\n",
    "\n",
    "names = [\"Original\", \"Ablated\", \"MLP5 path patched\", f\"MLP5 path patched + Top {top_neurons_count} MLP5 neurons ablated\", f\"MLP5 path patched + Bottom {top_neurons_count} MLP5 neurons ablated\"]\n",
    "short_names = [\"Original\", \"Ablated\", \"MLP5 path patched\", f\"Top MLP5 removed\", f\"Bottom MLP5 removed\"]\n",
    "\n",
    "values = [original_loss.tolist(), ablated_loss.tolist(), all_MLP5_loss.tolist(), top_MLP5_ablated_loss.tolist(), bottom_MLP5_ablated_loss.tolist()]\n",
    "haystack_utils.plot_barplot(values, names, short_names=short_names, ylabel=\"Loss\", title=f\"Average last token loss when removing top / bottom neurons from path patching\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigate top / bottom neuron boosts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Patch in MLP5 neurons, ablated bottom neurons, compare with patched MLP5 neurons without ablating bottom neurons\n",
    "\n",
    "with model.hooks(deactivate_neurons_fwd_hooks):\n",
    "    ablated_logits, ablated_cache = model.run_with_cache(prompts)\n",
    "\n",
    "ablate_top_neurons_hook = get_ablate_neurons_hook(top_neurons, ablated_cache)\n",
    "ablate_bottom_neurons_hook = get_ablate_neurons_hook(bottom_neurons, ablated_cache)\n",
    "\n",
    "original_logits, ablated_logprobs, _, all_MLP5_logprobs = haystack_utils.get_direct_effect(prompts, model, pos=-2, context_ablation_hooks=deactivate_neurons_fwd_hooks, context_activation_hooks=activate_neurons_fwd_hooks, return_type='logprobs')\n",
    "_, _, _, top_MLP5_ablated_logprobs = haystack_utils.get_direct_effect(prompts, model, pos=-2, context_ablation_hooks=deactivate_neurons_fwd_hooks, context_activation_hooks=activate_neurons_fwd_hooks+ablate_top_neurons_hook, return_type='logprobs')\n",
    "_, _, _, bottom_MLP5_ablated_logprobs = haystack_utils.get_direct_effect(prompts, model, pos=-2, context_ablation_hooks=deactivate_neurons_fwd_hooks, context_activation_hooks=activate_neurons_fwd_hooks+ablate_bottom_neurons_hook, return_type='logprobs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_difference_neurons(baseline_logprobs, ablated_logprops, positive=True, logprob_threshold=-7, k=50):\n",
    "    neuron_logprob_difference = (baseline_logprobs - ablated_logprops).mean(0)\n",
    "    neuron_logprob_difference[baseline_logprobs.mean(0) < logprob_threshold] = 0\n",
    "    if positive:\n",
    "        non_zero_count = (neuron_logprob_difference > 0).sum()\n",
    "    else:\n",
    "        non_zero_count = (neuron_logprob_difference < 0).sum()\n",
    "    top_logprob_difference, top_neurons = haystack_utils.top_k_with_exclude(neuron_logprob_difference, min(non_zero_count, k), all_ignore, largest=positive)\n",
    "    return top_logprob_difference, top_neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boosted tokens\n",
    "bottom_neuron_pos_difference_logprobs, bottom_pos_indices = get_top_difference_neurons(all_MLP5_logprobs, bottom_MLP5_ablated_logprobs, positive=True)\n",
    "top_neuron_pos_difference_logprobs, top_pos_indices = get_top_difference_neurons(all_MLP5_logprobs, top_MLP5_ablated_logprobs, positive=True)\n",
    "# Deboosted tokens\n",
    "bottom_neuron_neg_difference_logprobs, bottom_neg_indices = get_top_difference_neurons(all_MLP5_logprobs, bottom_MLP5_ablated_logprobs, positive=False)\n",
    "top_neuron_neg_difference_logprobs, top_neg_indices = get_top_difference_neurons(all_MLP5_logprobs, top_MLP5_ablated_logprobs, positive=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b8742e9c0c2442d9d2ea605acbb4aed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='Tokens:', options=('Boosted', 'Deboosted'), value='Boosted'), Drop…"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def plot_data(boosted_deboosted, top_bottom):\n",
    "    if boosted_deboosted == 'Boosted' and top_bottom == 'Top':\n",
    "        logprobs = top_neuron_pos_difference_logprobs\n",
    "        indices = top_pos_indices\n",
    "        title_change = 'increase'\n",
    "    elif boosted_deboosted == 'Boosted' and top_bottom == 'Bottom':\n",
    "        logprobs = bottom_neuron_pos_difference_logprobs\n",
    "        indices = bottom_pos_indices\n",
    "        title_change = 'increase'\n",
    "    elif boosted_deboosted == 'Deboosted' and top_bottom == 'Top':\n",
    "        logprobs = top_neuron_neg_difference_logprobs\n",
    "        indices = top_neg_indices\n",
    "        title_change = 'decrease'\n",
    "    else:  # 'Deboosted' and 'Bottom'\n",
    "        logprobs = bottom_neuron_neg_difference_logprobs\n",
    "        indices = bottom_neg_indices\n",
    "        title_change = 'decrease'\n",
    "\n",
    "    xlabel = boosted_deboosted + \" tokens\"\n",
    "    ylabel = \"full_logprob - ablated_logprop\"\n",
    "    title = 'Logprob ' + title_change + ' from ' + top_bottom.lower() + ' neurons'\n",
    "    xticks = [model.to_str_tokens([i])[0] for i in indices]\n",
    "\n",
    "    haystack_utils.line(logprobs.cpu().numpy(), xlabel=xlabel, ylabel=ylabel, title=title, xticks=xticks, show_legend=False)\n",
    "\n",
    "\n",
    "boosted_deboosted = widgets.Dropdown(options=['Boosted', 'Deboosted'], value='Boosted', description='Tokens:')\n",
    "top_bottom = widgets.Dropdown(options=['Top', 'Bottom'], value='Top', description='Neurons:')\n",
    "widgets.interactive(plot_data, boosted_deboosted=boosted_deboosted, top_bottom=top_bottom)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decompose neuron effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get logprob change for individual neuron\n",
    "\n",
    "def get_individual_neuron_logprob_effect(sorted_indices, neuron_pos=0, top=True, positive=True, plot=True):\n",
    "    if top:\n",
    "        neurons = sorted_indices[-(neuron_pos+1)]\n",
    "    else:\n",
    "        neurons = sorted_indices[neuron_pos]\n",
    "    neurons_hook = get_ablate_neurons_hook([neurons], ablated_cache)\n",
    "\n",
    "    original_logits, ablated_logprobs, _, all_MLP5_logprobs = haystack_utils.get_direct_effect(prompts, model, pos=-2, context_ablation_hooks=deactivate_neurons_fwd_hooks, context_activation_hooks=activate_neurons_fwd_hooks, return_type='logprobs')\n",
    "    _, _, _, neuron_ablated_logprobs = haystack_utils.get_direct_effect(prompts, model, pos=-2, context_ablation_hooks=deactivate_neurons_fwd_hooks, context_activation_hooks=activate_neurons_fwd_hooks+neurons_hook, return_type='logprobs')\n",
    "    neuron_difference_logprobs, difference_indices = \\\n",
    "        get_top_difference_neurons(all_MLP5_logprobs, neuron_ablated_logprobs, positive=positive)\n",
    "    \n",
    "    if not plot:\n",
    "        return neuron_difference_logprobs, difference_indices\n",
    "    \n",
    "    if positive:\n",
    "        xlabel = 'Boosted tokens'\n",
    "        title = 'Logprob increase from neuron ' + str(neurons.item())\n",
    "    else: \n",
    "        xlabel = 'Deboosted tokens'\n",
    "        title = 'Logprob decrease from neuron ' + str(neurons.item())\n",
    "    if top:\n",
    "        title += f' (top {neuron_pos+1} neuron)'\n",
    "    else:\n",
    "        title += f' (bottom {neuron_pos+1} neuron)'\n",
    "\n",
    "    xticks = [model.to_str_tokens([i])[0] for i in difference_indices]\n",
    "    haystack_utils.line(neuron_difference_logprobs.cpu().numpy(), xlabel=xlabel, ylabel=\"full_logprob - ablated_logprop\", title=title, xticks=xticks, show_legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3133e37850324c17a50ca61d0650d48b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='Neuron Pos:', max=20), Dropdown(description='Neurons:', …"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a function to call your function with widget inputs\n",
    "def plot_individual_neuron(neuron_pos, top_bottom, pos_neg):\n",
    "    top_bool = True if top_bottom == 'Top' else False\n",
    "    positive_bool = True if pos_neg == 'Positive' else False\n",
    "    get_individual_neuron_logprob_effect(indices, neuron_pos=neuron_pos, top=top_bool, positive=positive_bool)\n",
    "\n",
    "# Define widgets\n",
    "neuron_pos_slider = widgets.IntSlider(min=0, max=20, step=1, value=0, description='Neuron Pos:')\n",
    "top_bottom_dropdown = widgets.Dropdown(options=['Top', 'Bottom'], value='Top', description='Neurons:')\n",
    "pos_neg_dropdown = widgets.Dropdown(options=['Positive', 'Negative'], value='Positive', description='Pos/Neg:')\n",
    "\n",
    "# Use interactive function to bind the widgets to the plotting function\n",
    "widgets.interactive(plot_individual_neuron, neuron_pos=neuron_pos_slider, top_bottom=top_bottom_dropdown, pos_neg=pos_neg_dropdown)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum individual weighted positive / negative effects\n",
    "def summed_neuron_differences(num_neurons=10, top=True, positive=True, k=50):\n",
    "    summed_neuron_diffs = torch.zeros(model.cfg.d_vocab).cuda()\n",
    "    for neuron_pos in range(num_neurons):\n",
    "        # It would probably be more principle to do our filtering on the summed diffs instead of per neuron\n",
    "        neuron_diff, token_indices = get_individual_neuron_logprob_effect(indices, neuron_pos=neuron_pos, top=top, positive=positive, plot=False)\n",
    "        summed_neuron_diffs[token_indices] += neuron_diff\n",
    "    if positive:\n",
    "        non_zero_count = (summed_neuron_diffs > 0).sum()\n",
    "    else:\n",
    "        non_zero_count = (summed_neuron_diffs < 0).sum()\n",
    "    top_logprob_difference, top_tokens = haystack_utils.top_k_with_exclude(summed_neuron_diffs, min(non_zero_count, k), all_ignore, largest=positive)\n",
    "    return top_logprob_difference, top_tokens\n",
    "\n",
    "def plot_summed_neuron_differences(num_neurons=10, top=True, positive=True):\n",
    "    top_logprob_difference, top_logprob_tokens = summed_neuron_differences(num_neurons=num_neurons, top=top, positive=positive)\n",
    "    xticks = [model.to_str_tokens([i])[0] for i in top_logprob_tokens]\n",
    "    \n",
    "    if positive:\n",
    "        title = \"Summed individual boosts by\"\n",
    "        xlabel = \"Boosted tokens\"\n",
    "    else:\n",
    "        title = \"Summed individual deboosts by\"\n",
    "        xlabel = \"Deboosted tokens\"\n",
    "    if top:\n",
    "        title += f\" top {num_neurons} neurons\"\n",
    "    else:\n",
    "        title += f\" bottom {num_neurons} neurons\"\n",
    "    haystack_utils.line(top_logprob_difference.cpu().numpy(), xlabel=xlabel, ylabel=\"full_logprob - ablated_logprop\", title=title, xticks=xticks, show_legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93611f550d4d4a8d8b3e6f55517f2fee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=10, description='Num Neurons:', max=20, min=1), Dropdown(description='Ne…"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a function to call your function with widget inputs\n",
    "def plot_summed_neurons_widget(num_neurons, top_bottom, pos_neg):\n",
    "    top_bool = True if top_bottom == 'Top' else False\n",
    "    positive_bool = True if pos_neg == 'Positive' else False\n",
    "    plot_summed_neuron_differences(num_neurons=num_neurons, top=top_bool, positive=positive_bool)\n",
    "\n",
    "# Define widgets\n",
    "num_neuron_slider = widgets.IntSlider(min=1, max=20, step=1, value=10, description='Num Neurons:')\n",
    "top_bottom_dropdown = widgets.Dropdown(options=['Top', 'Bottom'], value='Top', description='Neurons:')\n",
    "pos_neg_dropdown = widgets.Dropdown(options=['Positive', 'Negative'], value='Positive', description='Pos/Neg:')\n",
    "\n",
    "# Use interactive function to bind the widgets to the plotting function\n",
    "widgets.interactive(plot_summed_neurons_widget, num_neurons=num_neuron_slider, top_bottom=top_bottom_dropdown, pos_neg=pos_neg_dropdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss is logprob(gen) = logit(gen) - logsumexp(other vocab)\n",
    "Loss = -logprob\n",
    "\n",
    "If the logsumexp is higher the loss is higher\n",
    "\n",
    "If the neuron engages in destructive interference of a high logit for a non-answer token, then the exp(logit) for the token will be lower and so the logsumexp will be lower so the loss will be lower\n",
    "\n",
    "If the neuron engages in destructive interference of a low logit for a non-answer token, then the exp(logit) for the the token won't really change and so the logsumexp and the loss will both be the same\n",
    "\n",
    "For a single neuron:\n",
    "1. For each \"gen\" prompt, zero centre the logits and record the logit of \"gen\". Calculate the mean \"gen\" logit.\n",
    "2. For each \"gen\" prompt, disable the neuron under test, zero centre the logits, edit the logit of \"gen\" to the mean with a hook, then get the logprob of \"gen\" and calculate logsumexp(all vocab) = logit(gen) - logprob(gen).\n",
    "2. For each \"gen\" prompt, enable the neuron, zero centre the logits, edit the logit of \"gen\" to the mean with a hook, then get the logprob of \"gen\" and calculate logsumexp(all vocab) = logit(gen) - logprob(gen).\n",
    "3. Take the difference in logsum exps. If it's positive, the neuron is reducing the loss via destructive interference by the difference.\n",
    "Can use the same procedure for sets of neurons, or for all neurons, to find high level effects of the context neuron\n",
    "\n",
    "Logprobs are logits with a constant subtracted and the constant is the same for every logit within a prompt.\n",
    "\n",
    "Taking the difference in terms with and without a neuron's effect via the context neuron:\n",
    "- If log sum exp increases, the neuron is boosting tokens on average. \n",
    "- If logit increases, the neuron is boosting the correct token\n",
    "\n",
    "\n",
    "Remove the neuron's effects on the gen logit. Take the mean on the prompt and position?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(11.2179, device='cuda:0')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Calculate the mean \"gen\" logit.\n",
    "gen_index = model.to_single_token('gen')\n",
    "gen_logits = []\n",
    "logits = model(prompts, return_type='logits') # batch pos vocab\n",
    "logits = logits - logits.mean(-1).unsqueeze(-1) # batch pos vocab, batch pos 1\n",
    "\n",
    "mean_gen_logit = logits[:, -1, gen_index].mean(0)\n",
    "mean_gen_logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import numpy as np\n",
    "from transformer_lens import utils\n",
    "\n",
    "# px.histogram(np.random.choice(logits[:, -1, gen_index].flatten().cpu().numpy(), 1000), nbins=100)\n",
    "# utils.test_prompt(\"\".join(model.to_str_tokens(prompts[0, :-1])), \"gen\", model, prepend_space_to_answer=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 50304])\n",
      "tensor([-0.5820, -0.2429, -0.7858, -0.5219, -0.4645], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>                            <div id=\"b497bbcb-3dbb-48da-a1fa-ab13f7bf1e75\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"b497bbcb-3dbb-48da-a1fa-ab13f7bf1e75\")) {                    Plotly.newPlot(                        \"b497bbcb-3dbb-48da-a1fa-ab13f7bf1e75\",                        [{\"alignmentgroup\":\"True\",\"bingroup\":\"x\",\"hovertemplate\":\"variable=0\\u003cbr\\u003evalue=%{x}\\u003cbr\\u003ecount=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"0\",\"marker\":{\"color\":\"#636efa\",\"pattern\":{\"shape\":\"\"}},\"name\":\"0\",\"offsetgroup\":\"0\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[-0.5820446014404297,-0.24291038513183594,-0.7857952117919922,-0.5218582153320312,-0.4645252227783203,-0.3501415252685547,-0.5110721588134766,-0.27910804748535156,-0.18342971801757812,-0.6384639739990234,-0.3081512451171875,-0.4160118103027344,-0.24183082580566406,-0.2811698913574219,-0.16000747680664062,-0.2222614288330078,-0.733489990234375,-0.5351314544677734,-0.25864219665527344,-0.5250797271728516,-0.3163127899169922,-0.5837230682373047,-0.4867115020751953,-0.3630027770996094,-0.6082763671875,-0.2371349334716797,-0.49123382568359375,-0.5633983612060547,-0.26566123962402344,-0.23460960388183594,-0.4637870788574219,-0.34066009521484375,-0.4494495391845703,-0.25560760498046875,-0.5083560943603516,-0.5741233825683594,-0.22797584533691406,-0.47657203674316406,-0.5060100555419922,-0.32666587829589844,-0.3644142150878906,-0.48896026611328125,-0.5295314788818359,-0.2144451141357422,-0.6232891082763672,-0.4071235656738281,-0.34662818908691406,-0.553955078125,-0.3961963653564453,-0.2319183349609375,-0.6241874694824219,-0.5401515960693359,-0.25835609436035156,-0.45772552490234375,-0.4278430938720703,-0.41927337646484375,-0.2455768585205078,-0.3614158630371094,-0.39170265197753906,-0.3391284942626953,-0.3780384063720703,-0.2686939239501953,-0.32433509826660156,-0.3543834686279297,-0.15135574340820312,-0.12297439575195312,-0.3176231384277344,-0.2612037658691406,-0.407073974609375,-0.27227020263671875,-0.3304328918457031,-0.49149513244628906,-0.3113594055175781,-0.4071483612060547,-0.5363388061523438,-0.3336219787597656,-0.26128387451171875,-0.3260154724121094,-0.3063011169433594,-0.24935150146484375,-0.4798908233642578,-0.4613323211669922,-0.44049072265625,-0.3592414855957031,-0.4473133087158203,-0.37334632873535156,-0.7396659851074219,-0.37581634521484375,-0.4448223114013672,-0.5018825531005859,-0.6949977874755859,-0.3009147644042969,-0.2336750030517578,-0.33490562438964844,-0.3910408020019531,-0.3080615997314453,-0.5257797241210938,-0.45420265197753906,-0.3094196319580078,-0.3591346740722656],\"xaxis\":\"x\",\"yaxis\":\"y\",\"type\":\"histogram\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"value\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"count\"}},\"legend\":{\"title\":{\"text\":\"variable\"},\"tracegroupgap\":0},\"margin\":{\"t\":60},\"barmode\":\"relative\"},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('b497bbcb-3dbb-48da-a1fa-ab13f7bf1e75');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Disable context neuron, later expand to downstream neurons\n",
    "with model.hooks(deactivate_neurons_fwd_hooks):\n",
    "    ablated_logits, ablated_cache = model.run_with_cache(prompts, return_type='logits')\n",
    "\n",
    "ablated_logits = ablated_logits[:, -1, :] # batch vocab\n",
    "ablated_logits = ablated_logits - ablated_logits.mean(-1).unsqueeze(-1)\n",
    "print(ablated_logits.shape)\n",
    "ablated_logits[:, gen_index] = mean_gen_logit\n",
    "\n",
    "ablated_logits_destructive_interference = ablated_logits.exp().sum(-1).log()\n",
    "\n",
    "\n",
    "ablate_top_neuron_hook = get_ablate_neurons_hook([top_neurons[0]], ablated_cache)\n",
    "_, ablated_logits, _, top_neuron_ablated_logits = haystack_utils.get_direct_effect(prompts, model, pos=-1, context_ablation_hooks=deactivate_neurons_fwd_hooks, context_activation_hooks=activate_neurons_fwd_hooks+ablate_top_neuron_hook, return_type='logits')\n",
    "top_neuron_ablated_logits = top_neuron_ablated_logits - top_neuron_ablated_logits.mean(-1).unsqueeze(-1)\n",
    "top_neuron_ablated_logits[:, gen_index] = mean_gen_logit\n",
    "\n",
    "top_neuron_ablated_destructive_interference = top_neuron_ablated_logits.exp().sum(-1).log()\n",
    "\n",
    "diffs = top_neuron_ablated_destructive_interference - ablated_logits_destructive_interference\n",
    "# log sum exp is lower so loss is lowered when top neuron is ablated\n",
    "# top neuron ablated is not engaging in destructive interference on net\n",
    "\n",
    "# compare MLP5 direct with MLP5 direct - one neuron\n",
    "\n",
    "print(diffs[:5])\n",
    "\n",
    "px.histogram(diffs.flatten().cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neuron -> Token Logit Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.0346, -0.0237, -0.0347,  0.0397,  0.0595, -0.0274,  0.0501,  0.1405,\n",
      "         0.0417, -0.0392], device='cuda:0')\n",
      "tensor([0.1634, 0.1461, 0.1405, 0.1399, 0.1349, 0.1207, 0.1196, 0.1161, 0.1156,\n",
      "        0.1139, 0.1130, 0.1128, 0.1120, 0.1104, 0.1096, 0.1070, 0.1056, 0.1048,\n",
      "        0.1042, 0.1028], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Our top neurons are selected by the difference in their boost in gen based on the context neuron\n",
    "# Many other neurons boost gen more per unit of activation\n",
    "\n",
    "cosine_sim = torch.nn.CosineSimilarity(dim=1)\n",
    "answer_residual_direction = model.tokens_to_residual_directions(\"gen\")\n",
    "neuron_weights = model.state_dict()['blocks.5.mlp.W_out'][top_neurons]\n",
    "\n",
    "cosine_sims = cosine_sim(neuron_weights, answer_residual_direction.unsqueeze(0))\n",
    "print(cosine_sims)\n",
    "\n",
    "cosine_sim = torch.nn.CosineSimilarity(dim=1)\n",
    "answer_residual_direction = model.tokens_to_residual_directions(\"gen\")\n",
    "neuron_weights = model.state_dict()['blocks.5.mlp.W_out']\n",
    "\n",
    "cosine_sims = cosine_sim(neuron_weights, answer_residual_direction.unsqueeze(0))\n",
    "top, indices = torch.topk(cosine_sims, 20)\n",
    "print(top)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7239c5955204a4f93330d076dd2952c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "654f963413ec454f98fb6e54bd21cfd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47d15555af774e19b136f53047cfc859",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "haystack_utils.clean_cache()\n",
    "english_activations_l5 = {}\n",
    "german_activations_l5 = {}\n",
    "all_activations_l5 = {}\n",
    "for layer in range(5, 6):\n",
    "    english_activations_l5[layer] = get_mlp_activations(english_data, layer, model, mean=True)\n",
    "    german_activations_l5[layer] = get_mlp_activations(german_data, layer, model, mean=True)\n",
    "    all_activations_l5[layer] = get_mlp_activations(german_data[:50] + english_data[:150], layer, model, mean=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'unsqueeze'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 28\u001b[0m\n\u001b[1;32m     26\u001b[0m W_out \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mstate_dict()[\u001b[39m'\u001b[39m\u001b[39mblocks.5.mlp.W_out\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     27\u001b[0m W_U \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mW_U\n\u001b[0;32m---> 28\u001b[0m \u001b[39mprint\u001b[39m(W_out\u001b[39m.\u001b[39mshape, W_U\u001b[39m.\u001b[39mshape, all_activations_l5\u001b[39m.\u001b[39;49munsqueeze(\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mshape)\n\u001b[1;32m     30\u001b[0m average_boost \u001b[39m=\u001b[39m W_out \u001b[39m*\u001b[39m all_activations_l5\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m) \u001b[39m*\u001b[39m W_U\n\u001b[1;32m     31\u001b[0m \u001b[39mprint\u001b[39m(average_boost\u001b[39m.\u001b[39mshape) \u001b[39m# d_mlp d_vocab\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'unsqueeze'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# token based measures:\n",
    "# index\n",
    "# next index\n",
    "\n",
    "# neuron based measures:\n",
    "# index\n",
    "# cosine sim with gen\n",
    "# average activation when context neuron enabled\n",
    "# average activation when context neuron disabled\n",
    "# 5 if cosine sim > 0 else -5\n",
    "# loss change when neuron ablated\n",
    "\n",
    "# boost_gen_acts = cosine_sims\n",
    "# boost_gen_acts[cosine_sims > 0] = 5.0\n",
    "# boost_gen_acts[cosine_sims <= 0] = -5.0\n",
    "\n",
    "# def get_deactivate_neuron_hook(neuron):\n",
    "#     def deactivate_neurons_hook(value, hook):\n",
    "#         value[:, :, neuron] = MEAN_ACTIVATION_INACTIVE\n",
    "#         return value\n",
    "# deactivate_neurons_fwd_hooks=[(f'blocks.{LAYER_TO_ABLATE}.mlp.hook_post', deactivate_neurons_hook)]\n",
    "# loss_diffs = []\n",
    "# for i in range(2048):\n",
    "\n",
    "W_out = model.state_dict()['blocks.5.mlp.W_out']\n",
    "W_U = model.W_U\n",
    "print(W_out.shape, W_U.shape, all_activations_l5.unsqueeze(1).shape)\n",
    "\n",
    "average_boost = W_out * all_activations_l5.unsqueeze(1) * W_U\n",
    "print(average_boost.shape) # d_mlp d_vocab\n",
    "\n",
    "# 'Average gen boost': [i zip(all_activations_l5,\n",
    "\n",
    "# mean activation\n",
    "data = {\n",
    "    'Neuron index': list(range(2048)),\n",
    "    'Cosine similarity with \\\"gen\\\"': cosine_sims.tolist(),\n",
    "    'Average act - context neuron enabled': german_activations_l5.tolist(),\n",
    "    'Average act - context neuron disabled': english_activations_l5.tolist(),\n",
    "    'Average act': all_activations_l5.tolist()\n",
    "    # 'Boost \\\"gen\\\" act': boost_gen_acts.tolist(),\n",
    "    # 'Loss change when ablated': \n",
    "}\n",
    "\n",
    "# Create the dataframe\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Print the dataframe\n",
    "print(df)\n",
    "\n",
    "# More complete picture of neurons that boost gen.\n",
    "# Line plot of each neuron's gen boost with dotted vertical lines where the German context neuron-boosted ones are"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brainstorm notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to decompose an MLP5 neuron's effect into its boost to the correct logit and its deboost of other logits. I want to discover how these two effects change the log prob.\n",
    "\n",
    "metric like loss reduction vs. token boost\n",
    "\n",
    "~~run direct effect, patch each neuron in top 10 individually, get overall loss reduction from neuron controlled by context neuron (equivalent to logprob increase for correct token)~~\n",
    "decompose loss reduction into two parts:\n",
    "run direct effect, patch the correct token logit boost from the neuron (removing the neuron's other effects), get overall loss reduction from neuron (equivalent to logprob increase for correct token)\n",
    "destructive interference loss reduction = overall loss reduction - loss reduction from correct token logit boost (component of the logprob increase for correct token due to it deboosting incorrect token)\n",
    "\n",
    "Patch the correct token logit boost from the neuron (removing the neuron's other effects).\n",
    "1. Get baseline logprobs for a prompt\n",
    "2. Get difference in logits from activating the neuron under test. Can use get_direct_effects with return_type='logits'.\n",
    "3. Run the model with return_type='logits' and the neuron under test zero ablated. \n",
    "4. Add the correct token logit from step 1 to a copy of the output logits. Convert to logprobs\n",
    "5. Add the incorrect token logits from step 1 to a copy of the output logits. Convert to logprobs\n",
    "6. Compare A. lobprobs with correct answer token logit increase, B. logprobs with incorrect answer token logit increases, and C. baseline logprobs\n",
    "\n",
    "~~If the context neuron gives each neuron a flat boost then if we decompose the resulting flat boost to one MLP5 neuron into a boost to one logit vs. boosts to all other logits it will change the logprobs (first will increase answer probability and second will reduce answer probability). \n",
    "\n",
    "the two resulting log probs at the correct answer token won't add up to the original log probs (?). \n",
    "\n",
    "If the boost is flat the correct percentage decomposition is 1/50000 and 49999/50000? In practice/all other factors being equal\n",
    "\n",
    "New plan:\n",
    "\n",
    "Difference between baseline log prob and neuron log prob?\n",
    "Classify individual neurons by percentage constructive vs destructive by looking at their log probs and summing the incorrect token log probs\n",
    "\n",
    "Correct log prob difference\n",
    "Incorrect log prob difference (summed over every plausible token?)\n",
    "\n",
    "Largest boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the log prob for an incorrect token is significantly lower then that's where the extra probability density on the correct answer is coming from \n",
    "# Constructive interference increases correct token log prob and uniformly decreases other log probs\n",
    "# Destructive interference decreases specific other log probs and uniformly increases other log probs\n",
    " \n",
    "# Most neurons are a mixture of the above\n",
    "# Decompose neurons into what % of their effect is each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original_logits, ablated_logprobs, _, all_MLP5_logprobs = haystack_utils.get_direct_effect(prompts, model, pos=-2, context_ablation_hooks=deactivate_neurons_fwd_hooks, context_activation_hooks=activate_neurons_fwd_hooks, return_type='logprobs')\n",
    "# _, _, _, top_MLP5_ablated_logprobs = haystack_utils.get_direct_effect(prompts, model, pos=-2, context_ablation_hooks=deactivate_neurons_fwd_hooks, context_activation_hooks=activate_neurons_fwd_hooks+ablate_top_neurons_hook, return_type='logprobs')\n",
    "# _, _, _, bottom_MLP5_ablated_logprobs = haystack_utils.get_direct_effect(prompts, model, pos=-2, context_ablation_hooks=deactivate_neurons_fwd_hooks, context_activation_hooks=activate_neurons_fwd_hooks+ablate_bottom_neurons_hook, return_type='logprobs')\n",
    "\n",
    "# bottom_neuron_high_difference_logprobs = (all_MLP5_logprobs - bottom_MLP5_ablated_logprobs)\n",
    "# bottom_neuron_high_difference_logprobs[bottom_neuron_high_difference_logprobs < 0] = 0\n",
    "# bottom_neuron_high_difference_logprobs = bottom_neuron_high_difference_logprobs.mean(0)\n",
    "# bottom_neuron_high_difference_logprobs[all_MLP5_logprobs.mean(0) < -7] = 0\n",
    "\n",
    "# bottom_non_zero_count = (bottom_neuron_high_difference_logprobs > 0).sum()\n",
    "# bottom_neuron_high_difference_logprobs, bottom_indices = haystack_utils.top_k_with_exclude(bottom_neuron_high_difference_logprobs, min(bottom_non_zero_count, 50), all_ignore)\n",
    "# haystack_utils.line(bottom_neuron_high_difference_logprobs.cpu().numpy(), title='Largest positive difference in log probs for tokens when bottom neurons are not ablated', xticks=[model.to_str_tokens([i])[0] for i in bottom_indices])\n",
    "\n",
    "\n",
    "# top_neuron_high_difference_logprobs = (all_MLP5_logprobs - top_MLP5_ablated_logprobs)\n",
    "# top_neuron_high_difference_logprobs[top_neuron_high_difference_logprobs < 0] = 0\n",
    "# top_neuron_high_difference_logprobs = top_neuron_high_difference_logprobs.mean(0)\n",
    "# top_neuron_high_difference_logprobs[all_MLP5_logprobs.mean(0) < -7] = 0\n",
    "\n",
    "# top_non_zero_count = (top_neuron_high_difference_logprobs > 0).sum()\n",
    "# top_neuron_high_difference_logprobs, top_indices = haystack_utils.top_k_with_exclude(top_neuron_high_difference_logprobs, min(top_non_zero_count, 50), all_ignore)\n",
    "# haystack_utils.line(top_neuron_high_difference_logprobs.cpu().numpy(), title='Largest positive difference in log probs for tokens when bottom neurons are not ablated', xticks=[model.to_str_tokens([i])[0] for i in top_indices])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
