{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from transformer_lens import HookedTransformer, ActivationCache, utils, patching\n",
    "from jaxtyping import Float, Int, Bool\n",
    "from torch import Tensor\n",
    "from tqdm.auto import tqdm\n",
    "import plotly.io as pio\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "pio.renderers.default = \"notebook_connected\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.autograd.set_grad_enabled(False)\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "from haystack_utils import get_mlp_activations\n",
    "import haystack_utils\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model EleutherAI/pythia-70m into HookedTransformer\n",
      "data/german_europarl.json: Loaded 2000 examples with 152 to 2000 characters each.\n",
      "data/english_europarl.json: Loaded 2000 examples with 165 to 2000 characters each.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b635cff3908d45a98538159850f66d26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da26dbb9c04f4169881ec9a445f22b8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = HookedTransformer.from_pretrained(\"EleutherAI/pythia-70m\",\n",
    "    center_unembed=True,\n",
    "    center_writing_weights=True,\n",
    "    fold_ln=True,\n",
    "    device=device)\n",
    "\n",
    "german_data = haystack_utils.load_json_data(\"data/german_europarl.json\")[:200]\n",
    "english_data = haystack_utils.load_json_data(\"data/english_europarl.json\")[:200]\n",
    "\n",
    "\n",
    "english_activations = {}\n",
    "german_activations = {}\n",
    "for layer in range(3, 4):\n",
    "    english_activations[layer] = get_mlp_activations(english_data, layer, model, mean=False)\n",
    "    german_activations[layer] = get_mlp_activations(german_data, layer, model, mean=False)\n",
    "\n",
    "LAYER_TO_ABLATE = 3\n",
    "NEURONS_TO_ABLATE = [669]\n",
    "MEAN_ACTIVATION_ACTIVE = german_activations[LAYER_TO_ABLATE][:, NEURONS_TO_ABLATE].mean()\n",
    "MEAN_ACTIVATION_INACTIVE = english_activations[LAYER_TO_ABLATE][:, NEURONS_TO_ABLATE].mean()\n",
    "\n",
    "def deactivate_neurons_hook(value, hook):\n",
    "    value[:, :, NEURONS_TO_ABLATE] = MEAN_ACTIVATION_INACTIVE\n",
    "    return value\n",
    "deactivate_neurons_fwd_hooks=[(f'blocks.{LAYER_TO_ABLATE}.mlp.hook_post', deactivate_neurons_hook)]\n",
    "\n",
    "def activate_neurons_hook(value, hook):\n",
    "    value[:, :, NEURONS_TO_ABLATE] = MEAN_ACTIVATION_ACTIVE\n",
    "    return value\n",
    "activate_neurons_fwd_hooks=[(f'blocks.{LAYER_TO_ABLATE}.mlp.hook_post', activate_neurons_hook)]\n",
    "\n",
    "all_ignore, not_ignore = haystack_utils.get_weird_tokens(model, plot_norms=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find top common German tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7daa29c551f4d70a934867ab01449fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' der', 'en', ' die', ' und', 'ung', 'ä', ' in', ' den', ' des', ' zu', 'ch', 'n', 'st', 're', 'z', ' von', ' für', 'äsident', ' Pr', 'ischen', 't', 'ü', 'icht', 'gen', ' ist', ' auf', ' dass', 'ge', 'ig', ' im', 'in', ' über', 'g', ' das', 'te', ' er', 'men', ' w', 'es', ' an', 'ß', ' wir', ' eine', 'f', ' W', 'hen', 'w', ' Europ', ' ich', 'ungen', 'ren', 'le', ' dem', 'ten', ' ein', 'e', ' Z', ' Ver', 'der', ' B', ' mit', ' dies', 'h', ' nicht', 'ungs', 's', ' G', ' z', 'it', ' Herr', ' es', 'l', ' S', 'ich', 'lich', ' An', 'heit', 'ie', ' Er', ' zur', ' V', ' ver', 'u', 'hr', 'chaft', 'Der', ' Ich', ' Ab', ' haben', 'i', 'ant', 'chte', ' mö', 'er', ' K', 'igen', ' Ber', 'ür', ' Fra', 'em']\n"
     ]
    }
   ],
   "source": [
    "# Get top common german tokens excluding punctuation\n",
    "token_counts = torch.zeros(model.cfg.d_vocab).cuda()\n",
    "for example in tqdm(german_data):\n",
    "    tokens = model.to_tokens(example)\n",
    "    for token in tokens[0]:\n",
    "        token_counts[token.item()] += 1\n",
    "\n",
    "punctuation = [\"\\n\", \".\", \",\", \"!\", \"?\", \";\", \":\", \"-\", \"(\", \")\", \"[\", \"]\", \"{\", \"}\", \"<\", \">\", \"/\", \"\\\\\", \"\\\"\", \"'\"]\n",
    "leading_space_punctuation = [\" \" + char for char in punctuation]\n",
    "punctuation_tokens = model.to_tokens(punctuation + leading_space_punctuation + [' –', \" \", '  ', \"<|endoftext|>\"])[:, 1].flatten()\n",
    "token_counts[punctuation_tokens] = 0\n",
    "token_counts[all_ignore] = 0\n",
    "\n",
    "top_counts, top_tokens = torch.topk(token_counts, 100)\n",
    "print(model.to_str_tokens(top_tokens[:100]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of ngrams preceded by random prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_selection(tensor, n=12):\n",
    "    # Hacky replacement for np.random.choice\n",
    "    return tensor[torch.randperm(len(tensor))[:n]]\n",
    "\n",
    "def generate_random_prompts(end_string, n=50, length=12):\n",
    "    # Generate a batch of random prompts ending with a specific ngram\n",
    "    end_tokens = model.to_tokens(end_string).flatten()[1:]\n",
    "    prompts = []\n",
    "    for i in range(n):\n",
    "        prompt = get_random_selection(top_tokens[:max(50, length)], n=length).cuda()\n",
    "        prompt = torch.cat([prompt, end_tokens])\n",
    "        prompts.append(prompt)\n",
    "    prompts = torch.stack(prompts)\n",
    "    return prompts\n",
    "\n",
    "def replace_column(prompts: Int[Tensor, \"n_prompts n_tokens\"], token_index: int):\n",
    "    # Replaces a specific token position in a batch of prompts with random common German tokens\n",
    "    new_prompts = prompts.clone()\n",
    "    random_tokens = get_random_selection(top_tokens[:max(50, prompts.shape[0])], n=prompts.shape[0]).cuda()\n",
    "    new_prompts[:, token_index] = random_tokens\n",
    "    return new_prompts \n",
    "\n",
    "def loss_analysis(prompts: Tensor, title=\"\"):\n",
    "    # Loss plot for a batch of prompts\n",
    "    names = [\"Original\", \"Ablated\", \"MLP5 path patched\"]\n",
    "    original_loss, ablated_loss, _, only_activated_loss = \\\n",
    "        haystack_utils.get_direct_effect(prompts, model, pos=-1,\n",
    "                                        context_ablation_hooks=deactivate_neurons_fwd_hooks, \n",
    "                                        context_activation_hooks=activate_neurons_fwd_hooks, \n",
    "                                        )\n",
    "    haystack_utils.plot_barplot([original_loss.tolist(), ablated_loss.tolist(), only_activated_loss.tolist()], names, ylabel=\"Loss\", title=title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_analysis_random_prompts(end_string, n=50, length=12, replace_columns: list[int] | None = None):\n",
    "    # Loss plot for a batch of random prompts ending with a specific ngram and optionally replacing specific tokens\n",
    "    prompts = generate_random_prompts(end_string, n=n, length=length)\n",
    "    title=f\"Average last token loss on {length} random tokens ending in '{end_string}'\"\n",
    "    if replace_columns is not None:\n",
    "        replaced_tokens = model.to_str_tokens(prompts[0, replace_columns])\n",
    "        title += f\" replacing {replaced_tokens}\"\n",
    "        for column in replace_columns:\n",
    "            prompts = replace_column(prompts, column)\n",
    "    \n",
    "    loss_analysis(prompts, title=title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "# loss_analysis_random_prompts(\" Vorschlägen\", n=100, length=20)\n",
    "# loss_analysis_random_prompts(\" Vorschlägen\", n=100, length=20, replace_columns=[-2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f44c0dfacfd84ea0ae69a6f9a83fa144",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Ngram: ', options=(' Vorschlägen', ' Vorschläge', ' häufig', ' schließt', ' beweglich'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef408fb06b8b41ebb18840232923de25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SelectMultiple(description='Replace Columns:', index=(3,), options=('-2', '-3', '-4', 'None'), value=('None',)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eafd29498ca942d1b9bfb0959cdc9267",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a dropdown menu widget\n",
    "dropdown = widgets.Dropdown(\n",
    "    options = [\" Vorschlägen\", \" Vorschläge\", \" häufig\", \" schließt\", \" beweglich\"], \n",
    "    value = \" Vorschlägen\",\n",
    "    description = 'Ngram: ',\n",
    ")\n",
    "\n",
    "replace_columns_dropdown = widgets.SelectMultiple(\n",
    "    options = ['-2', '-3', '-4', 'None'],\n",
    "    value = ['None'],  # default selected value\n",
    "    description = 'Replace Columns:',\n",
    ")\n",
    "\n",
    "# Create an output widget to hold the plot\n",
    "output = widgets.Output()\n",
    "\n",
    "# Define a function to call when the widget's value changes\n",
    "def update_plot(*args):\n",
    "    # Clear the old plot from the output\n",
    "    with output:\n",
    "        clear_output(wait=True)\n",
    "\n",
    "        if 'None' in replace_columns_dropdown.value:\n",
    "            replace_columns = None\n",
    "        else:\n",
    "            # If 'None' not selected, convert the selected values to integers\n",
    "            replace_columns = [int(val) for val in replace_columns_dropdown.value]\n",
    "        \n",
    "        # Call your function with the values from the widgets\n",
    "        loss_analysis_random_prompts(dropdown.value, n=100, length=20, replace_columns=replace_columns)\n",
    "\n",
    "# Set the function to be called when the widget's value changes\n",
    "dropdown.observe(update_plot, 'value')\n",
    "replace_columns_dropdown.observe(update_plot, 'value')\n",
    "\n",
    "# Display the widget and the output\n",
    "display(dropdown, replace_columns_dropdown, output)\n",
    "\n",
    "# Run once at startup\n",
    "update_plot()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify relevant neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 24])\n"
     ]
    }
   ],
   "source": [
    "prompts = generate_random_prompts(\" Vorschlägen\", n=100, length=20)\n",
    "print(prompts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80c2de5678d04302be0b3b5a44f91ef6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2048 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0007)\n"
     ]
    }
   ],
   "source": [
    "with model.hooks(deactivate_neurons_fwd_hooks):\n",
    "    _, ablated_cache = model.run_with_cache(prompts)\n",
    "\n",
    "def get_ablate_neurons_hook(neuron: int | list[int], ablated_cache, layer=5):\n",
    "    def ablate_neurons_hook(value, hook):\n",
    "        value[:, :, neuron] = ablated_cache[f'blocks.{layer}.mlp.hook_post'][:, :, neuron]\n",
    "        return value\n",
    "    return [(f'blocks.{layer}.mlp.hook_post', ablate_neurons_hook)]\n",
    "\n",
    "diffs = torch.zeros(2048, prompts.shape[0])\n",
    "# Loss with path patched MLP5 neurons\n",
    "_, _, _, baseline_loss = haystack_utils.get_direct_effect(prompts, model, pos=-1, context_ablation_hooks=deactivate_neurons_fwd_hooks, context_activation_hooks=activate_neurons_fwd_hooks)\n",
    "for neuron in tqdm(range(2048)):\n",
    "    ablate_single_neuron_hook = get_ablate_neurons_hook(neuron, ablated_cache)\n",
    "    # Loss with path patched MLP5 neurons but a single neuron changed back to original ablated value\n",
    "    _, _, _, only_deactivated_loss = haystack_utils.get_direct_effect(prompts, model, pos=-1, context_ablation_hooks=deactivate_neurons_fwd_hooks, context_activation_hooks=activate_neurons_fwd_hooks+ablate_single_neuron_hook)\n",
    "    diffs[neuron] = only_deactivated_loss - baseline_loss\n",
    "\n",
    "print(diffs.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>                            <div id=\"59e49475-4b06-440a-ad41-62b78d85ca4e\" class=\"plotly-graph-div\" style=\"height:525px; width:800px;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"59e49475-4b06-440a-ad41-62b78d85ca4e\")) {                    Plotly.newPlot(                        \"59e49475-4b06-440a-ad41-62b78d85ca4e\",                        [{\"hovertemplate\":\"variable=0\\u003cbr\\u003eindex=%{x}\\u003cbr\\u003evalue=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"0\",\"line\":{\"color\":\"#636efa\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"0\",\"showlegend\":true,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542,543,544,545,546,547,548,549,550,551,552,553,554,555,556,557,558,559,560,561,562,563,564,565,566,567,568,569,570,571,572,573,574,575,576,577,578,579,580,581,582,583,584,585,586,587,588,589,590,591,592,593,594,595,596,597,598,599,600,601,602,603,604,605,606,607,608,609,610,611,612,613,614,615,616,617,618,619,620,621,622,623,624,625,626,627,628,629,630,631,632,633,634,635,636,637,638,639,640,641,642,643,644,645,646,647,648,649,650,651,652,653,654,655,656,657,658,659,660,661,662,663,664,665,666,667,668,669,670,671,672,673,674,675,676,677,678,679,680,681,682,683,684,685,686,687,688,689,690,691,692,693,694,695,696,697,698,699,700,701,702,703,704,705,706,707,708,709,710,711,712,713,714,715,716,717,718,719,720,721,722,723,724,725,726,727,728,729,730,731,732,733,734,735,736,737,738,739,740,741,742,743,744,745,746,747,748,749,750,751,752,753,754,755,756,757,758,759,760,761,762,763,764,765,766,767,768,769,770,771,772,773,774,775,776,777,778,779,780,781,782,783,784,785,786,787,788,789,790,791,792,793,794,795,796,797,798,799,800,801,802,803,804,805,806,807,808,809,810,811,812,813,814,815,816,817,818,819,820,821,822,823,824,825,826,827,828,829,830,831,832,833,834,835,836,837,838,839,840,841,842,843,844,845,846,847,848,849,850,851,852,853,854,855,856,857,858,859,860,861,862,863,864,865,866,867,868,869,870,871,872,873,874,875,876,877,878,879,880,881,882,883,884,885,886,887,888,889,890,891,892,893,894,895,896,897,898,899,900,901,902,903,904,905,906,907,908,909,910,911,912,913,914,915,916,917,918,919,920,921,922,923,924,925,926,927,928,929,930,931,932,933,934,935,936,937,938,939,940,941,942,943,944,945,946,947,948,949,950,951,952,953,954,955,956,957,958,959,960,961,962,963,964,965,966,967,968,969,970,971,972,973,974,975,976,977,978,979,980,981,982,983,984,985,986,987,988,989,990,991,992,993,994,995,996,997,998,999,1000,1001,1002,1003,1004,1005,1006,1007,1008,1009,1010,1011,1012,1013,1014,1015,1016,1017,1018,1019,1020,1021,1022,1023,1024,1025,1026,1027,1028,1029,1030,1031,1032,1033,1034,1035,1036,1037,1038,1039,1040,1041,1042,1043,1044,1045,1046,1047,1048,1049,1050,1051,1052,1053,1054,1055,1056,1057,1058,1059,1060,1061,1062,1063,1064,1065,1066,1067,1068,1069,1070,1071,1072,1073,1074,1075,1076,1077,1078,1079,1080,1081,1082,1083,1084,1085,1086,1087,1088,1089,1090,1091,1092,1093,1094,1095,1096,1097,1098,1099,1100,1101,1102,1103,1104,1105,1106,1107,1108,1109,1110,1111,1112,1113,1114,1115,1116,1117,1118,1119,1120,1121,1122,1123,1124,1125,1126,1127,1128,1129,1130,1131,1132,1133,1134,1135,1136,1137,1138,1139,1140,1141,1142,1143,1144,1145,1146,1147,1148,1149,1150,1151,1152,1153,1154,1155,1156,1157,1158,1159,1160,1161,1162,1163,1164,1165,1166,1167,1168,1169,1170,1171,1172,1173,1174,1175,1176,1177,1178,1179,1180,1181,1182,1183,1184,1185,1186,1187,1188,1189,1190,1191,1192,1193,1194,1195,1196,1197,1198,1199,1200,1201,1202,1203,1204,1205,1206,1207,1208,1209,1210,1211,1212,1213,1214,1215,1216,1217,1218,1219,1220,1221,1222,1223,1224,1225,1226,1227,1228,1229,1230,1231,1232,1233,1234,1235,1236,1237,1238,1239,1240,1241,1242,1243,1244,1245,1246,1247,1248,1249,1250,1251,1252,1253,1254,1255,1256,1257,1258,1259,1260,1261,1262,1263,1264,1265,1266,1267,1268,1269,1270,1271,1272,1273,1274,1275,1276,1277,1278,1279,1280,1281,1282,1283,1284,1285,1286,1287,1288,1289,1290,1291,1292,1293,1294,1295,1296,1297,1298,1299,1300,1301,1302,1303,1304,1305,1306,1307,1308,1309,1310,1311,1312,1313,1314,1315,1316,1317,1318,1319,1320,1321,1322,1323,1324,1325,1326,1327,1328,1329,1330,1331,1332,1333,1334,1335,1336,1337,1338,1339,1340,1341,1342,1343,1344,1345,1346,1347,1348,1349,1350,1351,1352,1353,1354,1355,1356,1357,1358,1359,1360,1361,1362,1363,1364,1365,1366,1367,1368,1369,1370,1371,1372,1373,1374,1375,1376,1377,1378,1379,1380,1381,1382,1383,1384,1385,1386,1387,1388,1389,1390,1391,1392,1393,1394,1395,1396,1397,1398,1399,1400,1401,1402,1403,1404,1405,1406,1407,1408,1409,1410,1411,1412,1413,1414,1415,1416,1417,1418,1419,1420,1421,1422,1423,1424,1425,1426,1427,1428,1429,1430,1431,1432,1433,1434,1435,1436,1437,1438,1439,1440,1441,1442,1443,1444,1445,1446,1447,1448,1449,1450,1451,1452,1453,1454,1455,1456,1457,1458,1459,1460,1461,1462,1463,1464,1465,1466,1467,1468,1469,1470,1471,1472,1473,1474,1475,1476,1477,1478,1479,1480,1481,1482,1483,1484,1485,1486,1487,1488,1489,1490,1491,1492,1493,1494,1495,1496,1497,1498,1499,1500,1501,1502,1503,1504,1505,1506,1507,1508,1509,1510,1511,1512,1513,1514,1515,1516,1517,1518,1519,1520,1521,1522,1523,1524,1525,1526,1527,1528,1529,1530,1531,1532,1533,1534,1535,1536,1537,1538,1539,1540,1541,1542,1543,1544,1545,1546,1547,1548,1549,1550,1551,1552,1553,1554,1555,1556,1557,1558,1559,1560,1561,1562,1563,1564,1565,1566,1567,1568,1569,1570,1571,1572,1573,1574,1575,1576,1577,1578,1579,1580,1581,1582,1583,1584,1585,1586,1587,1588,1589,1590,1591,1592,1593,1594,1595,1596,1597,1598,1599,1600,1601,1602,1603,1604,1605,1606,1607,1608,1609,1610,1611,1612,1613,1614,1615,1616,1617,1618,1619,1620,1621,1622,1623,1624,1625,1626,1627,1628,1629,1630,1631,1632,1633,1634,1635,1636,1637,1638,1639,1640,1641,1642,1643,1644,1645,1646,1647,1648,1649,1650,1651,1652,1653,1654,1655,1656,1657,1658,1659,1660,1661,1662,1663,1664,1665,1666,1667,1668,1669,1670,1671,1672,1673,1674,1675,1676,1677,1678,1679,1680,1681,1682,1683,1684,1685,1686,1687,1688,1689,1690,1691,1692,1693,1694,1695,1696,1697,1698,1699,1700,1701,1702,1703,1704,1705,1706,1707,1708,1709,1710,1711,1712,1713,1714,1715,1716,1717,1718,1719,1720,1721,1722,1723,1724,1725,1726,1727,1728,1729,1730,1731,1732,1733,1734,1735,1736,1737,1738,1739,1740,1741,1742,1743,1744,1745,1746,1747,1748,1749,1750,1751,1752,1753,1754,1755,1756,1757,1758,1759,1760,1761,1762,1763,1764,1765,1766,1767,1768,1769,1770,1771,1772,1773,1774,1775,1776,1777,1778,1779,1780,1781,1782,1783,1784,1785,1786,1787,1788,1789,1790,1791,1792,1793,1794,1795,1796,1797,1798,1799,1800,1801,1802,1803,1804,1805,1806,1807,1808,1809,1810,1811,1812,1813,1814,1815,1816,1817,1818,1819,1820,1821,1822,1823,1824,1825,1826,1827,1828,1829,1830,1831,1832,1833,1834,1835,1836,1837,1838,1839,1840,1841,1842,1843,1844,1845,1846,1847,1848,1849,1850,1851,1852,1853,1854,1855,1856,1857,1858,1859,1860,1861,1862,1863,1864,1865,1866,1867,1868,1869,1870,1871,1872,1873,1874,1875,1876,1877,1878,1879,1880,1881,1882,1883,1884,1885,1886,1887,1888,1889,1890,1891,1892,1893,1894,1895,1896,1897,1898,1899,1900,1901,1902,1903,1904,1905,1906,1907,1908,1909,1910,1911,1912,1913,1914,1915,1916,1917,1918,1919,1920,1921,1922,1923,1924,1925,1926,1927,1928,1929,1930,1931,1932,1933,1934,1935,1936,1937,1938,1939,1940,1941,1942,1943,1944,1945,1946,1947,1948,1949,1950,1951,1952,1953,1954,1955,1956,1957,1958,1959,1960,1961,1962,1963,1964,1965,1966,1967,1968,1969,1970,1971,1972,1973,1974,1975,1976,1977,1978,1979,1980,1981,1982,1983,1984,1985,1986,1987,1988,1989,1990,1991,1992,1993,1994,1995,1996,1997,1998,1999,2000,2001,2002,2003,2004,2005,2006,2007,2008,2009,2010,2011,2012,2013,2014,2015,2016,2017,2018,2019,2020,2021,2022,2023,2024,2025,2026,2027,2028,2029,2030,2031,2032,2033,2034,2035,2036,2037,2038,2039,2040,2041,2042,2043,2044,2045,2046,2047],\"xaxis\":\"x\",\"y\":[-0.6126193404197693,-0.32720115780830383,-0.21029725670814514,-0.16531100869178772,-0.16434670984745026,-0.15532943606376648,-0.14826013147830963,-0.13351939618587494,-0.12037757784128189,-0.11664645373821259,-0.11635738611221313,-0.11030160635709763,-0.10959489643573761,-0.10052996873855591,-0.09534437209367752,-0.09520170092582703,-0.08402495086193085,-0.08358590304851532,-0.08121497929096222,-0.07760149985551834,-0.07754404097795486,-0.07704772800207138,-0.07625085115432739,-0.07454576343297958,-0.07128231227397919,-0.0692586898803711,-0.06835481524467468,-0.06801808625459671,-0.06750240176916122,-0.06723789125680923,-0.06697875261306763,-0.06551308929920197,-0.06387604773044586,-0.06365034729242325,-0.05980602279305458,-0.05976038798689842,-0.058868683874607086,-0.05801180750131607,-0.05752520263195038,-0.057260483503341675,-0.05683273449540138,-0.056507762521505356,-0.05630411207675934,-0.056199654936790466,-0.05492168292403221,-0.054604750126600266,-0.052773043513298035,-0.05094224959611893,-0.05035863071680069,-0.04973592609167099,-0.04880852624773979,-0.04868188872933388,-0.04817921295762062,-0.04684597998857498,-0.04670649394392967,-0.04255736991763115,-0.04192435368895531,-0.04056784510612488,-0.04030844196677208,-0.039998672902584076,-0.03946192190051079,-0.0391545332968235,-0.038983240723609924,-0.03873322159051895,-0.038475483655929565,-0.03799981623888016,-0.03766031190752983,-0.03737124800682068,-0.036139726638793945,-0.035918813198804855,-0.0356740728020668,-0.035254765301942825,-0.0350850448012352,-0.034861091524362564,-0.03441318869590759,-0.034314535558223724,-0.033928100019693375,-0.03383693844079971,-0.03366681560873985,-0.03260977193713188,-0.0325629822909832,-0.03225599229335785,-0.03211048245429993,-0.03189723193645477,-0.03187740221619606,-0.03128602355718613,-0.031242594122886658,-0.031125150620937347,-0.030309148132801056,-0.030238742008805275,-0.030175793915987015,-0.030090712010860443,-0.02985699474811554,-0.02955089882016182,-0.0293204877525568,-0.029183614999055862,-0.02910986915230751,-0.028567809611558914,-0.02843729965388775,-0.028300734236836433,-0.02759407088160515,-0.02751883678138256,-0.027268733829259872,-0.027229759842157364,-0.026835734024643898,-0.026630382984876633,-0.02656850777566433,-0.026306381449103355,-0.026297368109226227,-0.02627086080610752,-0.026117227971553802,-0.025751374661922455,-0.025457562878727913,-0.02533600851893425,-0.025010596960783005,-0.024132829159498215,-0.024120936170220375,-0.02403639815747738,-0.023752296343445778,-0.02364281564950943,-0.023616069927811623,-0.023585529997944832,-0.02340453304350376,-0.02322860062122345,-0.023023834452033043,-0.022692013531923294,-0.022469304502010345,-0.022041045129299164,-0.022023949772119522,-0.021485505625605583,-0.020822718739509583,-0.020768674090504646,-0.020763039588928223,-0.02074390836060047,-0.02057495154440403,-0.020236540585756302,-0.02011631801724434,-0.019970951601862907,-0.01953539066016674,-0.019018175080418587,-0.018948636949062347,-0.018937446177005768,-0.018798068165779114,-0.018507134169340134,-0.018417885527014732,-0.018316999077796936,-0.018159624189138412,-0.018130062147974968,-0.01793999969959259,-0.017927277833223343,-0.01782258041203022,-0.01778564602136612,-0.01775817759335041,-0.0176259633153677,-0.01760607771575451,-0.017186209559440613,-0.01706194505095482,-0.016760988160967827,-0.01661549136042595,-0.01656285487115383,-0.016420673578977585,-0.015954740345478058,-0.015928542241454124,-0.015867453068494797,-0.015490412712097168,-0.015464036725461483,-0.015243721194565296,-0.015230942517518997,-0.015166230499744415,-0.015069409273564816,-0.014944328926503658,-0.014817945659160614,-0.014608245342969894,-0.014569559134542942,-0.014478078112006187,-0.01445794478058815,-0.014343805611133575,-0.014216735027730465,-0.014039706438779831,-0.013951320201158524,-0.013688750565052032,-0.013643050566315651,-0.013610494323074818,-0.013538242317736149,-0.013529013842344284,-0.013430037535727024,-0.013386514037847519,-0.013301822356879711,-0.013262292370200157,-0.013259351253509521,-0.01322220079600811,-0.013005061075091362,-0.012961916625499725,-0.012894894927740097,-0.012763411737978458,-0.012481552548706532,-0.012424452230334282,-0.0121651915833354,-0.012089140713214874,-0.01208279374986887,-0.01200602762401104,-0.011981135234236717,-0.011871464550495148,-0.011839143000543118,-0.01176045648753643,-0.011684278026223183,-0.011668574996292591,-0.011559879407286644,-0.011478058993816376,-0.01114440057426691,-0.011103374883532524,-0.011002279818058014,-0.010711506940424442,-0.01060553826391697,-0.010511788539588451,-0.010437089949846268,-0.01011415384709835,-0.010088293813169003,-0.01004437729716301,-0.010017693042755127,-0.010015125386416912,-0.010009149089455605,-0.010008961893618107,-0.010005831718444824,-0.009656187146902084,-0.009557931683957577,-0.009553642943501472,-0.009549797512590885,-0.009540497325360775,-0.009445748291909695,-0.009389017708599567,-0.0093158558011055,-0.009232587181031704,-0.00922755990177393,-0.009151790291070938,-0.009149676188826561,-0.009136979468166828,-0.009127983823418617,-0.009125917218625546,-0.009065390564501286,-0.00898961815983057,-0.00893538910895586,-0.008924813009798527,-0.008907494135200977,-0.008863050490617752,-0.008823530748486519,-0.00881966669112444,-0.008739553391933441,-0.00857483223080635,-0.008442748337984085,-0.008382127620279789,-0.008347618393599987,-0.008333977311849594,-0.008295624516904354,-0.008215660229325294,-0.008007965981960297,-0.00798175111413002,-0.007891437970101833,-0.007878635078668594,-0.007833540439605713,-0.007761871907860041,-0.007669986691325903,-0.007645368110388517,-0.007644870318472385,-0.007610443979501724,-0.007590601220726967,-0.007527712732553482,-0.007522785570472479,-0.007479899562895298,-0.007470713462680578,-0.0074502695351839066,-0.007447184529155493,-0.007431539706885815,-0.007425636053085327,-0.007399383932352066,-0.007368544116616249,-0.007359040901064873,-0.00732229370623827,-0.007300057448446751,-0.007294290699064732,-0.007289502769708633,-0.0072707622312009335,-0.007256511598825455,-0.007225802168250084,-0.0070954919792711735,-0.006999101489782333,-0.006988538429141045,-0.006972051691263914,-0.006962026469409466,-0.006912422366440296,-0.006900840904563665,-0.006826040800660849,-0.0068106260150671005,-0.006743899546563625,-0.0067184362560510635,-0.006702219136059284,-0.0066998181864619255,-0.006691341288387775,-0.006673323921859264,-0.006602504290640354,-0.006561160087585449,-0.006537299137562513,-0.006532685831189156,-0.006521433591842651,-0.006492563523352146,-0.006477015092968941,-0.0064674182794988155,-0.0064534349367022514,-0.006450140383094549,-0.006433545146137476,-0.0063983118161559105,-0.006330801174044609,-0.006327676586806774,-0.006281252950429916,-0.006200490053743124,-0.006113801617175341,-0.006099458318203688,-0.0060906801372766495,-0.006021902430802584,-0.0059306081384420395,-0.005922989919781685,-0.005899551324546337,-0.005880969576537609,-0.0058685545809566975,-0.005826558452099562,-0.0057995496317744255,-0.0057977475225925446,-0.005782237742096186,-0.005780520383268595,-0.005767011549323797,-0.005761416163295507,-0.005701995920389891,-0.00567947281524539,-0.0056769526563584805,-0.005674257408827543,-0.005663349758833647,-0.005627268459647894,-0.005596230737864971,-0.005563607905060053,-0.005560872610658407,-0.00555194029584527,-0.005510525777935982,-0.005399195943027735,-0.005381615366786718,-0.005347379948943853,-0.0053359209559857845,-0.0053002978675067425,-0.0052949609234929085,-0.005257749930024147,-0.0052294800989329815,-0.00522248912602663,-0.005168149247765541,-0.005120201501995325,-0.005067834164947271,-0.005001244600862265,-0.0049720280803740025,-0.004933941178023815,-0.004932961892336607,-0.004920382983982563,-0.004906997550278902,-0.0048877703957259655,-0.004869464784860611,-0.004850444383919239,-0.004841385874897242,-0.004830849822610617,-0.00480102701112628,-0.004779119975864887,-0.004745956510305405,-0.004742448218166828,-0.004716123454272747,-0.004663032945245504,-0.004659689497202635,-0.0046573481522500515,-0.004615061916410923,-0.004552044905722141,-0.004544493742287159,-0.004538442008197308,-0.004492457024753094,-0.004468109924346209,-0.004447197541594505,-0.004436777904629707,-0.004427488427609205,-0.004414873197674751,-0.004413999617099762,-0.004402924329042435,-0.0043616811744868755,-0.004325947258621454,-0.004232502542436123,-0.0042294710874557495,-0.0042023262940347195,-0.004179821815341711,-0.0041600000113248825,-0.004074346274137497,-0.004071512725204229,-0.004051863215863705,-0.004039420280605555,-0.004026684910058975,-0.003998651169240475,-0.003987440373748541,-0.003928836435079575,-0.003924805670976639,-0.003912838641554117,-0.0039068665355443954,-0.003886002581566572,-0.0038813515566289425,-0.0038621819112449884,-0.0038537762593477964,-0.003848070278763771,-0.0038407586980611086,-0.0038222279399633408,-0.0038050932344049215,-0.0038037803024053574,-0.0037731314077973366,-0.0037669255398213863,-0.003763836808502674,-0.0037454008124768734,-0.003745208028703928,-0.0037061346229165792,-0.0036927135661244392,-0.003627197118476033,-0.0036104172468185425,-0.0036040842533111572,-0.003554936032742262,-0.0035446181427687407,-0.003534357063472271,-0.003515780670568347,-0.003483687061816454,-0.003470495343208313,-0.0034654473420232534,-0.003453509882092476,-0.0034451307728886604,-0.0034420599695295095,-0.0034394029062241316,-0.0033975658006966114,-0.0033787123393267393,-0.003378222929313779,-0.003375939093530178,-0.0033709839917719364,-0.0033672719728201628,-0.0033604016061872244,-0.003331132000312209,-0.003328216029331088,-0.003328063990920782,-0.0032760121393948793,-0.0032279237639158964,-0.00320106721483171,-0.0031834894325584173,-0.0031673740595579147,-0.0031043081544339657,-0.0030944105237722397,-0.0030878018587827682,-0.0030715269967913628,-0.0030651954002678394,-0.0030543345492333174,-0.003050427185371518,-0.0030374410562217236,-0.0030055323150008917,-0.0030023478902876377,-0.0030011150520294905,-0.0029934418853372335,-0.0029441937804222107,-0.0029416370671242476,-0.002940759062767029,-0.0029269380029290915,-0.0029266371857374907,-0.0029229826759546995,-0.002903186483308673,-0.0028908229433000088,-0.002851902274414897,-0.002816556952893734,-0.0028038155287504196,-0.002803468145430088,-0.002801049267873168,-0.002772189909592271,-0.00276488671079278,-0.002761194948107004,-0.002745675388723612,-0.0027331961318850517,-0.002717225346714258,-0.0027143312618136406,-0.0027014445513486862,-0.0026915830094367266,-0.002682427177205682,-0.0026791496202349663,-0.002658169250935316,-0.0026538765523582697,-0.002652023918926716,-0.0026285925414413214,-0.002628335263580084,-0.0026181715074926615,-0.002609212649986148,-0.0025983431842178106,-0.002584504894912243,-0.002566942013800144,-0.002562955254688859,-0.0025555382017046213,-0.002536551095545292,-0.002525703050196171,-0.002525528660044074,-0.0025162342935800552,-0.0025038712192326784,-0.0024888247717171907,-0.0024628201499581337,-0.0024560445453971624,-0.0024496696423739195,-0.002449170220643282,-0.002436005976051092,-0.0024222489446401596,-0.0024186773225665092,-0.002413350623100996,-0.0023977786768227816,-0.0023881385568529367,-0.0023749142419546843,-0.002366012893617153,-0.0023633958771824837,-0.0023549478501081467,-0.0023481533862650394,-0.0023074308410286903,-0.002281852765008807,-0.0022751339711248875,-0.0022671250626444817,-0.0022548562847077847,-0.002253544284030795,-0.002247381489723921,-0.002227622317150235,-0.0022203209809958935,-0.002217721426859498,-0.0022052160929888487,-0.002200996968895197,-0.002198265865445137,-0.002186065772548318,-0.002173319458961487,-0.00217305775731802,-0.002161875367164612,-0.0021613643039017916,-0.002160040894523263,-0.0021585437934845686,-0.002154431538656354,-0.002131575718522072,-0.002126651117578149,-0.002123804297298193,-0.0021233188454061747,-0.0021055571269243956,-0.0021033065859228373,-0.002098716562613845,-0.0020848424173891544,-0.0020798782352358103,-0.002067022956907749,-0.002066578483209014,-0.00206400896422565,-0.002048269147053361,-0.0020408611744642258,-0.002027045702561736,-0.002023420063778758,-0.0020207941997796297,-0.00201924005523324,-0.0020173380617052317,-0.0020136521197855473,-0.0020117261447012424,-0.0020047533325850964,-0.0019935653544962406,-0.0019933353178203106,-0.0019867236260324717,-0.0019854670390486717,-0.001985105685889721,-0.0019635017961263657,-0.0019498978508636355,-0.0019497263710945845,-0.0019465023651719093,-0.0019291097996756434,-0.0019253757782280445,-0.0018987279618158937,-0.0018774750642478466,-0.0018679420463740826,-0.0018126240465790033,-0.0018122642068192363,-0.0018014598172158003,-0.0017863416578620672,-0.0017852720338851213,-0.0017847231356427073,-0.001779448357410729,-0.001760178478434682,-0.0017565506277605891,-0.001754601951688528,-0.0017414831090718508,-0.0017407808918505907,-0.001739861094392836,-0.001739025698043406,-0.0017327979439869523,-0.0017250661039724946,-0.0017143958248198032,-0.0016805770574137568,-0.0016733078518882394,-0.0016707238974049687,-0.0016686139861121774,-0.0016683520516380668,-0.0016619786620140076,-0.0016588412690907717,-0.0016570118023082614,-0.0016476416494697332,-0.0016300163697451353,-0.0016236769733950496,-0.0016221489058807492,-0.0016001206822693348,-0.0015942687168717384,-0.001587603590451181,-0.0015823084395378828,-0.0015820679254829884,-0.001559261349029839,-0.0015395510708913207,-0.0015356466174125671,-0.001529634348116815,-0.001527092419564724,-0.0014824458630755544,-0.0014786222018301487,-0.0014730340335518122,-0.0014688619412481785,-0.0014588185586035252,-0.0014287428930401802,-0.0014263780321925879,-0.0014192378148436546,-0.0014147982001304626,-0.0014103134162724018,-0.0014057961525395513,-0.001388767035678029,-0.001386218355037272,-0.0013578950893133879,-0.0013477674219757318,-0.0013420702889561653,-0.0013388267252594233,-0.0013292229268699884,-0.0013154841726645827,-0.0013138811336830258,-0.0013137778732925653,-0.0013000199105590582,-0.0012817237293347716,-0.00127607851754874,-0.0012754923664033413,-0.0012701967498287559,-0.001268431544303894,-0.0012668949784711003,-0.001259275246411562,-0.0012579191243276,-0.0012542882468551397,-0.001252510817721486,-0.0012416753452271223,-0.0012396147940307856,-0.0012383941793814301,-0.0012313248589634895,-0.001224034815095365,-0.0011960775591433048,-0.0011918727541342378,-0.00119156448636204,-0.001184004475362599,-0.0011832104064524174,-0.0011824223911389709,-0.0011742687784135342,-0.001173421391285956,-0.0011697118170559406,-0.0011621869634836912,-0.0011476814979687333,-0.0011288466630503535,-0.0011210711672902107,-0.0011124631855636835,-0.0011085834121331573,-0.0011056512594223022,-0.0011041160905733705,-0.0010956437326967716,-0.0010915171587839723,-0.00108953972812742,-0.0010815518908202648,-0.0010632050689309835,-0.0010612097103148699,-0.0010580498492345214,-0.0010570189915597439,-0.0010563596151769161,-0.0010498752817511559,-0.001038918737322092,-0.0010261492570862174,-0.0010258066467940807,-0.001023296732455492,-0.001020721159875393,-0.0010205237194895744,-0.0010157200740650296,-0.0010115329641848803,-0.0010102556552737951,-0.0010102448286488652,-0.001006611972115934,-0.0010040446650236845,-0.0010038596810773015,-0.0010030645644292235,-0.0010015537263825536,-0.0009948763763532043,-0.0009875898249447346,-0.0009856108808889985,-0.0009809244656935334,-0.0009788158349692822,-0.0009745399001985788,-0.000960242236033082,-0.0009583220817148685,-0.0009569599642418325,-0.0009300274541601539,-0.0009183539659716189,-0.0009178112959489226,-0.000912592513486743,-0.0009082750766538084,-0.0009004866587929428,-0.000887252448592335,-0.0008863872499205172,-0.0008790252613835037,-0.0008755031158216298,-0.0008753623114898801,-0.0008715845760889351,-0.000863690220285207,-0.0008620101143606007,-0.0008566251490265131,-0.0008467916632071137,-0.0008396492921747267,-0.0008263055933639407,-0.0008125897729769349,-0.0008120732964016497,-0.0007972074090503156,-0.000792830134741962,-0.0007908725528977811,-0.000789763405919075,-0.0007854339201003313,-0.0007769109797663987,-0.0007712231599725783,-0.0007638922543264925,-0.0007604623096995056,-0.0007583284168504179,-0.0007579538505524397,-0.0007539841462858021,-0.0007510270806960762,-0.0007491548894904554,-0.0007374060805886984,-0.000733494758605957,-0.0007257708348333836,-0.0007221365231089294,-0.0007187564624473453,-0.0007184453424997628,-0.0007109132129698992,-0.0007089916616678238,-0.0006895579281263053,-0.0006871640216559172,-0.0006866581970825791,-0.0006796403904445469,-0.0006734715425409377,-0.0006729403394274414,-0.000669631757773459,-0.0006444284226745367,-0.0006436945404857397,-0.0006428170017898083,-0.0006424917955882847,-0.0006381795392371714,-0.0006365512381307781,-0.0006301615503616631,-0.0006264258990995586,-0.0006247763521969318,-0.0006192836444824934,-0.0006124816718511283,-0.0006108515663072467,-0.0006026955670677125,-0.000600490893702954,-0.0005985437892377377,-0.0005930955521762371,-0.0005913993809372187,-0.0005902417469769716,-0.0005775330937467515,-0.0005721356137655675,-0.000565292895771563,-0.0005645300261676311,-0.000561350432690233,-0.0005521342973224819,-0.0005443617701530457,-0.0005352693842723966,-0.0005319897900335491,-0.0005307552055455744,-0.0005174459656700492,-0.0005141720175743103,-0.0005117071559652686,-0.0005036663496866822,-0.0005034066853113472,-0.0005004041595384479,-0.0004933428717777133,-0.0004898592014797032,-0.00048427999718114734,-0.00048276371671818197,-0.00048092834185808897,-0.00048081055865623057,-0.0004745011683553457,-0.0004741078591905534,-0.0004737874842248857,-0.00047361874021589756,-0.0004708777414634824,-0.00046945243957452476,-0.0004640438419301063,-0.0004627083253581077,-0.0004607066512107849,-0.0004603966954164207,-0.0004566526331473142,-0.0004533729807008058,-0.0004529131983872503,-0.0004387519438751042,-0.00043380408897064626,-0.0004259749548509717,-0.0004241977585479617,-0.0004192444612272084,-0.00041542446706444025,-0.0004150079330429435,-0.0004100074584130198,-0.0004072359297424555,-0.00040671543683856726,-0.00040269040619023144,-0.0003963734197895974,-0.00039189858944155276,-0.0003887461207341403,-0.0003871757653541863,-0.00038607342867180705,-0.000385158957215026,-0.0003794161311816424,-0.00037566310493275523,-0.0003713873738888651,-0.0003685912524815649,-0.000364710547728464,-0.00036470763734541833,-0.0003639186907093972,-0.0003626733960118145,-0.00035941466921940446,-0.0003585386148188263,-0.00035524190752767026,-0.000351530994521454,-0.00035090482560917735,-0.0003508884401526302,-0.0003504765045363456,-0.0003458952996879816,-0.00034226186107844114,-0.0003413697995711118,-0.00034051574766635895,-0.00033609889214858413,-0.0003279646625742316,-0.00032626846223138273,-0.0003251645830459893,-0.00032510250457562506,-0.00032494604238308966,-0.0003238172794226557,-0.0003225136606488377,-0.00032155952067114413,-0.0003211017756257206,-0.00031504363869316876,-0.00031359941931441426,-0.0003089208039455116,-0.00030341921956278384,-0.00030144548509269953,-0.00029686279594898224,-0.0002960294368676841,-0.000295169505989179,-0.0002948913024738431,-0.0002929873007815331,-0.00028867885703220963,-0.00028700142866000533,-0.00028127938276156783,-0.00027997783035971224,-0.0002761775394901633,-0.0002687860978767276,-0.00026368885301053524,-0.000260775675997138,-0.0002544540911912918,-0.00024826452136039734,-0.00024504453176632524,-0.0002425710845272988,-0.00024125128402374685,-0.00023910790332593024,-0.0002309291739948094,-0.00022506080858875066,-0.0002223562478320673,-0.00022156514751259238,-0.00022111453290563077,-0.00021953947725705802,-0.00021863468282390386,-0.00021694354654755443,-0.0002077163808280602,-0.00020583905279636383,-0.00020225979096721858,-0.00020155019592493773,-0.00020034074259456247,-0.00019996784976683557,-0.00019960269855801016,-0.0001967979915207252,-0.00019607372814789414,-0.00019442402117419988,-0.00018874675151892006,-0.0001860541815403849,-0.00018510549853090197,-0.00018460735736880451,-0.00018333450134377927,-0.00018158473540097475,-0.00018115490092895925,-0.00018087922944687307,-0.0001807873632060364,-0.00017894529446493834,-0.00017602839216124266,-0.00017413556633982807,-0.00017084933642763644,-0.00016701772983651608,-0.00016300134302582592,-0.00016168445290531963,-0.0001603917044121772,-0.00016016021254472435,-0.00015714496839791536,-0.00015712104504927993,-0.0001561059762025252,-0.00015253998572006822,-0.00015128224913496524,-0.00014987342001404613,-0.00014968276082072407,-0.00014676198770757765,-0.00014652431127615273,-0.00014403655950445682,-0.00014330551493912935,-0.00014208808715920895,-0.0001391567348036915,-0.00013427235535345972,-0.00013405502249952406,-0.0001282281446037814,-0.0001280107389902696,-0.0001275590038858354,-0.000127457533380948,-0.00012613020953722298,-0.00011965446174144745,-0.00011944256402784958,-0.00011540159903233871,-0.00011022209946531802,-0.00010905228555202484,-0.00010900042980210856,-0.00010775372356874868,-0.00010720014688558877,-0.00010591506725177169,-0.00010503851081011817,-0.00010225713049294427,-0.00010111875599250197,-0.00010105401452165097,-0.00010045103408629075,-9.880572906695306e-05,-9.688638238003477e-05,-9.17974830372259e-05,-8.127265027724206e-05,-8.067660382948816e-05,-7.587656727991998e-05,-7.522389205405489e-05,-7.503486995119601e-05,-7.450692646671087e-05,-7.17860457370989e-05,-7.102042582118884e-05,-6.958752783248201e-05,-6.904438487254083e-05,-6.72093010507524e-05,-6.580300396308303e-05,-6.494305853266269e-05,-6.222642696229741e-05,-5.8850051573244855e-05,-5.426935967989266e-05,-5.240425525698811e-05,-5.173250974621624e-05,-4.892014112556353e-05,-4.864267975790426e-05,-4.699573037214577e-05,-3.811851274804212e-05,-3.533385824994184e-05,-3.51256130670663e-05,-3.424756141612306e-05,-3.357700916239992e-05,-3.2184496376430616e-05,-3.07741756842006e-05,-2.940304511866998e-05,-2.1641179046127945e-05,-2.056881749012973e-05,-2.0523146304185502e-05,-2.0107552700210363e-05,-1.9801631424343213e-05,-1.959465407708194e-05,-1.9086077372776344e-05,-1.8489361536921933e-05,-1.779168815119192e-05,-1.6889720427570865e-05,-1.6377716747228988e-05,-1.5501529560424387e-05,-1.530460940557532e-05,-1.4516562259814236e-05,-1.4458224541158415e-05,-1.4369338714459445e-05,-1.3200045032135677e-05,-1.3157650755601935e-05,-1.267924926651176e-05,-1.2468844943214208e-05,-1.1031702342734206e-05,-9.32723287405679e-06,-8.629039257357363e-06,-6.576180567208212e-06,-6.326511538645718e-06,-3.7641823382728035e-06,-3.743693241631263e-06,-2.8366596325213322e-06,-2.8233230295882095e-06,-2.3774803139531286e-06,-8.708238397048262e-07,-3.4727156617009314e-07,3.993511299427155e-08,4.675239324569702e-07,1.4496595213131513e-06,1.6348808458133135e-06,8.629113835922908e-06,8.701309525349643e-06,8.856207386997994e-06,8.990243259177078e-06,9.387061254528817e-06,1.2267157217138447e-05,1.2848153346567415e-05,1.3040974408795591e-05,1.4740675396751612e-05,1.70816474565072e-05,1.818381315388251e-05,1.9322185835335404e-05,2.1017864128225483e-05,2.3258775399881415e-05,2.4146436771843582e-05,2.6399717171443626e-05,2.7865693482453935e-05,2.8104930606787093e-05,2.9579327019746415e-05,3.110103352810256e-05,3.126770388917066e-05,3.193408338120207e-05,3.3845455618575215e-05,3.51208436768502e-05,3.533661219989881e-05,3.5398901673033834e-05,3.8829744880786166e-05,3.913954060408287e-05,4.508078200160526e-05,4.5223310735309497e-05,4.716910552815534e-05,4.948690548189916e-05,5.0814374844776466e-05,5.099035843159072e-05,5.187429633224383e-05,5.269840403343551e-05,5.4547115723835304e-05,5.59014079044573e-05,5.642279938911088e-05,5.715131919714622e-05,5.756325845140964e-05,6.124205538071692e-05,6.152466085040942e-05,6.187133840285242e-05,6.33509480394423e-05,6.515450513688847e-05,6.515785935334861e-05,6.761454278603196e-05,6.86754283378832e-05,6.966695218579844e-05,7.447905954904854e-05,7.546626147814095e-05,7.827073568478227e-05,7.941104558994994e-05,8.076064113993198e-05,8.636802522232756e-05,8.793987217359245e-05,8.821964001981542e-05,8.939951658248901e-05,9.046763443620875e-05,9.152375423582271e-05,9.279847290599719e-05,9.667358972365037e-05,9.87306993920356e-05,9.90907137747854e-05,0.00010009489778894931,0.00010240390838589519,0.00010416180157335475,0.00010528378334129229,0.00010695591481635347,0.00010814823326654732,0.00011015869677066803,0.00011084638390457258,0.00011118046677438542,0.00011219270527362823,0.00012009404599666595,0.00012238584167789668,0.0001246011961484328,0.00012522548786364496,0.00012763775885105133,0.00013011082774028182,0.00013273171498440206,0.00013370037777349353,0.00013475530431605875,0.0001349620579276234,0.00013497553300112486,0.0001353646075585857,0.0001378742599627003,0.0001431466662324965,0.0001448621624149382,0.00014487899898085743,0.00014666073548141867,0.00014690496027469635,0.0001477959012845531,0.00015186116797849536,0.00015309244918171316,0.0001535078918095678,0.00015414140943903476,0.00015546605573035777,0.0001573491026647389,0.0001583962148288265,0.0001596619258634746,0.00016017384768929332,0.0001620505063328892,0.00016472741845063865,0.00016507618420291692,0.00016741662693675607,0.00017562948050908744,0.00017617038975004107,0.0001765110355336219,0.0001817526645027101,0.00018692224693950266,0.00018729575094766915,0.00018758467922452837,0.0001880170457297936,0.00018844156875275075,0.00019066214736085385,0.0001971367746591568,0.00020261332974769175,0.00020336025045253336,0.0002034161298070103,0.00020565054728649557,0.00021190881670918316,0.00021232120343483984,0.00021280489454511553,0.00021468952763825655,0.00021563455811701715,0.0002161216689273715,0.0002164913748856634,0.00021665006352122873,0.00022073328727856278,0.00022436112340074033,0.0002251298719784245,0.00022735551465302706,0.00022889100364409387,0.00022958264162298292,0.00022962770890444517,0.0002322164218639955,0.00023627496557310224,0.00023722923651803285,0.00024408161698374897,0.00024763838155195117,0.0002506920718587935,0.00025747879408299923,0.0002645944769028574,0.0002660251338966191,0.00026683180476538837,0.00026987321325577796,0.00027575285639613867,0.000282392866211012,0.0002827298012562096,0.0002877791994251311,0.0002887843584176153,0.00028922900673933327,0.000291149306576699,0.00029408789123408496,0.00029796577291563153,0.0003015846887137741,0.0003031549567822367,0.0003037598798982799,0.00030599639285355806,0.000306363042909652,0.00030827565933577716,0.00031006804783828557,0.0003130169934593141,0.0003133105346933007,0.0003157835453748703,0.0003191801952198148,0.0003202544176019728,0.00032182878931052983,0.0003272118337918073,0.0003297442162875086,0.0003317277878522873,0.0003359465335961431,0.0003364687436260283,0.0003461536834947765,0.00034857765422202647,0.0003537125885486603,0.00035551300970837474,0.00035686566843651235,0.00035899749491363764,0.0003634410386439413,0.0003665944095700979,0.00036833129706792533,0.0003749607421923429,0.00037636898923665285,0.0003841207944788039,0.00038740067975595593,0.0003883158497046679,0.0003895731351803988,0.0003895858535543084,0.0003927949001081288,0.0004030626150779426,0.00041066951234824955,0.00041119978413917124,0.00041253797826357186,0.0004141988465562463,0.00041455074097029865,0.00041462256922386587,0.0004182957927696407,0.0004285689501557499,0.00042876400402747095,0.00043385193566791713,0.0004344109329394996,0.00043911911780014634,0.0004490161081776023,0.00045037895324639976,0.00045325764222070575,0.00046328784083016217,0.0004724885511677712,0.00048271624837070704,0.0004845108196604997,0.0004894417361356318,0.0004921435029245913,0.0004927300033159554,0.0005007206927984953,0.0005023684352636337,0.0005092722130939364,0.0005202455795370042,0.0005251546972431242,0.0005401551607064903,0.0005439398228190839,0.0005448987940326333,0.0005491054616868496,0.0005512663628906012,0.0005520897102542222,0.000555393926333636,0.0005557577824220061,0.0005706429947167635,0.0005708621465601027,0.0005768798291683197,0.0005832695751450956,0.0005837032222189009,0.0005869042943231761,0.0005990851204842329,0.0006002250593155622,0.000610672403126955,0.0006135049625299871,0.0006142110214568675,0.0006181767676025629,0.000620042672380805,0.0006215492030605674,0.0006347557064145803,0.0006377890822477639,0.000643670791760087,0.0006488172803074121,0.0006518381997011602,0.0006547584198415279,0.0006570561090484262,0.0006587530951946974,0.0006649095448665321,0.0006913852412253618,0.0006923392647877336,0.0007106054690666497,0.0007210898911580443,0.0007288953638635576,0.0007373629487119615,0.0007436604937538505,0.0007459504413418472,0.0007532472955062985,0.0007582681719213724,0.0007597882067784667,0.0007630908512510359,0.0007634551730006933,0.0007696262910030782,0.0007779216975905001,0.0007814708515070379,0.0007854492287151515,0.000797074637375772,0.0007976808119565248,0.0008114989614114165,0.0008177052368409932,0.0008191528031602502,0.0008270894177258015,0.0008279992616735399,0.0008284274372272193,0.0008285462390631437,0.000839430489577353,0.0008413227624259889,0.0008615136030130088,0.0008624957990832627,0.0008641037275083363,0.0008837501518428326,0.0008947578608058393,0.0008971815695986152,0.0009096118155866861,0.0009124120115302503,0.000926629698369652,0.0009277957724407315,0.0009282449027523398,0.0009313997579738498,0.0009319314267486334,0.0009330676984973252,0.0009373578359372914,0.0009428395424038172,0.0009451049845665693,0.0009524361812509596,0.0009612231515347958,0.0009702768293209374,0.0009706347482278943,0.0009788615861907601,0.0009846944594755769,0.0009851624490693212,0.000996304559521377,0.0009975263383239508,0.0009993247222155333,0.001005323720164597,0.0010106978006660938,0.0010137625504285097,0.0010154228657484055,0.001026544370688498,0.0010335649130865932,0.0010377391008660197,0.001039545051753521,0.0010428131790831685,0.001048451755195856,0.0010548002319410443,0.0010566565906628966,0.0010685441084206104,0.0010803834302350879,0.0010803842451423407,0.001081322319805622,0.0010947802802547812,0.001096108928322792,0.0011114523513242602,0.0011339032789692283,0.0011520992266014218,0.0011545706074684858,0.001156904618255794,0.0011632487876340747,0.0011714271968230605,0.0011722781928256154,0.0011725940275937319,0.0011738571338355541,0.0011790540302172303,0.0011824493994936347,0.0011877231299877167,0.0011904517887160182,0.0012014936655759811,0.001204884727485478,0.0012111393734812737,0.0012145561631768942,0.0012272773310542107,0.0012336154468357563,0.001236340613104403,0.0012393348151817918,0.0012639370979741216,0.001266817795112729,0.0012801417615264654,0.0012824423611164093,0.0012942588655278087,0.001302017830312252,0.0013151132734492421,0.001322529511526227,0.0013261124258860946,0.0013322730083018541,0.001335039152763784,0.001335279899649322,0.0013380380114540458,0.0013582685496658087,0.0013608140870928764,0.0013657569652423263,0.0013709572376683354,0.0013818693114444613,0.0014025892596691847,0.001410492928698659,0.0014238902367651463,0.001450971351005137,0.0014529931358993053,0.0014638133579865098,0.0014694349374622107,0.0014724237844347954,0.001477348036132753,0.0014801854267716408,0.0014855520566925406,0.0014857733622193336,0.0014910477912053466,0.0014978385297581553,0.0015024730237200856,0.0015095346607267857,0.0015265329275280237,0.0015351629117503762,0.0015596055891364813,0.0015813685022294521,0.0015870884526520967,0.0016032386338338256,0.0016047043027356267,0.0016063475050032139,0.0016099992208182812,0.0016343002207577229,0.0016395956045016646,0.0016405275091528893,0.0016544145764783025,0.001655242987908423,0.0016556817572563887,0.0016590188024565578,0.0016604490811005235,0.0016678812680765986,0.0016951218713074923,0.0016984387766569853,0.0017211177619174123,0.0017313655698671937,0.0017361390637233853,0.0017444202676415443,0.0017553367651998997,0.0017615258693695068,0.0017639946890994906,0.0017771476414054632,0.0017907251603901386,0.0017916741780936718,0.001800425467081368,0.0018180054612457752,0.0018326792633160949,0.0018352058250457048,0.0018470165086910129,0.00186880340334028,0.0018754706252366304,0.0018827378517016768,0.001884432160295546,0.0018852370558306575,0.0019077998585999012,0.001915294793434441,0.0019154050387442112,0.001915810746140778,0.0019199141534045339,0.0019353782990947366,0.001938347239047289,0.0019402152393013239,0.0019508355762809515,0.0019651029724627733,0.0019673886708915234,0.001974870217964053,0.001977987354621291,0.00198960117995739,0.0019980669021606445,0.0020029053557664156,0.0020083249546587467,0.0020141678396612406,0.0020269707310944796,0.002035503974184394,0.002052957657724619,0.0020534845534712076,0.002054742304608226,0.0020586159080266953,0.002085058717057109,0.002093874616548419,0.0021026371978223324,0.002103555016219616,0.0021059508435428143,0.0021147674415260553,0.0021430975757539272,0.0021484079770743847,0.0021600392647087574,0.0021706789266318083,0.0021722493693232536,0.0021754675544798374,0.0021822545677423477,0.002184131182730198,0.002184721175581217,0.0021855493541806936,0.0021994973067194223,0.002200290560722351,0.002203562529757619,0.0022089898120611906,0.0022310821805149317,0.002242825459688902,0.002253035083413124,0.0022696787491440773,0.0022699469700455666,0.002274914877489209,0.002290345961228013,0.0022972910664975643,0.002298482460901141,0.0023162798024713993,0.0023452541790902615,0.00234794057905674,0.0023518188390880823,0.0023682587780058384,0.0023742634803056717,0.002377042779698968,0.002379937330260873,0.002399536781013012,0.002403728896752,0.0024051594082266092,0.0024074085522443056,0.002421353943645954,0.002423861064016819,0.002455097623169422,0.0024660206399858,0.002474171807989478,0.002485481556504965,0.002505026524886489,0.002514042891561985,0.002516756998375058,0.0025171188171952963,0.0025174329057335854,0.0025190392043441534,0.002548349555581808,0.0025489232502877712,0.002561528468504548,0.0025671871844679117,0.0025684041902422905,0.002580344444140792,0.0025966328103095293,0.002603033557534218,0.002619056962430477,0.0026224018074572086,0.0026517710648477077,0.002658712910488248,0.0026668626815080643,0.0026940973475575447,0.0027039730921387672,0.00273367902263999,0.002765462500974536,0.002777341054752469,0.0028417492285370827,0.002857910469174385,0.0028622776735574007,0.0028875970747321844,0.002912108087912202,0.0029170585330575705,0.0029284467454999685,0.002933213720098138,0.002983161248266697,0.0029851056169718504,0.002990707056596875,0.003004222409799695,0.00300461589358747,0.0030108552891761065,0.003026009304448962,0.0030605040956288576,0.0030661828350275755,0.0030774930492043495,0.003085786011070013,0.0031056725420057774,0.0031067689415067434,0.0031163906678557396,0.003121590008959174,0.0031245991121977568,0.0031423831824213266,0.003200945444405079,0.0032266743946820498,0.0032272585667669773,0.0032430184073746204,0.00325169344432652,0.003267718246206641,0.0032714647240936756,0.0032771858386695385,0.003283517435193062,0.0033023888245224953,0.003391401143744588,0.003392053535208106,0.0033953741658478975,0.003408878343179822,0.0034610703587532043,0.0034850919619202614,0.003490255679935217,0.0035035384353250265,0.0035326823126524687,0.0035371948033571243,0.0035638136323541403,0.0035707142669707537,0.0035783774219453335,0.0036017494276165962,0.0036155711859464645,0.0036322944797575474,0.003657989902421832,0.0036812725011259317,0.00372368935495615,0.0037356240209192038,0.0037554416339844465,0.0037734899669885635,0.0037825163453817368,0.003801074344664812,0.003822167869657278,0.0038248258642852306,0.0038343751803040504,0.0038640806451439857,0.003915931098163128,0.003943596500903368,0.003945938777178526,0.00397889968007803,0.003987705800682306,0.004064120817929506,0.004084690939635038,0.004102369770407677,0.004106440115720034,0.004113052971661091,0.004128327127546072,0.004200005438178778,0.0042228070087730885,0.004225204698741436,0.004253057762980461,0.004257598891854286,0.00426147598773241,0.004270448349416256,0.004378100391477346,0.004434949718415737,0.00449164304882288,0.004524394404143095,0.004534707870334387,0.004549690056592226,0.004556780215352774,0.004612367134541273,0.004645516164600849,0.004660322796553373,0.004690322559326887,0.004703724756836891,0.004730709362775087,0.004733865614980459,0.004763099364936352,0.004775329027324915,0.00482089864090085,0.004849411081522703,0.004867844749242067,0.004875586833804846,0.004945870954543352,0.004977490287274122,0.005006553139537573,0.005007897038012743,0.005017142742872238,0.005031996872276068,0.005041925702244043,0.005044351797550917,0.005106904543936253,0.005161082837730646,0.005176041275262833,0.005273420829325914,0.005279971286654472,0.005296105053275824,0.005305564496666193,0.005320967640727758,0.005325979553163052,0.005334265064448118,0.0053360420279204845,0.005373654421418905,0.005408171564340591,0.005416692700237036,0.005421857815235853,0.0054304879158735275,0.005441006273031235,0.005461826920509338,0.005503663793206215,0.005543950013816357,0.005550384521484375,0.0056558577343821526,0.005660397931933403,0.005765561014413834,0.0057702697813510895,0.005838452372699976,0.005840648431330919,0.005844461731612682,0.00597089109942317,0.0059850080870091915,0.006018653977662325,0.006032581441104412,0.00603486318141222,0.0060599446296691895,0.0060790847055613995,0.006095024291425943,0.006125335581600666,0.006131498608738184,0.006141240708529949,0.006225886754691601,0.006270689889788628,0.006313953548669815,0.0063958256505429745,0.00645936606451869,0.006471929606050253,0.006482154596596956,0.006506382022053003,0.0065140752121806145,0.0065277754329144955,0.006680861581116915,0.006693671923130751,0.006695321295410395,0.006708699278533459,0.0067178779281675816,0.006723427213728428,0.0067345090210437775,0.006744043901562691,0.006755447946488857,0.006756624672561884,0.006758581846952438,0.006783841177821159,0.006785606034100056,0.006795161869376898,0.006892968434840441,0.0068941544741392136,0.006938630249351263,0.006961420178413391,0.0070011247880756855,0.007009314373135567,0.007016531191766262,0.007019238546490669,0.007024210877716541,0.007078950293362141,0.007082888390868902,0.007107914891093969,0.007121381815522909,0.007180486805737019,0.007195094600319862,0.007209127768874168,0.007245474029332399,0.007248538546264172,0.007271185517311096,0.007285207975655794,0.0073126922361552715,0.00735031021758914,0.007390405051410198,0.007397432811558247,0.007404850795865059,0.007420517038553953,0.007426861673593521,0.007437786553055048,0.007451423443853855,0.007466057315468788,0.007499119266867638,0.0075349449180066586,0.0075478400103747845,0.007578992750495672,0.007588515058159828,0.007605100981891155,0.007607096806168556,0.007700851187109947,0.007719165179878473,0.007776328362524509,0.007894936949014664,0.007964646443724632,0.007991505786776543,0.008019637316465378,0.008084625005722046,0.008096011355519295,0.008120739832520485,0.008129670284688473,0.008144230581820011,0.008180618286132812,0.00820629671216011,0.008213918656110764,0.008254258893430233,0.00829349085688591,0.008340760134160519,0.008364802226424217,0.008484935387969017,0.008497298695147038,0.008506106212735176,0.008584137074649334,0.008604241535067558,0.00863182544708252,0.00863331463187933,0.00897363293915987,0.009064160287380219,0.00915205106139183,0.009175753220915794,0.009209778159856796,0.009280207566916943,0.00931856594979763,0.00932713970541954,0.009504829533398151,0.0095150051638484,0.009670350700616837,0.009673179127275944,0.00980975478887558,0.009849943220615387,0.009882129728794098,0.00994157139211893,0.010107670910656452,0.010313197039067745,0.010405399836599827,0.010430914349853992,0.010455306619405746,0.010492022149264812,0.010545659810304642,0.010619524866342545,0.010658404789865017,0.010739865712821484,0.010758094489574432,0.010800960473716259,0.010856783017516136,0.010927046649158001,0.010934271849691868,0.010956465266644955,0.011007064953446388,0.011079361662268639,0.011094924993813038,0.011141162365674973,0.011266202665865421,0.011316495947539806,0.011320572346448898,0.011360063217580318,0.01136104017496109,0.011414511129260063,0.011443465016782284,0.011514104902744293,0.01153331995010376,0.011630913242697716,0.011662688106298447,0.011688482947647572,0.011728210374712944,0.011910105124115944,0.011922018602490425,0.011923376470804214,0.011940248310565948,0.01200239546597004,0.012180576100945473,0.01218998059630394,0.012204787693917751,0.012295409105718136,0.01231581810861826,0.01239592395722866,0.012409459799528122,0.012558357790112495,0.01264986302703619,0.012752678245306015,0.012758778408169746,0.012831101194024086,0.01291135884821415,0.012961341068148613,0.012994099408388138,0.013006780296564102,0.013149688020348549,0.013264150358736515,0.013293249532580376,0.01329681184142828,0.013305160216987133,0.013379555195569992,0.0134248873218894,0.013503700494766235,0.01352034229785204,0.013558979146182537,0.013592064380645752,0.013634659349918365,0.013666819781064987,0.013727656565606594,0.01374733168631792,0.0138178039342165,0.013957145623862743,0.014161083847284317,0.014293283224105835,0.014494087547063828,0.014551961794495583,0.014635392464697361,0.014648900367319584,0.014755599200725555,0.014985666610300541,0.015001281164586544,0.015178795903921127,0.01532031036913395,0.0154535211622715,0.015563949011266232,0.015924932435154915,0.015938464552164078,0.01619354449212551,0.016272112727165222,0.016584841534495354,0.016633039340376854,0.01701856404542923,0.017053313553333282,0.01712941937148571,0.01722732186317444,0.017352456226944923,0.017405765131115913,0.01769237220287323,0.017759431153535843,0.018041599541902542,0.018048789352178574,0.018120748922228813,0.018154019489884377,0.018213516101241112,0.018278196454048157,0.018407370895147324,0.018502511084079742,0.018532082438468933,0.01855786144733429,0.01860949955880642,0.018872886896133423,0.019062843173742294,0.019098306074738503,0.019310131669044495,0.019485650584101677,0.019511628895998,0.020112907513976097,0.020127877593040466,0.02039429172873497,0.02040291577577591,0.020498614758253098,0.020606668666005135,0.020886965095996857,0.021032636985182762,0.02125752903521061,0.021356524899601936,0.021389739587903023,0.021909553557634354,0.02196987345814705,0.022481927648186684,0.022562015801668167,0.02318667247891426,0.02335961163043976,0.02339777909219265,0.02349989116191864,0.023514678701758385,0.023674942553043365,0.0239510890096426,0.02406631037592888,0.024595413357019424,0.02515597641468048,0.025252923369407654,0.02537301369011402,0.025683239102363586,0.025753572583198547,0.026251615956425667,0.026311421766877174,0.02652577869594097,0.02655528485774994,0.02683166228234768,0.026898428797721863,0.026898931711912155,0.027083994820713997,0.02712949365377426,0.027238735929131508,0.027689021080732346,0.02784675545990467,0.028212327510118484,0.028371816501021385,0.02853473648428917,0.028809690847992897,0.028912924230098724,0.029201602563261986,0.029241682961583138,0.029582910239696503,0.029996519908308983,0.030038364231586456,0.030595242977142334,0.03060208261013031,0.030603278428316116,0.03166107088327408,0.03168268874287605,0.031859856098890305,0.03196679800748825,0.03220352530479431,0.03244410455226898,0.03302740678191185,0.03336702287197113,0.03355826064944267,0.03375442326068878,0.03419741243124008,0.03504550829529762,0.03525688871741295,0.035611312836408615,0.035800445824861526,0.036847785115242004,0.03728296235203743,0.03764389827847481,0.03769226372241974,0.038062721490859985,0.03832712769508362,0.03979886695742607,0.04056927561759949,0.04108990728855133,0.04164426401257515,0.04237144812941551,0.04273683577775955,0.04306286200881004,0.04340050742030144,0.044041670858860016,0.04406657814979553,0.04447241872549057,0.04450041428208351,0.04594568908214569,0.04619457200169563,0.04686872661113739,0.04695476219058037,0.04762272909283638,0.04803501442074776,0.048157963901758194,0.048166610300540924,0.048443183302879333,0.048569902777671814,0.04983145371079445,0.05009216442704201,0.051076602190732956,0.05208862945437431,0.05230401083827019,0.05338184908032417,0.05400833487510681,0.0577402263879776,0.05811246857047081,0.05858558788895607,0.05910246819257736,0.06071747839450836,0.06145612895488739,0.06210891902446747,0.06282752752304077,0.065894216299057,0.06644336134195328,0.0682731568813324,0.06891372054815292,0.06895182281732559,0.07073260098695755,0.07102195918560028,0.07131633907556534,0.07235263288021088,0.07423737645149231,0.07511632144451141,0.07669734954833984,0.07778475433588028,0.07849118858575821,0.08099222183227539,0.08486997336149216,0.08504639565944672,0.08734019100666046,0.08887413889169693,0.09052089601755142,0.09092331677675247,0.09142863005399704,0.09232436865568161,0.09308841079473495,0.09740366041660309,0.09889177232980728,0.10390975326299667,0.10527860373258591,0.10880443453788757,0.11599244922399521,0.11693021655082703,0.12138264626264572,0.12563477456569672,0.13013708591461182,0.1319034993648529,0.1493956595659256,0.15430589020252228,0.16302889585494995,0.16652508080005646,0.17122222483158112,0.17839248478412628,0.19428318738937378,0.21459774672985077,0.22186912596225739],\"yaxis\":\"y\",\"type\":\"scattergl\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Sorted neurons\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Loss change\"}},\"legend\":{\"title\":{\"text\":\"variable\"},\"tracegroupgap\":0},\"title\":{\"text\":\"Loss change from ablating MLP5 neuron\"},\"width\":800,\"showlegend\":true},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('59e49475-4b06-440a-ad41-62b78d85ca4e');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sorted_means, indices = torch.sort(diffs.mean(1))\n",
    "sorted_means = sorted_means.tolist()\n",
    "haystack_utils.line(sorted_means, xlabel=\"Sorted neurons\", ylabel=\"Loss change\", title=\"Loss change from ablating MLP5 neuron\") # xticks=indices\n",
    "sorted_top_neuron_indices = indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>                            <div id=\"887b7562-2959-415a-9b1b-95098a2c1b0b\" class=\"plotly-graph-div\" style=\"height:525px; width:1000px;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"887b7562-2959-415a-9b1b-95098a2c1b0b\")) {                    Plotly.newPlot(                        \"887b7562-2959-415a-9b1b-95098a2c1b0b\",                        [{\"error_y\":{\"array\":[1.4272304780717975],\"type\":\"data\",\"visible\":true},\"name\":\"Original\",\"x\":[\"Original\"],\"y\":[1.7170048448909074],\"type\":\"bar\"},{\"error_y\":{\"array\":[1.9646062013276773],\"type\":\"data\",\"visible\":true},\"name\":\"Ablated\",\"x\":[\"Ablated\"],\"y\":[3.7367643067240714],\"type\":\"bar\"},{\"error_y\":{\"array\":[1.3925685370654368],\"type\":\"data\",\"visible\":true},\"name\":\"MLP5 path patched\",\"x\":[\"MLP5 path patched\"],\"y\":[2.205931183621287],\"type\":\"bar\"},{\"error_y\":{\"array\":[1.8702686627922378],\"type\":\"data\",\"visible\":true},\"name\":\"MLP5 path patched + Top 10 MLP5 neurons ablated\",\"x\":[\"Top MLP5 removed\"],\"y\":[4.190492509007454],\"type\":\"bar\"},{\"error_y\":{\"array\":[0.652157984993019],\"type\":\"data\",\"visible\":true},\"name\":\"MLP5 path patched + Bottom 10 MLP5 neurons ablated\",\"x\":[\"Bottom MLP5 removed\"],\"y\":[0.592893252694048],\"type\":\"bar\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"title\":{\"text\":\"Average last token loss when removing top \\u002f bottom neurons from path patching\"},\"xaxis\":{\"title\":{\"text\":\"\"}},\"yaxis\":{\"title\":{\"text\":\"Loss\"}},\"barmode\":\"group\",\"width\":1000,\"showlegend\":true},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('887b7562-2959-415a-9b1b-95098a2c1b0b');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check loss change when ablating top / bottom neurons\n",
    "top_neurons_count = 10\n",
    "top_neurons = indices[-top_neurons_count:]\n",
    "bottom_neurons = indices[:top_neurons_count]\n",
    "\n",
    "with model.hooks(deactivate_neurons_fwd_hooks):\n",
    "    ablated_loss, ablated_cache = model.run_with_cache(prompts, return_type=\"loss\")\n",
    "\n",
    "ablate_top_neurons_hook = get_ablate_neurons_hook(top_neurons, ablated_cache)\n",
    "ablate_bottom_neurons_hook = get_ablate_neurons_hook(bottom_neurons, ablated_cache)\n",
    "\n",
    "original_loss, ablated_loss, _, all_MLP5_loss = haystack_utils.get_direct_effect(prompts, model, pos=-1, context_ablation_hooks=deactivate_neurons_fwd_hooks, context_activation_hooks=activate_neurons_fwd_hooks)\n",
    "_, _, _, top_MLP5_ablated_loss = haystack_utils.get_direct_effect(prompts, model, pos=-1, context_ablation_hooks=deactivate_neurons_fwd_hooks, context_activation_hooks=activate_neurons_fwd_hooks+ablate_top_neurons_hook)\n",
    "_, _, _, bottom_MLP5_ablated_loss = haystack_utils.get_direct_effect(prompts, model, pos=-1, context_ablation_hooks=deactivate_neurons_fwd_hooks, context_activation_hooks=activate_neurons_fwd_hooks+ablate_bottom_neurons_hook)\n",
    "\n",
    "names = [\"Original\", \"Ablated\", \"MLP5 path patched\", f\"MLP5 path patched + Top {top_neurons_count} MLP5 neurons ablated\", f\"MLP5 path patched + Bottom {top_neurons_count} MLP5 neurons ablated\"]\n",
    "short_names = [\"Original\", \"Ablated\", \"MLP5 path patched\", f\"Top MLP5 removed\", f\"Bottom MLP5 removed\"]\n",
    "\n",
    "values = [original_loss.tolist(), ablated_loss.tolist(), all_MLP5_loss.tolist(), top_MLP5_ablated_loss.tolist(), bottom_MLP5_ablated_loss.tolist()]\n",
    "haystack_utils.plot_barplot(values, names, short_names=short_names, ylabel=\"Loss\", title=f\"Average last token loss when removing top / bottom neurons from path patching\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigate top / bottom neuron boosts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Patch in MLP5 neurons, ablated bottom neurons, compare with patched MLP5 neurons without ablating bottom neurons\n",
    "\n",
    "with model.hooks(deactivate_neurons_fwd_hooks):\n",
    "    ablated_logits, ablated_cache = model.run_with_cache(prompts)\n",
    "\n",
    "ablate_top_neurons_hook = get_ablate_neurons_hook(top_neurons, ablated_cache)\n",
    "ablate_bottom_neurons_hook = get_ablate_neurons_hook(bottom_neurons, ablated_cache)\n",
    "\n",
    "original_logits, ablated_logprobs, _, all_MLP5_logprobs = haystack_utils.get_direct_effect(prompts, model, pos=-2, context_ablation_hooks=deactivate_neurons_fwd_hooks, context_activation_hooks=activate_neurons_fwd_hooks, return_type='logprobs')\n",
    "_, _, _, top_MLP5_ablated_logprobs = haystack_utils.get_direct_effect(prompts, model, pos=-2, context_ablation_hooks=deactivate_neurons_fwd_hooks, context_activation_hooks=activate_neurons_fwd_hooks+ablate_top_neurons_hook, return_type='logprobs')\n",
    "_, _, _, bottom_MLP5_ablated_logprobs = haystack_utils.get_direct_effect(prompts, model, pos=-2, context_ablation_hooks=deactivate_neurons_fwd_hooks, context_activation_hooks=activate_neurons_fwd_hooks+ablate_bottom_neurons_hook, return_type='logprobs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_difference_neurons(baseline_logprobs, ablated_logprops, positive=True, logprob_threshold=-7, k=50):\n",
    "    neuron_logprob_difference = (baseline_logprobs - ablated_logprops).mean(0)\n",
    "    neuron_logprob_difference[baseline_logprobs.mean(0) < logprob_threshold] = 0\n",
    "    if positive:\n",
    "        non_zero_count = (neuron_logprob_difference > 0).sum()\n",
    "    else:\n",
    "        non_zero_count = (neuron_logprob_difference < 0).sum()\n",
    "    top_logprob_difference, top_neurons = haystack_utils.top_k_with_exclude(neuron_logprob_difference, min(non_zero_count, k), all_ignore, largest=positive)\n",
    "    return top_logprob_difference, top_neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boosted tokens\n",
    "bottom_neuron_pos_difference_logprobs, bottom_pos_indices = get_top_difference_neurons(all_MLP5_logprobs, bottom_MLP5_ablated_logprobs, positive=True)\n",
    "top_neuron_pos_difference_logprobs, top_pos_indices = get_top_difference_neurons(all_MLP5_logprobs, top_MLP5_ablated_logprobs, positive=True)\n",
    "# Deboosted tokens\n",
    "bottom_neuron_neg_difference_logprobs, bottom_neg_indices = get_top_difference_neurons(all_MLP5_logprobs, bottom_MLP5_ablated_logprobs, positive=False)\n",
    "top_neuron_neg_difference_logprobs, top_neg_indices = get_top_difference_neurons(all_MLP5_logprobs, top_MLP5_ablated_logprobs, positive=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "687848e782864c179de7ccae5f9a8e93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='Tokens:', options=('Boosted', 'Deboosted'), value='Boosted'), Drop…"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def plot_data(boosted_deboosted, top_bottom):\n",
    "    if boosted_deboosted == 'Boosted' and top_bottom == 'Top':\n",
    "        logprobs = top_neuron_pos_difference_logprobs\n",
    "        indices = top_pos_indices\n",
    "        title_change = 'increase'\n",
    "    elif boosted_deboosted == 'Boosted' and top_bottom == 'Bottom':\n",
    "        logprobs = bottom_neuron_pos_difference_logprobs\n",
    "        indices = bottom_pos_indices\n",
    "        title_change = 'increase'\n",
    "    elif boosted_deboosted == 'Deboosted' and top_bottom == 'Top':\n",
    "        logprobs = top_neuron_neg_difference_logprobs\n",
    "        indices = top_neg_indices\n",
    "        title_change = 'decrease'\n",
    "    else:  # 'Deboosted' and 'Bottom'\n",
    "        logprobs = bottom_neuron_neg_difference_logprobs\n",
    "        indices = bottom_neg_indices\n",
    "        title_change = 'decrease'\n",
    "\n",
    "    xlabel = boosted_deboosted + \" tokens\"\n",
    "    ylabel = \"full_logprob - ablated_logprop\"\n",
    "    title = 'Logprob ' + title_change + ' from ' + top_bottom.lower() + ' neurons'\n",
    "    xticks = [model.to_str_tokens([i])[0] for i in indices]\n",
    "\n",
    "    haystack_utils.line(logprobs.cpu().numpy(), xlabel=xlabel, ylabel=ylabel, title=title, xticks=xticks, show_legend=False)\n",
    "\n",
    "\n",
    "boosted_deboosted = widgets.Dropdown(options=['Boosted', 'Deboosted'], value='Boosted', description='Tokens:')\n",
    "top_bottom = widgets.Dropdown(options=['Top', 'Bottom'], value='Top', description='Neurons:')\n",
    "widgets.interactive(plot_data, boosted_deboosted=boosted_deboosted, top_bottom=top_bottom)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decompose neuron effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get logprob change for individual neuron\n",
    "\n",
    "def get_individual_neuron_logprob_effect(sorted_indices, neuron_pos=0, top=True, positive=True, plot=True):\n",
    "    if top:\n",
    "        neurons = sorted_indices[-(neuron_pos+1)]\n",
    "    else:\n",
    "        neurons = sorted_indices[neuron_pos]\n",
    "    neurons_hook = get_ablate_neurons_hook([neurons], ablated_cache)\n",
    "\n",
    "    original_logits, ablated_logprobs, _, all_MLP5_logprobs = haystack_utils.get_direct_effect(prompts, model, pos=-2, context_ablation_hooks=deactivate_neurons_fwd_hooks, context_activation_hooks=activate_neurons_fwd_hooks, return_type='logprobs')\n",
    "    _, _, _, neuron_ablated_logprobs = haystack_utils.get_direct_effect(prompts, model, pos=-2, context_ablation_hooks=deactivate_neurons_fwd_hooks, context_activation_hooks=activate_neurons_fwd_hooks+neurons_hook, return_type='logprobs')\n",
    "    neuron_difference_logprobs, difference_indices = \\\n",
    "        get_top_difference_neurons(all_MLP5_logprobs, neuron_ablated_logprobs, positive=positive)\n",
    "    \n",
    "    if not plot:\n",
    "        return neuron_difference_logprobs, difference_indices\n",
    "    \n",
    "    if positive:\n",
    "        xlabel = 'Boosted tokens'\n",
    "        title = 'Logprob increase from neuron ' + str(neurons.item())\n",
    "    else: \n",
    "        xlabel = 'Deboosted tokens'\n",
    "        title = 'Logprob decrease from neuron ' + str(neurons.item())\n",
    "    if top:\n",
    "        title += f' (top {neuron_pos+1} neuron)'\n",
    "    else:\n",
    "        title += f' (bottom {neuron_pos+1} neuron)'\n",
    "\n",
    "    xticks = [model.to_str_tokens([i])[0] for i in difference_indices]\n",
    "    haystack_utils.line(neuron_difference_logprobs.cpu().numpy(), xlabel=xlabel, ylabel=\"full_logprob - ablated_logprop\", title=title, xticks=xticks, show_legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ae95a5224c8467ca9c979a5f1633fbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='Neuron Pos:', max=20), Dropdown(description='Neurons:', …"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a function to call your function with widget inputs\n",
    "def plot_individual_neuron(neuron_pos, top_bottom, pos_neg):\n",
    "    top_bool = True if top_bottom == 'Top' else False\n",
    "    positive_bool = True if pos_neg == 'Positive' else False\n",
    "    get_individual_neuron_logprob_effect(indices, neuron_pos=neuron_pos, top=top_bool, positive=positive_bool)\n",
    "\n",
    "# Define widgets\n",
    "neuron_pos_slider = widgets.IntSlider(min=0, max=20, step=1, value=0, description='Neuron Pos:')\n",
    "top_bottom_dropdown = widgets.Dropdown(options=['Top', 'Bottom'], value='Top', description='Neurons:')\n",
    "pos_neg_dropdown = widgets.Dropdown(options=['Positive', 'Negative'], value='Positive', description='Pos/Neg:')\n",
    "\n",
    "# Use interactive function to bind the widgets to the plotting function\n",
    "widgets.interactive(plot_individual_neuron, neuron_pos=neuron_pos_slider, top_bottom=top_bottom_dropdown, pos_neg=pos_neg_dropdown)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum individual weighted positive / negative effects\n",
    "def summed_neuron_differences(num_neurons=10, top=True, positive=True, k=50):\n",
    "    summed_neuron_diffs = torch.zeros(model.cfg.d_vocab).cuda()\n",
    "    for neuron_pos in range(num_neurons):\n",
    "        # It would probably be more principle to do our filtering on the summed diffs instead of per neuron\n",
    "        neuron_diff, token_indices = get_individual_neuron_logprob_effect(indices, neuron_pos=neuron_pos, top=top, positive=positive, plot=False)\n",
    "        summed_neuron_diffs[token_indices] += neuron_diff\n",
    "    if positive:\n",
    "        non_zero_count = (summed_neuron_diffs > 0).sum()\n",
    "    else:\n",
    "        non_zero_count = (summed_neuron_diffs < 0).sum()\n",
    "    top_logprob_difference, top_tokens = haystack_utils.top_k_with_exclude(summed_neuron_diffs, min(non_zero_count, k), all_ignore, largest=positive)\n",
    "    return top_logprob_difference, top_tokens\n",
    "\n",
    "def plot_summed_neuron_differences(num_neurons=10, top=True, positive=True):\n",
    "    top_logprob_difference, top_logprob_tokens = summed_neuron_differences(num_neurons=num_neurons, top=top, positive=positive)\n",
    "    xticks = [model.to_str_tokens([i])[0] for i in top_logprob_tokens]\n",
    "    \n",
    "    if positive:\n",
    "        title = \"Summed individual boosts by\"\n",
    "        xlabel = \"Boosted tokens\"\n",
    "    else:\n",
    "        title = \"Summed individual deboosts by\"\n",
    "        xlabel = \"Deboosted tokens\"\n",
    "    if top:\n",
    "        title += f\" top {num_neurons} neurons\"\n",
    "    else:\n",
    "        title += f\" bottom {num_neurons} neurons\"\n",
    "    haystack_utils.line(top_logprob_difference.cpu().numpy(), xlabel=xlabel, ylabel=\"full_logprob - ablated_logprop\", title=title, xticks=xticks, show_legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc6f2248073c4ba99a5186a935708511",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=10, description='Num Neurons:', max=20, min=1), Dropdown(description='Ne…"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a function to call your function with widget inputs\n",
    "def plot_summed_neurons_widget(num_neurons, top_bottom, pos_neg):\n",
    "    top_bool = True if top_bottom == 'Top' else False\n",
    "    positive_bool = True if pos_neg == 'Positive' else False\n",
    "    plot_summed_neuron_differences(num_neurons=num_neurons, top=top_bool, positive=positive_bool)\n",
    "\n",
    "# Define widgets\n",
    "num_neuron_slider = widgets.IntSlider(min=1, max=20, step=1, value=10, description='Num Neurons:')\n",
    "top_bottom_dropdown = widgets.Dropdown(options=['Top', 'Bottom'], value='Top', description='Neurons:')\n",
    "pos_neg_dropdown = widgets.Dropdown(options=['Positive', 'Negative'], value='Positive', description='Pos/Neg:')\n",
    "\n",
    "# Use interactive function to bind the widgets to the plotting function\n",
    "widgets.interactive(plot_summed_neurons_widget, num_neurons=num_neuron_slider, top_bottom=top_bottom_dropdown, pos_neg=pos_neg_dropdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss is logprob(correct_token_logit) = logit(correct_token_logit) - LogSumExp(all_token_logits)\n",
    "Loss = -logprob\n",
    "\n",
    "LogSumExp approximates a maximum function. If the neuron engages in destructive interference of a high logit for a non-answer token, then the exp(logit) for the token will be lower and so the LogSumExp will be more similar to logit(correct_token_logit) so the loss will be lower. So a lower logsumexp(all vocab) is good.\n",
    "\n",
    "If the neuron engages in destructive interference of a low logit for a non-answer token, then the exp(logit) for the the token won't really change and so the logsumexp and the loss will both be the same.\n",
    "\n",
    "For a single neuron:\n",
    "1. For each \"gen\" prompt, zero centre the logits and record the logit of \"gen\". Calculate the mean \"gen\" logit.\n",
    "2. For each \"gen\" prompt, disable the neuron under test, zero centre the logits, edit the logit of \"gen\" to the mean with a hook, then get the logprob of \"gen\" and calculate logsumexp(all vocab) = logit(gen) - logprob(gen).\n",
    "2. For each \"gen\" prompt, enable the neuron, zero centre the logits, edit the logit of \"gen\" to the mean with a hook, then get the logprob of \"gen\" and calculate logsumexp(all vocab) = logit(gen) - logprob(gen).\n",
    "3. Take the difference in logsum exps. If it's positive, the neuron is reducing the loss via destructive interference by the difference.\n",
    "Can use the same procedure for sets of neurons, or for all neurons, to find high level effects of the context neuron\n",
    "\n",
    "Logprobs are logits with a constant subtracted and the constant is the same for every logit within a prompt.\n",
    "\n",
    "Taking the difference in terms with and without a neuron's effect via the context neuron:\n",
    "- If log sum exp increases, the neuron is boosting tokens on average. \n",
    "- If logit increases, the neuron is boosting the correct token\n",
    "\n",
    "\n",
    "Remove the neuron's effects on the gen logit. Take the mean on the prompt and position?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(40.0787, device='cuda:0')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Calculate the mean \"gen\" logit.\n",
    "gen_index = model.to_single_token('gen')\n",
    "gen_logits = []\n",
    "logits = model(prompts, return_type='logits') # batch pos vocab\n",
    "logits = logits - logits.mean(-1).unsqueeze(-1) # batch pos vocab, batch pos 1\n",
    "\n",
    "mean_gen_logit = logits[:, -2, gen_index].mean(0)\n",
    "mean_gen_logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>                            <div id=\"92664d61-45b7-44b7-b671-8e30364e5257\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"92664d61-45b7-44b7-b671-8e30364e5257\")) {                    Plotly.newPlot(                        \"92664d61-45b7-44b7-b671-8e30364e5257\",                        [{\"alignmentgroup\":\"True\",\"bingroup\":\"x\",\"hovertemplate\":\"variable=0\\u003cbr\\u003evalue=%{x}\\u003cbr\\u003ecount=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"0\",\"marker\":{\"color\":\"#636efa\",\"pattern\":{\"shape\":\"\"}},\"name\":\"0\",\"nbinsx\":100,\"offsetgroup\":\"0\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[11.828539848327637,10.094202995300293,11.033581733703613,10.383891105651855,11.223034858703613,10.094202995300293,10.115687370300293,11.46186351776123,12.294032096862793,12.140864372253418,12.22634506225586,10.783870697021484,11.408492088317871,10.434484481811523,11.27364730834961,10.51807975769043,11.97506332397461,10.20490550994873,11.27364730834961,12.164505958557129,12.140864372253418,11.048110008239746,11.745047569274902,11.7483549118042,11.122775077819824,11.10375690460205,11.009413719177246,10.841790199279785,10.115687370300293,10.684741020202637,11.466828346252441,11.408492088317871,10.305498123168945,10.383891105651855,10.316129684448242,10.285383224487305,11.585213661193848,10.368927001953125,10.600566864013672,11.074710845947266,10.841790199279785,11.122775077819824,11.745047569274902,11.751770973205566,10.316129684448242,11.132574081420898,10.318943977355957,11.788850784301758,11.828539848327637,10.600566864013672,13.789572715759277,10.78131103515625,11.404772758483887,10.886754989624023,10.64365291595459,11.074710845947266,12.069652557373047,11.404772758483887,12.145801544189453,12.145801544189453,11.585213661193848,12.424949645996094,12.140864372253418,12.069652557373047,11.373406410217285,10.841790199279785,10.305498123168945,11.535086631774902,10.379386901855469,10.642513275146484,12.069652557373047,12.36260986328125,10.101863861083984,12.303435325622559,9.653680801391602,10.612776756286621,11.223034858703613,12.140864372253418,11.46720027923584,12.424949645996094,10.820554733276367,12.755560874938965,11.404772758483887,11.891207695007324,12.755560874938965,11.036617279052734,11.585213661193848,10.368927001953125,10.115687370300293,10.20490550994873,11.292632102966309,11.151241302490234,12.500862121582031,11.404772758483887,11.97506332397461,11.535086631774902,11.828539848327637,12.439493179321289,13.270489692687988,12.500862121582031,10.318943977355957,11.46720027923584,10.316129684448242,11.10375690460205,11.27364730834961,11.46720027923584,11.155471801757812,11.292632102966309,10.115687370300293,11.404772758483887,10.305498123168945,11.46720027923584,11.157090187072754,11.007722854614258,10.841790199279785,11.151241302490234,11.745047569274902,11.46720027923584,11.373406410217285,10.625128746032715,12.578568458557129,10.730844497680664,12.303435325622559,12.140864372253418,11.828539848327637,10.684741020202637,11.248010635375977,11.751770973205566,11.404772758483887,11.132574081420898,10.383891105651855,10.584381103515625,11.557347297668457,11.175163269042969,11.46720027923584,11.7166748046875,10.783870697021484,11.27364730834961,11.175163269042969,11.122775077819824,10.64365291595459,11.009413719177246,11.007722854614258,12.286802291870117,10.684741020202637,11.176279067993164,12.439493179321289,11.122775077819824,10.783870697021484,12.755560874938965,11.122775077819824,12.069652557373047,12.424949645996094,11.151241302490234,11.122775077819824,12.069652557373047,10.762014389038086,11.160247802734375,12.500862121582031,11.074710845947266,11.671412467956543,10.434484481811523,11.97506332397461,12.294032096862793,10.625128746032715,11.46720027923584,10.600566864013672,12.163540840148926,12.303435325622559,12.500862121582031,12.145801544189453,12.140864372253418,12.069652557373047,10.612776756286621,11.151241302490234,12.439493179321289,10.873309135437012,11.585213661193848,12.622770309448242,12.578568458557129,10.101863861083984,10.316129684448242,12.286802291870117,12.500862121582031,10.316129684448242,12.303435325622559,10.684741020202637,11.466828346252441,10.242538452148438,10.16875171661377,10.89651870727539,13.270489692687988,11.585213661193848,13.270489692687988,10.944157600402832,10.20490550994873,11.891207695007324,10.434484481811523,12.069652557373047,10.684741020202637,10.101863861083984,12.145801544189453,11.40023422241211,10.64365291595459,10.625128746032715,11.40023422241211,10.820554733276367,12.439493179321289,12.303435325622559,12.145801544189453,11.751770973205566,11.585213661193848,10.115687370300293,11.751770973205566,12.069652557373047,10.873309135437012,11.157090187072754,10.318943977355957,11.585213661193848,10.51807975769043,11.404772758483887,10.684136390686035,10.368927001953125,10.910982131958008,13.270489692687988,10.318943977355957,11.074710845947266,11.7166748046875,12.36260986328125,10.614396095275879,12.140864372253418,10.886754989624023,10.434484481811523,12.069652557373047,12.500862121582031,12.140864372253418,11.7483549118042,10.318943977355957,12.500862121582031,10.101863861083984,11.40023422241211,10.285383224487305,12.286802291870117,12.140864372253418,13.270489692687988,10.642513275146484,11.97506332397461,11.466828346252441,10.16875171661377,12.424949645996094,11.132574081420898,12.163540840148926,12.069652557373047,10.886754989624023,11.585213661193848,10.783870697021484,11.7483549118042,10.614396095275879,11.891207695007324,10.762014389038086,11.751770973205566,11.466828346252441,11.223034858703613,12.500862121582031,11.7483549118042,10.316129684448242,10.368927001953125,13.789572715759277,11.46720027923584,10.305498123168945,10.20490550994873,11.466828346252441,10.886754989624023,10.20490550994873,10.642513275146484,11.535086631774902,11.669949531555176,11.788850784301758,12.578568458557129,10.16875171661377,11.373406410217285,11.46720027923584,11.788850784301758,10.684741020202637,11.408492088317871,10.625128746032715,11.157090187072754,11.007722854614258,10.434484481811523,11.891207695007324,12.286802291870117,10.285383224487305,10.873309135437012,11.46186351776123,12.128418922424316,12.145801544189453,11.751770973205566,11.155471801757812,11.176279067993164,10.094202995300293,11.176279067993164,10.684741020202637,12.622770309448242,10.614396095275879,11.007722854614258,11.557347297668457,10.886754989624023,11.7166748046875,11.151241302490234,11.7166748046875,12.128418922424316,10.873309135437012,12.303435325622559,10.285383224487305,12.500862121582031,11.745047569274902,10.16875171661377,10.51807975769043,11.788850784301758,11.557347297668457,12.424949645996094,10.242538452148438,10.316129684448242,10.20490550994873,12.578568458557129,11.074710845947266,10.094202995300293,11.40023422241211,13.164533615112305,11.10375690460205,10.684136390686035,11.40023422241211,11.557347297668457,12.286802291870117,10.841790199279785,12.069652557373047,10.584381103515625,11.007722854614258,10.318943977355957,11.408492088317871,10.910982131958008,11.175163269042969,11.328277587890625,11.7483549118042,11.292632102966309,12.164505958557129,10.762014389038086,10.51807975769043,10.820554733276367,10.379386901855469,11.788850784301758,10.873309135437012,10.305498123168945,10.285383224487305,11.27364730834961,10.625128746032715,10.78131103515625,11.122775077819824,11.40023422241211,11.404772758483887,11.175163269042969,12.164505958557129,12.36260986328125,11.40023422241211,11.7483549118042,11.074710845947266,11.745047569274902,11.745047569274902,11.751770973205566,12.622770309448242,11.176279067993164,11.176279067993164,10.762014389038086,11.7166748046875,10.873309135437012,10.101863861083984,12.36260986328125,11.155471801757812,12.294032096862793,13.270489692687988,11.671412467956543,11.97506332397461,10.170038223266602,10.434484481811523,10.316129684448242,11.033581733703613,12.424949645996094,11.074710845947266,10.094202995300293,12.500862121582031,10.316129684448242,12.424949645996094,11.160247802734375,11.048110008239746,11.122775077819824,11.074710845947266,12.069652557373047,10.379386901855469,10.89651870727539,11.057905197143555,10.944157600402832,11.27364730834961,10.625128746032715,10.873309135437012,12.145801544189453,10.584381103515625,10.383891105651855,11.46186351776123,10.316129684448242,10.285383224487305,10.379386901855469,12.145801544189453,12.622770309448242,10.584381103515625,11.160247802734375,11.7483549118042,11.745047569274902,12.303435325622559,10.285383224487305,10.944157600402832,12.424949645996094,12.36260986328125,10.730844497680664,11.155471801757812,12.303435325622559,9.653680801391602,10.20490550994873,11.585213661193848,9.653680801391602,13.164533615112305,11.585213661193848,11.46186351776123,10.841790199279785,11.155471801757812,10.101863861083984,11.328277587890625,10.886754989624023,12.164505958557129,10.612776756286621,11.745047569274902,10.783870697021484,11.788850784301758,10.684741020202637,11.248010635375977,11.223034858703613,10.383891105651855,10.873309135437012,10.014237403869629,11.155471801757812,10.101863861083984,12.755560874938965,11.248010635375977,12.303435325622559,10.318943977355957,11.036617279052734,10.379386901855469,11.788850784301758,11.248010635375977,11.677290916442871,11.007722854614258,11.669949531555176,10.684136390686035,10.684136390686035,10.368927001953125,11.033581733703613,11.176279067993164,10.16875171661377,10.684136390686035,10.820554733276367,10.783870697021484,11.160247802734375,12.164505958557129,10.612776756286621,11.057905197143555,11.466828346252441,12.164505958557129,10.316129684448242,10.305498123168945,9.653680801391602,11.97506332397461,11.036617279052734,10.584381103515625,10.584381103515625,10.64365291595459,11.828539848327637,12.164505958557129,11.46186351776123,10.014237403869629,10.783870697021484,11.248010635375977,12.424949645996094,11.373406410217285,11.007722854614258,10.51807975769043,11.585213661193848,11.373406410217285,11.036617279052734,11.27364730834961,11.671412467956543,10.642513275146484,11.404772758483887,11.033581733703613,10.434484481811523,11.248010635375977,11.074710845947266,11.248010635375977,10.910982131958008,10.944157600402832,12.622770309448242,12.286802291870117,11.10375690460205,11.048110008239746,10.316129684448242,10.379386901855469,12.069652557373047,11.248010635375977,10.600566864013672,10.600566864013672,10.64365291595459,11.175163269042969,11.891207695007324,12.622770309448242,11.033581733703613,10.64365291595459,12.36260986328125,11.466828346252441,10.78131103515625,11.828539848327637,9.653680801391602,12.500862121582031,10.318943977355957,11.292632102966309,11.669949531555176,10.368927001953125,11.157090187072754,11.891207695007324,10.379386901855469,10.625128746032715,10.584381103515625,11.009413719177246,10.115687370300293,10.383891105651855,11.036617279052734,11.535086631774902,10.305498123168945,11.466828346252441,11.057905197143555,11.7166748046875,10.642513275146484,11.97506332397461,11.160247802734375,11.466828346252441,10.383891105651855,11.677290916442871,12.163540840148926,11.132574081420898,12.622770309448242,12.145801544189453,11.175163269042969,10.730844497680664,12.140864372253418,10.170038223266602,10.51807975769043,11.535086631774902,10.383891105651855,11.151241302490234,11.033581733703613,12.755560874938965,10.64365291595459,12.164505958557129,11.466828346252441,10.841790199279785,10.64365291595459,10.684741020202637,12.069652557373047,11.745047569274902,10.910982131958008,12.294032096862793,11.97506332397461,10.89651870727539,11.157090187072754,12.294032096862793,11.27364730834961,11.009413719177246,12.36260986328125,10.614396095275879,10.684136390686035,12.36260986328125,10.886754989624023,10.684741020202637,10.101863861083984,10.612776756286621,10.783870697021484,11.97506332397461,12.164505958557129,11.074710845947266,11.7483549118042,11.122775077819824,11.97506332397461,11.46186351776123,10.612776756286621,13.164533615112305,10.625128746032715,10.16875171661377,13.789572715759277,11.132574081420898,10.434484481811523,10.841790199279785,11.248010635375977,11.557347297668457,11.404772758483887,11.122775077819824,10.730844497680664,11.048110008239746,10.730844497680664,11.176279067993164,10.841790199279785,11.033581733703613,10.78131103515625,10.379386901855469,11.46186351776123,10.886754989624023,10.841790199279785,11.074710845947266,10.318943977355957,12.424949645996094,12.145801544189453,10.625128746032715,11.404772758483887,13.789572715759277,11.751770973205566,11.248010635375977,11.585213661193848,11.160247802734375,11.788850784301758,11.122775077819824,10.886754989624023,13.270489692687988,12.164505958557129,12.145801544189453,11.788850784301758,11.408492088317871,12.163540840148926,11.751770973205566,13.164533615112305,10.612776756286621,11.40023422241211,10.584381103515625,10.014237403869629,11.46720027923584,13.789572715759277,11.669949531555176,11.155471801757812,10.684136390686035,11.677290916442871,10.873309135437012,10.910982131958008,11.828539848327637,11.328277587890625,11.223034858703613,10.170038223266602,10.094202995300293,11.122775077819824,11.828539848327637,11.7483549118042,10.642513275146484,12.622770309448242,10.285383224487305,10.115687370300293,12.578568458557129,10.316129684448242,10.89651870727539,10.600566864013672,10.101863861083984,11.122775077819824,10.101863861083984,10.642513275146484,10.873309135437012,11.404772758483887,10.115687370300293,11.677290916442871,11.132574081420898,12.22634506225586,11.007722854614258,11.10375690460205,11.373406410217285,11.40023422241211,11.46720027923584,10.379386901855469,11.788850784301758,12.36260986328125,12.140864372253418,10.684136390686035,13.789572715759277,11.132574081420898,10.318943977355957,11.151241302490234,12.622770309448242,11.175163269042969,11.151241302490234,12.36260986328125,11.7166748046875,11.176279067993164,11.671412467956543,12.145801544189453,10.014237403869629,11.751770973205566,11.057905197143555,10.584381103515625,11.97506332397461,10.684136390686035,10.584381103515625,11.466828346252441,11.404772758483887,10.64365291595459,10.820554733276367,10.242538452148438,10.944157600402832,10.684741020202637,12.424949645996094,11.404772758483887,11.122775077819824,10.51807975769043,10.684136390686035,10.20490550994873,10.625128746032715,12.22634506225586,10.242538452148438,11.009413719177246,12.303435325622559,11.057905197143555,11.155471801757812,12.36260986328125,11.27364730834961,12.145801544189453,11.328277587890625,10.625128746032715,10.242538452148438,12.303435325622559,10.584381103515625,11.373406410217285,11.404772758483887,10.094202995300293,12.622770309448242,10.89651870727539,10.886754989624023,10.51807975769043,11.891207695007324,10.16875171661377,10.014237403869629,10.762014389038086,11.557347297668457,10.873309135437012,10.820554733276367,11.036617279052734,11.535086631774902,11.828539848327637,11.7166748046875,11.157090187072754,12.069652557373047,10.434484481811523,10.20490550994873,12.145801544189453,10.115687370300293,10.944157600402832,12.303435325622559,10.318943977355957,11.036617279052734,11.408492088317871,11.328277587890625,10.16875171661377,12.163540840148926,10.820554733276367,12.22634506225586,11.175163269042969,10.841790199279785,10.170038223266602,10.170038223266602,12.163540840148926,12.069652557373047,11.248010635375977,10.944157600402832,11.036617279052734,12.128418922424316,11.155471801757812,11.248010635375977,11.033581733703613,11.828539848327637,10.612776756286621,11.404772758483887,11.157090187072754,12.424949645996094,11.751770973205566,11.7166748046875,11.669949531555176,10.841790199279785,10.642513275146484,11.40023422241211,11.160247802734375,11.248010635375977,10.014237403869629,13.270489692687988,11.151241302490234,12.424949645996094,12.424949645996094,10.783870697021484,10.014237403869629,10.820554733276367,10.170038223266602,11.048110008239746,10.170038223266602,11.373406410217285,11.007722854614258,11.151241302490234,11.788850784301758,12.145801544189453,12.439493179321289,10.368927001953125,10.684136390686035,10.64365291595459,11.828539848327637,12.164505958557129,12.145801544189453,11.373406410217285,10.305498123168945,10.094202995300293,10.730844497680664,11.132574081420898,11.404772758483887,10.316129684448242,11.891207695007324,11.745047569274902,11.10375690460205,10.841790199279785,12.755560874938965,10.101863861083984,11.328277587890625,12.500862121582031,12.286802291870117,11.248010635375977,11.223034858703613,11.007722854614258,10.51807975769043,11.057905197143555,11.176279067993164,11.557347297668457,10.612776756286621,13.270489692687988,11.160247802734375,12.578568458557129,10.642513275146484,12.424949645996094,11.007722854614258,10.730844497680664,10.886754989624023,12.069652557373047,10.16875171661377,12.140864372253418,12.145801544189453,12.439493179321289,11.328277587890625,11.46720027923584,11.007722854614258,12.303435325622559,11.223034858703613,11.175163269042969,10.841790199279785,11.292632102966309,10.094202995300293,11.132574081420898,11.007722854614258,11.033581733703613,10.78131103515625,12.500862121582031,11.009413719177246,10.910982131958008,10.242538452148438,12.36260986328125,10.115687370300293,10.383891105651855,13.164533615112305,13.789572715759277,11.46720027923584,10.115687370300293,10.16875171661377,11.669949531555176,10.783870697021484,11.27364730834961,11.745047569274902,11.7166748046875,10.910982131958008,11.122775077819824,11.157090187072754,11.007722854614258,12.22634506225586,10.820554733276367,11.7483549118042,12.303435325622559,11.828539848327637,12.439493179321289,12.303435325622559,10.944157600402832,10.242538452148438,10.841790199279785,10.584381103515625,11.007722854614258,11.007722854614258,11.248010635375977,13.164533615112305,10.910982131958008,11.669949531555176,10.094202995300293,10.612776756286621,11.009413719177246,9.653680801391602,10.614396095275879,11.751770973205566,11.7483549118042,12.164505958557129,11.157090187072754,12.500862121582031,11.176279067993164,10.642513275146484,11.151241302490234,10.316129684448242,10.383891105651855,10.783870697021484,10.600566864013672,11.466828346252441,10.944157600402832,10.614396095275879,10.094202995300293,11.7166748046875,11.048110008239746,11.828539848327637,12.22634506225586,10.762014389038086,10.316129684448242,10.242538452148438,12.163540840148926,12.145801544189453,11.751770973205566,11.328277587890625,11.057905197143555,11.46186351776123,11.408492088317871,13.164533615112305,10.379386901855469,11.7166748046875,12.500862121582031,11.160247802734375,12.578568458557129,11.891207695007324,10.944157600402832,10.614396095275879,11.404772758483887,10.783870697021484,10.434484481811523,11.328277587890625,11.292632102966309,10.379386901855469,11.248010635375977,11.404772758483887,11.292632102966309,12.286802291870117,12.164505958557129,11.466828346252441,10.305498123168945,11.557347297668457,11.328277587890625,12.755560874938965,10.094202995300293,10.820554733276367,12.303435325622559,10.094202995300293,11.27364730834961,10.684741020202637,12.439493179321289,10.730844497680664,11.7483549118042,10.383891105651855,11.10375690460205,10.318943977355957,12.164505958557129,11.048110008239746,11.074710845947266,11.176279067993164,12.424949645996094,11.7166748046875,10.78131103515625,10.305498123168945,12.163540840148926,10.78131103515625,11.585213661193848,12.286802291870117,11.535086631774902,10.316129684448242,11.557347297668457,11.048110008239746,11.557347297668457],\"xaxis\":\"x\",\"yaxis\":\"y\",\"type\":\"histogram\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"value\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"count\"}},\"legend\":{\"title\":{\"text\":\"variable\"},\"tracegroupgap\":0},\"margin\":{\"t\":60},\"barmode\":\"relative\"},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('92664d61-45b7-44b7-b671-8e30364e5257');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import plotly.express as px\n",
    "import numpy as np\n",
    "# from transformer_lens import utils\n",
    "# \n",
    "# px.histogram(np.random.choice(logits[:, -1, gen_index].flatten().cpu().numpy(), 1000), nbins=100)\n",
    "# utils.test_prompt(\"\".join(model.to_str_tokens(prompts[0, :-1])), \"gen\", model, prepend_space_to_answer=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constructive interference diff, destructive interference diff\n",
      "0.115395 0.001012\n",
      "0.125306 0.004347\n",
      "0.165871 0.004114\n",
      "0.255768 -0.002472\n",
      "0.153542 0.004875\n",
      "0.242432 0.000291\n",
      "-0.383107 0.044685\n",
      "0.353284 -0.007274\n",
      "0.214405 0.005348\n",
      "0.041719 0.018188\n",
      "1.309840 0.093216\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>                            <div id=\"b787e9de-e685-4f37-bb43-69b04c1badfb\" class=\"plotly-graph-div\" style=\"height:525px; width:1000px;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"b787e9de-e685-4f37-bb43-69b04c1badfb\")) {                    Plotly.newPlot(                        \"b787e9de-e685-4f37-bb43-69b04c1badfb\",                        [{\"error_y\":{\"array\":[0.0],\"type\":\"data\",\"visible\":true},\"name\":\"Neuron 0\",\"x\":[\"Neuron 0\"],\"y\":[0.001011924701742828],\"type\":\"bar\"},{\"error_y\":{\"array\":[0.0],\"type\":\"data\",\"visible\":true},\"name\":\"Neuron 1\",\"x\":[\"Neuron 1\"],\"y\":[0.00434673298150301],\"type\":\"bar\"},{\"error_y\":{\"array\":[0.0],\"type\":\"data\",\"visible\":true},\"name\":\"Neuron 2\",\"x\":[\"Neuron 2\"],\"y\":[0.00411441782489419],\"type\":\"bar\"},{\"error_y\":{\"array\":[0.0],\"type\":\"data\",\"visible\":true},\"name\":\"Neuron 3\",\"x\":[\"Neuron 3\"],\"y\":[-0.0024716185871511698],\"type\":\"bar\"},{\"error_y\":{\"array\":[0.0],\"type\":\"data\",\"visible\":true},\"name\":\"Neuron 4\",\"x\":[\"Neuron 4\"],\"y\":[0.004875373560935259],\"type\":\"bar\"},{\"error_y\":{\"array\":[0.0],\"type\":\"data\",\"visible\":true},\"name\":\"Neuron 5\",\"x\":[\"Neuron 5\"],\"y\":[0.0002910613839048892],\"type\":\"bar\"},{\"error_y\":{\"array\":[0.0],\"type\":\"data\",\"visible\":true},\"name\":\"Neuron 6\",\"x\":[\"Neuron 6\"],\"y\":[0.04468532279133797],\"type\":\"bar\"},{\"error_y\":{\"array\":[0.0],\"type\":\"data\",\"visible\":true},\"name\":\"Neuron 7\",\"x\":[\"Neuron 7\"],\"y\":[-0.00727436039596796],\"type\":\"bar\"},{\"error_y\":{\"array\":[0.0],\"type\":\"data\",\"visible\":true},\"name\":\"Neuron 8\",\"x\":[\"Neuron 8\"],\"y\":[0.005347938276827335],\"type\":\"bar\"},{\"error_y\":{\"array\":[0.0],\"type\":\"data\",\"visible\":true},\"name\":\"Neuron 9\",\"x\":[\"Neuron 9\"],\"y\":[0.018187522888183594],\"type\":\"bar\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"title\":{\"text\":\"LogSumExp reduction from ablating top neurons\"},\"xaxis\":{\"title\":{\"text\":\"\"}},\"yaxis\":{\"title\":{\"text\":\"LogSumExp reduction\"}},\"barmode\":\"group\",\"width\":1000,\"showlegend\":true},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('b787e9de-e685-4f37-bb43-69b04c1badfb');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def decompose_interference_diff(ablated_cache, disabled_neurons=[top_neurons[0]], mean=True\n",
    "                                ) -> tuple[float, float] | tuple[Float[Tensor, \"n_prompts\"], Float[Tensor, \"n_prompts\"]]:\n",
    "    '''\n",
    "    Finds the effect of the German context neuron ablation via a given set of MLP5 neurons on the logit of each final \n",
    "    token. Decomposes it into constructive and destructive interference. A positive constructive interference difference\n",
    "    means the neurons boost the logit of the correct token, a positive destructive interference difference means the\n",
    "    LogSumExp is getting closer to the correct token logit because neurons deboost the logits of the higher probability \n",
    "    incorrect tokens.\n",
    "\n",
    "    Loss = -logprob(correct_token_logit)\n",
    "    logprob(correct_token_logit) = correct_token_logit - LogSumExp(all_token_logits)\n",
    "\n",
    "    LogSumExp is a smooth maximum function, so it approximates the max of all logits. A neuron that destructively \n",
    "    interfers with a non-answer token with a high logit lowers the LogSumExp(all_token_logits) and thus the loss.'''\n",
    "    ablate_top_neuron_hook = get_ablate_neurons_hook(disabled_neurons, ablated_cache)\n",
    "\n",
    "    _, _, _, mlp5_enabled_logits = haystack_utils.get_direct_effect(prompts, model, pos=-2, \n",
    "                                                                    context_ablation_hooks=deactivate_neurons_fwd_hooks, \n",
    "                                                                    context_activation_hooks=activate_neurons_fwd_hooks, return_type='logits')\n",
    "    _, _, _, mlp5_enabled_top_neuron_ablated_logits = haystack_utils.get_direct_effect(prompts, model, pos=-2, \n",
    "                                                                                       context_ablation_hooks=deactivate_neurons_fwd_hooks, \n",
    "                                                                                       context_activation_hooks=activate_neurons_fwd_hooks+ablate_top_neuron_hook, return_type='logits')\n",
    "    \n",
    "    # Mean center logits to avoid picking up on constant boosts / deboosts\n",
    "    mlp5_enabled_logits = mlp5_enabled_logits - mlp5_enabled_logits.mean(-1).unsqueeze(-1)\n",
    "    mlp5_enabled_top_neuron_ablated_logits = mlp5_enabled_top_neuron_ablated_logits - mlp5_enabled_top_neuron_ablated_logits.mean(-1).unsqueeze(-1)\n",
    "    \n",
    "    # 1. Constructive interference difference\n",
    "    # This is the change in the correct answer token logit from ablating the neuron, positive is good\n",
    "    constructive_interference_diffs = mlp5_enabled_logits[:, gen_index] - mlp5_enabled_top_neuron_ablated_logits[:, gen_index]\n",
    "\n",
    "    # 2. Destructive interference difference\n",
    "    # This is the change in the LogSumExp of all logits from not ablating the neuron positive/increase in LogSumExp is bad \n",
    "    # even though it means that all tokens get deboosted more by a constant amount because the LogSumExp is guaranteed to be\n",
    "    # >= the correct token logit\n",
    "\n",
    "    # Set the gen logit to its mean value so the neuron's constructive interference doesn't affect the LogSumExp difference\n",
    "    mlp5_enabled_logits[:, gen_index] = mean_gen_logit\n",
    "    mlp5_enabled_top_neuron_ablated_logits[:, gen_index] = mean_gen_logit\n",
    "    \n",
    "    # Compute logsumexp\n",
    "    mlp5_enabled_log_sum_exp = mlp5_enabled_logits.exp().sum(-1).log()\n",
    "    mlp5_enabled_top_neuron_ablated_log_sum_exp = mlp5_enabled_top_neuron_ablated_logits.exp().sum(-1).log()\n",
    "\n",
    "    # Check for errors\n",
    "    assert torch.allclose(mlp5_enabled_log_sum_exp, mlp5_enabled_logits[:, gen_index] - mlp5_enabled_logits.log_softmax(-1)[:, gen_index])\n",
    "    assert torch.allclose(mlp5_enabled_top_neuron_ablated_log_sum_exp, mlp5_enabled_top_neuron_ablated_logits[:, gen_index] - mlp5_enabled_top_neuron_ablated_logits.log_softmax(-1)[:, gen_index])\n",
    "\n",
    "    # Difference in logsumexp\n",
    "    # Logsumexp of enabled should be higher than ablated if the neuron does something good\n",
    "    # Negative results are good - they mean that all tokens are deboosted more when the neuron is active\n",
    "    destructive_interference_diffs = mlp5_enabled_log_sum_exp - mlp5_enabled_top_neuron_ablated_log_sum_exp\n",
    "    # Convert the results so positive is good\n",
    "    destructive_interference_diffs *= -1\n",
    "\n",
    "    if mean:\n",
    "        return constructive_interference_diffs.mean().item(), destructive_interference_diffs.mean().item(), \n",
    "    return constructive_interference_diffs, destructive_interference_diffs,\n",
    "\n",
    "\n",
    "destructive_diffs = []\n",
    "# Calculate neuron-wise loss change\n",
    "print(\"constructive interference diff, destructive interference diff\")\n",
    "for i in range(10):\n",
    "    constructive_diff, destructive_diff = decompose_interference_diff(ablated_cache, disabled_neurons=[top_neurons[i]], mean=True)\n",
    "    print(f'{constructive_diff:2f}', f'{destructive_diff:2f}')\n",
    "    destructive_diffs.append(destructive_diff)\n",
    "\n",
    "constructive_diff, destructive_diff = decompose_interference_diff(ablated_cache, top_neurons)\n",
    "print(f'{constructive_diff:2f}', f'{destructive_diff:2f}')\n",
    "\n",
    "# line plot of logsumexp\n",
    "haystack_utils.plot_barplot([[item] for item in destructive_diffs], names=[f\"Neuron {i}\" for i in range(10)], ylabel=\"LogSumExp reduction\", title=\"LogSumExp reduction from ablating top neurons\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neuron -> Token Logit Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0237, -0.0347,  0.0346,  0.0595,  0.0397,  0.0501, -0.0274,  0.1405,\n",
      "         0.0417, -0.0392], device='cuda:0')\n",
      "tensor([0.1634, 0.1461, 0.1405, 0.1399, 0.1349, 0.1207, 0.1196, 0.1161, 0.1156,\n",
      "        0.1139, 0.1130, 0.1128, 0.1120, 0.1104, 0.1096, 0.1070, 0.1056, 0.1048,\n",
      "        0.1042, 0.1028], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Our top neurons are selected by the difference in their boost in gen based on the context neuron\n",
    "# Many other neurons boost gen more per unit of activation\n",
    "\n",
    "cosine_sim = torch.nn.CosineSimilarity(dim=1)\n",
    "answer_residual_direction = model.tokens_to_residual_directions(\"gen\")\n",
    "neuron_weights = model.state_dict()['blocks.5.mlp.W_out'][top_neurons]\n",
    "\n",
    "cosine_sims = cosine_sim(neuron_weights, answer_residual_direction.unsqueeze(0))\n",
    "print(cosine_sims)\n",
    "\n",
    "cosine_sim = torch.nn.CosineSimilarity(dim=1)\n",
    "answer_residual_direction = model.tokens_to_residual_directions(\"gen\")\n",
    "neuron_weights = model.state_dict()['blocks.5.mlp.W_out']\n",
    "\n",
    "cosine_sims = cosine_sim(neuron_weights, answer_residual_direction.unsqueeze(0))\n",
    "top, indices = torch.topk(cosine_sims, 20)\n",
    "print(top)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15636f0d0bb841c5a1b6f90350b03ef9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcd651b6e5904f3d94caeeb8d63ff145",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e57eefd4b5ce40dcb12c22b6a0c7e4ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "haystack_utils.clean_cache()\n",
    "english_activations_l5 = {}\n",
    "german_activations_l5 = {}\n",
    "all_activations_l5 = {}\n",
    "for layer in range(5, 6):\n",
    "    english_activations_l5 = get_mlp_activations(english_data, layer, model, mean=True)\n",
    "    german_activations_l5 = get_mlp_activations(german_data, layer, model, mean=True)\n",
    "    all_activations_l5 = get_mlp_activations(german_data[:50] + english_data[:150], layer, model, mean=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gen dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b031dcc267d0450aa3eb193b9686ffca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2272eeac034f4fa092fabda61b71e3b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ce327275a824d2c8a09224e633c4e38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9078cc3f24404fd7b593a24e60912394",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f38e41284b4a4564b58d1e5a8185d489",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import einops\n",
    "\n",
    "W_out = model.state_dict()['blocks.5.mlp.W_out']\n",
    "W_U = model.W_U\n",
    "avg_W_out_all = einops.einsum(all_activations_l5, W_out, 'd_mlp, d_mlp d_model -> d_mlp d_model')\n",
    "avg_mlp5_boosts_all = einops.einsum(avg_W_out_all, W_U, 'd_mlp d_model, d_model d_vocab -> d_mlp d_vocab')\n",
    "avg_W_out_enabled = einops.einsum(german_activations_l5, W_out, 'd_mlp, d_mlp d_model -> d_mlp d_model')\n",
    "avg_mlp5_boosts_enabled = einops.einsum(avg_W_out_enabled, W_U, 'd_mlp d_model, d_model d_vocab -> d_mlp d_vocab')\n",
    "avg_W_out_disabled = einops.einsum(english_activations_l5, W_out, 'd_mlp, d_mlp d_model -> d_mlp d_model')\n",
    "avg_mlp5_boosts_disabled = einops.einsum(avg_W_out_disabled, W_U, 'd_mlp d_model, d_model d_vocab -> d_mlp d_vocab')\n",
    "\n",
    "prompt_strs = [model.tokenizer.decode(prompts[i].tolist()) for i in range(prompts.shape[0])]\n",
    "\n",
    "gen_acts_l5_all = get_mlp_activations(prompt_strs, 5, model, mean=True, pos=-2)\n",
    "with model.hooks(deactivate_neurons_fwd_hooks):\n",
    "    gen_acts_l5_disabled = get_mlp_activations(prompt_strs, 5, model, mean=True, pos=-2)\n",
    "with model.hooks(activate_neurons_fwd_hooks):\n",
    "    gen_acts_l5_enabled = get_mlp_activations(prompt_strs, 5, model, mean=True, pos=-2)\n",
    "\n",
    "with model.hooks(deactivate_neurons_fwd_hooks):\n",
    "    all_gen_acts_l5_disabled = get_mlp_activations(prompt_strs, 5, model, mean=False, pos=-2)\n",
    "with model.hooks(activate_neurons_fwd_hooks):\n",
    "    all_gen_acts_l5_enabled = get_mlp_activations(prompt_strs, 5, model, mean=False, pos=-2)\n",
    "\n",
    "data = {\n",
    "    'firing count diff': ((all_gen_acts_l5_enabled > 0).sum(0) - (all_gen_acts_l5_disabled > 0).sum(0)).tolist(),\n",
    "    # 'neuron index': list(range(2048)),\n",
    "    'cos sim gen': cosine_sims.tolist(),\n",
    "    # 'avg act (enabled)': german_activations_l5.tolist(),\n",
    "    # 'avg act (disabled)': english_activations_l5.tolist(),\n",
    "    'avg act increase enabled': (german_activations_l5 - english_activations_l5).tolist(),\n",
    "    'avg act': all_activations_l5.tolist(),\n",
    "    'avg gen act': gen_acts_l5_all.tolist(),\n",
    "    'avg gen act enabled': gen_acts_l5_enabled.tolist(),\n",
    "    'avg gen act disabled': gen_acts_l5_disabled.tolist(),\n",
    "    # 'avg boost gen': avg_mlp5_boosts_all[:, gen_index].tolist(),\n",
    "    # 'avg boost enabled gen': avg_mlp5_boosts_enabled[:, gen_index].tolist(),\n",
    "    # 'avg boost disabled gen': avg_mlp5_boosts_disabled[:, gen_index].tolist(),\n",
    "    'enabled firing count': (all_gen_acts_l5_enabled > 0).sum(0).tolist(),\n",
    "    'disabled firing count': (all_gen_acts_l5_disabled > 0).sum(0).tolist(),\n",
    "    'firing count': (all_gen_acts_l5_disabled.bool().sum(0).tolist())\n",
    "    \n",
    "    # 'Loss change when ablated': \n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "# Line plot of each neuron's gen boost with dotted vertical lines where the German context neuron-boosted ones are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' and', ' creating', ' the', ' Sc', 'hen', 'gen', ' area', '.', ' From', ' 2012']\n",
      "[' Application', ' of', ' the', ' Sc', 'hen', 'gen', ' acquis', ' relating', ' to', ' the']\n",
      "[' Bulgaria', ' to', ' the', ' Sc', 'hen', 'gen', ' Area', ',', ' nor', ' the']\n",
      "[' can', ' enter', ' the', ' Sc', 'hen', 'gen', ' area', ' without', ' having', ' to']\n",
      "[' to', ' restrict', ' the', ' Sc', 'hen', 'gen', ' area', ',', ' compatible', ' with']\n"
     ]
    }
   ],
   "source": [
    "for prompt in english_data:\n",
    "    prompt_tokens = model.to_tokens(prompt)\n",
    "    if gen_index in prompt_tokens:\n",
    "        index = prompt_tokens[0].tolist().index(gen_index)\n",
    "        print(model.to_str_tokens(prompt_tokens[0, index-5:index+5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1107/2048 MLP5 neurons generically increase \"gen\", 54.05%\n",
      "461 of these fire more when context neuron enabled, 22.51%\n",
      "646 of these fire less when context neuron enabled, 31.54%\n",
      "277 of these fire on avg when context neuron enabled, 13.53%\n",
      "251 of these fire on avg when context neuron disabled, 12.26%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>firing count diff</th>\n",
       "      <th>cos sim gen</th>\n",
       "      <th>avg act increase enabled</th>\n",
       "      <th>avg act</th>\n",
       "      <th>avg gen act</th>\n",
       "      <th>avg gen act enabled</th>\n",
       "      <th>avg gen act disabled</th>\n",
       "      <th>enabled firing count</th>\n",
       "      <th>disabled firing count</th>\n",
       "      <th>firing count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2038</th>\n",
       "      <td>93</td>\n",
       "      <td>0.003579</td>\n",
       "      <td>0.793564</td>\n",
       "      <td>0.159707</td>\n",
       "      <td>0.378852</td>\n",
       "      <td>0.259211</td>\n",
       "      <td>-0.075702</td>\n",
       "      <td>100</td>\n",
       "      <td>7</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1747</th>\n",
       "      <td>93</td>\n",
       "      <td>0.053287</td>\n",
       "      <td>0.200168</td>\n",
       "      <td>-0.063503</td>\n",
       "      <td>0.170113</td>\n",
       "      <td>0.099123</td>\n",
       "      <td>-0.104430</td>\n",
       "      <td>94</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1927</th>\n",
       "      <td>93</td>\n",
       "      <td>0.025486</td>\n",
       "      <td>0.347759</td>\n",
       "      <td>0.020567</td>\n",
       "      <td>0.210589</td>\n",
       "      <td>0.148217</td>\n",
       "      <td>-0.077320</td>\n",
       "      <td>99</td>\n",
       "      <td>6</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>93</td>\n",
       "      <td>0.057594</td>\n",
       "      <td>0.739129</td>\n",
       "      <td>0.160572</td>\n",
       "      <td>0.263156</td>\n",
       "      <td>0.166516</td>\n",
       "      <td>-0.121527</td>\n",
       "      <td>94</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1667</th>\n",
       "      <td>88</td>\n",
       "      <td>0.038450</td>\n",
       "      <td>0.321733</td>\n",
       "      <td>0.052155</td>\n",
       "      <td>0.160309</td>\n",
       "      <td>0.085960</td>\n",
       "      <td>-0.123786</td>\n",
       "      <td>89</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358</th>\n",
       "      <td>88</td>\n",
       "      <td>0.008868</td>\n",
       "      <td>0.307440</td>\n",
       "      <td>0.041471</td>\n",
       "      <td>0.376901</td>\n",
       "      <td>0.263298</td>\n",
       "      <td>-0.058402</td>\n",
       "      <td>100</td>\n",
       "      <td>12</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1877</th>\n",
       "      <td>86</td>\n",
       "      <td>0.030004</td>\n",
       "      <td>0.049302</td>\n",
       "      <td>0.190596</td>\n",
       "      <td>0.440926</td>\n",
       "      <td>0.341423</td>\n",
       "      <td>-0.056909</td>\n",
       "      <td>100</td>\n",
       "      <td>14</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>85</td>\n",
       "      <td>0.041687</td>\n",
       "      <td>0.345945</td>\n",
       "      <td>-0.001036</td>\n",
       "      <td>0.633800</td>\n",
       "      <td>0.455694</td>\n",
       "      <td>-0.055810</td>\n",
       "      <td>100</td>\n",
       "      <td>15</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>987</th>\n",
       "      <td>83</td>\n",
       "      <td>0.005353</td>\n",
       "      <td>0.352996</td>\n",
       "      <td>0.051033</td>\n",
       "      <td>0.144413</td>\n",
       "      <td>0.083695</td>\n",
       "      <td>-0.108847</td>\n",
       "      <td>84</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1490</th>\n",
       "      <td>83</td>\n",
       "      <td>0.028981</td>\n",
       "      <td>0.277170</td>\n",
       "      <td>-0.010279</td>\n",
       "      <td>0.250364</td>\n",
       "      <td>0.171001</td>\n",
       "      <td>-0.055530</td>\n",
       "      <td>97</td>\n",
       "      <td>14</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>898</th>\n",
       "      <td>81</td>\n",
       "      <td>0.011997</td>\n",
       "      <td>0.234033</td>\n",
       "      <td>-0.033518</td>\n",
       "      <td>0.261481</td>\n",
       "      <td>0.168796</td>\n",
       "      <td>-0.058418</td>\n",
       "      <td>95</td>\n",
       "      <td>14</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1716</th>\n",
       "      <td>81</td>\n",
       "      <td>0.146135</td>\n",
       "      <td>1.097669</td>\n",
       "      <td>0.571957</td>\n",
       "      <td>0.539957</td>\n",
       "      <td>0.346225</td>\n",
       "      <td>-0.082334</td>\n",
       "      <td>97</td>\n",
       "      <td>16</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1227</th>\n",
       "      <td>77</td>\n",
       "      <td>0.073344</td>\n",
       "      <td>0.133045</td>\n",
       "      <td>-0.022945</td>\n",
       "      <td>0.115361</td>\n",
       "      <td>0.053602</td>\n",
       "      <td>-0.120085</td>\n",
       "      <td>77</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>819</th>\n",
       "      <td>76</td>\n",
       "      <td>0.011484</td>\n",
       "      <td>-0.025496</td>\n",
       "      <td>-0.051579</td>\n",
       "      <td>0.211821</td>\n",
       "      <td>0.126946</td>\n",
       "      <td>-0.068953</td>\n",
       "      <td>89</td>\n",
       "      <td>13</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1076</th>\n",
       "      <td>74</td>\n",
       "      <td>0.020529</td>\n",
       "      <td>0.425295</td>\n",
       "      <td>0.047409</td>\n",
       "      <td>0.112791</td>\n",
       "      <td>0.045184</td>\n",
       "      <td>-0.106668</td>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      firing count diff  cos sim gen  avg act increase enabled   avg act  \\\n",
       "2038                 93     0.003579                  0.793564  0.159707   \n",
       "1747                 93     0.053287                  0.200168 -0.063503   \n",
       "1927                 93     0.025486                  0.347759  0.020567   \n",
       "13                   93     0.057594                  0.739129  0.160572   \n",
       "1667                 88     0.038450                  0.321733  0.052155   \n",
       "358                  88     0.008868                  0.307440  0.041471   \n",
       "1877                 86     0.030004                  0.049302  0.190596   \n",
       "84                   85     0.041687                  0.345945 -0.001036   \n",
       "987                  83     0.005353                  0.352996  0.051033   \n",
       "1490                 83     0.028981                  0.277170 -0.010279   \n",
       "898                  81     0.011997                  0.234033 -0.033518   \n",
       "1716                 81     0.146135                  1.097669  0.571957   \n",
       "1227                 77     0.073344                  0.133045 -0.022945   \n",
       "819                  76     0.011484                 -0.025496 -0.051579   \n",
       "1076                 74     0.020529                  0.425295  0.047409   \n",
       "\n",
       "      avg gen act  avg gen act enabled  avg gen act disabled  \\\n",
       "2038     0.378852             0.259211             -0.075702   \n",
       "1747     0.170113             0.099123             -0.104430   \n",
       "1927     0.210589             0.148217             -0.077320   \n",
       "13       0.263156             0.166516             -0.121527   \n",
       "1667     0.160309             0.085960             -0.123786   \n",
       "358      0.376901             0.263298             -0.058402   \n",
       "1877     0.440926             0.341423             -0.056909   \n",
       "84       0.633800             0.455694             -0.055810   \n",
       "987      0.144413             0.083695             -0.108847   \n",
       "1490     0.250364             0.171001             -0.055530   \n",
       "898      0.261481             0.168796             -0.058418   \n",
       "1716     0.539957             0.346225             -0.082334   \n",
       "1227     0.115361             0.053602             -0.120085   \n",
       "819      0.211821             0.126946             -0.068953   \n",
       "1076     0.112791             0.045184             -0.106668   \n",
       "\n",
       "      enabled firing count  disabled firing count  firing count  \n",
       "2038                   100                      7           100  \n",
       "1747                    94                      1           100  \n",
       "1927                    99                      6           100  \n",
       "13                      94                      1           100  \n",
       "1667                    89                      1           100  \n",
       "358                    100                     12           100  \n",
       "1877                   100                     14           100  \n",
       "84                     100                     15           100  \n",
       "987                     84                      1           100  \n",
       "1490                    97                     14           100  \n",
       "898                     95                     14           100  \n",
       "1716                    97                     16           100  \n",
       "1227                    77                      0           100  \n",
       "819                     89                     13           100  \n",
       "1076                    74                      0           100  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Weight analysis\n",
    "gen_df = df[(df['cos sim gen'] > 0)]\n",
    "print(f\"{len(gen_df)}/{len(df)} MLP5 neurons generically increase \\\"gen\\\", {len(gen_df)/len(df)*100:.2f}%\")\n",
    "\n",
    "# Activation analysis\n",
    "# All on German prompts\n",
    "filtered_df = gen_df[(gen_df['avg act increase enabled'] > 0)]\n",
    "print(f\"{len(filtered_df)} of these fire more when context neuron enabled, {len(filtered_df)/len(df)*100:.2f}%\")\n",
    "filtered_df = gen_df[(gen_df['avg act increase enabled'] < 0)]\n",
    "print(f\"{len(filtered_df)} of these fire less when context neuron enabled, {len(filtered_df)/len(df)*100:.2f}%\")\n",
    "\n",
    "filtered_df = gen_df[(gen_df['avg gen act enabled'] > 0)]\n",
    "print(f\"{len(filtered_df)} of these fire on avg when context neuron enabled, {len(filtered_df)/len(df)*100:.2f}%\")\n",
    "filtered_df = gen_df[(gen_df['avg gen act disabled'] > 0)]\n",
    "print(f\"{len(filtered_df)} of these fire on avg when context neuron disabled, {len(filtered_df)/len(df)*100:.2f}%\")\n",
    "\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "sorted = gen_df.sort_values(by=['firing count diff'], ascending=False)\n",
    "\n",
    "sorted.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interested in how often the context neuron makes the difference between firing and not, rather than firing on average.\n",
    "Neurons with top difference between number of fires with neuron enabled vs. disabled\n",
    "x% more likely to fire\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top/bottom neuron activation boxplot with and without context neuron ablated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d0e7f28b58341aa80625ab39e33aa87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a1a2d1d77c94cf89ca2d6bae7d7daca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with model.hooks(activate_neurons_fwd_hooks):\n",
    "    enabled_acts = get_mlp_activations(german_data, 5, model, mean=False)\n",
    "with model.hooks(deactivate_neurons_fwd_hooks):\n",
    "    disabled_acts = get_mlp_activations(german_data, 5, model, mean=False)\n",
    "\n",
    "top_neuron_enabled_acts = enabled_acts[:, top_neurons]\n",
    "top_neuron_disabled_acts = disabled_acts[:, top_neurons]\n",
    "bottom_neuron_enabled_acts = enabled_acts[:, bottom_neurons]\n",
    "bottom_neuron_disabled_acts = disabled_acts[:, bottom_neurons]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m df_melted \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mmelt(var_name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mColumn\u001b[39m\u001b[39m'\u001b[39m, value_name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mValue\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      7\u001b[0m fig \u001b[39m=\u001b[39m px\u001b[39m.\u001b[39mbox(df_melted, x\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mColumn\u001b[39m\u001b[39m'\u001b[39m, y\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mValue\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m fig\u001b[39m.\u001b[39;49mshow()\n\u001b[1;32m     10\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame({\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m: disabled_acts[:, i]\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy() \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m top_neurons})\n\u001b[1;32m     11\u001b[0m df_melted \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mmelt(var_name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mColumn\u001b[39m\u001b[39m'\u001b[39m, value_name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mValue\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/plotly/basedatatypes.py:3409\u001b[0m, in \u001b[0;36mBaseFigure.show\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3376\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   3377\u001b[0m \u001b[39mShow a figure using either the default renderer(s) or the renderer(s)\u001b[39;00m\n\u001b[1;32m   3378\u001b[0m \u001b[39mspecified by the renderer argument\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3405\u001b[0m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   3406\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   3407\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mplotly\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mio\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpio\u001b[39;00m\n\u001b[0;32m-> 3409\u001b[0m \u001b[39mreturn\u001b[39;00m pio\u001b[39m.\u001b[39;49mshow(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/plotly/io/_renderers.py:388\u001b[0m, in \u001b[0;36mshow\u001b[0;34m(fig, renderer, validate, **kwargs)\u001b[0m\n\u001b[1;32m    385\u001b[0m fig_dict \u001b[39m=\u001b[39m validate_coerce_fig_to_dict(fig, validate)\n\u001b[1;32m    387\u001b[0m \u001b[39m# Mimetype renderers\u001b[39;00m\n\u001b[0;32m--> 388\u001b[0m bundle \u001b[39m=\u001b[39m renderers\u001b[39m.\u001b[39;49m_build_mime_bundle(fig_dict, renderers_string\u001b[39m=\u001b[39;49mrenderer, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    389\u001b[0m \u001b[39mif\u001b[39;00m bundle:\n\u001b[1;32m    390\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m ipython_display:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/plotly/io/_renderers.py:296\u001b[0m, in \u001b[0;36mRenderersConfig._build_mime_bundle\u001b[0;34m(self, fig_dict, renderers_string, **kwargs)\u001b[0m\n\u001b[1;32m    293\u001b[0m             \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(renderer, k):\n\u001b[1;32m    294\u001b[0m                 \u001b[39msetattr\u001b[39m(renderer, k, v)\n\u001b[0;32m--> 296\u001b[0m         bundle\u001b[39m.\u001b[39mupdate(renderer\u001b[39m.\u001b[39;49mto_mimebundle(fig_dict))\n\u001b[1;32m    298\u001b[0m \u001b[39mreturn\u001b[39;00m bundle\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/plotly/io/_base_renderers.py:379\u001b[0m, in \u001b[0;36mHtmlRenderer.to_mimebundle\u001b[0;34m(self, fig_dict)\u001b[0m\n\u001b[1;32m    376\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    377\u001b[0m         post_script\u001b[39m.\u001b[39mextend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpost_script)\n\u001b[0;32m--> 379\u001b[0m html \u001b[39m=\u001b[39m to_html(\n\u001b[1;32m    380\u001b[0m     fig_dict,\n\u001b[1;32m    381\u001b[0m     config\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig,\n\u001b[1;32m    382\u001b[0m     auto_play\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mauto_play,\n\u001b[1;32m    383\u001b[0m     include_plotlyjs\u001b[39m=\u001b[39;49minclude_plotlyjs,\n\u001b[1;32m    384\u001b[0m     include_mathjax\u001b[39m=\u001b[39;49minclude_mathjax,\n\u001b[1;32m    385\u001b[0m     post_script\u001b[39m=\u001b[39;49mpost_script,\n\u001b[1;32m    386\u001b[0m     full_html\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfull_html,\n\u001b[1;32m    387\u001b[0m     animation_opts\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49manimation_opts,\n\u001b[1;32m    388\u001b[0m     default_width\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m100\u001b[39;49m\u001b[39m%\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    389\u001b[0m     default_height\u001b[39m=\u001b[39;49m\u001b[39m525\u001b[39;49m,\n\u001b[1;32m    390\u001b[0m     validate\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    391\u001b[0m )\n\u001b[1;32m    393\u001b[0m \u001b[39mreturn\u001b[39;00m {\u001b[39m\"\u001b[39m\u001b[39mtext/html\u001b[39m\u001b[39m\"\u001b[39m: html}\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/plotly/io/_html.py:144\u001b[0m, in \u001b[0;36mto_html\u001b[0;34m(fig, config, auto_play, include_plotlyjs, include_mathjax, post_script, full_html, animation_opts, default_width, default_height, validate, div_id)\u001b[0m\n\u001b[1;32m    141\u001b[0m plotdivid \u001b[39m=\u001b[39m div_id \u001b[39mor\u001b[39;00m \u001b[39mstr\u001b[39m(uuid\u001b[39m.\u001b[39muuid4())\n\u001b[1;32m    143\u001b[0m \u001b[39m# ## Serialize figure ##\u001b[39;00m\n\u001b[0;32m--> 144\u001b[0m jdata \u001b[39m=\u001b[39m to_json_plotly(fig_dict\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mdata\u001b[39;49m\u001b[39m\"\u001b[39;49m, []))\n\u001b[1;32m    145\u001b[0m jlayout \u001b[39m=\u001b[39m to_json_plotly(fig_dict\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mlayout\u001b[39m\u001b[39m\"\u001b[39m, {}))\n\u001b[1;32m    147\u001b[0m \u001b[39mif\u001b[39;00m fig_dict\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mframes\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/plotly/io/_json.py:143\u001b[0m, in \u001b[0;36mto_json_plotly\u001b[0;34m(plotly_object, pretty, engine)\u001b[0m\n\u001b[1;32m    138\u001b[0m         opts[\u001b[39m\"\u001b[39m\u001b[39mseparators\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m (\u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m:\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    140\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m_plotly_utils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m PlotlyJSONEncoder\n\u001b[1;32m    142\u001b[0m     \u001b[39mreturn\u001b[39;00m _safe(\n\u001b[0;32m--> 143\u001b[0m         json\u001b[39m.\u001b[39;49mdumps(plotly_object, \u001b[39mcls\u001b[39;49m\u001b[39m=\u001b[39;49mPlotlyJSONEncoder, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mopts), _swap_json\n\u001b[1;32m    144\u001b[0m     )\n\u001b[1;32m    145\u001b[0m \u001b[39melif\u001b[39;00m engine \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39morjson\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    146\u001b[0m     JsonConfig\u001b[39m.\u001b[39mvalidate_orjson()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/json/__init__.py:238\u001b[0m, in \u001b[0;36mdumps\u001b[0;34m(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     \u001b[39mcls\u001b[39m \u001b[39m=\u001b[39m JSONEncoder\n\u001b[1;32m    234\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m(\n\u001b[1;32m    235\u001b[0m     skipkeys\u001b[39m=\u001b[39;49mskipkeys, ensure_ascii\u001b[39m=\u001b[39;49mensure_ascii,\n\u001b[1;32m    236\u001b[0m     check_circular\u001b[39m=\u001b[39;49mcheck_circular, allow_nan\u001b[39m=\u001b[39;49mallow_nan, indent\u001b[39m=\u001b[39;49mindent,\n\u001b[1;32m    237\u001b[0m     separators\u001b[39m=\u001b[39;49mseparators, default\u001b[39m=\u001b[39;49mdefault, sort_keys\u001b[39m=\u001b[39;49msort_keys,\n\u001b[0;32m--> 238\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\u001b[39m.\u001b[39;49mencode(obj)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/_plotly_utils/utils.py:59\u001b[0m, in \u001b[0;36mPlotlyJSONEncoder.encode\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[39mLoad and then dump the result using parse_constant kwarg\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \n\u001b[1;32m     55\u001b[0m \u001b[39mNote that setting invalid separators will cause a failure at this step.\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \n\u001b[1;32m     57\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[39m# this will raise errors in a normal-expected way\u001b[39;00m\n\u001b[0;32m---> 59\u001b[0m encoded_o \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m(PlotlyJSONEncoder, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49mencode(o)\n\u001b[1;32m     60\u001b[0m \u001b[39m# Brute force guessing whether NaN or Infinity values are in the string\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[39m# We catch false positive cases (e.g. strings such as titles, labels etc.)\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[39m# but this is ok since the intention is to skip the decoding / reencoding\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[39m# step when it's completely safe\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39m\"\u001b[39m\u001b[39mNaN\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m encoded_o \u001b[39mor\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mInfinity\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m encoded_o):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/json/encoder.py:199\u001b[0m, in \u001b[0;36mJSONEncoder.encode\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[39mreturn\u001b[39;00m encode_basestring(o)\n\u001b[1;32m    196\u001b[0m \u001b[39m# This doesn't pass the iterator directly to ''.join() because the\u001b[39;00m\n\u001b[1;32m    197\u001b[0m \u001b[39m# exceptions aren't as detailed.  The list call should be roughly\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# equivalent to the PySequence_Fast that ''.join() would do.\u001b[39;00m\n\u001b[0;32m--> 199\u001b[0m chunks \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49miterencode(o, _one_shot\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    200\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(chunks, (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m)):\n\u001b[1;32m    201\u001b[0m     chunks \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(chunks)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/json/encoder.py:257\u001b[0m, in \u001b[0;36mJSONEncoder.iterencode\u001b[0;34m(self, o, _one_shot)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    253\u001b[0m     _iterencode \u001b[39m=\u001b[39m _make_iterencode(\n\u001b[1;32m    254\u001b[0m         markers, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefault, _encoder, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindent, floatstr,\n\u001b[1;32m    255\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkey_separator, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitem_separator, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msort_keys,\n\u001b[1;32m    256\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mskipkeys, _one_shot)\n\u001b[0;32m--> 257\u001b[0m \u001b[39mreturn\u001b[39;00m _iterencode(o, \u001b[39m0\u001b[39;49m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "top_neurons = indices[-100:]\n",
    "bottom_neurons = indices[:100]\n",
    "print(len(top_neurons))\n",
    "\n",
    "df = pd.DataFrame({f'{i}': enabled_acts[:, i].cpu().numpy() for i in top_neurons})\n",
    "df_melted = df.melt(var_name='Column', value_name='Value')\n",
    "fig = px.box(df_melted, x='Column', y='Value')\n",
    "fig.show()\n",
    "\n",
    "df = pd.DataFrame({f'{i}': disabled_acts[:, i].cpu().numpy() for i in top_neurons})\n",
    "df_melted = df.melt(var_name='Column', value_name='Value')\n",
    "fig = px.box(df_melted, x='Column', y='Value')\n",
    "fig.show()\n",
    "\n",
    "df = pd.DataFrame({f'{i}': enabled_acts[:, i].cpu().numpy() for i in bottom_neurons})\n",
    "df_melted = df.melt(var_name='Column', value_name='Value')\n",
    "fig = px.box(df_melted, x='Column', y='Value')\n",
    "fig.show()\n",
    "\n",
    "df = pd.DataFrame({f'{i}': disabled_acts[:, i].cpu().numpy() for i in bottom_neurons})\n",
    "df_melted = df.melt(var_name='Column', value_name='Value')\n",
    "fig = px.box(df_melted, x='Column', y='Value')\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
