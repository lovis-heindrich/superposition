{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from transformer_lens import HookedTransformer, ActivationCache, utils, patching\n",
    "from jaxtyping import Float, Int, Bool\n",
    "from torch import Tensor\n",
    "from tqdm.auto import tqdm\n",
    "import plotly.io as pio\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "pio.renderers.default = \"notebook_connected+notebook\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.autograd.set_grad_enabled(False)\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "from haystack_utils import get_mlp_activations\n",
    "import haystack_utils\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model EleutherAI/pythia-70m into HookedTransformer\n",
      "data/german_europarl.json: Loaded 2000 examples with 152 to 2000 characters each.\n",
      "data/english_europarl.json: Loaded 2000 examples with 165 to 2000 characters each.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9961a4bdb87242e4b55b5d61dcb2ccc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bd3fd25b6a542879473bf96c77e8679",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = HookedTransformer.from_pretrained(\"EleutherAI/pythia-70m\",\n",
    "    center_unembed=True,\n",
    "    center_writing_weights=True,\n",
    "    fold_ln=True,\n",
    "    device=device)\n",
    "\n",
    "german_data = haystack_utils.load_json_data(\"data/german_europarl.json\")[:200]\n",
    "english_data = haystack_utils.load_json_data(\"data/english_europarl.json\")[:200]\n",
    "\n",
    "\n",
    "english_activations = {}\n",
    "german_activations = {}\n",
    "for layer in range(3, 4):\n",
    "    english_activations[layer] = get_mlp_activations(english_data, layer, model, mean=False)\n",
    "    german_activations[layer] = get_mlp_activations(german_data, layer, model, mean=False)\n",
    "\n",
    "LAYER_TO_ABLATE = 3\n",
    "NEURONS_TO_ABLATE = [669]\n",
    "MEAN_ACTIVATION_ACTIVE = german_activations[LAYER_TO_ABLATE][:, NEURONS_TO_ABLATE].mean()\n",
    "MEAN_ACTIVATION_INACTIVE = english_activations[LAYER_TO_ABLATE][:, NEURONS_TO_ABLATE].mean()\n",
    "\n",
    "def deactivate_neurons_hook(value, hook):\n",
    "    value[:, :, NEURONS_TO_ABLATE] = MEAN_ACTIVATION_INACTIVE\n",
    "    return value\n",
    "deactivate_neurons_fwd_hooks=[(f'blocks.{LAYER_TO_ABLATE}.mlp.hook_post', deactivate_neurons_hook)]\n",
    "\n",
    "def activate_neurons_hook(value, hook):\n",
    "    value[:, :, NEURONS_TO_ABLATE] = MEAN_ACTIVATION_ACTIVE\n",
    "    return value\n",
    "activate_neurons_fwd_hooks=[(f'blocks.{LAYER_TO_ABLATE}.mlp.hook_post', activate_neurons_hook)]\n",
    "\n",
    "all_ignore, not_ignore = haystack_utils.get_weird_tokens(model, plot_norms=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find top common German tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91642c65262a41da8d8888fdb817f075",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' der', 'en', ' die', ' und', 'ung', 'ä', ' in', ' den', ' des', ' zu', 'ch', 'n', 'st', 're', 'z', ' von', ' für', 'äsident', ' Pr', 'ischen', 't', 'ü', 'icht', 'gen', ' ist', ' auf', ' dass', 'ge', 'ig', ' im', 'in', ' über', 'g', ' das', 'te', ' er', 'men', ' w', 'es', ' an', 'ß', ' wir', ' eine', 'f', ' W', 'hen', 'w', ' Europ', ' ich', 'ungen', 'ren', 'le', ' dem', 'ten', ' ein', 'e', ' Z', ' Ver', 'der', ' B', ' mit', ' dies', 'h', ' nicht', 'ungs', 's', ' G', ' z', 'it', ' Herr', ' es', 'l', ' S', 'ich', 'lich', ' An', 'heit', 'ie', ' Er', ' zur', ' V', ' ver', 'u', 'hr', 'chaft', 'Der', ' Ich', ' Ab', ' haben', 'i', 'ant', 'chte', ' mö', 'er', ' K', 'igen', ' Ber', 'ür', ' Fra', 'em']\n"
     ]
    }
   ],
   "source": [
    "# Get top common german tokens excluding punctuation\n",
    "token_counts = torch.zeros(model.cfg.d_vocab).cuda()\n",
    "for example in tqdm(german_data):\n",
    "    tokens = model.to_tokens(example)\n",
    "    for token in tokens[0]:\n",
    "        token_counts[token.item()] += 1\n",
    "\n",
    "punctuation = [\"\\n\", \".\", \",\", \"!\", \"?\", \";\", \":\", \"-\", \"(\", \")\", \"[\", \"]\", \"{\", \"}\", \"<\", \">\", \"/\", \"\\\\\", \"\\\"\", \"'\"]\n",
    "leading_space_punctuation = [\" \" + char for char in punctuation]\n",
    "punctuation_tokens = model.to_tokens(punctuation + leading_space_punctuation + [' –', \" \", '  ', \"<|endoftext|>\"])[:, 1].flatten()\n",
    "token_counts[punctuation_tokens] = 0\n",
    "token_counts[all_ignore] = 0\n",
    "\n",
    "top_counts, top_tokens = torch.topk(token_counts, 100)\n",
    "print(model.to_str_tokens(top_tokens[:100]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of ngrams preceded by random prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_selection(tensor, n=12):\n",
    "    # Hacky replacement for np.random.choice\n",
    "    return tensor[torch.randperm(len(tensor))[:n]]\n",
    "\n",
    "def generate_random_prompts(end_string, n=50, length=12):\n",
    "    # Generate a batch of random prompts ending with a specific ngram\n",
    "    end_tokens = model.to_tokens(end_string).flatten()[1:]\n",
    "    prompts = []\n",
    "    for i in range(n):\n",
    "        prompt = get_random_selection(top_tokens[:max(50, length)], n=length).cuda()\n",
    "        prompt = torch.cat([prompt, end_tokens])\n",
    "        prompts.append(prompt)\n",
    "    prompts = torch.stack(prompts)\n",
    "    return prompts\n",
    "\n",
    "def replace_column(prompts: Int[Tensor, \"n_prompts n_tokens\"], token_index: int):\n",
    "    # Replaces a specific token position in a batch of prompts with random common German tokens\n",
    "    new_prompts = prompts.clone()\n",
    "    random_tokens = get_random_selection(top_tokens[:max(50, prompts.shape[0])], n=prompts.shape[0]).cuda()\n",
    "    new_prompts[:, token_index] = random_tokens\n",
    "    return new_prompts \n",
    "\n",
    "def loss_analysis(prompts: Tensor, title=\"\"):\n",
    "    # Loss plot for a batch of prompts\n",
    "    names = [\"Original\", \"Ablated\", \"MLP5 path patched\"]\n",
    "    original_loss, ablated_loss, _, only_activated_loss = \\\n",
    "        haystack_utils.get_direct_effect(prompts, model, pos=-1,\n",
    "                                        context_ablation_hooks=deactivate_neurons_fwd_hooks, \n",
    "                                        context_activation_hooks=activate_neurons_fwd_hooks, \n",
    "                                        )\n",
    "    haystack_utils.plot_barplot([original_loss.tolist(), ablated_loss.tolist(), only_activated_loss.tolist()], names, ylabel=\"Loss\", title=title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_analysis_random_prompts(end_string, n=50, length=12, replace_columns: list[int] | None = None):\n",
    "    # Loss plot for a batch of random prompts ending with a specific ngram and optionally replacing specific tokens\n",
    "    prompts = generate_random_prompts(end_string, n=n, length=length)\n",
    "    title=f\"Average last token loss on {length} random tokens ending in '{end_string}'\"\n",
    "    if replace_columns is not None:\n",
    "        replaced_tokens = model.to_str_tokens(prompts[0, replace_columns])\n",
    "        title += f\" replacing {replaced_tokens}\"\n",
    "        for column in replace_columns:\n",
    "            prompts = replace_column(prompts, column)\n",
    "    \n",
    "    loss_analysis(prompts, title=title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "# loss_analysis_random_prompts(\" Vorschlägen\", n=100, length=20)\n",
    "# loss_analysis_random_prompts(\" Vorschlägen\", n=100, length=20, replace_columns=[-2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ed6606f9ae746069b7fb3ed401136aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Ngram: ', index=1, options=(' statt', ' Vorschlägen', ' Vorschläge', ' häufig', ' schlie…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f932f4ecfe2644c2b0145b42cf46af51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SelectMultiple(description='Replace Columns:', index=(3,), options=('-2', '-3', '-4', 'None'), value=('None',)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba199cb6046444b48a636abdac2cb74f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a dropdown menu widget\n",
    "dropdown = widgets.Dropdown(\n",
    "    options = [\" statt\", \" Vorschlägen\", \" Vorschläge\", \" häufig\", \" schließt\", \" beweglich\", \" seine Ansicht\"], \n",
    "    value = \" Vorschlägen\",\n",
    "    description = 'Ngram: ',\n",
    ")\n",
    "\n",
    "replace_columns_dropdown = widgets.SelectMultiple(\n",
    "    options = ['-2', '-3', '-4', 'None'],\n",
    "    value = ['None'],  # default selected value\n",
    "    description = 'Replace Columns:',\n",
    ")\n",
    "\n",
    "# Create an output widget to hold the plot\n",
    "output = widgets.Output()\n",
    "\n",
    "# Define a function to call when the widget's value changes\n",
    "def update_plot(*args):\n",
    "    # Clear the old plot from the output\n",
    "    with output:\n",
    "        clear_output(wait=True)\n",
    "\n",
    "        if 'None' in replace_columns_dropdown.value:\n",
    "            replace_columns = None\n",
    "        else:\n",
    "            # If 'None' not selected, convert the selected values to integers\n",
    "            replace_columns = [int(val) for val in replace_columns_dropdown.value]\n",
    "        \n",
    "        # Call your function with the values from the widgets\n",
    "        loss_analysis_random_prompts(dropdown.value, n=100, length=20, replace_columns=replace_columns)\n",
    "\n",
    "# Set the function to be called when the widget's value changes\n",
    "dropdown.observe(update_plot, 'value')\n",
    "replace_columns_dropdown.observe(update_plot, 'value')\n",
    "\n",
    "# Display the widget and the output\n",
    "display(dropdown, replace_columns_dropdown, output)\n",
    "\n",
    "# Run once at startup\n",
    "update_plot()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify relevant neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 22])\n"
     ]
    }
   ],
   "source": [
    "prompts = generate_random_prompts(\" statt\", n=100, length=20)\n",
    "print(prompts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c38f466e4ea941db8c7334b91b81a923",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2048 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0008)\n"
     ]
    }
   ],
   "source": [
    "with model.hooks(deactivate_neurons_fwd_hooks):\n",
    "    _, ablated_cache = model.run_with_cache(prompts)\n",
    "\n",
    "def get_ablate_neurons_hook(neuron: int | list[int], ablated_cache, layer=5):\n",
    "    def ablate_neurons_hook(value, hook):\n",
    "        value[:, :, neuron] = ablated_cache[f'blocks.{layer}.mlp.hook_post'][:, :, neuron]\n",
    "        return value\n",
    "    return [(f'blocks.{layer}.mlp.hook_post', ablate_neurons_hook)]\n",
    "\n",
    "diffs = torch.zeros(2048, prompts.shape[0])\n",
    "# Loss with path patched MLP5 neurons\n",
    "_, _, _, baseline_loss = haystack_utils.get_direct_effect(prompts, model, pos=-1, context_ablation_hooks=deactivate_neurons_fwd_hooks, context_activation_hooks=activate_neurons_fwd_hooks)\n",
    "for neuron in tqdm(range(2048)):\n",
    "    ablate_single_neuron_hook = get_ablate_neurons_hook(neuron, ablated_cache)\n",
    "    # Loss with path patched MLP5 neurons but a single neuron changed back to original ablated value\n",
    "    _, _, _, only_deactivated_loss = haystack_utils.get_direct_effect(prompts, model, pos=-1, context_ablation_hooks=deactivate_neurons_fwd_hooks, context_activation_hooks=activate_neurons_fwd_hooks+ablate_single_neuron_hook)\n",
    "    diffs[neuron] = only_deactivated_loss - baseline_loss\n",
    "\n",
    "print(diffs.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>                            <div id=\"2d684eb9-c32f-45e5-b990-deef2069d245\" class=\"plotly-graph-div\" style=\"height:525px; width:800px;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"2d684eb9-c32f-45e5-b990-deef2069d245\")) {                    Plotly.newPlot(                        \"2d684eb9-c32f-45e5-b990-deef2069d245\",                        [{\"hovertemplate\":\"variable=0\\u003cbr\\u003eindex=%{x}\\u003cbr\\u003evalue=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"0\",\"line\":{\"color\":\"#636efa\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"0\",\"showlegend\":true,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542,543,544,545,546,547,548,549,550,551,552,553,554,555,556,557,558,559,560,561,562,563,564,565,566,567,568,569,570,571,572,573,574,575,576,577,578,579,580,581,582,583,584,585,586,587,588,589,590,591,592,593,594,595,596,597,598,599,600,601,602,603,604,605,606,607,608,609,610,611,612,613,614,615,616,617,618,619,620,621,622,623,624,625,626,627,628,629,630,631,632,633,634,635,636,637,638,639,640,641,642,643,644,645,646,647,648,649,650,651,652,653,654,655,656,657,658,659,660,661,662,663,664,665,666,667,668,669,670,671,672,673,674,675,676,677,678,679,680,681,682,683,684,685,686,687,688,689,690,691,692,693,694,695,696,697,698,699,700,701,702,703,704,705,706,707,708,709,710,711,712,713,714,715,716,717,718,719,720,721,722,723,724,725,726,727,728,729,730,731,732,733,734,735,736,737,738,739,740,741,742,743,744,745,746,747,748,749,750,751,752,753,754,755,756,757,758,759,760,761,762,763,764,765,766,767,768,769,770,771,772,773,774,775,776,777,778,779,780,781,782,783,784,785,786,787,788,789,790,791,792,793,794,795,796,797,798,799,800,801,802,803,804,805,806,807,808,809,810,811,812,813,814,815,816,817,818,819,820,821,822,823,824,825,826,827,828,829,830,831,832,833,834,835,836,837,838,839,840,841,842,843,844,845,846,847,848,849,850,851,852,853,854,855,856,857,858,859,860,861,862,863,864,865,866,867,868,869,870,871,872,873,874,875,876,877,878,879,880,881,882,883,884,885,886,887,888,889,890,891,892,893,894,895,896,897,898,899,900,901,902,903,904,905,906,907,908,909,910,911,912,913,914,915,916,917,918,919,920,921,922,923,924,925,926,927,928,929,930,931,932,933,934,935,936,937,938,939,940,941,942,943,944,945,946,947,948,949,950,951,952,953,954,955,956,957,958,959,960,961,962,963,964,965,966,967,968,969,970,971,972,973,974,975,976,977,978,979,980,981,982,983,984,985,986,987,988,989,990,991,992,993,994,995,996,997,998,999,1000,1001,1002,1003,1004,1005,1006,1007,1008,1009,1010,1011,1012,1013,1014,1015,1016,1017,1018,1019,1020,1021,1022,1023,1024,1025,1026,1027,1028,1029,1030,1031,1032,1033,1034,1035,1036,1037,1038,1039,1040,1041,1042,1043,1044,1045,1046,1047,1048,1049,1050,1051,1052,1053,1054,1055,1056,1057,1058,1059,1060,1061,1062,1063,1064,1065,1066,1067,1068,1069,1070,1071,1072,1073,1074,1075,1076,1077,1078,1079,1080,1081,1082,1083,1084,1085,1086,1087,1088,1089,1090,1091,1092,1093,1094,1095,1096,1097,1098,1099,1100,1101,1102,1103,1104,1105,1106,1107,1108,1109,1110,1111,1112,1113,1114,1115,1116,1117,1118,1119,1120,1121,1122,1123,1124,1125,1126,1127,1128,1129,1130,1131,1132,1133,1134,1135,1136,1137,1138,1139,1140,1141,1142,1143,1144,1145,1146,1147,1148,1149,1150,1151,1152,1153,1154,1155,1156,1157,1158,1159,1160,1161,1162,1163,1164,1165,1166,1167,1168,1169,1170,1171,1172,1173,1174,1175,1176,1177,1178,1179,1180,1181,1182,1183,1184,1185,1186,1187,1188,1189,1190,1191,1192,1193,1194,1195,1196,1197,1198,1199,1200,1201,1202,1203,1204,1205,1206,1207,1208,1209,1210,1211,1212,1213,1214,1215,1216,1217,1218,1219,1220,1221,1222,1223,1224,1225,1226,1227,1228,1229,1230,1231,1232,1233,1234,1235,1236,1237,1238,1239,1240,1241,1242,1243,1244,1245,1246,1247,1248,1249,1250,1251,1252,1253,1254,1255,1256,1257,1258,1259,1260,1261,1262,1263,1264,1265,1266,1267,1268,1269,1270,1271,1272,1273,1274,1275,1276,1277,1278,1279,1280,1281,1282,1283,1284,1285,1286,1287,1288,1289,1290,1291,1292,1293,1294,1295,1296,1297,1298,1299,1300,1301,1302,1303,1304,1305,1306,1307,1308,1309,1310,1311,1312,1313,1314,1315,1316,1317,1318,1319,1320,1321,1322,1323,1324,1325,1326,1327,1328,1329,1330,1331,1332,1333,1334,1335,1336,1337,1338,1339,1340,1341,1342,1343,1344,1345,1346,1347,1348,1349,1350,1351,1352,1353,1354,1355,1356,1357,1358,1359,1360,1361,1362,1363,1364,1365,1366,1367,1368,1369,1370,1371,1372,1373,1374,1375,1376,1377,1378,1379,1380,1381,1382,1383,1384,1385,1386,1387,1388,1389,1390,1391,1392,1393,1394,1395,1396,1397,1398,1399,1400,1401,1402,1403,1404,1405,1406,1407,1408,1409,1410,1411,1412,1413,1414,1415,1416,1417,1418,1419,1420,1421,1422,1423,1424,1425,1426,1427,1428,1429,1430,1431,1432,1433,1434,1435,1436,1437,1438,1439,1440,1441,1442,1443,1444,1445,1446,1447,1448,1449,1450,1451,1452,1453,1454,1455,1456,1457,1458,1459,1460,1461,1462,1463,1464,1465,1466,1467,1468,1469,1470,1471,1472,1473,1474,1475,1476,1477,1478,1479,1480,1481,1482,1483,1484,1485,1486,1487,1488,1489,1490,1491,1492,1493,1494,1495,1496,1497,1498,1499,1500,1501,1502,1503,1504,1505,1506,1507,1508,1509,1510,1511,1512,1513,1514,1515,1516,1517,1518,1519,1520,1521,1522,1523,1524,1525,1526,1527,1528,1529,1530,1531,1532,1533,1534,1535,1536,1537,1538,1539,1540,1541,1542,1543,1544,1545,1546,1547,1548,1549,1550,1551,1552,1553,1554,1555,1556,1557,1558,1559,1560,1561,1562,1563,1564,1565,1566,1567,1568,1569,1570,1571,1572,1573,1574,1575,1576,1577,1578,1579,1580,1581,1582,1583,1584,1585,1586,1587,1588,1589,1590,1591,1592,1593,1594,1595,1596,1597,1598,1599,1600,1601,1602,1603,1604,1605,1606,1607,1608,1609,1610,1611,1612,1613,1614,1615,1616,1617,1618,1619,1620,1621,1622,1623,1624,1625,1626,1627,1628,1629,1630,1631,1632,1633,1634,1635,1636,1637,1638,1639,1640,1641,1642,1643,1644,1645,1646,1647,1648,1649,1650,1651,1652,1653,1654,1655,1656,1657,1658,1659,1660,1661,1662,1663,1664,1665,1666,1667,1668,1669,1670,1671,1672,1673,1674,1675,1676,1677,1678,1679,1680,1681,1682,1683,1684,1685,1686,1687,1688,1689,1690,1691,1692,1693,1694,1695,1696,1697,1698,1699,1700,1701,1702,1703,1704,1705,1706,1707,1708,1709,1710,1711,1712,1713,1714,1715,1716,1717,1718,1719,1720,1721,1722,1723,1724,1725,1726,1727,1728,1729,1730,1731,1732,1733,1734,1735,1736,1737,1738,1739,1740,1741,1742,1743,1744,1745,1746,1747,1748,1749,1750,1751,1752,1753,1754,1755,1756,1757,1758,1759,1760,1761,1762,1763,1764,1765,1766,1767,1768,1769,1770,1771,1772,1773,1774,1775,1776,1777,1778,1779,1780,1781,1782,1783,1784,1785,1786,1787,1788,1789,1790,1791,1792,1793,1794,1795,1796,1797,1798,1799,1800,1801,1802,1803,1804,1805,1806,1807,1808,1809,1810,1811,1812,1813,1814,1815,1816,1817,1818,1819,1820,1821,1822,1823,1824,1825,1826,1827,1828,1829,1830,1831,1832,1833,1834,1835,1836,1837,1838,1839,1840,1841,1842,1843,1844,1845,1846,1847,1848,1849,1850,1851,1852,1853,1854,1855,1856,1857,1858,1859,1860,1861,1862,1863,1864,1865,1866,1867,1868,1869,1870,1871,1872,1873,1874,1875,1876,1877,1878,1879,1880,1881,1882,1883,1884,1885,1886,1887,1888,1889,1890,1891,1892,1893,1894,1895,1896,1897,1898,1899,1900,1901,1902,1903,1904,1905,1906,1907,1908,1909,1910,1911,1912,1913,1914,1915,1916,1917,1918,1919,1920,1921,1922,1923,1924,1925,1926,1927,1928,1929,1930,1931,1932,1933,1934,1935,1936,1937,1938,1939,1940,1941,1942,1943,1944,1945,1946,1947,1948,1949,1950,1951,1952,1953,1954,1955,1956,1957,1958,1959,1960,1961,1962,1963,1964,1965,1966,1967,1968,1969,1970,1971,1972,1973,1974,1975,1976,1977,1978,1979,1980,1981,1982,1983,1984,1985,1986,1987,1988,1989,1990,1991,1992,1993,1994,1995,1996,1997,1998,1999,2000,2001,2002,2003,2004,2005,2006,2007,2008,2009,2010,2011,2012,2013,2014,2015,2016,2017,2018,2019,2020,2021,2022,2023,2024,2025,2026,2027,2028,2029,2030,2031,2032,2033,2034,2035,2036,2037,2038,2039,2040,2041,2042,2043,2044,2045,2046,2047],\"xaxis\":\"x\",\"y\":[-0.11408159881830215,-0.08636737614870071,-0.07183496654033661,-0.06907375156879425,-0.06442699581384659,-0.06233665347099304,-0.061052631586790085,-0.060806483030319214,-0.0595172755420208,-0.052635595202445984,-0.051296167075634,-0.051279082894325256,-0.05053113028407097,-0.04587283730506897,-0.04435083270072937,-0.043951887637376785,-0.04324619844555855,-0.0427396334707737,-0.03800768032670021,-0.03533421456813812,-0.03473249077796936,-0.03452618792653084,-0.03205539658665657,-0.03186517581343651,-0.03167340159416199,-0.030756933614611626,-0.03034336492419243,-0.03017830103635788,-0.028824593871831894,-0.028237486258149147,-0.026321496814489365,-0.0260760635137558,-0.025627408176660538,-0.025562778115272522,-0.02551577240228653,-0.02550096996128559,-0.024768788367509842,-0.024572400376200676,-0.02404024638235569,-0.023351237177848816,-0.022508258000016212,-0.022357795387506485,-0.022082379087805748,-0.021824155002832413,-0.02181197702884674,-0.021710330620408058,-0.02163383737206459,-0.021178770810365677,-0.020605850964784622,-0.020542075857520103,-0.019952844828367233,-0.01991054229438305,-0.019159572198987007,-0.01896672509610653,-0.018009094521403313,-0.016861414536833763,-0.01651071570813656,-0.016129354014992714,-0.016117220744490623,-0.016099007800221443,-0.016079330816864967,-0.015757175162434578,-0.015731720253825188,-0.015641218051314354,-0.015556744299829006,-0.015527980402112007,-0.015511554665863514,-0.015445048920810223,-0.015354085713624954,-0.015214093029499054,-0.01506914384663105,-0.014381255023181438,-0.014128253795206547,-0.013902115635573864,-0.013882091268897057,-0.013860809616744518,-0.01378488540649414,-0.013464630581438541,-0.01325991004705429,-0.013211830519139767,-0.012949805706739426,-0.012788352556526661,-0.012615866959095001,-0.012454943731427193,-0.012301541864871979,-0.012215541675686836,-0.012177896685898304,-0.011957253329455853,-0.01192031055688858,-0.011872037313878536,-0.01177919376641512,-0.011655465699732304,-0.011478086933493614,-0.011464187875390053,-0.011455421335995197,-0.011270811781287193,-0.01090958807617426,-0.010865308344364166,-0.010789955034852028,-0.010706506669521332,-0.010627266950905323,-0.010562760755419731,-0.010488309897482395,-0.010437973774969578,-0.010389737784862518,-0.010305103845894337,-0.010173401795327663,-0.01012169849127531,-0.010057125240564346,-0.009920243173837662,-0.009886175394058228,-0.009877028875052929,-0.00942646712064743,-0.00929331686347723,-0.009180544875562191,-0.009111382067203522,-0.009030148386955261,-0.008977082557976246,-0.008945325389504433,-0.00863773562014103,-0.008103122003376484,-0.008096308447420597,-0.008078421466052532,-0.007988040335476398,-0.00795032735913992,-0.007899664342403412,-0.007802021689713001,-0.00775523204356432,-0.007744790520519018,-0.0076792119070887566,-0.007578566204756498,-0.007564364466816187,-0.007501653395593166,-0.007471082732081413,-0.007424330338835716,-0.007401544600725174,-0.0073816110379993916,-0.007330694235861301,-0.007265780121088028,-0.0071635558269917965,-0.007106664590537548,-0.0070573389530181885,-0.006860241759568453,-0.006811065599322319,-0.006774595007300377,-0.006751672830432653,-0.006719787605106831,-0.006544891744852066,-0.00653831847012043,-0.0064744059927761555,-0.006366149056702852,-0.006277907639741898,-0.006154023576527834,-0.006085306406021118,-0.006078158039599657,-0.006038631312549114,-0.006025478709489107,-0.005880696699023247,-0.005764922127127647,-0.005761630367487669,-0.005748152732849121,-0.00573488837108016,-0.005728110671043396,-0.005653998348861933,-0.005644623190164566,-0.005623838864266872,-0.005589518696069717,-0.00554813863709569,-0.005533507093787193,-0.005482958164066076,-0.005449448712170124,-0.005408253520727158,-0.0053894054144620895,-0.005368007346987724,-0.005365177523344755,-0.005361388437449932,-0.005340942647308111,-0.005327984225004911,-0.005306018516421318,-0.005222348030656576,-0.005117252469062805,-0.0049828700721263885,-0.004944541025906801,-0.0049399929121136665,-0.004896786063909531,-0.004760498180985451,-0.00474936980754137,-0.0046652089804410934,-0.0046243080869317055,-0.004604576155543327,-0.004583386704325676,-0.004494885914027691,-0.004466561134904623,-0.004458176903426647,-0.00444987416267395,-0.00444709463045001,-0.004446027800440788,-0.004390086978673935,-0.004385362844914198,-0.0042951107025146484,-0.0042785173282027245,-0.0042318613268435,-0.004228632897138596,-0.004219277296215296,-0.004204552620649338,-0.00417367834597826,-0.004146792460232973,-0.004124518949538469,-0.004122896585613489,-0.0041218907572329044,-0.004086480010300875,-0.004081529099494219,-0.004031255375593901,-0.0040284316055476665,-0.0039781793020665646,-0.00397740863263607,-0.003951996099203825,-0.003923539072275162,-0.003906158497557044,-0.0039022627752274275,-0.0038745298516005278,-0.0038371551781892776,-0.0038120588287711143,-0.003804850624874234,-0.003764855209738016,-0.0037402205634862185,-0.0036881715059280396,-0.0036625287029892206,-0.0036394023336470127,-0.0035671177320182323,-0.003541388548910618,-0.00353446532972157,-0.0034714213106781244,-0.003470371011644602,-0.0034485170617699623,-0.0034434485714882612,-0.003380542155355215,-0.0033578660804778337,-0.0033214499708265066,-0.0032766929361969233,-0.0032745706848800182,-0.0032680118456482887,-0.003242654260247946,-0.0031682250555604696,-0.0031515618320554495,-0.003136057872325182,-0.0031316778622567654,-0.0031185182742774487,-0.0030926561448723078,-0.0030866998713463545,-0.0030807023867964745,-0.0030693504959344864,-0.0030258900951594114,-0.0030175638385117054,-0.0029821456409990788,-0.002924158936366439,-0.0028987962286919355,-0.002850667340680957,-0.002849697135388851,-0.0028353447560220957,-0.002780535724014044,-0.002766938880085945,-0.0027570014353841543,-0.0027480581775307655,-0.002740908181294799,-0.0027366795111447573,-0.0027308498974889517,-0.002716743852943182,-0.002706431085243821,-0.002679701428860426,-0.0026743337512016296,-0.0026622693985700607,-0.0026604013983160257,-0.002660130150616169,-0.002658554818481207,-0.002635218435898423,-0.0026279890444129705,-0.0026275075506418943,-0.002624558750540018,-0.0026229415088891983,-0.0026195563841611147,-0.0025984456297010183,-0.002596070058643818,-0.002572585828602314,-0.0025648707523941994,-0.0025638260412961245,-0.002562438603490591,-0.0025533626321703196,-0.0025078512262552977,-0.0025073301512748003,-0.0024967901408672333,-0.002476181136444211,-0.002469812287017703,-0.002460241550579667,-0.002431716537103057,-0.002423045923933387,-0.0024225376546382904,-0.0023992492351680994,-0.002359150443226099,-0.0023549306206405163,-0.0023467421997338533,-0.002344064647331834,-0.002331374678760767,-0.0023222817108035088,-0.002311746124178171,-0.002304932801052928,-0.002281598746776581,-0.0022765696048736572,-0.00223858212120831,-0.0022250490728765726,-0.002215114887803793,-0.0022024097852408886,-0.0021268243435770273,-0.002123898593708873,-0.0020918874070048332,-0.002091202652081847,-0.0020831299480050802,-0.002081748330965638,-0.002077972050756216,-0.0020764791406691074,-0.0020690793171525,-0.0020618936978280544,-0.002059713937342167,-0.002048063324764371,-0.0020471285097301006,-0.0020351989660412073,-0.00202920101583004,-0.0020090462639927864,-0.001965759787708521,-0.001959314104169607,-0.00195490219630301,-0.0019396740244701505,-0.00193161447532475,-0.0019283283036202192,-0.0019091602880507708,-0.0018999985186383128,-0.0018891607178375125,-0.0018769758753478527,-0.0018661868525668979,-0.0018600602634251118,-0.001858384464867413,-0.0018572746776044369,-0.0018496568081900477,-0.0018354688072577119,-0.0018321939278393984,-0.0018318459624424577,-0.0018318149959668517,-0.001829038723371923,-0.001822405494749546,-0.001822044374421239,-0.0018218164332211018,-0.0018207835964858532,-0.0018207260873168707,-0.0018142475746572018,-0.001809341018088162,-0.0018029191996902227,-0.0018002826254814863,-0.0017769704572856426,-0.0017696303548291326,-0.001742699882015586,-0.0017405201215296984,-0.0017377600306645036,-0.001734683639369905,-0.0017330640694126487,-0.001701213070191443,-0.0016962136141955853,-0.001695674262009561,-0.001683780807070434,-0.001679117209278047,-0.0016783135943114758,-0.0016612926265224814,-0.0016582761891186237,-0.0016471731942147017,-0.001641479437239468,-0.0016332102240994573,-0.001629239646717906,-0.0016243105055764318,-0.0016187385190278292,-0.0016088261036202312,-0.0016053677536547184,-0.0016036608722060919,-0.0016033671563491225,-0.0015967446379363537,-0.0015932002570480108,-0.0015895627439022064,-0.0015894569223746657,-0.0015868933405727148,-0.0015853442018851638,-0.0015847369795665145,-0.001572618493810296,-0.0015696431510150433,-0.0015631277346983552,-0.0015454692766070366,-0.0015203310176730156,-0.001515903975814581,-0.0015135934809222817,-0.0015101049793884158,-0.0015063377795740962,-0.0014984976733103395,-0.001492961193434894,-0.0014844206161797047,-0.0014729760587215424,-0.0014713142300024629,-0.001467216876335442,-0.0014624219620600343,-0.0014526897575706244,-0.001449216273613274,-0.0014444774715229869,-0.0014291980769485235,-0.0014216084964573383,-0.0014201136073097587,-0.0014115585945546627,-0.0014076470397412777,-0.0013992579188197851,-0.0013978559290990233,-0.0013919973280280828,-0.0013883472420275211,-0.0013782941969111562,-0.0013716318644583225,-0.001366222626529634,-0.0013559896033257246,-0.0013536530314013362,-0.0013503804802894592,-0.0013497619656845927,-0.00134926731698215,-0.0013471244601532817,-0.0013304587919265032,-0.001327806618064642,-0.0013232886558398604,-0.0013189143501222134,-0.0013018015306442976,-0.001301255659200251,-0.0013000094331800938,-0.0012936340644955635,-0.0012860537972301245,-0.0012656551552936435,-0.001256090123206377,-0.0012509376974776387,-0.0012508626095950603,-0.001244496670551598,-0.0012429541675373912,-0.0012368265306577086,-0.001233489834703505,-0.0012299600057303905,-0.001224742503836751,-0.0012235328322276473,-0.0012182556092739105,-0.001209755428135395,-0.001203687395900488,-0.001199586084112525,-0.0011922207195311785,-0.0011775754392147064,-0.001147376256994903,-0.0011443281546235085,-0.0011399075156077743,-0.001138872466981411,-0.00113628467079252,-0.0011011975584551692,-0.0010970169678330421,-0.001086449483409524,-0.0010814659763127565,-0.0010813996195793152,-0.001079839887097478,-0.0010795474518090487,-0.0010786601342260838,-0.0010704979067668319,-0.0010648006573319435,-0.0010635063517838717,-0.0010558468056842685,-0.0010547843994572759,-0.0010531003354117274,-0.0010530208237469196,-0.0010474930750206113,-0.0010343381436541677,-0.0010177018120884895,-0.0009984188945963979,-0.000997062656097114,-0.00098699820227921,-0.0009869475616142154,-0.0009864335879683495,-0.0009839384583756328,-0.000981644494459033,-0.0009715447085909545,-0.0009712238679639995,-0.0009685212280601263,-0.000951459864154458,-0.0009513465920463204,-0.0009425521129742265,-0.0009405900491401553,-0.0009374739020131528,-0.0009308238513767719,-0.0009205107344314456,-0.0009181976201944053,-0.0009181350469589233,-0.0009096154826693237,-0.000904328073374927,-0.0008909119642339647,-0.000888753856997937,-0.0008874791674315929,-0.00088669074466452,-0.0008746497333049774,-0.000870377873070538,-0.0008701056358404458,-0.000861417967826128,-0.0008561086724512279,-0.0008523306460119784,-0.0008520340779796243,-0.0008506044978275895,-0.0008482486009597778,-0.0008399843936786056,-0.000839649757836014,-0.0008306330419145525,-0.0008282138151116669,-0.0008266438380815089,-0.0008190303924493492,-0.0008173286914825439,-0.0008161687874235213,-0.0008102758438326418,-0.0008098889957182109,-0.000796731561422348,-0.000796646170783788,-0.000796359614469111,-0.000793732120655477,-0.0007925149984657764,-0.0007902958896011114,-0.0007856225711293519,-0.0007845695363357663,-0.0007844969513826072,-0.0007838791352696717,-0.0007827039225958288,-0.000774725223891437,-0.0007736384868621826,-0.0007686128956265748,-0.0007654437213204801,-0.0007569871959276497,-0.000753053289372474,-0.0007527786656282842,-0.0007417504675686359,-0.0007406233344227076,-0.000730336585547775,-0.0007299214485101402,-0.0007291333167813718,-0.0007257459801621735,-0.000725476595107466,-0.0007092609885148704,-0.0007026629173196852,-0.0006987491506151855,-0.0006844614399597049,-0.0006824296433478594,-0.0006767110317014158,-0.0006766289588995278,-0.000676429714076221,-0.0006747535080648959,-0.0006714609335176647,-0.0006650164723396301,-0.0006644444074481726,-0.0006628878181800246,-0.000661692931316793,-0.000646976986899972,-0.0006427705520763993,-0.0006368616013787687,-0.0006366662564687431,-0.0006364543805830181,-0.0006304633570834994,-0.000629852875135839,-0.0006272259633988142,-0.0006256821798160672,-0.0006065261550247669,-0.00060610770015046,-0.0006020796135999262,-0.0005989183555357158,-0.0005968913319520652,-0.0005944545846432447,-0.0005869839806109667,-0.0005784760578535497,-0.0005725668161176145,-0.0005700502078980207,-0.0005698160966858268,-0.0005696663283742964,-0.0005690777325071394,-0.0005634631379507482,-0.0005602915771305561,-0.0005599844735115767,-0.0005587154882960021,-0.0005577257252298295,-0.000556228740606457,-0.0005495875957421958,-0.0005492280470207334,-0.0005472080665640533,-0.0005421587848104537,-0.0005400361260399222,-0.0005327457329258323,-0.0005286894738674164,-0.0005283352802507579,-0.0005217362777329981,-0.0005210849922150373,-0.0005177654093131423,-0.000511097488924861,-0.0005091445054858923,-0.00050686567556113,-0.0005027158767916262,-0.0004999120719730854,-0.0004987858119420707,-0.000498572422657162,-0.0004979853401891887,-0.0004921440850012004,-0.00048429935122840106,-0.0004825992800761014,-0.00047953083412721753,-0.0004790194216184318,-0.00047785951755940914,-0.00047677860129624605,-0.0004766695201396942,-0.00047601148253306746,-0.00047529488801956177,-0.0004742859164252877,-0.00047269463539123535,-0.0004613146302290261,-0.0004604361893143505,-0.0004566959978546947,-0.0004555344639811665,-0.0004535993793979287,-0.00044940979569219053,-0.0004476855683606118,-0.00044535769848152995,-0.00044158860691823065,-0.0004402167978696525,-0.000440161966253072,-0.00043649302097037435,-0.000435579422628507,-0.0004305036272853613,-0.0004281192959751934,-0.0004249380435794592,-0.0004246880125720054,-0.00041827483801171184,-0.0004175665962975472,-0.00041730597149580717,-0.00041400850750505924,-0.0004136566712986678,-0.00040836737025529146,-0.00040832028025761247,-0.0004077959165442735,-0.0004068407288286835,-0.0004066245164722204,-0.00040572465513832867,-0.00040397272096015513,-0.00039754793397150934,-0.0003970678080804646,-0.00039562329766340554,-0.00039361120434477925,-0.0003886596823576838,-0.0003872428787872195,-0.0003789442707784474,-0.0003783217107411474,-0.00037812336813658476,-0.0003780415572691709,-0.00037766157765872777,-0.0003751330077648163,-0.00037037403672002256,-0.000368379958672449,-0.00036729127168655396,-0.00036609292146749794,-0.00036186276702210307,-0.0003613294684328139,-0.0003608517290558666,-0.00036048368201591074,-0.0003581494092941284,-0.0003560499753803015,-0.000353490118868649,-0.00035020962241105735,-0.0003457756247371435,-0.00034175426117144525,-0.0003360813716426492,-0.0003319852112326771,-0.0003318286035209894,-0.0003309716412331909,-0.00032989218016155064,-0.00032870128052309155,-0.00032819629996083677,-0.00032792671117931604,-0.00032608851324766874,-0.0003243717656005174,-0.000322395411785692,-0.0003221438964828849,-0.00032210125937126577,-0.000322076230077073,-0.00032031460432335734,-0.000315040786517784,-0.00031458615558221936,-0.0003080238529946655,-0.00030597165459766984,-0.00030372932087630033,-0.00030311779119074345,-0.00029788882238790393,-0.0002969233610201627,-0.0002954904630314559,-0.00029196799732744694,-0.000291849224595353,-0.00028574749012477696,-0.0002818338689394295,-0.00027966275229118764,-0.0002782756055239588,-0.00026842369697988033,-0.0002681018377188593,-0.00026596232783049345,-0.00026437133783474565,-0.0002641598985064775,-0.0002637176075950265,-0.00025984077365137637,-0.00025796995032578707,-0.0002569658972788602,-0.00025693251518532634,-0.000256879924563691,-0.00025556294713169336,-0.0002538509725127369,-0.0002447386214043945,-0.0002446414437144995,-0.0002445589052513242,-0.0002435304195387289,-0.00024080589355435222,-0.00023585304734297097,-0.0002290788252139464,-0.00022830665693618357,-0.00022633731714449823,-0.00021875411039218307,-0.0002168118953704834,-0.00021542265312746167,-0.00021157681476324797,-0.0002109347260557115,-0.00020821497309952974,-0.00020704447524622083,-0.00020466923888307065,-0.00020378798944875598,-0.00020241782476659864,-0.00020138054969720542,-0.0002013605844695121,-0.00020080611284356564,-0.00019966154650319368,-0.00019946157408412546,-0.00019868105300702155,-0.00019484430958982557,-0.00019264727598056197,-0.00019152701133862138,-0.00019050062110181898,-0.00018832937348634005,-0.00018685236864257604,-0.00018650785204954445,-0.00018550634558778256,-0.0001842682104324922,-0.0001818603341234848,-0.00018112898396793753,-0.00018068536883220077,-0.00017857446800917387,-0.00017805233073886484,-0.00017789662524592131,-0.00017774566367734224,-0.0001766748755471781,-0.0001756685960572213,-0.00017556280363351107,-0.0001738378341542557,-0.00017354398733004928,-0.00017221496091224253,-0.00017110511544160545,-0.00016763001622166485,-0.00016675189544912428,-0.00016567438433412462,-0.00016537428018637002,-0.00016416072321590036,-0.0001639582187635824,-0.00016186937864404172,-0.00016151070303749293,-0.00015821024135220796,-0.0001577672373969108,-0.00015774428902659565,-0.00015675037866458297,-0.0001560989039717242,-0.00015483125753235072,-0.00015444606833625585,-0.00015339448873419315,-0.00015191540296655148,-0.00015179708134382963,-0.00015094279660843313,-0.00014990224735811353,-0.00014918670058250427,-0.0001469926501158625,-0.00014655858103651553,-0.00014611512597184628,-0.00014569953782483935,-0.00014557092799805105,-0.0001450979762012139,-0.00014240115706343204,-0.00014118418039288372,-0.00014051706239115447,-0.00014032125181984156,-0.00013969391875434667,-0.0001377499138470739,-0.0001369676028843969,-0.0001350444508716464,-0.0001343734620604664,-0.00012878701090812683,-0.0001266141189262271,-0.00012649684504140168,-0.00012646467075683177,-0.00012630030687432736,-0.0001256453979294747,-0.0001251697540283203,-0.00012140214676037431,-0.00011626794730545953,-0.00011539310071384534,-0.00011403992539271712,-0.00011251986143179238,-0.00011206984345335513,-0.00010972306336043403,-0.00010888591350521892,-0.00010849162936210632,-0.00010840833419933915,-0.00010711908544180915,-0.0001061356088030152,-0.0001060666108969599,-0.00010593250044621527,-0.00010528937127673998,-0.0001035694804158993,-0.00010341376037104055,-0.00010257616668241099,-0.00010174855560762808,-9.893372771330178e-05,-9.840712300501764e-05,-9.681373921921477e-05,-9.459376451559365e-05,-9.397819667356089e-05,-9.328931628260761e-05,-9.268432768294588e-05,-8.988365880213678e-05,-8.885130228009075e-05,-8.82042950252071e-05,-8.774369780439883e-05,-8.617445564595982e-05,-8.615255501354113e-05,-8.337616600329056e-05,-8.018314838409424e-05,-8.018091466510668e-05,-7.85861921031028e-05,-7.857724995119497e-05,-7.614284550072625e-05,-7.494345481973141e-05,-7.455438026227057e-05,-7.166817522374913e-05,-7.158875814639032e-05,-7.103919779183343e-05,-7.044449739623815e-05,-6.965413922443986e-05,-6.895914702909067e-05,-6.843343726359308e-05,-6.733387999702245e-05,-6.595239392481744e-05,-6.552502600243315e-05,-6.52675298624672e-05,-6.51143491268158e-05,-6.469353684224188e-05,-6.328224844764918e-05,-6.100728933233768e-05,-6.034016769262962e-05,-5.940168921370059e-05,-5.894377682125196e-05,-5.8669895224738866e-05,-5.793452146463096e-05,-5.639389200950973e-05,-5.631118983728811e-05,-5.62722998438403e-05,-5.522489664144814e-05,-5.4334999731509015e-05,-5.2894651162205264e-05,-5.261883052298799e-05,-5.2554754802258685e-05,-5.200639498070814e-05,-5.137965126777999e-05,-5.1081031415378675e-05,-4.877224637311883e-05,-4.8604011681163684e-05,-4.7178120439639315e-05,-4.6137422032188624e-05,-4.565447670756839e-05,-4.521340088103898e-05,-4.5192391553428024e-05,-4.388242814457044e-05,-4.3217241909587756e-05,-4.273742524674162e-05,-4.235580490785651e-05,-4.165515201748349e-05,-4.079565405845642e-05,-4.0444432670483366e-05,-4.0211678424384445e-05,-3.925502460333519e-05,-3.845438186544925e-05,-3.838211341644637e-05,-3.7219524529064074e-05,-3.627851765486412e-05,-3.612771615735255e-05,-3.5397260944591835e-05,-3.431156437727623e-05,-3.343745993333869e-05,-3.279656084487215e-05,-3.2061783713288605e-05,-3.1191557354759425e-05,-2.9827504476998e-05,-2.7552097890293226e-05,-2.5856048523564823e-05,-2.578049861767795e-05,-2.5094597731367685e-05,-2.451658292557113e-05,-2.3764372599544004e-05,-2.170622428820934e-05,-2.1670311980415136e-05,-2.0642875824705698e-05,-2.044618122454267e-05,-1.9995271941297688e-05,-1.9830615201499313e-05,-1.9036680896533653e-05,-1.8608272512210533e-05,-1.810461253626272e-05,-1.799717574613169e-05,-1.79202852450544e-05,-1.6923844668781385e-05,-1.6692130884621292e-05,-1.5960484233801253e-05,-1.5763938790769316e-05,-1.3627261068904772e-05,-1.2111514479329344e-05,-1.185670498671243e-05,-1.1498033927637152e-05,-1.0467618267284706e-05,-8.877366781234741e-06,-6.821453553129686e-06,-5.712807251256891e-06,-5.7011843637155835e-06,-5.413889994088095e-06,-4.898309725831496e-06,-4.249662197253201e-06,-3.934949745598715e-06,-3.2784043924038997e-06,-3.262162181272288e-06,-2.575218786660116e-06,-2.53885991696734e-06,-2.1682678834622493e-06,-2.1316111542546423e-06,-2.0138918443990406e-06,-1.9565225102269324e-06,-1.7301738353125984e-06,-1.4270841575125814e-06,-1.3549625919040409e-06,4.900991825707024e-07,9.72747784544481e-07,1.2995302540730336e-06,1.7726421219776967e-06,1.8164515722673968e-06,2.231299959021271e-06,2.419203610770637e-06,2.9490888664440718e-06,3.0072033041506074e-06,3.5123528050462482e-06,3.550201654434204e-06,3.6107003325014375e-06,4.253685347066494e-06,4.580616860039299e-06,4.7869980335235596e-06,5.789250280940905e-06,5.868524112884188e-06,6.710886736982502e-06,7.674992048123386e-06,7.977485438459553e-06,9.723752555146348e-06,9.823292202781886e-06,1.0854601896426175e-05,1.1162906957906671e-05,1.1447966244304553e-05,1.2445300853869412e-05,1.2522936231107451e-05,1.3186484466132242e-05,1.4898479093972128e-05,1.5058070857776329e-05,1.6102492736536078e-05,1.623094067326747e-05,1.650571903155651e-05,1.695275386737194e-05,1.787349538062699e-05,1.8196404198533855e-05,1.9519626221153885e-05,1.9697547031682916e-05,1.9759834685828537e-05,2.0366460375953466e-05,2.0476132704061456e-05,2.1281242879922502e-05,2.149313695554156e-05,2.2226720830076374e-05,2.3690759917371906e-05,2.4018585463636555e-05,2.4209619368775748e-05,2.5066583475563675e-05,2.520427187846508e-05,2.601459709694609e-05,2.690285509743262e-05,2.8370022846502252e-05,2.847611904144287e-05,2.9582231945823878e-05,3.148272662656382e-05,3.244355320930481e-05,3.266662315581925e-05,3.3999829611275345e-05,3.4052878618240356e-05,3.414869206608273e-05,3.470316369202919e-05,3.476202618912794e-05,3.49080546584446e-05,3.504633787088096e-05,3.626823308877647e-05,3.6595462006516755e-05,3.749996540136635e-05,3.753840792342089e-05,3.805026426562108e-05,3.80718702217564e-05,4.067942427354865e-05,4.083260864717886e-05,4.1187704482581466e-05,4.382476254249923e-05,4.4436157622840255e-05,4.448726758710109e-05,4.502907540882006e-05,4.58107897429727e-05,4.73082072858233e-05,4.740178701467812e-05,4.761770469485782e-05,4.781335519510321e-05,4.847600939683616e-05,4.882395296590403e-05,4.897356120636687e-05,4.931301009492017e-05,5.009591404814273e-05,5.176186459721066e-05,5.184873953112401e-05,5.2172094001434743e-05,5.22243972227443e-05,5.3288044000510126e-05,5.3998381190467626e-05,5.541041537071578e-05,5.632326065097004e-05,5.878910451428965e-05,6.017371924826875e-05,6.36638724245131e-05,6.40504076727666e-05,6.542593473568559e-05,6.622597720706835e-05,6.730571476509795e-05,6.844579911557958e-05,6.978496821830049e-05,7.000014011282474e-05,7.105246186256409e-05,7.407530938507989e-05,7.433995779138058e-05,7.968097634147853e-05,8.072465425357223e-05,8.174791582860053e-05,8.18453700048849e-05,8.258342859335244e-05,8.408084249822423e-05,8.550584607291967e-05,8.57728737173602e-05,8.671611431054771e-05,8.680119935888797e-05,8.796617476036772e-05,8.833214815240353e-05,8.862778486218303e-05,8.891805919120088e-05,8.95953198778443e-05,8.984789019450545e-05,9.34569543460384e-05,9.371772466693074e-05,9.405001765117049e-05,9.434998355573043e-05,9.45726060308516e-05,9.473755926592276e-05,9.691536251921207e-05,9.755417704582214e-05,0.0001007202299661003,0.00010198041854891926,0.0001033003645716235,0.00010335803381167352,0.00010363116598455235,0.0001038777845678851,0.00010464846855029464,0.00010705753811635077,0.00010708585614338517,0.00010735556134022772,0.00010804072371684015,0.00010976910562021658,0.0001103165777749382,0.00011036395881092176,0.00011158704728586599,0.00011421948875067756,0.00011431724124122411,0.00011711388651747257,0.00011728018580470234,0.00011800229549407959,0.00012009188503725454,0.00012092828546883538,0.00012119785242248327,0.00012196183524793014,0.00012239933130331337,0.00012240275100339204,0.00012354641512501985,0.0001236429816344753,0.00012403800792526454,0.00012466550106182694,0.00012479483848437667,0.0001252278743777424,0.00012549445091281086,0.0001258319680346176,0.00012613460421562195,0.00012645185051951557,0.00012652009900193661,0.0001271665096282959,0.0001292741362703964,0.00012950539530720562,0.00013084277452435344,0.0001320183218922466,0.00013384535850491375,0.00013457343447953463,0.000137288574478589,0.00013853833661414683,0.00014041588292457163,0.0001432463468518108,0.0001441633648937568,0.00014519094838760793,0.00014717191515956074,0.0001473304582759738,0.00015031517250463367,0.00015320151578634977,0.00015362948761321604,0.00015546873328275979,0.00015563637134619057,0.00015607282693963498,0.0001593753695487976,0.00015948251530062407,0.00015957519644871354,0.0001626503508305177,0.00016435369616374373,0.00016610443708486855,0.00017153978114947677,0.00017206146731041372,0.00017297059821430594,0.00017367451800964773,0.00017460674280300736,0.0001746542693581432,0.00017583474982529879,0.00017678350559435785,0.0001776526914909482,0.00017839312204159796,0.00017887487774714828,0.0001789964735507965,0.0001791325194062665,0.00017991929780691862,0.00018100961460731924,0.00018189341062679887,0.0001828770327847451,0.00018380105029791594,0.00018418714171275496,0.00018790543253999203,0.00018798767996486276,0.0001887305115815252,0.0001888261758722365,0.00018915980763267726,0.0001910430146381259,0.0001917162589961663,0.0001959279179573059,0.0001961986708920449,0.00019641027029138058,0.00019875899306498468,0.00020232990209478885,0.0002077665994875133,0.00021109789668116719,0.00021190449479036033,0.0002132773370249197,0.0002168118953704834,0.0002195736742578447,0.00022226705914363265,0.0002252768026664853,0.00022588208958040923,0.00022663071285933256,0.00022726625320501626,0.00022967756376601756,0.00023405120009556413,0.0002361875813221559,0.00024254218442365527,0.0002463927958160639,0.0002465181169100106,0.0002478633832652122,0.00024912506341934204,0.0002506202436052263,0.0002518102410249412,0.00025383889442309737,0.0002556395484134555,0.00025675579672679305,0.0002567623450886458,0.0002570745418779552,0.00025812641251832247,0.00026074782363139093,0.0002612769603729248,0.00026532396441325545,0.00026824205997399986,0.00026967257144860923,0.00026990234619006515,0.00027050793869420886,0.00027070389478467405,0.0002709464752115309,0.00027127147768624127,0.0002729529223870486,0.0002773901796899736,0.00028077512979507446,0.0002833029720932245,0.00028371275402605534,0.0002857582294382155,0.00028577877674251795,0.00028747052419930696,0.000287906383164227,0.0002916845551226288,0.00029505445854738355,0.0002958272525575012,0.00029745607753284276,0.00030022099963389337,0.00030072644585743546,0.00030382483964785933,0.0003052332904189825,0.00030549109214916825,0.00030738263740204275,0.00030739500652998686,0.00030918687116354704,0.00031010477687232196,0.0003115460276603699,0.00031203002436086535,0.0003167645772919059,0.0003174583544023335,0.00031793848029337823,0.0003209961869288236,0.0003300701209809631,0.00033177106524817646,0.0003349235630594194,0.00033523127785883844,0.00033666446688584983,0.0003413969243410975,0.0003427219344303012,0.00034427447826601565,0.00034620374208316207,0.0003499822341836989,0.00035114018828608096,0.0003512394323479384,0.000351527618477121,0.0003519064048305154,0.0003566037048585713,0.0003568480897229165,0.00036103278398513794,0.0003640936338342726,0.0003671461308840662,0.00036750585422851145,0.00037108405376784503,0.0003731003380380571,0.00037393823731690645,0.00037438541767187417,0.0003786517772823572,0.00038232997758314013,0.0003841075231321156,0.0003862892044708133,0.0003912627580575645,0.00039158330764621496,0.0003947172954212874,0.0003958575543947518,0.00039657458546571434,0.00039925173041410744,0.00040156111936084926,0.00040788008482195437,0.0004125676932744682,0.0004131308232899755,0.0004133152833674103,0.0004155518254265189,0.0004169505846221,0.000419378571677953,0.00042170181404799223,0.00042821571696549654,0.000428285013185814,0.0004289782082196325,0.00043162674410268664,0.00043320373515598476,0.0004336567362770438,0.0004360273596830666,0.000446608813945204,0.00044755623093806207,0.00044829861144535244,0.0004498027265071869,0.000450470601208508,0.0004523842071648687,0.0004559786757454276,0.00045654759742319584,0.0004644322325475514,0.0004662129213102162,0.00046637863852083683,0.00046797306276857853,0.0004682767321355641,0.00046956853475421667,0.0004761941672768444,0.0004864896764047444,0.0004891990101896226,0.0004903716035187244,0.0004957573255524039,0.0004972247988916934,0.0004982109530828893,0.0005011366447433829,0.0005040197283960879,0.0005045290454290807,0.0005049678729847074,0.0005068276659585536,0.0005086116725578904,0.000511291204020381,0.0005124064045958221,0.000517423904966563,0.0005236898432485759,0.0005269563407637179,0.0005414762999862432,0.0005428087897598743,0.000543220667168498,0.00054879707749933,0.0005500702536664903,0.000550220487639308,0.0005537620163522661,0.0005558617413043976,0.0005569478962570429,0.0005572263617068529,0.0005589143838733435,0.0005608874489553273,0.0005610790685750544,0.0005642551113851368,0.0005678841262124479,0.0005708846729248762,0.0005788177368231118,0.0005791811854578555,0.0005805724649690092,0.0005844752304255962,0.0005854962510056794,0.0005903932615183294,0.0005950382328592241,0.0005960646085441113,0.0005964328302070498,0.0005966353346593678,0.0005989739438518882,0.000599748338572681,0.0006001372821629047,0.0006021796143613756,0.0006045536720193923,0.0006058583967387676,0.0006070549716241658,0.0006090338574722409,0.0006120465695858002,0.0006135287694633007,0.000614579941611737,0.0006160839111544192,0.0006208994891494513,0.000621938263066113,0.0006242308299988508,0.0006248040590435266,0.000628200767096132,0.0006283910479396582,0.0006311926408670843,0.0006327654700726271,0.0006358067621476948,0.0006359121180139482,0.0006408595945686102,0.0006408943445421755,0.0006443348247557878,0.000645564345177263,0.0006458395509980619,0.0006512318504974246,0.0006604929221794009,0.0006634402088820934,0.0006639267667196691,0.0006665725959464908,0.0006674097385257483,0.0006695043994113803,0.0006740294629707932,0.0006785626756027341,0.0006826479802839458,0.0006873097736388445,0.0006909473449923098,0.0006909527000971138,0.0006942911422811449,0.0006992012495175004,0.000702021992765367,0.0007029604748822749,0.0007264029700309038,0.0007297199917957187,0.0007366216159425676,0.0007400570902973413,0.0007438884931616485,0.000744252058211714,0.0007492995355278254,0.0007497033220715821,0.0007509030401706696,0.0007521529332734644,0.0007529522408731282,0.0007535594631917775,0.0007540187216363847,0.000759934657253325,0.0007618263480253518,0.0007647265447303653,0.0007679818663746119,0.0007688940968364477,0.0007725316099822521,0.000772701227106154,0.0007789593655616045,0.000779789115767926,0.0007865618099458516,0.0007871788693591952,0.0007917245966382325,0.0007969950092956424,0.0008015866624191403,0.0008037790539674461,0.000806830357760191,0.0008090056362561882,0.0008097545942291617,0.0008145168540067971,0.0008173880050890148,0.0008194309193640947,0.0008229743107222021,0.0008267052471637726,0.0008330075652338564,0.0008342351065948606,0.0008346402901224792,0.0008355894824489951,0.0008369433926418424,0.0008375455508939922,0.0008380982326343656,0.0008389826398342848,0.0008404158288612962,0.0008411991875618696,0.0008433286566287279,0.0008483879501000047,0.0008484928403049707,0.0008688871748745441,0.0008702967897988856,0.0008716312004253268,0.0008764739613980055,0.0008778402116149664,0.0008787569240666926,0.0008907123119570315,0.0008909564930945635,0.0008961644489318132,0.0009005001047626138,0.0009007648914121091,0.0009019991848617792,0.0009044341859407723,0.0009045131737366319,0.0009055068949237466,0.000911917828489095,0.0009143993374891579,0.0009238123893737793,0.0009244318353012204,0.0009252176969312131,0.0009269929141737521,0.0009314125636592507,0.0009340329561382532,0.0009341363329440355,0.000935262767598033,0.0009436032269150019,0.0009469506330788136,0.000954344286583364,0.0009572256822139025,0.0009573620627634227,0.0009598085307516158,0.0009672713349573314,0.0009703136747702956,0.0009712274186313152,0.0009785518050193787,0.0009807073511183262,0.0009867885382845998,0.0009931433014571667,0.0009948527440428734,0.0009978909511119127,0.0010064287343993783,0.0010137318167835474,0.001016566646285355,0.001016620546579361,0.0010271838400512934,0.001037505455315113,0.001039471011608839,0.0010482381330803037,0.0010515233734622598,0.0010692659998312593,0.00107602181378752,0.0010807348880916834,0.0010837445734068751,0.0010838012676686049,0.0010844416683539748,0.00108648044988513,0.0010941855143755674,0.0011029316810891032,0.0011064144782721996,0.0011094900546595454,0.001111300429329276,0.0011138857807964087,0.0011210977099835873,0.0011299902107566595,0.0011347195832058787,0.001135326223447919,0.0011384579120203853,0.0011396344052627683,0.0011417879723012447,0.0011439499212428927,0.001145153888501227,0.0011489780154079199,0.0011522471904754639,0.0011604293249547482,0.0011749766999855638,0.0011763302609324455,0.0011818173807114363,0.0011871978640556335,0.0011892698239535093,0.0011970143532380462,0.0011985652381554246,0.0012005510507151484,0.001212668721564114,0.0012176005402579904,0.001218187971971929,0.0012194524751976132,0.0012273644097149372,0.0012363289715722203,0.001238692202605307,0.0012418084079399705,0.001250881003215909,0.0012516235001385212,0.0012546360958367586,0.0012602049391716719,0.0012662081280723214,0.0012668394483625889,0.0012669390998780727,0.0012688941787928343,0.001271748566068709,0.0012717879144474864,0.001272291410714388,0.0012822059215977788,0.0012825694866478443,0.0012826037127524614,0.0012870493810623884,0.0012880004942417145,0.0012904272880405188,0.0012985893990844488,0.0012999316677451134,0.001301840296946466,0.0013018612517043948,0.0013189431047067046,0.0013227384770289063,0.0013309232890605927,0.0013315468095242977,0.001344435615465045,0.0013502332149073482,0.0013569690054282546,0.0013598180375993252,0.0013917726464569569,0.001394113409332931,0.001396377687342465,0.0014045407297089696,0.0014051556354388595,0.0014082950074225664,0.0014085019938647747,0.0014109051553532481,0.0014361708890646696,0.0014398940838873386,0.0014403872191905975,0.0014717080630362034,0.001486514462158084,0.0014877136563882232,0.001490973518230021,0.0014923615381121635,0.0014930105535313487,0.0014939929824322462,0.0015038973651826382,0.0015104241902008653,0.001513624913059175,0.001515375915914774,0.0015214555896818638,0.0015240657376125455,0.0015376447699964046,0.001546573592349887,0.0015524148475378752,0.0015603606589138508,0.0015644464874640107,0.0015678961062803864,0.0015684026293456554,0.0015724918339401484,0.001574224210344255,0.0015844813315197825,0.0015910613583400846,0.001599072478711605,0.0016270343912765384,0.001630313228815794,0.0016347989439964294,0.0016486215172335505,0.0016533122397959232,0.001655338448472321,0.0016757437260821462,0.0016919915797188878,0.0017176904948428273,0.0017178511479869485,0.0017303882632404566,0.0017318108584731817,0.001734092365950346,0.0017360661877319217,0.0017412889283150434,0.0017625377513468266,0.001768360729329288,0.0017827439587563276,0.001788571011275053,0.0018052496016025543,0.001810492598451674,0.0018115320708602667,0.00181233836337924,0.0018192927818745375,0.0018431928474456072,0.0018433666555210948,0.0018557218136265874,0.0018559544114395976,0.0018590124091133475,0.001859163399785757,0.001898039598017931,0.0019037157762795687,0.0019125809194520116,0.0019130578730255365,0.0019189986633136868,0.0019192786421626806,0.001928921090438962,0.0019367405911907554,0.0019510629354044795,0.0019557324703782797,0.0019692815840244293,0.0019794534891843796,0.0019877543672919273,0.0020026215352118015,0.0020107124000787735,0.002015062840655446,0.002038291422650218,0.0020490961614996195,0.0020566051825881004,0.0020602496806532145,0.0020657640416175127,0.0020697040017694235,0.002074546879157424,0.002074655843898654,0.0020836808253079653,0.002089657122269273,0.0021028576884418726,0.002107964362949133,0.0021224385127425194,0.002128642750903964,0.0021367298904806376,0.0021445986349135637,0.0021494782995432615,0.0021500373259186745,0.002151773776859045,0.00217459280975163,0.002178126247599721,0.0021807102020829916,0.0021839819382876158,0.002191327279433608,0.002221287228167057,0.002226622076705098,0.0022421209141612053,0.0022530914284288883,0.0022756061516702175,0.002294571604579687,0.0023261324968189,0.0023429961875081062,0.0023581197019666433,0.0023594440426677465,0.0023604969028383493,0.002364320447668433,0.0023656729608774185,0.0023923933040350676,0.0023925004061311483,0.002409935463219881,0.002425563521683216,0.002436754060909152,0.0024466796312481165,0.002450961386784911,0.002479385584592819,0.002497578039765358,0.0025000469759106636,0.002517862245440483,0.0025185560807585716,0.002521123271435499,0.002553880214691162,0.002569559495896101,0.0025910064578056335,0.0025922758504748344,0.0025948970578610897,0.0025989962741732597,0.0026097851805388927,0.002625478897243738,0.0026314293500036,0.0026425733231008053,0.0026647441554814577,0.002700568176805973,0.002702465746551752,0.0027047835756093264,0.0027231452986598015,0.002724532736465335,0.0027694953605532646,0.0027700522914528847,0.0027709249407052994,0.002782355761155486,0.0027920196298509836,0.0027966201305389404,0.002806299366056919,0.0028140803333371878,0.002816610038280487,0.002821297850459814,0.002823691349476576,0.002836574800312519,0.0028465010691434145,0.0028546417597681284,0.002858845517039299,0.0028675401117652655,0.0028771734796464443,0.002897904720157385,0.0029042651876807213,0.0029395895544439554,0.0029897228814661503,0.0029960304964333773,0.003050193889066577,0.003068268997594714,0.0030966117046773434,0.0031079372856765985,0.003109778743237257,0.003133641555905342,0.0031744614243507385,0.00320444256067276,0.003238576464354992,0.0032582946587353945,0.003258629236370325,0.003300267504528165,0.003300382988527417,0.0033147272188216448,0.003355541033670306,0.003358815098181367,0.003360031172633171,0.003365980926901102,0.003419646993279457,0.0034277555532753468,0.0034474870190024376,0.003457502229139209,0.003458076622337103,0.0035664725583046675,0.00359379593282938,0.003610581159591675,0.0036195209249854088,0.0036301768850535154,0.0036380251403898,0.003647061064839363,0.0036721632350236177,0.003695472376421094,0.0037000852171331644,0.0037113758735358715,0.0037373793311417103,0.0037395900581032038,0.0037536064628511667,0.003788689384236932,0.0038432274013757706,0.003850835608318448,0.0038556335493922234,0.0038737552240490913,0.003888639621436596,0.003910853061825037,0.003934711683541536,0.003935051150619984,0.003963286057114601,0.003982454538345337,0.003984001465141773,0.004004193935543299,0.004036493133753538,0.004050975665450096,0.004071258474141359,0.0040930164977908134,0.0041145519353449345,0.004157325252890587,0.004176275804638863,0.004208400845527649,0.004225582350045443,0.0042527541518211365,0.004268183838576078,0.004351469688117504,0.004398483783006668,0.004404145292937756,0.004459354095160961,0.0045561399310827255,0.004563959315419197,0.00467571709305048,0.004774157889187336,0.004783962853252888,0.004793060012161732,0.004867864307016134,0.004867875948548317,0.004918462131172419,0.004948112182319164,0.004961047787219286,0.005057899747043848,0.005101219285279512,0.005111815873533487,0.005153555888682604,0.005154635291546583,0.005222637671977282,0.005250865127891302,0.005259933415800333,0.005265364423394203,0.005269254557788372,0.005278137978166342,0.005298799369484186,0.005305209197103977,0.00531143881380558,0.0053594233468174934,0.005379797890782356,0.005380377639085054,0.005388012621551752,0.005459916777908802,0.005511922761797905,0.0056417095474898815,0.0056497566401958466,0.005683066789060831,0.005707056727260351,0.005771939642727375,0.005876580253243446,0.005991954822093248,0.00600895332172513,0.0060159582644701,0.006022472400218248,0.006035068072378635,0.006042239256203175,0.00609457865357399,0.006094926502555609,0.006220728158950806,0.006258232519030571,0.006297983694821596,0.006326013710349798,0.006347014103084803,0.006364766508340836,0.006369070615619421,0.0063910637982189655,0.0064065842889249325,0.006492248736321926,0.006503007840365171,0.006575702223926783,0.006577621679753065,0.00659060338512063,0.006593023426830769,0.006614052224904299,0.006679435260593891,0.006804060190916061,0.006809576880186796,0.0068344068713486195,0.006898655090481043,0.006949435919523239,0.007014358416199684,0.007055261172354221,0.0071648359298706055,0.0072035337798297405,0.00736330496147275,0.007380542811006308,0.007483579684048891,0.007505057845264673,0.007527083158493042,0.007578594144433737,0.00769508583471179,0.0077034663408994675,0.007813865318894386,0.007924233563244343,0.008052103221416473,0.008070484735071659,0.008107353001832962,0.00814806018024683,0.008269021287560463,0.008305143564939499,0.008331561461091042,0.008399482816457748,0.008401183411478996,0.008468525484204292,0.008505798876285553,0.008524175733327866,0.008562944829463959,0.008654857985675335,0.008800501003861427,0.008829915896058083,0.008852633647620678,0.008930515497922897,0.009049274027347565,0.00913352519273758,0.009375962428748608,0.00938331801444292,0.0094838235527277,0.009535425342619419,0.009709018282592297,0.009789071045815945,0.009790884330868721,0.010085717774927616,0.010089436545968056,0.010201322846114635,0.010235936380922794,0.010265320539474487,0.010265802033245564,0.01027594693005085,0.010327961295843124,0.010401950217783451,0.010489306412637234,0.010493764653801918,0.010581149719655514,0.010625442489981651,0.010686254128813744,0.010837817564606667,0.010857317596673965,0.01090487465262413,0.01093384064733982,0.011007720604538918,0.011021146550774574,0.011032923124730587,0.01110008079558611,0.011298218742012978,0.01140144094824791,0.011619186028838158,0.011654263362288475,0.011782879941165447,0.012034738436341286,0.012064668349921703,0.012171861715614796,0.012177199125289917,0.012327554635703564,0.012371011078357697,0.012414133176207542,0.0128880450502038,0.013253849931061268,0.013543489389121532,0.013548718765377998,0.013576998375356197,0.013615547679364681,0.013934498652815819,0.014022698625922203,0.014091613702476025,0.0141014214605093,0.014324507676064968,0.014393474906682968,0.01448174100369215,0.014523518271744251,0.014560849405825138,0.014914032071828842,0.01497330330312252,0.01521905418485403,0.015315549448132515,0.015593918971717358,0.015699278563261032,0.015805887058377266,0.01590585894882679,0.015956079587340355,0.016165662556886673,0.016171619296073914,0.016326148062944412,0.016780996695160866,0.016799643635749817,0.01682168059051037,0.016832485795021057,0.01685722917318344,0.017130110412836075,0.017479384317994118,0.017612366005778313,0.017813382670283318,0.018792694434523582,0.018908658996224403,0.018972095102071762,0.019151456654071808,0.01974276639521122,0.020288119092583656,0.020760275423526764,0.02099648490548134,0.0213344506919384,0.021347513422369957,0.021352462470531464,0.02152617648243904,0.021862627938389778,0.022185033187270164,0.022195495665073395,0.02226148173213005,0.022284207865595818,0.022992119193077087,0.023645155131816864,0.024295711889863014,0.024348601698875427,0.024803318083286285,0.025031976401805878,0.025166794657707214,0.02518133446574211,0.02632603794336319,0.027146225795149803,0.02747873030602932,0.027798395603895187,0.028403164818882942,0.0287000872194767,0.029886219650506973,0.0300322063267231,0.030591053888201714,0.031775347888469696,0.032460395246744156,0.0325072817504406,0.034112002700567245,0.03477739170193672,0.03509931638836861,0.03613166883587837,0.036260612308979034,0.03752078115940094,0.037663184106349945,0.039045874029397964,0.039534907788038254,0.040757399052381516,0.04149869084358215,0.04255785048007965,0.04530903324484825,0.04583922028541565,0.04797150939702988,0.04804333671927452,0.0482843741774559,0.04855562373995781,0.052966173738241196,0.05322442948818207,0.05330909788608551,0.053906191140413284,0.05535488203167915,0.05723786726593971,0.06328567862510681,0.07425059378147125,0.07729722559452057,0.0807795599102974,0.0807913988828659,0.09862048923969269,0.09903118014335632,0.09936141967773438,0.10375267267227173,0.12045883387327194,0.1380511075258255,0.1853087991476059],\"yaxis\":\"y\",\"type\":\"scattergl\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Sorted neurons\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Loss change\"}},\"legend\":{\"title\":{\"text\":\"variable\"},\"tracegroupgap\":0},\"title\":{\"text\":\"Loss change from ablating MLP5 neuron\"},\"width\":800,\"showlegend\":true},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('2d684eb9-c32f-45e5-b990-deef2069d245');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sorted_means, indices = torch.sort(diffs.mean(1))\n",
    "sorted_means = sorted_means.tolist()\n",
    "haystack_utils.line(sorted_means, xlabel=\"Sorted neurons\", ylabel=\"Loss change\", title=\"Loss change from ablating MLP5 neuron\") # xticks=indices\n",
    "sorted_top_neuron_indices = indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>                            <div id=\"d7b90413-b21c-42dc-9a42-a861a8dbf304\" class=\"plotly-graph-div\" style=\"height:525px; width:1000px;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"d7b90413-b21c-42dc-9a42-a861a8dbf304\")) {                    Plotly.newPlot(                        \"d7b90413-b21c-42dc-9a42-a861a8dbf304\",                        [{\"error_y\":{\"array\":[0.9840341393665195],\"type\":\"data\",\"visible\":true},\"name\":\"Original\",\"x\":[\"Original\"],\"y\":[1.6139547308534383],\"type\":\"bar\"},{\"error_y\":{\"array\":[1.3926855254063564],\"type\":\"data\",\"visible\":true},\"name\":\"Ablated\",\"x\":[\"Ablated\"],\"y\":[3.881795574426651],\"type\":\"bar\"},{\"error_y\":{\"array\":[0.8625324705387765],\"type\":\"data\",\"visible\":true},\"name\":\"MLP5 path patched\",\"x\":[\"MLP5 path patched\"],\"y\":[1.8690502901375294],\"type\":\"bar\"},{\"error_y\":{\"array\":[1.041107329114758],\"type\":\"data\",\"visible\":true},\"name\":\"MLP5 path patched + Top 10 MLP5 neurons ablated\",\"x\":[\"Top MLP5 removed\"],\"y\":[3.0648298299312593],\"type\":\"bar\"},{\"error_y\":{\"array\":[0.6964508478396878],\"type\":\"data\",\"visible\":true},\"name\":\"MLP5 path patched + Bottom 10 MLP5 neurons ablated\",\"x\":[\"Bottom MLP5 removed\"],\"y\":[1.2263662429898978],\"type\":\"bar\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"title\":{\"text\":\"Average last token loss when removing top \\u002f bottom neurons from path patching\"},\"xaxis\":{\"title\":{\"text\":\"\"}},\"yaxis\":{\"title\":{\"text\":\"Loss\"}},\"barmode\":\"group\",\"width\":1000,\"showlegend\":true},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('d7b90413-b21c-42dc-9a42-a861a8dbf304');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check loss change when ablating top / bottom neurons\n",
    "top_neurons_count = 10\n",
    "top_neurons = indices[-top_neurons_count:]\n",
    "bottom_neurons = indices[:top_neurons_count]\n",
    "\n",
    "with model.hooks(deactivate_neurons_fwd_hooks):\n",
    "    ablated_loss, ablated_cache = model.run_with_cache(prompts, return_type=\"loss\")\n",
    "\n",
    "ablate_top_neurons_hook = get_ablate_neurons_hook(top_neurons, ablated_cache)\n",
    "ablate_bottom_neurons_hook = get_ablate_neurons_hook(bottom_neurons, ablated_cache)\n",
    "\n",
    "original_loss, ablated_loss, _, all_MLP5_loss = haystack_utils.get_direct_effect(prompts, model, pos=-1, context_ablation_hooks=deactivate_neurons_fwd_hooks, context_activation_hooks=activate_neurons_fwd_hooks)\n",
    "_, _, _, top_MLP5_ablated_loss = haystack_utils.get_direct_effect(prompts, model, pos=-1, context_ablation_hooks=deactivate_neurons_fwd_hooks, context_activation_hooks=activate_neurons_fwd_hooks+ablate_top_neurons_hook)\n",
    "_, _, _, bottom_MLP5_ablated_loss = haystack_utils.get_direct_effect(prompts, model, pos=-1, context_ablation_hooks=deactivate_neurons_fwd_hooks, context_activation_hooks=activate_neurons_fwd_hooks+ablate_bottom_neurons_hook)\n",
    "\n",
    "names = [\"Original\", \"Ablated\", \"MLP5 path patched\", f\"MLP5 path patched + Top {top_neurons_count} MLP5 neurons ablated\", f\"MLP5 path patched + Bottom {top_neurons_count} MLP5 neurons ablated\"]\n",
    "short_names = [\"Original\", \"Ablated\", \"MLP5 path patched\", f\"Top MLP5 removed\", f\"Bottom MLP5 removed\"]\n",
    "\n",
    "values = [original_loss.tolist(), ablated_loss.tolist(), all_MLP5_loss.tolist(), top_MLP5_ablated_loss.tolist(), bottom_MLP5_ablated_loss.tolist()]\n",
    "haystack_utils.plot_barplot(values, names, short_names=short_names, ylabel=\"Loss\", title=f\"Average last token loss when removing top / bottom neurons from path patching\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigate top / bottom neuron boosts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Patch in MLP5 neurons, ablated bottom neurons, compare with patched MLP5 neurons without ablating bottom neurons\n",
    "\n",
    "with model.hooks(deactivate_neurons_fwd_hooks):\n",
    "    ablated_logits, ablated_cache = model.run_with_cache(prompts)\n",
    "\n",
    "ablate_top_neurons_hook = get_ablate_neurons_hook(top_neurons, ablated_cache)\n",
    "ablate_bottom_neurons_hook = get_ablate_neurons_hook(bottom_neurons, ablated_cache)\n",
    "\n",
    "original_logits, ablated_logprobs, _, all_MLP5_logprobs = haystack_utils.get_direct_effect(prompts, model, pos=-2, context_ablation_hooks=deactivate_neurons_fwd_hooks, context_activation_hooks=activate_neurons_fwd_hooks, return_type='logprobs')\n",
    "_, _, _, top_MLP5_ablated_logprobs = haystack_utils.get_direct_effect(prompts, model, pos=-2, context_ablation_hooks=deactivate_neurons_fwd_hooks, context_activation_hooks=activate_neurons_fwd_hooks+ablate_top_neurons_hook, return_type='logprobs')\n",
    "_, _, _, bottom_MLP5_ablated_logprobs = haystack_utils.get_direct_effect(prompts, model, pos=-2, context_ablation_hooks=deactivate_neurons_fwd_hooks, context_activation_hooks=activate_neurons_fwd_hooks+ablate_bottom_neurons_hook, return_type='logprobs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_difference_neurons(baseline_logprobs, ablated_logprops, positive=True, logprob_threshold=-7, k=50):\n",
    "    neuron_logprob_difference = (baseline_logprobs - ablated_logprops).mean(0)\n",
    "    neuron_logprob_difference[baseline_logprobs.mean(0) < logprob_threshold] = 0\n",
    "    if positive:\n",
    "        non_zero_count = (neuron_logprob_difference > 0).sum()\n",
    "    else:\n",
    "        non_zero_count = (neuron_logprob_difference < 0).sum()\n",
    "    print(neuron_logprob_difference.shape)\n",
    "    top_logprob_difference, top_neurons = haystack_utils.top_k_with_exclude(neuron_logprob_difference, min(non_zero_count, k), all_ignore, largest=positive)\n",
    "    return top_logprob_difference, top_neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50304])\n",
      "torch.Size([50304])\n",
      "torch.Size([50304])\n",
      "torch.Size([50304])\n"
     ]
    }
   ],
   "source": [
    "# Boosted tokens\n",
    "bottom_neuron_pos_difference_logprobs, bottom_pos_indices = get_top_difference_neurons(all_MLP5_logprobs, bottom_MLP5_ablated_logprobs, positive=True)\n",
    "top_neuron_pos_difference_logprobs, top_pos_indices = get_top_difference_neurons(all_MLP5_logprobs, top_MLP5_ablated_logprobs, positive=True)\n",
    "# Deboosted tokens\n",
    "bottom_neuron_neg_difference_logprobs, bottom_neg_indices = get_top_difference_neurons(all_MLP5_logprobs, bottom_MLP5_ablated_logprobs, positive=False)\n",
    "top_neuron_neg_difference_logprobs, top_neg_indices = get_top_difference_neurons(all_MLP5_logprobs, top_MLP5_ablated_logprobs, positive=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "064f6d09b6194ae5902d84a8f1b505ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='Tokens:', options=('Boosted', 'Deboosted'), value='Boosted'), Drop…"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def plot_data(boosted_deboosted, top_bottom):\n",
    "    if boosted_deboosted == 'Boosted' and top_bottom == 'Top':\n",
    "        logprobs = top_neuron_pos_difference_logprobs\n",
    "        indices = top_pos_indices\n",
    "        title_change = 'increase'\n",
    "    elif boosted_deboosted == 'Boosted' and top_bottom == 'Bottom':\n",
    "        logprobs = bottom_neuron_pos_difference_logprobs\n",
    "        indices = bottom_pos_indices\n",
    "        title_change = 'increase'\n",
    "    elif boosted_deboosted == 'Deboosted' and top_bottom == 'Top':\n",
    "        logprobs = top_neuron_neg_difference_logprobs\n",
    "        indices = top_neg_indices\n",
    "        title_change = 'decrease'\n",
    "    else:  # 'Deboosted' and 'Bottom'\n",
    "        logprobs = bottom_neuron_neg_difference_logprobs\n",
    "        indices = bottom_neg_indices\n",
    "        title_change = 'decrease'\n",
    "\n",
    "    xlabel = boosted_deboosted + \" tokens\"\n",
    "    ylabel = \"full_logprob - ablated_logprop\"\n",
    "    title = 'Logprob ' + title_change + ' from ' + top_bottom.lower() + ' neurons'\n",
    "    xticks = [model.to_str_tokens([i])[0] for i in indices]\n",
    "\n",
    "    haystack_utils.line(logprobs.cpu().numpy(), xlabel=xlabel, ylabel=ylabel, title=title, xticks=xticks, show_legend=False)\n",
    "\n",
    "\n",
    "boosted_deboosted = widgets.Dropdown(options=['Boosted', 'Deboosted'], value='Boosted', description='Tokens:')\n",
    "top_bottom = widgets.Dropdown(options=['Top', 'Bottom'], value='Top', description='Neurons:')\n",
    "widgets.interactive(plot_data, boosted_deboosted=boosted_deboosted, top_bottom=top_bottom)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decompose neuron effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get logprob change for individual neuron\n",
    "\n",
    "def get_individual_neuron_logprob_effect(sorted_indices, neuron_pos=0, top=True, positive=True, plot=True):\n",
    "    if top:\n",
    "        neurons = sorted_indices[-(neuron_pos+1)]\n",
    "    else:\n",
    "        neurons = sorted_indices[neuron_pos]\n",
    "    neurons_hook = get_ablate_neurons_hook([neurons], ablated_cache)\n",
    "\n",
    "    original_logits, ablated_logprobs, _, all_MLP5_logprobs = haystack_utils.get_direct_effect(prompts, model, pos=-2, context_ablation_hooks=deactivate_neurons_fwd_hooks, context_activation_hooks=activate_neurons_fwd_hooks, return_type='logprobs')\n",
    "    _, _, _, neuron_ablated_logprobs = haystack_utils.get_direct_effect(prompts, model, pos=-2, context_ablation_hooks=deactivate_neurons_fwd_hooks, context_activation_hooks=activate_neurons_fwd_hooks+neurons_hook, return_type='logprobs')\n",
    "    neuron_difference_logprobs, difference_indices = \\\n",
    "        get_top_difference_neurons(all_MLP5_logprobs, neuron_ablated_logprobs, positive=positive)\n",
    "    \n",
    "    if not plot:\n",
    "        return neuron_difference_logprobs, difference_indices\n",
    "    \n",
    "    if positive:\n",
    "        xlabel = 'Boosted tokens'\n",
    "        title = 'Logprob increase from neuron ' + str(neurons.item())\n",
    "    else: \n",
    "        xlabel = 'Deboosted tokens'\n",
    "        title = 'Logprob decrease from neuron ' + str(neurons.item())\n",
    "    if top:\n",
    "        title += f' (top {neuron_pos+1} neuron)'\n",
    "    else:\n",
    "        title += f' (bottom {neuron_pos+1} neuron)'\n",
    "\n",
    "    xticks = [model.to_str_tokens([i])[0] for i in difference_indices]\n",
    "    haystack_utils.line(neuron_difference_logprobs.cpu().numpy(), xlabel=xlabel, ylabel=\"full_logprob - ablated_logprop\", title=title, xticks=xticks, show_legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "638cee45581041a9b8152f4a70b8d02c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='Neuron Pos:', max=20), Dropdown(description='Neurons:', …"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a function to call your function with widget inputs\n",
    "def plot_individual_neuron(neuron_pos, top_bottom, pos_neg):\n",
    "    top_bool = True if top_bottom == 'Top' else False\n",
    "    positive_bool = True if pos_neg == 'Positive' else False\n",
    "    get_individual_neuron_logprob_effect(indices, neuron_pos=neuron_pos, top=top_bool, positive=positive_bool)\n",
    "\n",
    "# Define widgets\n",
    "neuron_pos_slider = widgets.IntSlider(min=0, max=20, step=1, value=0, description='Neuron Pos:')\n",
    "top_bottom_dropdown = widgets.Dropdown(options=['Top', 'Bottom'], value='Top', description='Neurons:')\n",
    "pos_neg_dropdown = widgets.Dropdown(options=['Positive', 'Negative'], value='Positive', description='Pos/Neg:')\n",
    "\n",
    "# Use interactive function to bind the widgets to the plotting function\n",
    "widgets.interactive(plot_individual_neuron, neuron_pos=neuron_pos_slider, top_bottom=top_bottom_dropdown, pos_neg=pos_neg_dropdown)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum individual weighted positive / negative effects\n",
    "def summed_neuron_differences(num_neurons=10, top=True, positive=True, k=50):\n",
    "    summed_neuron_diffs = torch.zeros(model.cfg.d_vocab).cuda()\n",
    "    for neuron_pos in range(num_neurons):\n",
    "        # It would probably be more principle to do our filtering on the summed diffs instead of per neuron\n",
    "        neuron_diff, token_indices = get_individual_neuron_logprob_effect(indices, neuron_pos=neuron_pos, top=top, positive=positive, plot=False)\n",
    "        summed_neuron_diffs[token_indices] += neuron_diff\n",
    "    if positive:\n",
    "        non_zero_count = (summed_neuron_diffs > 0).sum()\n",
    "    else:\n",
    "        non_zero_count = (summed_neuron_diffs < 0).sum()\n",
    "    top_logprob_difference, top_tokens = haystack_utils.top_k_with_exclude(summed_neuron_diffs, min(non_zero_count, k), all_ignore, largest=positive)\n",
    "    return top_logprob_difference, top_tokens\n",
    "\n",
    "def plot_summed_neuron_differences(num_neurons=10, top=True, positive=True):\n",
    "    top_logprob_difference, top_logprob_tokens = summed_neuron_differences(num_neurons=num_neurons, top=top, positive=positive)\n",
    "    xticks = [model.to_str_tokens([i])[0] for i in top_logprob_tokens]\n",
    "    \n",
    "    if positive:\n",
    "        title = \"Summed individual boosts by\"\n",
    "        xlabel = \"Boosted tokens\"\n",
    "    else:\n",
    "        title = \"Summed individual deboosts by\"\n",
    "        xlabel = \"Deboosted tokens\"\n",
    "    if top:\n",
    "        title += f\" top {num_neurons} neurons\"\n",
    "    else:\n",
    "        title += f\" bottom {num_neurons} neurons\"\n",
    "    haystack_utils.line(top_logprob_difference.cpu().numpy(), xlabel=xlabel, ylabel=\"full_logprob - ablated_logprop\", title=title, xticks=xticks, show_legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0c63967f16844c0843fcb9151e7a953",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=10, description='Num Neurons:', max=20, min=1), Dropdown(description='Ne…"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a function to call your function with widget inputs\n",
    "def plot_summed_neurons_widget(num_neurons, top_bottom, pos_neg):\n",
    "    top_bool = True if top_bottom == 'Top' else False\n",
    "    positive_bool = True if pos_neg == 'Positive' else False\n",
    "    plot_summed_neuron_differences(num_neurons=num_neurons, top=top_bool, positive=positive_bool)\n",
    "\n",
    "# Define widgets\n",
    "num_neuron_slider = widgets.IntSlider(min=1, max=20, step=1, value=10, description='Num Neurons:')\n",
    "top_bottom_dropdown = widgets.Dropdown(options=['Top', 'Bottom'], value='Top', description='Neurons:')\n",
    "pos_neg_dropdown = widgets.Dropdown(options=['Positive', 'Negative'], value='Positive', description='Pos/Neg:')\n",
    "\n",
    "# Use interactive function to bind the widgets to the plotting function\n",
    "widgets.interactive(plot_summed_neurons_widget, num_neurons=num_neuron_slider, top_bottom=top_bottom_dropdown, pos_neg=pos_neg_dropdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss is logprob(correct_token_logit) = logit(correct_token_logit) - LogSumExp(all_token_logits)\n",
    "Loss = -logprob\n",
    "\n",
    "LogSumExp approximates a maximum function. If the neuron engages in destructive interference of a high logit for a non-answer token, then the exp(logit) for the token will be lower and so the LogSumExp will be more similar to logit(correct_token_logit) so the loss will be lower. So a lower logsumexp(all vocab) is good.\n",
    "\n",
    "If the neuron engages in destructive interference of a low logit for a non-answer token, then the exp(logit) for the the token won't really change and so the logsumexp and the loss will both be the same.\n",
    "\n",
    "For a single neuron:\n",
    "1. For each \"gen\" prompt, zero centre the logits and record the logit of \"gen\". Calculate the mean \"gen\" logit.\n",
    "2. For each \"gen\" prompt, disable the neuron under test, zero centre the logits, edit the logit of \"gen\" to the mean with a hook, then get the logprob of \"gen\" and calculate logsumexp(all vocab) = logit(gen) - logprob(gen).\n",
    "2. For each \"gen\" prompt, enable the neuron, zero centre the logits, edit the logit of \"gen\" to the mean with a hook, then get the logprob of \"gen\" and calculate logsumexp(all vocab) = logit(gen) - logprob(gen).\n",
    "3. Take the difference in logsum exps. If it's positive, the neuron is reducing the loss via destructive interference by the difference.\n",
    "Can use the same procedure for sets of neurons, or for all neurons, to find high level effects of the context neuron\n",
    "\n",
    "Logprobs are logits with a constant subtracted and the constant is the same for every logit within a prompt.\n",
    "\n",
    "Taking the difference in terms with and without a neuron's effect via the context neuron:\n",
    "- If log sum exp increases, the neuron is boosting tokens on average. \n",
    "- If logit increases, the neuron is boosting the correct token\n",
    "\n",
    "\n",
    "Remove the neuron's effects on the gen logit. Take the mean on the prompt and position?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(38.6459, device='cuda:0')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Calculate the mean \"gen\" logit.\n",
    "gen_index = model.to_single_token('icht')\n",
    "gen_logits = []\n",
    "logits = model(prompts, return_type='logits') # batch pos vocab\n",
    "logits = logits - logits.mean(-1).unsqueeze(-1) # batch pos vocab, batch pos 1\n",
    "\n",
    "mean_gen_logit = logits[:, -2, gen_index].mean(0)\n",
    "mean_gen_logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import numpy as np\n",
    "# from transformer_lens import utils\n",
    "# \n",
    "# px.histogram(np.random.choice(logits[:, -1, gen_index].flatten().cpu().numpy(), 1000), nbins=100)\n",
    "# utils.test_prompt(\"\".join(model.to_str_tokens(prompts[0, :-1])), \"gen\", model, prepend_space_to_answer=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constructive interference diff, destructive interference diff\n",
      "0.053818 0.069388\n",
      "0.157467 0.031159\n",
      "0.273463 -0.016081\n",
      "0.163135 0.037566\n",
      "0.089304 0.073434\n",
      "0.092345 0.071882\n",
      "-0.005005 0.118236\n",
      "0.256493 0.025861\n",
      "-0.076165 0.150736\n",
      "0.625586 -0.014790\n",
      "1.725928 0.645747\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>                            <div id=\"73d8168f-eacd-4d9f-8331-b26115503703\" class=\"plotly-graph-div\" style=\"height:525px; width:1000px;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"73d8168f-eacd-4d9f-8331-b26115503703\")) {                    Plotly.newPlot(                        \"73d8168f-eacd-4d9f-8331-b26115503703\",                        [{\"error_y\":{\"array\":[0.0],\"type\":\"data\",\"visible\":true},\"name\":\"Neuron 0\",\"x\":[\"Neuron 0\"],\"y\":[0.06938766688108444],\"type\":\"bar\"},{\"error_y\":{\"array\":[0.0],\"type\":\"data\",\"visible\":true},\"name\":\"Neuron 1\",\"x\":[\"Neuron 1\"],\"y\":[0.03115897998213768],\"type\":\"bar\"},{\"error_y\":{\"array\":[0.0],\"type\":\"data\",\"visible\":true},\"name\":\"Neuron 2\",\"x\":[\"Neuron 2\"],\"y\":[-0.016080817207694054],\"type\":\"bar\"},{\"error_y\":{\"array\":[0.0],\"type\":\"data\",\"visible\":true},\"name\":\"Neuron 3\",\"x\":[\"Neuron 3\"],\"y\":[0.037565842270851135],\"type\":\"bar\"},{\"error_y\":{\"array\":[0.0],\"type\":\"data\",\"visible\":true},\"name\":\"Neuron 4\",\"x\":[\"Neuron 4\"],\"y\":[0.07343383878469467],\"type\":\"bar\"},{\"error_y\":{\"array\":[0.0],\"type\":\"data\",\"visible\":true},\"name\":\"Neuron 5\",\"x\":[\"Neuron 5\"],\"y\":[0.07188212871551514],\"type\":\"bar\"},{\"error_y\":{\"array\":[0.0],\"type\":\"data\",\"visible\":true},\"name\":\"Neuron 6\",\"x\":[\"Neuron 6\"],\"y\":[0.11823642253875732],\"type\":\"bar\"},{\"error_y\":{\"array\":[0.0],\"type\":\"data\",\"visible\":true},\"name\":\"Neuron 7\",\"x\":[\"Neuron 7\"],\"y\":[0.025861242786049843],\"type\":\"bar\"},{\"error_y\":{\"array\":[0.0],\"type\":\"data\",\"visible\":true},\"name\":\"Neuron 8\",\"x\":[\"Neuron 8\"],\"y\":[0.150735542178154],\"type\":\"bar\"},{\"error_y\":{\"array\":[0.0],\"type\":\"data\",\"visible\":true},\"name\":\"Neuron 9\",\"x\":[\"Neuron 9\"],\"y\":[-0.014789809472858906],\"type\":\"bar\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"title\":{\"text\":\"LogSumExp reduction from ablating top neurons\"},\"xaxis\":{\"title\":{\"text\":\"\"}},\"yaxis\":{\"title\":{\"text\":\"LogSumExp reduction\"}},\"barmode\":\"group\",\"width\":1000,\"showlegend\":true},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('73d8168f-eacd-4d9f-8331-b26115503703');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def decompose_interference_diff(ablated_cache, disabled_neurons=[top_neurons[0]], mean=True\n",
    "                                ) -> tuple[float, float] | tuple[Float[Tensor, \"n_prompts\"], Float[Tensor, \"n_prompts\"]]:\n",
    "    '''\n",
    "    Finds the effect of the German context neuron ablation via a given set of MLP5 neurons on the logit of each final \n",
    "    token. Decomposes it into constructive and destructive interference. A positive constructive interference difference\n",
    "    means the neurons boost the logit of the correct token, a positive destructive interference difference means the\n",
    "    LogSumExp is getting closer to the correct token logit because neurons deboost the logits of the higher probability \n",
    "    incorrect tokens.\n",
    "\n",
    "    Loss = -logprob(correct_token_logit)\n",
    "    logprob(correct_token_logit) = correct_token_logit - LogSumExp(all_token_logits)\n",
    "\n",
    "    LogSumExp is a smooth maximum function, so it approximates the max of all logits. A neuron that destructively \n",
    "    interfers with a non-answer token with a high logit lowers the LogSumExp(all_token_logits) and thus the loss.'''\n",
    "    ablate_top_neuron_hook = get_ablate_neurons_hook(disabled_neurons, ablated_cache)\n",
    "\n",
    "    _, _, _, mlp5_enabled_logits = haystack_utils.get_direct_effect(prompts, model, pos=-2, \n",
    "                                                                    context_ablation_hooks=deactivate_neurons_fwd_hooks, \n",
    "                                                                    context_activation_hooks=activate_neurons_fwd_hooks, return_type='logits')\n",
    "    _, _, _, mlp5_enabled_top_neuron_ablated_logits = haystack_utils.get_direct_effect(prompts, model, pos=-2, \n",
    "                                                                                       context_ablation_hooks=deactivate_neurons_fwd_hooks, \n",
    "                                                                                       context_activation_hooks=activate_neurons_fwd_hooks+ablate_top_neuron_hook, return_type='logits')\n",
    "    \n",
    "    # Mean center logits to avoid picking up on constant boosts / deboosts\n",
    "    mlp5_enabled_logits = mlp5_enabled_logits - mlp5_enabled_logits.mean(-1).unsqueeze(-1)\n",
    "    mlp5_enabled_top_neuron_ablated_logits = mlp5_enabled_top_neuron_ablated_logits - mlp5_enabled_top_neuron_ablated_logits.mean(-1).unsqueeze(-1)\n",
    "    \n",
    "    # 1. Constructive interference difference\n",
    "    # This is the change in the correct answer token logit from ablating the neuron, positive is good\n",
    "    constructive_interference_diffs = mlp5_enabled_logits[:, gen_index] - mlp5_enabled_top_neuron_ablated_logits[:, gen_index]\n",
    "\n",
    "    # 2. Destructive interference difference\n",
    "    # This is the change in the LogSumExp of all logits from not ablating the neuron positive/increase in LogSumExp is bad \n",
    "    # even though it means that all tokens get deboosted more by a constant amount because the LogSumExp is guaranteed to be\n",
    "    # >= the correct token logit\n",
    "\n",
    "    # Set the gen logit to its mean value so the neuron's constructive interference doesn't affect the LogSumExp difference\n",
    "    mlp5_enabled_logits[:, gen_index] = mean_gen_logit\n",
    "    mlp5_enabled_top_neuron_ablated_logits[:, gen_index] = mean_gen_logit\n",
    "    \n",
    "    # Compute logsumexp\n",
    "    mlp5_enabled_log_sum_exp = mlp5_enabled_logits.exp().sum(-1).log()\n",
    "    mlp5_enabled_top_neuron_ablated_log_sum_exp = mlp5_enabled_top_neuron_ablated_logits.exp().sum(-1).log()\n",
    "\n",
    "    # Check for errors\n",
    "    assert torch.allclose(mlp5_enabled_log_sum_exp, mlp5_enabled_logits[:, gen_index] - mlp5_enabled_logits.log_softmax(-1)[:, gen_index])\n",
    "    assert torch.allclose(mlp5_enabled_top_neuron_ablated_log_sum_exp, mlp5_enabled_top_neuron_ablated_logits[:, gen_index] - mlp5_enabled_top_neuron_ablated_logits.log_softmax(-1)[:, gen_index])\n",
    "\n",
    "    # Difference in logsumexp\n",
    "    # Logsumexp of enabled should be higher than ablated if the neuron does something good\n",
    "    # Negative results are good - they mean that all tokens are deboosted more when the neuron is active\n",
    "    destructive_interference_diffs = mlp5_enabled_log_sum_exp - mlp5_enabled_top_neuron_ablated_log_sum_exp\n",
    "    # Convert the results so positive is good\n",
    "    destructive_interference_diffs *= -1\n",
    "\n",
    "    if mean:\n",
    "        return constructive_interference_diffs.mean().item(), destructive_interference_diffs.mean().item(), \n",
    "    return constructive_interference_diffs, destructive_interference_diffs,\n",
    "\n",
    "\n",
    "destructive_diffs = []\n",
    "# Calculate neuron-wise loss change\n",
    "print(\"constructive interference diff, destructive interference diff\")\n",
    "for i in range(10):\n",
    "    constructive_diff, destructive_diff = decompose_interference_diff(ablated_cache, disabled_neurons=[top_neurons[i]], mean=True)\n",
    "    print(f'{constructive_diff:2f}', f'{destructive_diff:2f}')\n",
    "    destructive_diffs.append(destructive_diff)\n",
    "\n",
    "constructive_diff, destructive_diff = decompose_interference_diff(ablated_cache, top_neurons)\n",
    "print(f'{constructive_diff:2f}', f'{destructive_diff:2f}')\n",
    "\n",
    "# line plot of logsumexp\n",
    "haystack_utils.plot_barplot([[item] for item in destructive_diffs], names=[f\"Neuron {i}\" for i in range(10)], ylabel=\"LogSumExp reduction\", title=\"LogSumExp reduction from ablating top neurons\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neuron -> Token Logit Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.0346, -0.0237, -0.0347,  0.0397,  0.0595,  0.0501, -0.0274,  0.1405,\n",
      "         0.0417, -0.0392], device='cuda:0')\n",
      "tensor([0.1634, 0.1461, 0.1405, 0.1399, 0.1349, 0.1207, 0.1196, 0.1161, 0.1156,\n",
      "        0.1139, 0.1130, 0.1128, 0.1120, 0.1104, 0.1096, 0.1070, 0.1056, 0.1048,\n",
      "        0.1042, 0.1028], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Our top neurons are selected by the difference in their boost in gen based on the context neuron\n",
    "# Many other neurons boost gen more per unit of activation\n",
    "\n",
    "cosine_sim = torch.nn.CosineSimilarity(dim=1)\n",
    "answer_residual_direction = model.tokens_to_residual_directions(\"gen\")\n",
    "neuron_weights = model.state_dict()['blocks.5.mlp.W_out'][top_neurons]\n",
    "\n",
    "cosine_sims = cosine_sim(neuron_weights, answer_residual_direction.unsqueeze(0))\n",
    "print(cosine_sims)\n",
    "\n",
    "cosine_sim = torch.nn.CosineSimilarity(dim=1)\n",
    "answer_residual_direction = model.tokens_to_residual_directions(\"gen\")\n",
    "neuron_weights = model.state_dict()['blocks.5.mlp.W_out']\n",
    "\n",
    "cosine_sims = cosine_sim(neuron_weights, answer_residual_direction.unsqueeze(0))\n",
    "top, indices = torch.topk(cosine_sims, 20)\n",
    "print(top)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81effddac5574c43bdc0733f53753924",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5f5993eb227471fb553cac35f9f6c71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "648dddd9dff54e3bb667818a292ccacc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "haystack_utils.clean_cache()\n",
    "english_activations_l5 = {}\n",
    "german_activations_l5 = {}\n",
    "all_activations_l5 = {}\n",
    "for layer in range(5, 6):\n",
    "    english_activations_l5 = get_mlp_activations(english_data, layer, model, mean=True)\n",
    "    german_activations_l5 = get_mlp_activations(german_data, layer, model, mean=True)\n",
    "    all_activations_l5 = get_mlp_activations(german_data[:50] + english_data[:150], layer, model, mean=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gen dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa2b2f1910484b85b0e8786e962150b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cafb6c03cb54244870836378d690729",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "552a4ada6254437e9f1b80378959bef0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8498995f4de349178dbc31c402918c0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6c44596fff94c3c9c440a3a482a769c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import einops\n",
    "\n",
    "W_out = model.state_dict()['blocks.5.mlp.W_out']\n",
    "W_U = model.W_U\n",
    "avg_W_out_all = einops.einsum(all_activations_l5, W_out, 'd_mlp, d_mlp d_model -> d_mlp d_model')\n",
    "avg_mlp5_boosts_all = einops.einsum(avg_W_out_all, W_U, 'd_mlp d_model, d_model d_vocab -> d_mlp d_vocab')\n",
    "avg_W_out_enabled = einops.einsum(german_activations_l5, W_out, 'd_mlp, d_mlp d_model -> d_mlp d_model')\n",
    "avg_mlp5_boosts_enabled = einops.einsum(avg_W_out_enabled, W_U, 'd_mlp d_model, d_model d_vocab -> d_mlp d_vocab')\n",
    "avg_W_out_disabled = einops.einsum(english_activations_l5, W_out, 'd_mlp, d_mlp d_model -> d_mlp d_model')\n",
    "avg_mlp5_boosts_disabled = einops.einsum(avg_W_out_disabled, W_U, 'd_mlp d_model, d_model d_vocab -> d_mlp d_vocab')\n",
    "\n",
    "prompt_strs = [model.tokenizer.decode(prompts[i].tolist()) for i in range(prompts.shape[0])]\n",
    "\n",
    "gen_acts_l5_all = get_mlp_activations(prompt_strs, 5, model, mean=True, pos=-2)\n",
    "with model.hooks(deactivate_neurons_fwd_hooks):\n",
    "    gen_acts_l5_disabled = get_mlp_activations(prompt_strs, 5, model, mean=True, pos=-2)\n",
    "with model.hooks(activate_neurons_fwd_hooks):\n",
    "    gen_acts_l5_enabled = get_mlp_activations(prompt_strs, 5, model, mean=True, pos=-2)\n",
    "\n",
    "with model.hooks(deactivate_neurons_fwd_hooks):\n",
    "    all_gen_acts_l5_disabled = get_mlp_activations(prompt_strs, 5, model, mean=False, pos=-2)\n",
    "with model.hooks(activate_neurons_fwd_hooks):\n",
    "    all_gen_acts_l5_enabled = get_mlp_activations(prompt_strs, 5, model, mean=False, pos=-2)\n",
    "\n",
    "data = {\n",
    "    'firing count diff': ((all_gen_acts_l5_enabled > 0).sum(0) - (all_gen_acts_l5_disabled > 0).sum(0)).tolist(),\n",
    "    # 'neuron index': list(range(2048)),\n",
    "    'cos sim gen': cosine_sims.tolist(),\n",
    "    # 'avg act (enabled)': german_activations_l5.tolist(),\n",
    "    # 'avg act (disabled)': english_activations_l5.tolist(),\n",
    "    'avg act increase enabled': (german_activations_l5 - english_activations_l5).tolist(),\n",
    "    'avg act': all_activations_l5.tolist(),\n",
    "    'avg gen act': gen_acts_l5_all.tolist(),\n",
    "    'avg gen act enabled': gen_acts_l5_enabled.tolist(),\n",
    "    'avg gen act disabled': gen_acts_l5_disabled.tolist(),\n",
    "    # 'avg boost gen': avg_mlp5_boosts_all[:, gen_index].tolist(),\n",
    "    # 'avg boost enabled gen': avg_mlp5_boosts_enabled[:, gen_index].tolist(),\n",
    "    # 'avg boost disabled gen': avg_mlp5_boosts_disabled[:, gen_index].tolist(),\n",
    "    'enabled firing count': (all_gen_acts_l5_enabled > 0).sum(0).tolist(),\n",
    "    'disabled firing count': (all_gen_acts_l5_disabled > 0).sum(0).tolist(),\n",
    "    'firing count': (all_gen_acts_l5_disabled.bool().sum(0).tolist())\n",
    "    \n",
    "    # 'Loss change when ablated': \n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "# Line plot of each neuron's gen boost with dotted vertical lines where the German context neuron-boosted ones are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' and', ' creating', ' the', ' Sc', 'hen', 'gen', ' area', '.', ' From', ' 2012']\n",
      "[' Application', ' of', ' the', ' Sc', 'hen', 'gen', ' acquis', ' relating', ' to', ' the']\n",
      "[' Bulgaria', ' to', ' the', ' Sc', 'hen', 'gen', ' Area', ',', ' nor', ' the']\n",
      "[' can', ' enter', ' the', ' Sc', 'hen', 'gen', ' area', ' without', ' having', ' to']\n",
      "[' to', ' restrict', ' the', ' Sc', 'hen', 'gen', ' area', ',', ' compatible', ' with']\n"
     ]
    }
   ],
   "source": [
    "for prompt in english_data:\n",
    "    prompt_tokens = model.to_tokens(prompt)\n",
    "    if gen_index in prompt_tokens:\n",
    "        index = prompt_tokens[0].tolist().index(gen_index)\n",
    "        print(model.to_str_tokens(prompt_tokens[0, index-5:index+5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1107/2048 MLP5 neurons generically increase \"gen\", 54.05%\n",
      "461 of these fire more when context neuron enabled, 22.51%\n",
      "646 of these fire less when context neuron enabled, 31.54%\n",
      "277 of these fire on avg when context neuron enabled, 13.53%\n",
      "250 of these fire on avg when context neuron disabled, 12.21%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>firing count diff</th>\n",
       "      <th>cos sim gen</th>\n",
       "      <th>avg act increase enabled</th>\n",
       "      <th>avg act</th>\n",
       "      <th>avg gen act</th>\n",
       "      <th>avg gen act enabled</th>\n",
       "      <th>avg gen act disabled</th>\n",
       "      <th>enabled firing count</th>\n",
       "      <th>disabled firing count</th>\n",
       "      <th>firing count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>93</td>\n",
       "      <td>0.057594</td>\n",
       "      <td>0.739129</td>\n",
       "      <td>0.160572</td>\n",
       "      <td>0.251590</td>\n",
       "      <td>0.151860</td>\n",
       "      <td>-0.124767</td>\n",
       "      <td>94</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1927</th>\n",
       "      <td>92</td>\n",
       "      <td>0.025486</td>\n",
       "      <td>0.347759</td>\n",
       "      <td>0.020567</td>\n",
       "      <td>0.212794</td>\n",
       "      <td>0.149408</td>\n",
       "      <td>-0.075781</td>\n",
       "      <td>96</td>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2038</th>\n",
       "      <td>91</td>\n",
       "      <td>0.003579</td>\n",
       "      <td>0.793564</td>\n",
       "      <td>0.159707</td>\n",
       "      <td>0.416826</td>\n",
       "      <td>0.289465</td>\n",
       "      <td>-0.061763</td>\n",
       "      <td>100</td>\n",
       "      <td>9</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1667</th>\n",
       "      <td>90</td>\n",
       "      <td>0.038450</td>\n",
       "      <td>0.321733</td>\n",
       "      <td>0.052155</td>\n",
       "      <td>0.176787</td>\n",
       "      <td>0.096939</td>\n",
       "      <td>-0.121565</td>\n",
       "      <td>90</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1716</th>\n",
       "      <td>88</td>\n",
       "      <td>0.146135</td>\n",
       "      <td>1.097669</td>\n",
       "      <td>0.571957</td>\n",
       "      <td>0.511899</td>\n",
       "      <td>0.311726</td>\n",
       "      <td>-0.100262</td>\n",
       "      <td>98</td>\n",
       "      <td>10</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>987</th>\n",
       "      <td>88</td>\n",
       "      <td>0.005353</td>\n",
       "      <td>0.352996</td>\n",
       "      <td>0.051033</td>\n",
       "      <td>0.144142</td>\n",
       "      <td>0.081435</td>\n",
       "      <td>-0.109428</td>\n",
       "      <td>89</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>87</td>\n",
       "      <td>0.041687</td>\n",
       "      <td>0.345945</td>\n",
       "      <td>-0.001036</td>\n",
       "      <td>0.625658</td>\n",
       "      <td>0.441772</td>\n",
       "      <td>-0.063952</td>\n",
       "      <td>100</td>\n",
       "      <td>13</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358</th>\n",
       "      <td>87</td>\n",
       "      <td>0.008868</td>\n",
       "      <td>0.307440</td>\n",
       "      <td>0.041471</td>\n",
       "      <td>0.372113</td>\n",
       "      <td>0.258444</td>\n",
       "      <td>-0.056477</td>\n",
       "      <td>100</td>\n",
       "      <td>13</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1877</th>\n",
       "      <td>87</td>\n",
       "      <td>0.030004</td>\n",
       "      <td>0.049302</td>\n",
       "      <td>0.190596</td>\n",
       "      <td>0.420401</td>\n",
       "      <td>0.318768</td>\n",
       "      <td>-0.066352</td>\n",
       "      <td>100</td>\n",
       "      <td>13</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1747</th>\n",
       "      <td>86</td>\n",
       "      <td>0.053287</td>\n",
       "      <td>0.200168</td>\n",
       "      <td>-0.063503</td>\n",
       "      <td>0.163022</td>\n",
       "      <td>0.089683</td>\n",
       "      <td>-0.109547</td>\n",
       "      <td>89</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1869</th>\n",
       "      <td>85</td>\n",
       "      <td>0.018846</td>\n",
       "      <td>-0.069149</td>\n",
       "      <td>0.087601</td>\n",
       "      <td>0.193943</td>\n",
       "      <td>0.126662</td>\n",
       "      <td>-0.046839</td>\n",
       "      <td>97</td>\n",
       "      <td>12</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>819</th>\n",
       "      <td>83</td>\n",
       "      <td>0.011484</td>\n",
       "      <td>-0.025496</td>\n",
       "      <td>-0.051579</td>\n",
       "      <td>0.220504</td>\n",
       "      <td>0.132783</td>\n",
       "      <td>-0.064537</td>\n",
       "      <td>93</td>\n",
       "      <td>10</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>898</th>\n",
       "      <td>83</td>\n",
       "      <td>0.011997</td>\n",
       "      <td>0.234033</td>\n",
       "      <td>-0.033518</td>\n",
       "      <td>0.241521</td>\n",
       "      <td>0.147876</td>\n",
       "      <td>-0.070099</td>\n",
       "      <td>93</td>\n",
       "      <td>10</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1227</th>\n",
       "      <td>77</td>\n",
       "      <td>0.073344</td>\n",
       "      <td>0.133045</td>\n",
       "      <td>-0.022945</td>\n",
       "      <td>0.117388</td>\n",
       "      <td>0.053527</td>\n",
       "      <td>-0.119858</td>\n",
       "      <td>77</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1490</th>\n",
       "      <td>77</td>\n",
       "      <td>0.028981</td>\n",
       "      <td>0.277170</td>\n",
       "      <td>-0.010279</td>\n",
       "      <td>0.263554</td>\n",
       "      <td>0.180924</td>\n",
       "      <td>-0.049311</td>\n",
       "      <td>95</td>\n",
       "      <td>18</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      firing count diff  cos sim gen  avg act increase enabled   avg act  \\\n",
       "13                   93     0.057594                  0.739129  0.160572   \n",
       "1927                 92     0.025486                  0.347759  0.020567   \n",
       "2038                 91     0.003579                  0.793564  0.159707   \n",
       "1667                 90     0.038450                  0.321733  0.052155   \n",
       "1716                 88     0.146135                  1.097669  0.571957   \n",
       "987                  88     0.005353                  0.352996  0.051033   \n",
       "84                   87     0.041687                  0.345945 -0.001036   \n",
       "358                  87     0.008868                  0.307440  0.041471   \n",
       "1877                 87     0.030004                  0.049302  0.190596   \n",
       "1747                 86     0.053287                  0.200168 -0.063503   \n",
       "1869                 85     0.018846                 -0.069149  0.087601   \n",
       "819                  83     0.011484                 -0.025496 -0.051579   \n",
       "898                  83     0.011997                  0.234033 -0.033518   \n",
       "1227                 77     0.073344                  0.133045 -0.022945   \n",
       "1490                 77     0.028981                  0.277170 -0.010279   \n",
       "\n",
       "      avg gen act  avg gen act enabled  avg gen act disabled  \\\n",
       "13       0.251590             0.151860             -0.124767   \n",
       "1927     0.212794             0.149408             -0.075781   \n",
       "2038     0.416826             0.289465             -0.061763   \n",
       "1667     0.176787             0.096939             -0.121565   \n",
       "1716     0.511899             0.311726             -0.100262   \n",
       "987      0.144142             0.081435             -0.109428   \n",
       "84       0.625658             0.441772             -0.063952   \n",
       "358      0.372113             0.258444             -0.056477   \n",
       "1877     0.420401             0.318768             -0.066352   \n",
       "1747     0.163022             0.089683             -0.109547   \n",
       "1869     0.193943             0.126662             -0.046839   \n",
       "819      0.220504             0.132783             -0.064537   \n",
       "898      0.241521             0.147876             -0.070099   \n",
       "1227     0.117388             0.053527             -0.119858   \n",
       "1490     0.263554             0.180924             -0.049311   \n",
       "\n",
       "      enabled firing count  disabled firing count  firing count  \n",
       "13                      94                      1           100  \n",
       "1927                    96                      4           100  \n",
       "2038                   100                      9           100  \n",
       "1667                    90                      0           100  \n",
       "1716                    98                     10           100  \n",
       "987                     89                      1           100  \n",
       "84                     100                     13           100  \n",
       "358                    100                     13           100  \n",
       "1877                   100                     13           100  \n",
       "1747                    89                      3           100  \n",
       "1869                    97                     12           100  \n",
       "819                     93                     10           100  \n",
       "898                     93                     10           100  \n",
       "1227                    77                      0           100  \n",
       "1490                    95                     18           100  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Weight analysis\n",
    "gen_df = df[(df['cos sim gen'] > 0)]\n",
    "print(f\"{len(gen_df)}/{len(df)} MLP5 neurons generically increase \\\"gen\\\", {len(gen_df)/len(df)*100:.2f}%\")\n",
    "\n",
    "# Activation analysis\n",
    "# All on German prompts\n",
    "filtered_df = gen_df[(gen_df['avg act increase enabled'] > 0)]\n",
    "print(f\"{len(filtered_df)} of these fire more when context neuron enabled, {len(filtered_df)/len(df)*100:.2f}%\")\n",
    "filtered_df = gen_df[(gen_df['avg act increase enabled'] < 0)]\n",
    "print(f\"{len(filtered_df)} of these fire less when context neuron enabled, {len(filtered_df)/len(df)*100:.2f}%\")\n",
    "\n",
    "filtered_df = gen_df[(gen_df['avg gen act enabled'] > 0)]\n",
    "print(f\"{len(filtered_df)} of these fire on avg when context neuron enabled, {len(filtered_df)/len(df)*100:.2f}%\")\n",
    "filtered_df = gen_df[(gen_df['avg gen act disabled'] > 0)]\n",
    "print(f\"{len(filtered_df)} of these fire on avg when context neuron disabled, {len(filtered_df)/len(df)*100:.2f}%\")\n",
    "\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "sorted = gen_df.sort_values(by=['firing count diff'], ascending=False)\n",
    "\n",
    "sorted.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interested in how often the context neuron makes the difference between firing and not, rather than firing on average.\n",
    "Neurons with top difference between number of fires with neuron enabled vs. disabled\n",
    "x% more likely to fire\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
