{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from transformer_lens import HookedTransformer, ActivationCache, utils, patching\n",
    "from jaxtyping import Float, Int, Bool\n",
    "from torch import Tensor\n",
    "from tqdm.auto import tqdm\n",
    "import plotly.io as pio\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "pio.renderers.default = \"notebook_connected\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.autograd.set_grad_enabled(False)\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "from haystack_utils import get_mlp_activations\n",
    "import haystack_utils\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model EleutherAI/pythia-70m into HookedTransformer\n",
      "data/german_europarl.json: Loaded 2000 examples with 152 to 2000 characters each.\n",
      "data/english_europarl.json: Loaded 2000 examples with 165 to 2000 characters each.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de4b85742bbe48198bc0728f1bc21517",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7efaeffacbca433895ac312e3f0400fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = HookedTransformer.from_pretrained(\"EleutherAI/pythia-70m\",\n",
    "    center_unembed=True,\n",
    "    center_writing_weights=True,\n",
    "    fold_ln=True,\n",
    "    device=device)\n",
    "\n",
    "german_data = haystack_utils.load_json_data(\"data/german_europarl.json\")[:200]\n",
    "english_data = haystack_utils.load_json_data(\"data/english_europarl.json\")[:200]\n",
    "\n",
    "\n",
    "english_activations = {}\n",
    "german_activations = {}\n",
    "for layer in range(3, 4):\n",
    "    english_activations[layer] = get_mlp_activations(english_data, layer, model, mean=False)\n",
    "    german_activations[layer] = get_mlp_activations(german_data, layer, model, mean=False)\n",
    "\n",
    "LAYER_TO_ABLATE = 3\n",
    "NEURONS_TO_ABLATE = [669]\n",
    "MEAN_ACTIVATION_ACTIVE = german_activations[LAYER_TO_ABLATE][:, NEURONS_TO_ABLATE].mean()\n",
    "MEAN_ACTIVATION_INACTIVE = english_activations[LAYER_TO_ABLATE][:, NEURONS_TO_ABLATE].mean()\n",
    "\n",
    "def deactivate_neurons_hook(value, hook):\n",
    "    value[:, :, NEURONS_TO_ABLATE] = MEAN_ACTIVATION_INACTIVE\n",
    "    return value\n",
    "deactivate_neurons_fwd_hooks=[(f'blocks.{LAYER_TO_ABLATE}.mlp.hook_post', deactivate_neurons_hook)]\n",
    "\n",
    "def activate_neurons_hook(value, hook):\n",
    "    value[:, :, NEURONS_TO_ABLATE] = MEAN_ACTIVATION_ACTIVE\n",
    "    return value\n",
    "activate_neurons_fwd_hooks=[(f'blocks.{LAYER_TO_ABLATE}.mlp.hook_post', activate_neurons_hook)]\n",
    "\n",
    "all_ignore, not_ignore = haystack_utils.get_weird_tokens(model, plot_norms=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find top common German tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7da11e396b84b9692dfc229d7abdfaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' der', 'en', ' die', ' und', 'ung', 'ä', ' in', ' den', ' des', ' zu', 'ch', 'n', 'st', 're', 'z', ' von', ' für', 'äsident', ' Pr', 'ischen', 't', 'ü', 'icht', 'gen', ' ist', ' auf', ' dass', 'ge', 'ig', ' im', 'in', ' über', 'g', ' das', 'te', ' er', 'men', ' w', 'es', ' an', 'ß', ' wir', ' eine', 'f', ' W', 'hen', 'w', ' Europ', ' ich', 'ungen', 'ren', 'le', ' dem', 'ten', ' ein', 'e', ' Z', ' Ver', 'der', ' B', ' mit', ' dies', 'h', ' nicht', 'ungs', 's', ' G', ' z', 'it', ' Herr', ' es', 'l', ' S', 'ich', 'lich', ' An', 'heit', 'ie', ' Er', ' zur', ' V', ' ver', 'u', 'hr', 'chaft', 'Der', ' Ich', ' Ab', ' haben', 'i', 'ant', 'chte', ' mö', 'er', ' K', 'igen', ' Ber', 'ür', ' Fra', 'em']\n"
     ]
    }
   ],
   "source": [
    "# Get top common german tokens excluding punctuation\n",
    "token_counts = torch.zeros(model.cfg.d_vocab).cuda()\n",
    "for example in tqdm(german_data):\n",
    "    tokens = model.to_tokens(example)\n",
    "    for token in tokens[0]:\n",
    "        token_counts[token.item()] += 1\n",
    "\n",
    "punctuation = [\"\\n\", \".\", \",\", \"!\", \"?\", \";\", \":\", \"-\", \"(\", \")\", \"[\", \"]\", \"{\", \"}\", \"<\", \">\", \"/\", \"\\\\\", \"\\\"\", \"'\"]\n",
    "leading_space_punctuation = [\" \" + char for char in punctuation]\n",
    "punctuation_tokens = model.to_tokens(punctuation + leading_space_punctuation + [' –', \" \", '  ', \"<|endoftext|>\"])[:, 1].flatten()\n",
    "token_counts[punctuation_tokens] = 0\n",
    "token_counts[all_ignore] = 0\n",
    "\n",
    "top_counts, top_tokens = torch.topk(token_counts, 100)\n",
    "print(model.to_str_tokens(top_tokens[:100]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of ngrams preceded by random prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_selection(tensor, n=12):\n",
    "    # Hacky replacement for np.random.choice\n",
    "    return tensor[torch.randperm(len(tensor))[:n]]\n",
    "\n",
    "def generate_random_prompts(end_string, n=50, length=12):\n",
    "    # Generate a batch of random prompts ending with a specific ngram\n",
    "    end_tokens = model.to_tokens(end_string).flatten()[1:]\n",
    "    prompts = []\n",
    "    for i in range(n):\n",
    "        prompt = get_random_selection(top_tokens[:max(50, length)], n=length).cuda()\n",
    "        prompt = torch.cat([prompt, end_tokens])\n",
    "        prompts.append(prompt)\n",
    "    prompts = torch.stack(prompts)\n",
    "    return prompts\n",
    "\n",
    "def replace_column(prompts: Int[Tensor, \"n_prompts n_tokens\"], token_index: int):\n",
    "    # Replaces a specific token position in a batch of prompts with random common German tokens\n",
    "    new_prompts = prompts.clone()\n",
    "    random_tokens = get_random_selection(top_tokens[:max(50, prompts.shape[0])], n=prompts.shape[0]).cuda()\n",
    "    new_prompts[:, token_index] = random_tokens\n",
    "    return new_prompts \n",
    "\n",
    "def loss_analysis(prompts: Tensor, title=\"\"):\n",
    "    # Loss plot for a batch of prompts\n",
    "    names = [\"Original\", \"Ablated\", \"MLP5 path patched\"]\n",
    "    original_loss, ablated_loss, _, only_activated_loss = \\\n",
    "        haystack_utils.get_direct_effect(prompts, model, pos=-1,\n",
    "                                        context_ablation_hooks=deactivate_neurons_fwd_hooks, \n",
    "                                        context_activation_hooks=activate_neurons_fwd_hooks, \n",
    "                                        )\n",
    "    haystack_utils.plot_barplot([original_loss.tolist(), ablated_loss.tolist(), only_activated_loss.tolist()], names, ylabel=\"Loss\", title=title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_analysis_random_prompts(end_string, n=50, length=12, replace_columns: list[int] | None = None):\n",
    "    # Loss plot for a batch of random prompts ending with a specific ngram and optionally replacing specific tokens\n",
    "    prompts = generate_random_prompts(end_string, n=n, length=length)\n",
    "    title=f\"Average last token loss on {length} random tokens ending in '{end_string}'\"\n",
    "    if replace_columns is not None:\n",
    "        replaced_tokens = model.to_str_tokens(prompts[0, replace_columns])\n",
    "        title += f\" replacing {replaced_tokens}\"\n",
    "        for column in replace_columns:\n",
    "            prompts = replace_column(prompts, column)\n",
    "    \n",
    "    loss_analysis(prompts, title=title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "# loss_analysis_random_prompts(\" Vorschlägen\", n=100, length=20)\n",
    "# loss_analysis_random_prompts(\" Vorschlägen\", n=100, length=20, replace_columns=[-2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee64af7363c84a169a6c7b88e2ecceec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Ngram: ', options=(' Vorschlägen', ' Vorschläge', ' häufig', ' schließt', ' beweglich'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c4601f310274ca585d37d79e321806c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SelectMultiple(description='Replace Columns:', index=(3,), options=('-2', '-3', '-4', 'None'), value=('None',)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4da1b9884c4b4ebe8c5e43f000fb3da3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a dropdown menu widget\n",
    "dropdown = widgets.Dropdown(\n",
    "    options = [\" Vorschlägen\", \" Vorschläge\", \" häufig\", \" schließt\", \" beweglich\"], \n",
    "    value = \" Vorschlägen\",\n",
    "    description = 'Ngram: ',\n",
    ")\n",
    "\n",
    "replace_columns_dropdown = widgets.SelectMultiple(\n",
    "    options = ['-2', '-3', '-4', 'None'],\n",
    "    value = ['None'],  # default selected value\n",
    "    description = 'Replace Columns:',\n",
    ")\n",
    "\n",
    "# Create an output widget to hold the plot\n",
    "output = widgets.Output()\n",
    "\n",
    "# Define a function to call when the widget's value changes\n",
    "def update_plot(*args):\n",
    "    # Clear the old plot from the output\n",
    "    with output:\n",
    "        clear_output(wait=True)\n",
    "\n",
    "        if 'None' in replace_columns_dropdown.value:\n",
    "            replace_columns = None\n",
    "        else:\n",
    "            # If 'None' not selected, convert the selected values to integers\n",
    "            replace_columns = [int(val) for val in replace_columns_dropdown.value]\n",
    "        \n",
    "        # Call your function with the values from the widgets\n",
    "        loss_analysis_random_prompts(dropdown.value, n=100, length=20, replace_columns=replace_columns)\n",
    "\n",
    "# Set the function to be called when the widget's value changes\n",
    "dropdown.observe(update_plot, 'value')\n",
    "replace_columns_dropdown.observe(update_plot, 'value')\n",
    "\n",
    "# Display the widget and the output\n",
    "display(dropdown, replace_columns_dropdown, output)\n",
    "\n",
    "# Run once at startup\n",
    "update_plot()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify relevant neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 24])\n"
     ]
    }
   ],
   "source": [
    "prompts = generate_random_prompts(\" Vorschlägen\", n=100, length=20)\n",
    "print(prompts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dd870419ed543b4abb35609447bbdfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2048 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0007)\n"
     ]
    }
   ],
   "source": [
    "with model.hooks(deactivate_neurons_fwd_hooks):\n",
    "    _, ablated_cache = model.run_with_cache(prompts)\n",
    "\n",
    "def get_ablate_neurons_hook(neuron: int | list[int], ablated_cache, layer=5):\n",
    "    def ablate_neurons_hook(value, hook):\n",
    "        value[:, :, neuron] = ablated_cache[f'blocks.{layer}.mlp.hook_post'][:, :, neuron]\n",
    "        return value\n",
    "    return [(f'blocks.{layer}.mlp.hook_post', ablate_neurons_hook)]\n",
    "\n",
    "diffs = torch.zeros(2048, prompts.shape[0])\n",
    "# Loss with path patched MLP5 neurons\n",
    "_, _, _, baseline_loss = haystack_utils.get_direct_effect(prompts, model, pos=-1, context_ablation_hooks=deactivate_neurons_fwd_hooks, context_activation_hooks=activate_neurons_fwd_hooks)\n",
    "for neuron in tqdm(range(2048)):\n",
    "    ablate_single_neuron_hook = get_ablate_neurons_hook(neuron, ablated_cache)\n",
    "    # Loss with path patched MLP5 neurons but a single neuron changed back to original ablated value\n",
    "    _, _, _, only_deactivated_loss = haystack_utils.get_direct_effect(prompts, model, pos=-1, context_ablation_hooks=deactivate_neurons_fwd_hooks, context_activation_hooks=activate_neurons_fwd_hooks+ablate_single_neuron_hook)\n",
    "    diffs[neuron] = only_deactivated_loss - baseline_loss\n",
    "\n",
    "print(diffs.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>                            <div id=\"5ade98a8-a9b1-4140-aa9d-92af4f3dcacb\" class=\"plotly-graph-div\" style=\"height:525px; width:800px;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"5ade98a8-a9b1-4140-aa9d-92af4f3dcacb\")) {                    Plotly.newPlot(                        \"5ade98a8-a9b1-4140-aa9d-92af4f3dcacb\",                        [{\"hovertemplate\":\"variable=0\\u003cbr\\u003eindex=%{x}\\u003cbr\\u003evalue=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"0\",\"line\":{\"color\":\"#636efa\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"0\",\"showlegend\":true,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542,543,544,545,546,547,548,549,550,551,552,553,554,555,556,557,558,559,560,561,562,563,564,565,566,567,568,569,570,571,572,573,574,575,576,577,578,579,580,581,582,583,584,585,586,587,588,589,590,591,592,593,594,595,596,597,598,599,600,601,602,603,604,605,606,607,608,609,610,611,612,613,614,615,616,617,618,619,620,621,622,623,624,625,626,627,628,629,630,631,632,633,634,635,636,637,638,639,640,641,642,643,644,645,646,647,648,649,650,651,652,653,654,655,656,657,658,659,660,661,662,663,664,665,666,667,668,669,670,671,672,673,674,675,676,677,678,679,680,681,682,683,684,685,686,687,688,689,690,691,692,693,694,695,696,697,698,699,700,701,702,703,704,705,706,707,708,709,710,711,712,713,714,715,716,717,718,719,720,721,722,723,724,725,726,727,728,729,730,731,732,733,734,735,736,737,738,739,740,741,742,743,744,745,746,747,748,749,750,751,752,753,754,755,756,757,758,759,760,761,762,763,764,765,766,767,768,769,770,771,772,773,774,775,776,777,778,779,780,781,782,783,784,785,786,787,788,789,790,791,792,793,794,795,796,797,798,799,800,801,802,803,804,805,806,807,808,809,810,811,812,813,814,815,816,817,818,819,820,821,822,823,824,825,826,827,828,829,830,831,832,833,834,835,836,837,838,839,840,841,842,843,844,845,846,847,848,849,850,851,852,853,854,855,856,857,858,859,860,861,862,863,864,865,866,867,868,869,870,871,872,873,874,875,876,877,878,879,880,881,882,883,884,885,886,887,888,889,890,891,892,893,894,895,896,897,898,899,900,901,902,903,904,905,906,907,908,909,910,911,912,913,914,915,916,917,918,919,920,921,922,923,924,925,926,927,928,929,930,931,932,933,934,935,936,937,938,939,940,941,942,943,944,945,946,947,948,949,950,951,952,953,954,955,956,957,958,959,960,961,962,963,964,965,966,967,968,969,970,971,972,973,974,975,976,977,978,979,980,981,982,983,984,985,986,987,988,989,990,991,992,993,994,995,996,997,998,999,1000,1001,1002,1003,1004,1005,1006,1007,1008,1009,1010,1011,1012,1013,1014,1015,1016,1017,1018,1019,1020,1021,1022,1023,1024,1025,1026,1027,1028,1029,1030,1031,1032,1033,1034,1035,1036,1037,1038,1039,1040,1041,1042,1043,1044,1045,1046,1047,1048,1049,1050,1051,1052,1053,1054,1055,1056,1057,1058,1059,1060,1061,1062,1063,1064,1065,1066,1067,1068,1069,1070,1071,1072,1073,1074,1075,1076,1077,1078,1079,1080,1081,1082,1083,1084,1085,1086,1087,1088,1089,1090,1091,1092,1093,1094,1095,1096,1097,1098,1099,1100,1101,1102,1103,1104,1105,1106,1107,1108,1109,1110,1111,1112,1113,1114,1115,1116,1117,1118,1119,1120,1121,1122,1123,1124,1125,1126,1127,1128,1129,1130,1131,1132,1133,1134,1135,1136,1137,1138,1139,1140,1141,1142,1143,1144,1145,1146,1147,1148,1149,1150,1151,1152,1153,1154,1155,1156,1157,1158,1159,1160,1161,1162,1163,1164,1165,1166,1167,1168,1169,1170,1171,1172,1173,1174,1175,1176,1177,1178,1179,1180,1181,1182,1183,1184,1185,1186,1187,1188,1189,1190,1191,1192,1193,1194,1195,1196,1197,1198,1199,1200,1201,1202,1203,1204,1205,1206,1207,1208,1209,1210,1211,1212,1213,1214,1215,1216,1217,1218,1219,1220,1221,1222,1223,1224,1225,1226,1227,1228,1229,1230,1231,1232,1233,1234,1235,1236,1237,1238,1239,1240,1241,1242,1243,1244,1245,1246,1247,1248,1249,1250,1251,1252,1253,1254,1255,1256,1257,1258,1259,1260,1261,1262,1263,1264,1265,1266,1267,1268,1269,1270,1271,1272,1273,1274,1275,1276,1277,1278,1279,1280,1281,1282,1283,1284,1285,1286,1287,1288,1289,1290,1291,1292,1293,1294,1295,1296,1297,1298,1299,1300,1301,1302,1303,1304,1305,1306,1307,1308,1309,1310,1311,1312,1313,1314,1315,1316,1317,1318,1319,1320,1321,1322,1323,1324,1325,1326,1327,1328,1329,1330,1331,1332,1333,1334,1335,1336,1337,1338,1339,1340,1341,1342,1343,1344,1345,1346,1347,1348,1349,1350,1351,1352,1353,1354,1355,1356,1357,1358,1359,1360,1361,1362,1363,1364,1365,1366,1367,1368,1369,1370,1371,1372,1373,1374,1375,1376,1377,1378,1379,1380,1381,1382,1383,1384,1385,1386,1387,1388,1389,1390,1391,1392,1393,1394,1395,1396,1397,1398,1399,1400,1401,1402,1403,1404,1405,1406,1407,1408,1409,1410,1411,1412,1413,1414,1415,1416,1417,1418,1419,1420,1421,1422,1423,1424,1425,1426,1427,1428,1429,1430,1431,1432,1433,1434,1435,1436,1437,1438,1439,1440,1441,1442,1443,1444,1445,1446,1447,1448,1449,1450,1451,1452,1453,1454,1455,1456,1457,1458,1459,1460,1461,1462,1463,1464,1465,1466,1467,1468,1469,1470,1471,1472,1473,1474,1475,1476,1477,1478,1479,1480,1481,1482,1483,1484,1485,1486,1487,1488,1489,1490,1491,1492,1493,1494,1495,1496,1497,1498,1499,1500,1501,1502,1503,1504,1505,1506,1507,1508,1509,1510,1511,1512,1513,1514,1515,1516,1517,1518,1519,1520,1521,1522,1523,1524,1525,1526,1527,1528,1529,1530,1531,1532,1533,1534,1535,1536,1537,1538,1539,1540,1541,1542,1543,1544,1545,1546,1547,1548,1549,1550,1551,1552,1553,1554,1555,1556,1557,1558,1559,1560,1561,1562,1563,1564,1565,1566,1567,1568,1569,1570,1571,1572,1573,1574,1575,1576,1577,1578,1579,1580,1581,1582,1583,1584,1585,1586,1587,1588,1589,1590,1591,1592,1593,1594,1595,1596,1597,1598,1599,1600,1601,1602,1603,1604,1605,1606,1607,1608,1609,1610,1611,1612,1613,1614,1615,1616,1617,1618,1619,1620,1621,1622,1623,1624,1625,1626,1627,1628,1629,1630,1631,1632,1633,1634,1635,1636,1637,1638,1639,1640,1641,1642,1643,1644,1645,1646,1647,1648,1649,1650,1651,1652,1653,1654,1655,1656,1657,1658,1659,1660,1661,1662,1663,1664,1665,1666,1667,1668,1669,1670,1671,1672,1673,1674,1675,1676,1677,1678,1679,1680,1681,1682,1683,1684,1685,1686,1687,1688,1689,1690,1691,1692,1693,1694,1695,1696,1697,1698,1699,1700,1701,1702,1703,1704,1705,1706,1707,1708,1709,1710,1711,1712,1713,1714,1715,1716,1717,1718,1719,1720,1721,1722,1723,1724,1725,1726,1727,1728,1729,1730,1731,1732,1733,1734,1735,1736,1737,1738,1739,1740,1741,1742,1743,1744,1745,1746,1747,1748,1749,1750,1751,1752,1753,1754,1755,1756,1757,1758,1759,1760,1761,1762,1763,1764,1765,1766,1767,1768,1769,1770,1771,1772,1773,1774,1775,1776,1777,1778,1779,1780,1781,1782,1783,1784,1785,1786,1787,1788,1789,1790,1791,1792,1793,1794,1795,1796,1797,1798,1799,1800,1801,1802,1803,1804,1805,1806,1807,1808,1809,1810,1811,1812,1813,1814,1815,1816,1817,1818,1819,1820,1821,1822,1823,1824,1825,1826,1827,1828,1829,1830,1831,1832,1833,1834,1835,1836,1837,1838,1839,1840,1841,1842,1843,1844,1845,1846,1847,1848,1849,1850,1851,1852,1853,1854,1855,1856,1857,1858,1859,1860,1861,1862,1863,1864,1865,1866,1867,1868,1869,1870,1871,1872,1873,1874,1875,1876,1877,1878,1879,1880,1881,1882,1883,1884,1885,1886,1887,1888,1889,1890,1891,1892,1893,1894,1895,1896,1897,1898,1899,1900,1901,1902,1903,1904,1905,1906,1907,1908,1909,1910,1911,1912,1913,1914,1915,1916,1917,1918,1919,1920,1921,1922,1923,1924,1925,1926,1927,1928,1929,1930,1931,1932,1933,1934,1935,1936,1937,1938,1939,1940,1941,1942,1943,1944,1945,1946,1947,1948,1949,1950,1951,1952,1953,1954,1955,1956,1957,1958,1959,1960,1961,1962,1963,1964,1965,1966,1967,1968,1969,1970,1971,1972,1973,1974,1975,1976,1977,1978,1979,1980,1981,1982,1983,1984,1985,1986,1987,1988,1989,1990,1991,1992,1993,1994,1995,1996,1997,1998,1999,2000,2001,2002,2003,2004,2005,2006,2007,2008,2009,2010,2011,2012,2013,2014,2015,2016,2017,2018,2019,2020,2021,2022,2023,2024,2025,2026,2027,2028,2029,2030,2031,2032,2033,2034,2035,2036,2037,2038,2039,2040,2041,2042,2043,2044,2045,2046,2047],\"xaxis\":\"x\",\"y\":[-0.617233157157898,-0.33419564366340637,-0.22000952064990997,-0.17032939195632935,-0.16973353922367096,-0.1666564792394638,-0.14912518858909607,-0.13433367013931274,-0.12215464562177658,-0.12205848842859268,-0.12072032690048218,-0.1155034750699997,-0.10655862838029861,-0.10061012208461761,-0.09858690202236176,-0.09225684404373169,-0.08426584303379059,-0.08374012261629105,-0.08223166316747665,-0.0810726135969162,-0.08029710501432419,-0.0796358585357666,-0.07921406626701355,-0.07765228301286697,-0.07216592878103256,-0.07195678353309631,-0.07166681438684464,-0.07039245963096619,-0.07006807625293732,-0.06932634115219116,-0.06915783882141113,-0.06806939840316772,-0.06590021401643753,-0.06589191406965256,-0.06585822999477386,-0.06249089539051056,-0.061585236340761185,-0.06126110255718231,-0.06082702800631523,-0.060719043016433716,-0.060335464775562286,-0.059164926409721375,-0.057395413517951965,-0.05673553794622421,-0.05654824897646904,-0.0558633916079998,-0.05369070544838905,-0.05341802537441254,-0.053031910210847855,-0.05204024910926819,-0.049761977046728134,-0.048853300511837006,-0.046989645808935165,-0.046627629548311234,-0.04621666297316551,-0.04337218776345253,-0.04293645918369293,-0.04246506839990616,-0.04188520833849907,-0.041329849511384964,-0.04108310863375664,-0.040621720254421234,-0.04055143520236015,-0.04002317413687706,-0.03991500288248062,-0.039911989122629166,-0.03989962115883827,-0.039217159152030945,-0.03897606208920479,-0.03865982964634895,-0.03813949227333069,-0.03798257187008858,-0.03715099021792412,-0.036971211433410645,-0.035905417054891586,-0.0354488343000412,-0.035167887806892395,-0.034335073083639145,-0.034072376787662506,-0.03404449298977852,-0.033998649567365646,-0.033866290003061295,-0.033634915947914124,-0.033140622079372406,-0.0323369987308979,-0.032210685312747955,-0.03199610486626625,-0.031861335039138794,-0.03185518831014633,-0.03136938810348511,-0.03111559897661209,-0.03071703389286995,-0.030653614550828934,-0.0305867288261652,-0.03050134889781475,-0.03050006367266178,-0.03021439164876938,-0.029605723917484283,-0.029245596379041672,-0.028851637616753578,-0.02855093404650688,-0.028548067435622215,-0.02829745225608349,-0.028038721531629562,-0.027699461206793785,-0.027632251381874084,-0.02762121707201004,-0.027126746252179146,-0.027063386514782906,-0.027007972821593285,-0.026683064177632332,-0.026471929624676704,-0.026199445128440857,-0.026055337861180305,-0.025776129215955734,-0.025580234825611115,-0.025350527837872505,-0.02497726120054722,-0.024744322523474693,-0.02464599348604679,-0.024512404575943947,-0.02415628172457218,-0.024010315537452698,-0.023558948189020157,-0.02325880527496338,-0.022191859781742096,-0.022128598764538765,-0.02168002538383007,-0.021669914945960045,-0.02156270109117031,-0.0215588491410017,-0.021413175389170647,-0.021347269415855408,-0.021028215065598488,-0.020669372752308846,-0.020667875185608864,-0.020504970103502274,-0.020209576934576035,-0.01979491300880909,-0.01972675323486328,-0.019683584570884705,-0.0194518081843853,-0.01944410800933838,-0.019241755828261375,-0.01909099519252777,-0.019062725827097893,-0.018872825428843498,-0.018817277625203133,-0.018579689785838127,-0.018500905483961105,-0.018492532894015312,-0.018429158255457878,-0.018254704773426056,-0.018209468573331833,-0.017877502366900444,-0.017737513408064842,-0.017384566366672516,-0.01724644936621189,-0.01681974157691002,-0.0167671050876379,-0.016760464757680893,-0.01637798734009266,-0.016342146322131157,-0.01632905751466751,-0.016267679631710052,-0.01626027561724186,-0.016085032373666763,-0.016047151759266853,-0.015898294746875763,-0.015785057097673416,-0.015549927018582821,-0.015441065654158592,-0.015258587896823883,-0.01509046833962202,-0.014988877810537815,-0.014970357529819012,-0.014887112192809582,-0.014850332401692867,-0.01474071480333805,-0.014592272229492664,-0.014527732506394386,-0.014459299854934216,-0.014344805851578712,-0.014267964288592339,-0.014147072099149227,-0.014123966917395592,-0.013818645849823952,-0.01358409970998764,-0.01350465789437294,-0.013432812876999378,-0.013398474082350731,-0.013126862235367298,-0.012905153445899487,-0.012805037200450897,-0.012770605273544788,-0.01252756454050541,-0.012458821758627892,-0.012385940179228783,-0.012341421097517014,-0.012282676994800568,-0.012277629226446152,-0.01227404922246933,-0.012268129736185074,-0.012241006828844547,-0.01218303944915533,-0.011950547806918621,-0.011665873229503632,-0.011613845825195312,-0.01138261053711176,-0.011328289285302162,-0.011306792497634888,-0.01096491701900959,-0.01081465557217598,-0.010778733529150486,-0.010550562292337418,-0.010545487515628338,-0.010542608797550201,-0.010539365001022816,-0.010276571847498417,-0.010222773998975754,-0.010219969786703587,-0.010187648236751556,-0.010139966383576393,-0.010130583308637142,-0.00997120887041092,-0.0099256606772542,-0.00987942423671484,-0.009852861054241657,-0.009816094301640987,-0.009806125424802303,-0.009785766713321209,-0.009783662855625153,-0.009639719501137733,-0.009625744074583054,-0.009614142589271069,-0.009593959897756577,-0.009562122635543346,-0.009447268210351467,-0.00939963012933731,-0.009362403303384781,-0.009275959804654121,-0.009221506305038929,-0.009165655821561813,-0.009163052774965763,-0.009096885100007057,-0.009076233021914959,-0.009029919281601906,-0.009019946679472923,-0.00900171883404255,-0.008965857326984406,-0.008571842685341835,-0.008468844927847385,-0.008417189121246338,-0.008350363001227379,-0.008293680846691132,-0.00822494551539421,-0.00821771938353777,-0.008087921887636185,-0.007946083322167397,-0.007920932024717331,-0.007907024584710598,-0.00785561092197895,-0.0077924239449203014,-0.007784401066601276,-0.007776092737913132,-0.0077578164637088776,-0.007707760203629732,-0.007667569909244776,-0.007613737136125565,-0.007611348759382963,-0.00757262110710144,-0.007564844097942114,-0.007556768599897623,-0.007433728780597448,-0.007424454670399427,-0.007384306285530329,-0.007379800081253052,-0.007375556975603104,-0.007322611287236214,-0.007259666454046965,-0.007249658927321434,-0.007219798397272825,-0.0072163124568760395,-0.007207907270640135,-0.007135292980819941,-0.0071005309000611305,-0.00709424028173089,-0.007092192303389311,-0.007090317085385323,-0.007083259057253599,-0.006957098841667175,-0.006952762138098478,-0.006893931422382593,-0.006890588905662298,-0.006873350124806166,-0.006858367472887039,-0.006847092881798744,-0.006770131643861532,-0.006761365570127964,-0.006729958113282919,-0.006727567408233881,-0.006627504248172045,-0.006621749605983496,-0.006501459516584873,-0.0064930664375424385,-0.006487620063126087,-0.006422836799174547,-0.006412717513740063,-0.006326328497380018,-0.006317418068647385,-0.006303080823272467,-0.006273169536143541,-0.006259437650442123,-0.006233178544789553,-0.006162197794765234,-0.006110954098403454,-0.006105770356953144,-0.006103324703872204,-0.006081840023398399,-0.0060717444866895676,-0.006045956164598465,-0.006043862085789442,-0.006037440150976181,-0.006019046995788813,-0.005997797939926386,-0.005996338091790676,-0.005977556109428406,-0.005973166786134243,-0.005952352192252874,-0.005932614207267761,-0.00592624070122838,-0.005835156422108412,-0.005818306002765894,-0.005762371234595776,-0.0057599954307079315,-0.005750174634158611,-0.005744740832597017,-0.005743491463363171,-0.005740574095398188,-0.005729115568101406,-0.0057051158510148525,-0.00567722087725997,-0.005664936266839504,-0.005591873079538345,-0.005571582354605198,-0.005555242765694857,-0.005532367154955864,-0.005511303432285786,-0.005505623761564493,-0.005443870555609465,-0.00543623324483633,-0.005401157308369875,-0.005314520560204983,-0.00522449379786849,-0.005183813627809286,-0.0051484908908605576,-0.005136845633387566,-0.005086916498839855,-0.005063530057668686,-0.005041226744651794,-0.00501655088737607,-0.004999286960810423,-0.0049796076491475105,-0.004943548701703548,-0.0049106464721262455,-0.004908563569188118,-0.004870780743658543,-0.0048691327683627605,-0.004861273802816868,-0.00485270144417882,-0.004852429497987032,-0.004838918335735798,-0.004834781400859356,-0.004831849131733179,-0.004831030033528805,-0.0047988854348659515,-0.004717780742794275,-0.004711607005447149,-0.00470432173460722,-0.00469697592779994,-0.0046789334155619144,-0.004652137868106365,-0.004610650707036257,-0.0045814174227416515,-0.004527287557721138,-0.004521687515079975,-0.004506887402385473,-0.00449735764414072,-0.0044856322929263115,-0.004449584987014532,-0.004421846009790897,-0.004306252580136061,-0.004272546619176865,-0.0042497324757277966,-0.004244712181389332,-0.004244285169988871,-0.0042378525249660015,-0.004229782614856958,-0.004203880205750465,-0.004105197265744209,-0.004094727803021669,-0.0040765260346233845,-0.004063419532030821,-0.004033891949802637,-0.003992840647697449,-0.003992727026343346,-0.003973649814724922,-0.003951567225158215,-0.003947097808122635,-0.003861828474327922,-0.0038542530965059996,-0.003819894976913929,-0.00381115498021245,-0.0037931969854980707,-0.003792370669543743,-0.003777462290599942,-0.003754248609766364,-0.0037504942156374454,-0.00370989553630352,-0.0036995229311287403,-0.003691301913931966,-0.0036456033121794462,-0.0035803657956421375,-0.0035726435016840696,-0.003541792742908001,-0.0035404020454734564,-0.003521207021549344,-0.0035162947606295347,-0.003507617162540555,-0.003504866734147072,-0.0034893827978521585,-0.0034718841779977083,-0.003454458201304078,-0.0034481827169656754,-0.003440684173256159,-0.0033917566761374474,-0.0033879789989441633,-0.003381785936653614,-0.0033647059462964535,-0.003355212276801467,-0.003349639242514968,-0.003346855053678155,-0.0033178161829710007,-0.003314119065180421,-0.0033098573330789804,-0.003301328979432583,-0.003237102646380663,-0.0032339429017156363,-0.003203602973371744,-0.0032032805029302835,-0.003203169209882617,-0.0032011002767831087,-0.003175675868988037,-0.003155204700306058,-0.0031518330797553062,-0.0031423396430909634,-0.003131054574623704,-0.0030999004375189543,-0.0030901844147592783,-0.0030706184916198254,-0.0030551862437278032,-0.0030498518608510494,-0.00302591803483665,-0.003007984720170498,-0.0029501905664801598,-0.002949152374640107,-0.002943004947155714,-0.002941136946901679,-0.0029403900261968374,-0.002932744100689888,-0.0029274621047079563,-0.0029138887766748667,-0.0029125125147402287,-0.0029093038756400347,-0.0028454780112951994,-0.0028387648053467274,-0.00283743510954082,-0.0028343466110527515,-0.00281345727853477,-0.0028124963864684105,-0.0028055990114808083,-0.0028042877092957497,-0.0028040388133376837,-0.0027998355217278004,-0.0027963244356215,-0.002768190111964941,-0.0027490095235407352,-0.002699156291782856,-0.002695434493944049,-0.002694733440876007,-0.0026937180664390326,-0.0026709435041993856,-0.002656990895047784,-0.002637938829138875,-0.002637682482600212,-0.002619371982291341,-0.002611219184473157,-0.0026048419531434774,-0.0025828771758824587,-0.0025800561998039484,-0.0025725150480866432,-0.0025643985718488693,-0.0025630302261561155,-0.0025376295670866966,-0.002531331731006503,-0.00251509016379714,-0.002507895929738879,-0.0025042095221579075,-0.00248497212305665,-0.002481470350176096,-0.0024540252052247524,-0.0024435771629214287,-0.002426345832645893,-0.0023975460790097713,-0.0023755780421197414,-0.0023740099277347326,-0.0023720108438283205,-0.0023692611139267683,-0.002357713645324111,-0.0023507648147642612,-0.002340719336643815,-0.002328312024474144,-0.002319095656275749,-0.002316890750080347,-0.002313393633812666,-0.002289962023496628,-0.0022893401328474283,-0.0022851969115436077,-0.0022825116757303476,-0.002273706253618002,-0.002259316388517618,-0.0022532553412020206,-0.0022519829217344522,-0.0022299066185951233,-0.0022224211134016514,-0.002221518661826849,-0.0022037457674741745,-0.002198833739385009,-0.0021958663128316402,-0.002188424114137888,-0.0021877840626984835,-0.0021627566311508417,-0.0021470871288329363,-0.00214042654260993,-0.0021365517750382423,-0.002121396828442812,-0.002117972355335951,-0.0020972788333892822,-0.0020889502484351397,-0.002066670684143901,-0.0020598825067281723,-0.0020590294152498245,-0.0020499946549534798,-0.002035668585449457,-0.0020321477204561234,-0.002020330633968115,-0.002011342439800501,-0.0020012902095913887,-0.0019844959024339914,-0.0019806246273219585,-0.001971091842278838,-0.0019669942557811737,-0.001965642673894763,-0.0019647085573524237,-0.0019580901134759188,-0.0019518547924235463,-0.001948418328538537,-0.0019456822192296386,-0.0019432969857007265,-0.001926601049490273,-0.0019133874448016286,-0.0019056029850617051,-0.001905027194879949,-0.0018975911661982536,-0.0018869602354243398,-0.0018776479410007596,-0.001876187277957797,-0.0018711016746237874,-0.0018674141028895974,-0.001864889869466424,-0.0018625486409291625,-0.0018458939157426357,-0.0018434524536132812,-0.001841518678702414,-0.001828501233831048,-0.0018236244795843959,-0.001819855417124927,-0.0018186569213867188,-0.001813069568015635,-0.0017988444305956364,-0.001793208415620029,-0.0017878244398161769,-0.0017874918412417173,-0.0017711049877107143,-0.0017550487536936998,-0.0017452238826081157,-0.0017190739745274186,-0.0016903631621971726,-0.001688807038590312,-0.0016814353875815868,-0.0016591172898188233,-0.001652435166761279,-0.0016403358895331621,-0.0016361053567379713,-0.0016225534491240978,-0.0016220920952036977,-0.0015785719733685255,-0.0015728524886071682,-0.0015642314683645964,-0.0015594707801938057,-0.001550115761347115,-0.0015432093059644103,-0.0015342673286795616,-0.001529733999632299,-0.001521305413916707,-0.0015142462216317654,-0.0015020172577351332,-0.001492718467488885,-0.0014905105344951153,-0.0014876993373036385,-0.0014828969724476337,-0.0014705103822052479,-0.0014657324645668268,-0.0014656950952485204,-0.0014588527847081423,-0.0014558738330379128,-0.0014513448113575578,-0.0014510313048958778,-0.001442360575310886,-0.0014407753478735685,-0.001423536567017436,-0.0014182630693539977,-0.0014180957805365324,-0.0014091735938563943,-0.0014033467741683125,-0.0014002391835674644,-0.0013681768905371428,-0.00135736761149019,-0.0013470605481415987,-0.0013393099652603269,-0.0013334491522982717,-0.0013291791547089815,-0.0013129374710842967,-0.0013047155225649476,-0.0012908113421872258,-0.0012823203578591347,-0.0012811161577701569,-0.0012783086858689785,-0.00126580148935318,-0.0012651697034016252,-0.0012646653922274709,-0.0012613924918696284,-0.0012585067888721824,-0.00124825700186193,-0.0012480574660003185,-0.0012401682324707508,-0.0012351946206763387,-0.0012304724659770727,-0.0012287815334275365,-0.0012250514701008797,-0.001197433564811945,-0.0011961671989411116,-0.0011905988212674856,-0.0011841357918456197,-0.0011747227981686592,-0.0011706481454893947,-0.0011663173791021109,-0.001165534369647503,-0.0011637997813522816,-0.0011493964120745659,-0.001145650283433497,-0.0011441686656326056,-0.0011410044971853495,-0.0011357085313647985,-0.0011285083601251245,-0.001118972199037671,-0.0011098770191892982,-0.0010941117070615292,-0.0010909298434853554,-0.0010870950063690543,-0.0010830601677298546,-0.0010732447262853384,-0.001061229151673615,-0.0010591564932838082,-0.0010589666198939085,-0.0010563876712694764,-0.0010509738931432366,-0.0010448419488966465,-0.0010424734791740775,-0.0010342826135456562,-0.0010280549759045243,-0.0010218257084488869,-0.001021290896460414,-0.0010117541532963514,-0.0010088755516335368,-0.0010026952950283885,-0.0010011206613853574,-0.0009973838459700346,-0.0009950643870979548,-0.0009814377408474684,-0.0009809995535761118,-0.0009775301441550255,-0.0009674663888290524,-0.0009639086783863604,-0.000960054574534297,-0.0009539145976305008,-0.0009534029522910714,-0.0009459057473577559,-0.0009425662574358284,-0.0009164640214294195,-0.000912754621822387,-0.0008952162461355329,-0.0008853566832840443,-0.0008715955191291869,-0.0008682261686772108,-0.0008677387959323823,-0.0008471070905216038,-0.0008450743625871837,-0.0008390319417230785,-0.0008302464848384261,-0.0008116704411804676,-0.0008103391737677157,-0.0008077078382484615,-0.0008060138789005578,-0.0008041081018745899,-0.0008007764117792249,-0.000796744367107749,-0.0007939793867990375,-0.0007871073321439326,-0.0007869653636589646,-0.0007847842061892152,-0.0007838992751203477,-0.0007749574724584818,-0.0007746720802970231,-0.0007609314634464681,-0.0007509010029025376,-0.0007424070499837399,-0.0007352213724516332,-0.0007309745997190475,-0.0007212937343865633,-0.000717831018846482,-0.0007165978313423693,-0.0007102511008270085,-0.0007003745413385332,-0.0006915682461112738,-0.0006843438604846597,-0.0006819571717642248,-0.0006802222924306989,-0.00067997066071257,-0.0006786159356124699,-0.000659426674246788,-0.0006523170159198344,-0.0006379932165145874,-0.0006333974888548255,-0.0006317748920992017,-0.0006299674278125167,-0.0006207724800333381,-0.0006198384799063206,-0.000618971127551049,-0.0006089075468480587,-0.0006039442378096282,-0.0005963512230664492,-0.0005929369945079088,-0.0005791492876596749,-0.0005746575188823044,-0.0005684407660737634,-0.0005505062872543931,-0.0005483776330947876,-0.00054525415180251,-0.000542623398359865,-0.0005408755969256163,-0.0005407945718616247,-0.0005381901282817125,-0.0005318265175446868,-0.0005246253567747772,-0.0005234427517279983,-0.0005213306285440922,-0.0005134146194905043,-0.0005073464126326144,-0.0005049451719969511,-0.0005037591326981783,-0.000493942410685122,-0.0004933541058562696,-0.0004924133536405861,-0.0004839521716348827,-0.0004793385451193899,-0.0004711862711701542,-0.0004697928670793772,-0.00046322145499289036,-0.00046170345740392804,-0.00045912645873613656,-0.00045574092655442655,-0.0004543012473732233,-0.0004507047706283629,-0.00044971078750677407,-0.0004388850065879524,-0.0004362747713457793,-0.00043600655044429004,-0.00043297052616253495,-0.0004278617270756513,-0.0004251109203323722,-0.0004190661129541695,-0.00041800522012636065,-0.0004162969416938722,-0.0004098278295714408,-0.00040449664811603725,-0.00040178210474550724,-0.00039448178722523153,-0.0003934474952984601,-0.00039275825838558376,-0.00039176747668534517,-0.00038985550054349005,-0.00038888753624632955,-0.0003857498522847891,-0.0003846266772598028,-0.00038322992622852325,-0.0003732690238393843,-0.00037313028587959707,-0.00036578133585862815,-0.00036319487844593823,-0.00036247828393243253,-0.0003603504737839103,-0.0003601385687943548,-0.0003565024526324123,-0.0003551511326804757,-0.00035335280699655414,-0.000351452617906034,-0.0003502396575640887,-0.0003440225846134126,-0.00034191281883977354,-0.00033655433799140155,-0.00033561422605998814,-0.00033453607466071844,-0.0003342575510032475,-0.0003318590752314776,-0.0003304345882497728,-0.0003260480589233339,-0.0003191318246535957,-0.00031751408823765814,-0.0003163118672091514,-0.00030352675821632147,-0.0003030329244211316,-0.000302646221825853,-0.00030107342172414064,-0.00029952064505778253,-0.0002978338161483407,-0.00029580629779957235,-0.00029468192951753736,-0.00029067962896078825,-0.0002892225165851414,-0.00028708874015137553,-0.0002843293477781117,-0.00028211294556967914,-0.0002747586404439062,-0.0002731213753577322,-0.00027097880956716835,-0.0002673023263923824,-0.0002661776670720428,-0.00026450902805663645,-0.00026411638827994466,-0.0002583354653324932,-0.0002530653728172183,-0.00024821600527502596,-0.00024721227237023413,-0.00024256609322037548,-0.0002417946670902893,-0.00024170003598555923,-0.00024131587997544557,-0.0002403269027126953,-0.00023863918613642454,-0.0002362831000937149,-0.0002342002117075026,-0.00023084305576048791,-0.0002254129940411076,-0.00022519171761814505,-0.00022470898693427444,-0.00022386349155567586,-0.00021897174883633852,-0.00021753735200036317,-0.00021652392752002925,-0.00020938717352692038,-0.00020751103875227273,-0.00020713485719170421,-0.00020526564912870526,-0.0002036147634498775,-0.00020238816796336323,-0.0002003297267947346,-0.00019823289767373353,-0.00019302107102703303,-0.0001877370523288846,-0.0001867245155153796,-0.00018539503798820078,-0.00018263682432007045,-0.00017876199854072183,-0.00017699725867714733,-0.00017687276704236865,-0.0001760088634910062,-0.00017594851669855416,-0.000172412910615094,-0.00017148733604699373,-0.00016808048530947417,-0.00016789398796390742,-0.00016755066462792456,-0.00016699946718290448,-0.00016355350089725107,-0.00016349420184269547,-0.00016262322606053203,-0.0001612263877177611,-0.0001601178228156641,-0.00015591703413520008,-0.00015278444334398955,-0.00015120759780984372,-0.00014517136150971055,-0.00014336928143166006,-0.00014089301112107933,-0.00014049246965441853,-0.00014016263594385237,-0.0001391316909575835,-0.00013705529272556305,-0.00013645843137055635,-0.00013561308151111007,-0.00013267718895804137,-0.000132563931401819,-0.00013178176595829427,-0.00013159394438844174,-0.00013051502173766494,-0.0001279266143683344,-0.00012245416291989386,-0.00011976264067925513,-0.00011887810978805646,-0.00011762440408347175,-0.0001160870524472557,-0.00011584371532080695,-0.00011092380009358749,-0.0001087415948859416,-0.00010688655311241746,-0.00010607309377519414,-0.00010589882731437683,-0.00010035671584773809,-9.854324162006378e-05,-9.605094965081662e-05,-9.499788575340062e-05,-9.198792395181954e-05,-8.902735862648115e-05,-8.588626951677725e-05,-8.286722004413605e-05,-8.161157165886834e-05,-8.014358900254592e-05,-7.783644105074927e-05,-7.476881000911817e-05,-7.458410982508212e-05,-7.126770651666448e-05,-7.04434496583417e-05,-6.889917131047696e-05,-6.716705684084445e-05,-6.711289461236447e-05,-6.237700290512294e-05,-6.222099182195961e-05,-5.8003217418445274e-05,-5.7454108173260465e-05,-5.690880061592907e-05,-5.191169839235954e-05,-5.087114914203994e-05,-5.011498797102831e-05,-4.863783760811202e-05,-4.526644988800399e-05,-4.454210284166038e-05,-4.341870589996688e-05,-3.924183693015948e-05,-3.577426105039194e-05,-3.2694788387743756e-05,-3.1590683647664264e-05,-3.062814357690513e-05,-2.982728256029077e-05,-2.7767866413341835e-05,-2.7370304451324046e-05,-2.6413648811285384e-05,-2.208501064160373e-05,-2.1708830900024623e-05,-2.0060389942955226e-05,-1.7349719200865366e-05,-1.587025872140657e-05,-1.5037655430205632e-05,-1.416400118614547e-05,-1.4050751815375406e-05,-1.4034211744728964e-05,-1.2690499715972692e-05,-1.2611746569746174e-05,-1.1541023923200555e-05,-1.1474788152554538e-05,-1.093916580430232e-05,-1.0928511983365752e-05,-8.709281246410683e-06,-8.145720130414702e-06,-4.825368705496658e-06,-4.3937561713391915e-06,-4.311352768127108e-06,-2.62960793406819e-06,-1.7949938637684681e-06,-1.5717745327492594e-06,0.0,4.9248338740426334e-08,8.136034068684239e-08,1.0646134569469723e-06,2.9321759029699024e-06,3.520697418935015e-06,3.522410906953155e-06,3.708750000441796e-06,4.5570732254418544e-06,4.725456165033393e-06,4.7922135308908764e-06,5.154833161213901e-06,6.451085027947556e-06,7.695555723330472e-06,9.972154657589272e-06,1.0476112038304564e-05,1.46454576679389e-05,1.5701056327088736e-05,1.65402889251709e-05,1.6661286281305365e-05,1.6911701095523313e-05,1.7155483874375932e-05,1.793973206076771e-05,1.8444061424816027e-05,1.888871156552341e-05,1.906253419292625e-05,1.948796125361696e-05,1.955881634785328e-05,2.208598016295582e-05,2.2572650777874514e-05,2.481743649696e-05,2.7545243938220665e-05,2.8996913897572085e-05,3.0508786949212663e-05,3.1999126804294065e-05,3.244563777116127e-05,3.2755509892012924e-05,3.357946843607351e-05,3.37095552822575e-05,3.384485899005085e-05,3.3957287087105215e-05,3.718994412338361e-05,3.743626075447537e-05,4.098787758266553e-05,4.228614125167951e-05,4.435390292201191e-05,4.7359764721477404e-05,4.823655035579577e-05,4.891760545433499e-05,5.1137059926986694e-05,5.133166996529326e-05,5.286201849230565e-05,5.626484926324338e-05,5.682468326995149e-05,5.694188075722195e-05,5.7960824051406235e-05,5.818136196467094e-05,5.860194505658001e-05,5.86607311561238e-05,5.9094578318763524e-05,5.975872409180738e-05,6.09295821050182e-05,6.144583312561736e-05,6.459593714680523e-05,6.622828368563205e-05,6.778650276828557e-05,6.782263517379761e-05,6.865419709356502e-05,6.94038753863424e-05,6.988331733737141e-05,7.025725790299475e-05,7.362350879702717e-05,7.528908463427797e-05,7.702357834205031e-05,7.968343561515212e-05,8.116922981571406e-05,8.280001929961145e-05,8.650384552311152e-05,8.80056613823399e-05,8.877911022864282e-05,9.095333371078596e-05,9.217731712851673e-05,9.235173638444394e-05,9.371414489578456e-05,9.472914098296314e-05,9.665392281021923e-05,9.737253276398405e-05,9.942874021362513e-05,0.00010062016372103244,0.00010339759319322184,0.00010727145127020776,0.00010830074461409822,0.00010846599616343156,0.00011088788596680388,0.00011091530177509412,0.00011205583723494783,0.0001144855486927554,0.0001176399746327661,0.00011972576612606645,0.00012068733485648409,0.0001221696293214336,0.00012285560660529882,0.0001241894788108766,0.00012907557538710535,0.00013081908400636166,0.00013551748997997493,0.00013927883992437273,0.00014033041952643543,0.0001427472452633083,0.00014516904775518924,0.0001492024166509509,0.00015083893958944827,0.00015092834655661136,0.00015755236381664872,0.00015824146976228803,0.00015913859533611685,0.00016067757678683847,0.00016153066826518625,0.00016496397438459098,0.00016553774185013026,0.00016759999562054873,0.00016765757754910737,0.00016969167336355895,0.00017736195877660066,0.00018420949345454574,0.00018462374282535166,0.000196479115402326,0.00019984811660833657,0.00020019545627292246,0.00020169868366792798,0.00020202800806146115,0.00020393848535604775,0.00020472414325922728,0.0002050784241873771,0.00020682952890638262,0.00020897849753964692,0.00020977250824216753,0.00021234825544524938,0.00021457106049638242,0.00022188163711689413,0.0002220517344539985,0.00022292777430266142,0.00022591576271224767,0.00022969335259404033,0.00022987536794971675,0.00023158594558481127,0.00023517086810898036,0.00023657403653487563,0.00023926980793476105,0.00024165793729480356,0.0002460540854372084,0.00024743593530729413,0.0002569271600805223,0.0002582308661658317,0.0002614039112813771,0.00026773192803375423,0.00026813134900294244,0.00026947646983899176,0.0002696463488973677,0.00027619750471785665,0.00028101823409087956,0.0002841382520273328,0.0002843485854100436,0.0002875989011954516,0.00028915301663801074,0.00029024481773376465,0.0002903218555729836,0.0002909567847382277,0.0002940348640549928,0.0002963647129945457,0.00030641243210993707,0.00030700876959599555,0.0003117958549410105,0.00031224475242197514,0.00031311012571677566,0.00031792381196282804,0.0003219216305296868,0.0003249036381021142,0.0003272081958130002,0.00033657989115454257,0.000336930388584733,0.00034257760853506625,0.0003516030265018344,0.0003554755530785769,0.00035876163747161627,0.0003671392914839089,0.0003717461950145662,0.0003725736460182816,0.0003801599086727947,0.0003875918046105653,0.0003881221928168088,0.0003884109028149396,0.000392259651562199,0.00039960487629286945,0.0004013373691122979,0.0004021029162686318,0.0004066305700689554,0.00040915465797297657,0.0004124505794607103,0.00041911297012120485,0.00042047008173540235,0.00042204625788144767,0.0004262946604285389,0.000428401748649776,0.0004339028091635555,0.0004379655292723328,0.0004434884467627853,0.0004443491925485432,0.00044920051004737616,0.00046037533320486546,0.00046583652147091925,0.00046795286471024156,0.00046824634773656726,0.000468257290776819,0.00046929894597269595,0.00047043844824656844,0.00047259486746042967,0.00048093899386003613,0.0004965878324583173,0.0004982806276530027,0.0005045285215601325,0.0005063558928668499,0.0005091921193525195,0.00051139882998541,0.000512565253302455,0.0005127399927005172,0.0005290553672239184,0.0005318897310644388,0.0005331449792720377,0.0005342443473637104,0.0005369053687900305,0.0005388714489527047,0.0005439253291115165,0.0005487314774654806,0.0005550154019147158,0.0005576538387686014,0.0005606914055533707,0.0005617502611130476,0.0005632269312627614,0.0005660786991938949,0.0005727820680476725,0.0005839726072736084,0.0005899749230593443,0.0005989443743601441,0.0006055302219465375,0.0006138350581750274,0.0006149736582301557,0.0006172021967358887,0.0006221922230906785,0.000622910272795707,0.0006243509124033153,0.0006293334299698472,0.0006421040161512792,0.0006468580104410648,0.0006558623863384128,0.0006606047390960157,0.0006813375512138009,0.0006829072372056544,0.0006879097782075405,0.0006905409973114729,0.0007078326307237148,0.0007145344279706478,0.0007207412854768336,0.0007259196718223393,0.00074903474887833,0.0007550125010311604,0.0007885641534812748,0.0008055453072302043,0.0008158892160281539,0.0008169240318238735,0.0008190164226107299,0.0008193915127776563,0.0008318684995174408,0.0008326148963533342,0.0008368994458578527,0.0008409915026277304,0.0008412674069404602,0.0008464574348181486,0.0008493278874084353,0.0008523155120201409,0.0008576323743909597,0.0008615023689344525,0.0008681562030687928,0.000868569768499583,0.0008738606702536345,0.0008781336946412921,0.0008891112520359457,0.0008939496474340558,0.00090201006969437,0.0009069012594409287,0.0009159633773379028,0.0009166237432509661,0.0009243327658623457,0.0009285047417506576,0.0009323825943283737,0.0009337343508377671,0.0009398689726367593,0.0009402397554367781,0.0009602262871339917,0.0009621693752706051,0.000967938220128417,0.0009761980036273599,0.0009781914995983243,0.0009825688321143389,0.0009900571312755346,0.0009925176855176687,0.0009945773053914309,0.0010126405395567417,0.0010157623328268528,0.001032703323289752,0.0010391916148364544,0.0010405213106423616,0.0010421067709103227,0.001042256597429514,0.001044305507093668,0.0010538765927776694,0.0010560709051787853,0.0010563064133748412,0.0010610503377392888,0.0010621139081194997,0.0010638876119628549,0.0010668783215805888,0.001074713421985507,0.001079431502148509,0.001081189257092774,0.0010908562690019608,0.0010929646668955684,0.001094185747206211,0.001102239592000842,0.001106809824705124,0.0011107478057965636,0.0011151916114613414,0.0011152902152389288,0.0011162110604345798,0.0011182230664417148,0.001129901735112071,0.0011299520265311003,0.001133609563112259,0.0011357488110661507,0.001144055975601077,0.001155857928097248,0.0011558763217180967,0.0011574198724702,0.0011696272995322943,0.0011726112570613623,0.0011772451689466834,0.0011872689938172698,0.0011948858154937625,0.0012039165012538433,0.0012107763905078173,0.001228666165843606,0.0012504142941907048,0.0012518641306087375,0.0012540885945782065,0.0012554782442748547,0.0012653331505134702,0.001271675108000636,0.001275568618439138,0.00127820810303092,0.0012898200657218695,0.0012920681620016694,0.0012930056545883417,0.0012965265195816755,0.0012981861364096403,0.0013091147411614656,0.001311961910687387,0.001312324427999556,0.0013289572671055794,0.0013393711997196078,0.0013851870317012072,0.0013893473660573363,0.0013943840749561787,0.00140357855707407,0.0014036509674042463,0.0014062008121982217,0.0014121107524260879,0.0014223795151337981,0.0014270137762650847,0.0014286243822425604,0.0014579759445041418,0.001465559471398592,0.001481671817600727,0.0014838932547718287,0.0014880746603012085,0.0014951865887269378,0.0015055692056193948,0.0015095643466338515,0.0015134698478505015,0.0015134961577132344,0.0015245750546455383,0.001535356161184609,0.0015565342037007213,0.0015728340949863195,0.0015749159501865506,0.0015866102185100317,0.0015870928764343262,0.0015871779760345817,0.0015873439842835069,0.0015909681096673012,0.0015915930271148682,0.0016087499679997563,0.0016145904082804918,0.0016194188501685858,0.0016287588514387608,0.0016574895707890391,0.0016773573588579893,0.0016795882256701589,0.0016883723437786102,0.0016955494647845626,0.0017046122811734676,0.0017242844915017486,0.0017334256554022431,0.0017449643928557634,0.0017553700599819422,0.001773107098415494,0.0017869796138256788,0.0017904449487105012,0.0017928130691871047,0.001794563140720129,0.0018041373696178198,0.0018052991945296526,0.0018253950402140617,0.001842282828874886,0.0018513303948566318,0.0018561227479949594,0.0018775372300297022,0.0018909684149548411,0.0018992975819855928,0.0019023719942197204,0.0019182598916813731,0.0019199126400053501,0.0019295456586405635,0.0019498371984809637,0.0019569764845073223,0.001964335097000003,0.001973050879314542,0.0019738248083740473,0.001974082551896572,0.0019876123405992985,0.0020066192373633385,0.00203006062656641,0.0020325961522758007,0.0020345908123999834,0.0020352296996861696,0.002061696257442236,0.002067816210910678,0.0020694872364401817,0.002071014605462551,0.0020717892330139875,0.002076676581054926,0.002089346991851926,0.002091027097776532,0.0021020565181970596,0.002103395527228713,0.002107574138790369,0.0021085336338728666,0.002114546252414584,0.002116466872394085,0.0021205046214163303,0.0021283887326717377,0.0021405797451734543,0.0021411709021776915,0.0021449304185807705,0.0021475404500961304,0.0021808468736708164,0.002183355623856187,0.0021920278668403625,0.0021924437023699284,0.0022020156029611826,0.002206814242526889,0.002221370581537485,0.00222833757288754,0.0022445458453148603,0.002257529180496931,0.002260582521557808,0.0022733521182090044,0.002274434082210064,0.002281906083226204,0.002282446250319481,0.0022839661687612534,0.0022943788208067417,0.002301120199263096,0.0023235308472067118,0.0023255234118551016,0.002333913929760456,0.0023558891844004393,0.0023569006007164717,0.002359098754823208,0.0023855739273130894,0.0024058115668594837,0.002414778107777238,0.0024157653097063303,0.0024304555263370275,0.0024354816414415836,0.002448422834277153,0.002459055045619607,0.0024594031274318695,0.0024643726646900177,0.0024880478158593178,0.0024902536533772945,0.002493118867278099,0.0025002826005220413,0.002511468483135104,0.002519233850762248,0.002559143351390958,0.0025911424309015274,0.002609055256471038,0.002622210420668125,0.0026412957813590765,0.0026424743700772524,0.0026427716948091984,0.00264292536303401,0.002644318388774991,0.0026532309129834175,0.0026638919953256845,0.0026715246494859457,0.002675928408280015,0.0026807989925146103,0.002684136852622032,0.0026861706282943487,0.002687138272449374,0.002712810877710581,0.002767361933365464,0.0027678227052092552,0.0027726609259843826,0.002786831231787801,0.002796712564304471,0.0028005861677229404,0.002802568720653653,0.0028594958130270243,0.002862210851162672,0.0028716258239001036,0.0029176261741667986,0.002934148069471121,0.002949886256828904,0.0029615811072289944,0.0029893291648477316,0.003000962780788541,0.0030084485188126564,0.003014352871105075,0.0030149605590850115,0.003047146135941148,0.003050262341275811,0.0030604188796132803,0.003061140887439251,0.0030844081193208694,0.0030845720320940018,0.003102007554844022,0.0031108481343835592,0.0031459382735192776,0.0032127737067639828,0.00321396766230464,0.0032513446640223265,0.003266231156885624,0.0032842981163412333,0.003291022963821888,0.0032982444390654564,0.0033060756977647543,0.003311114851385355,0.0033211112022399902,0.003341713920235634,0.0033741784282028675,0.003382275113835931,0.0034175748005509377,0.003474015509709716,0.003509375499561429,0.003511482384055853,0.0035519187804311514,0.0036150969099253416,0.00362073490396142,0.0036414999049156904,0.0036460019182413816,0.003652215236797929,0.0036603135522454977,0.0036881088744848967,0.0036987229250371456,0.00375358946621418,0.0037578782066702843,0.0037638437934219837,0.0038020634092390537,0.0038175033405423164,0.003841761965304613,0.0038427000399678946,0.003846424166113138,0.0038633407093584538,0.003877463284879923,0.003928261809051037,0.003940395079553127,0.003947373945266008,0.003986232914030552,0.003986695781350136,0.004019996151328087,0.004053729586303234,0.004107845947146416,0.004143816418945789,0.0041487072594463825,0.00415407307446003,0.004188543185591698,0.004220671020448208,0.004232459235936403,0.004249956458806992,0.004280377179384232,0.004294203594326973,0.0043226731941103935,0.004351252689957619,0.00437294552102685,0.0044818464666605,0.0045443265698850155,0.004569819197058678,0.004606191534548998,0.004636222030967474,0.004651681054383516,0.004681186750531197,0.004723947495222092,0.004848729353398085,0.00485977903008461,0.004870313219726086,0.004904188681393862,0.004906482063233852,0.004914522171020508,0.004930024966597557,0.004948897287249565,0.004968082997947931,0.004971976391971111,0.004979583900421858,0.004981908947229385,0.004982038866728544,0.00498706242069602,0.005004629958420992,0.005033010151237249,0.005043867975473404,0.005054650362581015,0.005059889983385801,0.005122445058077574,0.005126496776938438,0.005138922482728958,0.005153498612344265,0.005221719853579998,0.005263314116746187,0.005272073205560446,0.005299016367644072,0.005315259099006653,0.005324041936546564,0.005325468257069588,0.00532582588493824,0.005333480890840292,0.005343189928680658,0.0053865001536905766,0.005409594625234604,0.005414589773863554,0.005470850504934788,0.005498781334608793,0.005533912219107151,0.005538574419915676,0.005603393539786339,0.005662156268954277,0.005732027348130941,0.005754806566983461,0.005775086116045713,0.0058576385490596294,0.0058867172338068485,0.005893624387681484,0.005905489902943373,0.005969693884253502,0.00601143529638648,0.006054671946913004,0.006077538710087538,0.006101963575929403,0.006115097552537918,0.006116805132478476,0.0061257509514689445,0.006142883095890284,0.00616537407040596,0.006193290930241346,0.006267004646360874,0.006325670517981052,0.0063346922397613525,0.00633885245770216,0.006369456183165312,0.006371930241584778,0.0063872081227600574,0.00640185596421361,0.006469885818660259,0.006472802255302668,0.0065277013927698135,0.006532845553010702,0.006547620519995689,0.0066033704206347466,0.006608471740037203,0.006674772594124079,0.006787573918700218,0.006818268448114395,0.006831498350948095,0.006847082171589136,0.006853356491774321,0.006861987058073282,0.00688504334539175,0.006886019837111235,0.0068863858468830585,0.006920594722032547,0.006941352970898151,0.006979407276958227,0.007042703218758106,0.007092571817338467,0.007094880566000938,0.007114192936569452,0.007188593968749046,0.007195381913334131,0.007208812050521374,0.0072105866856873035,0.007239078171551228,0.007289478555321693,0.007323929574340582,0.007341031916439533,0.007348335813730955,0.0073639643378555775,0.007379345130175352,0.007407351862639189,0.00742263812571764,0.007510143332183361,0.007529629860073328,0.007543112616986036,0.007573000155389309,0.007615972775965929,0.007622099947184324,0.0076249404810369015,0.00770204421132803,0.007711144629865885,0.007735054008662701,0.007773902267217636,0.007774776313453913,0.0077759381383657455,0.00780955795198679,0.00782010331749916,0.007824762724339962,0.007909456267952919,0.007931627333164215,0.007987947203218937,0.008041173219680786,0.00807137694209814,0.008082443848252296,0.008100533857941628,0.008130612783133984,0.00817866064608097,0.008264168165624142,0.008348069153726101,0.008352227509021759,0.0084671750664711,0.008519630879163742,0.008528023026883602,0.008534683845937252,0.008580587804317474,0.008657724596560001,0.008753953501582146,0.008819685317575932,0.008835532702505589,0.00897720456123352,0.009173106402158737,0.009202742949128151,0.00921229925006628,0.009303310886025429,0.00934839341789484,0.009355072863399982,0.009364599362015724,0.009431177750229836,0.009444343857467175,0.009484644047915936,0.009497024118900299,0.009533990174531937,0.009624497033655643,0.009630030021071434,0.009697319008409977,0.009859733283519745,0.009868158958852291,0.009908489882946014,0.009928890503942966,0.009991783648729324,0.010037065483629704,0.01004166528582573,0.010117191821336746,0.010117674246430397,0.010560891591012478,0.010656185448169708,0.010710720904171467,0.010734457522630692,0.010804095305502415,0.010838287882506847,0.010888136923313141,0.010919605381786823,0.010922551155090332,0.010954253375530243,0.010996767319738865,0.0110428836196661,0.011254746466875076,0.01131398230791092,0.011388082057237625,0.011424083262681961,0.011434887535870075,0.011477985419332981,0.011534064076840878,0.011555389501154423,0.01162563357502222,0.011705391108989716,0.011716454289853573,0.011775858700275421,0.011858850717544556,0.011928712949156761,0.012068189680576324,0.012117650359869003,0.01214070338755846,0.012180272489786148,0.012211926281452179,0.012254280038177967,0.012513415887951851,0.012531343847513199,0.012639674358069897,0.012642431072890759,0.012680313549935818,0.012708238326013088,0.012724163942039013,0.012808788567781448,0.012817330658435822,0.012878022156655788,0.012904655188322067,0.01290906872600317,0.012935199774801731,0.013111804611980915,0.013113372027873993,0.013143103569746017,0.013232171535491943,0.013276299461722374,0.013511944562196732,0.013543801382184029,0.013601596467196941,0.013623296283185482,0.013676576316356659,0.013723857700824738,0.013732594437897205,0.013838309794664383,0.01401993166655302,0.014100197702646255,0.01416594535112381,0.014252947643399239,0.014391463249921799,0.014437624253332615,0.014475016854703426,0.014538245275616646,0.014619430527091026,0.014933816157281399,0.014961416833102703,0.015199539251625538,0.015278518199920654,0.015406020916998386,0.015456922352313995,0.015517898835241795,0.01564018428325653,0.015901414677500725,0.016115127131342888,0.016202710568904877,0.01620309241116047,0.016544820740818977,0.016772864386439323,0.01681014709174633,0.01726789027452469,0.017430856823921204,0.017518101260066032,0.017577707767486572,0.017852993682026863,0.018033184111118317,0.018226783722639084,0.018372228369116783,0.018386200070381165,0.018564468249678612,0.01858900487422943,0.018673794344067574,0.018777620047330856,0.01883777789771557,0.018991662189364433,0.01901177130639553,0.019061315804719925,0.01908932812511921,0.019135378301143646,0.01921033300459385,0.019222091883420944,0.019272124394774437,0.019362540915608406,0.019495539367198944,0.01955452188849449,0.01984231173992157,0.01987908035516739,0.019960986450314522,0.020197773352265358,0.020748063921928406,0.021085236221551895,0.021173594519495964,0.02128622494637966,0.021347956731915474,0.021715035662055016,0.021752016618847847,0.02181982435286045,0.02183365635573864,0.0223137978464365,0.022374527528882027,0.02297692932188511,0.02312060445547104,0.023156555369496346,0.02325022779405117,0.023653794080018997,0.02378729358315468,0.023854203522205353,0.02393351122736931,0.024306606501340866,0.024503199383616447,0.02451462298631668,0.024530114606022835,0.0245324969291687,0.02479991875588894,0.025115685537457466,0.025302821770310402,0.02545146644115448,0.02563028782606125,0.02584085240960121,0.026336774230003357,0.02638193592429161,0.026500524953007698,0.02673141285777092,0.026964129880070686,0.027166858315467834,0.027262840420007706,0.028341174125671387,0.028487998992204666,0.028505800291895866,0.028786661103367805,0.02882050722837448,0.029003771021962166,0.02933589369058609,0.029796941205859184,0.030009305104613304,0.03019263781607151,0.030227629467844963,0.030320726335048676,0.030511759221553802,0.030602972954511642,0.03149191290140152,0.03200782835483551,0.032069217413663864,0.03265205770730972,0.032835137099027634,0.032894376665353775,0.032940253615379333,0.03316911309957504,0.03417889401316643,0.034186460077762604,0.03442961350083351,0.03476346284151077,0.035580411553382874,0.03588342294096947,0.03604701906442642,0.03623087331652641,0.03625708818435669,0.03727108612656593,0.03771696984767914,0.03912698104977608,0.03942130506038666,0.040559329092502594,0.04092817008495331,0.04101305082440376,0.0410529188811779,0.0412500761449337,0.04173784703016281,0.043115463107824326,0.0433744341135025,0.04385748878121376,0.044796302914619446,0.04481082782149315,0.04506918042898178,0.0452200323343277,0.04525583237409592,0.04575475677847862,0.04605744406580925,0.04611705616116524,0.04656204208731651,0.04696941003203392,0.04725717008113861,0.04757579416036606,0.047590307891368866,0.04925842210650444,0.04997730255126953,0.05037961155176163,0.050732627511024475,0.05084693804383278,0.05103156715631485,0.05278974026441574,0.054516199976205826,0.055772509425878525,0.056163396686315536,0.059235960245132446,0.060558319091796875,0.060651715844869614,0.061323389410972595,0.06143355742096901,0.06202288717031479,0.06374046951532364,0.06705212593078613,0.06801664084196091,0.06822182238101959,0.06969934701919556,0.07028278708457947,0.07089021801948547,0.07143180072307587,0.0738152340054512,0.07489971816539764,0.07611691206693649,0.07668230682611465,0.07960682362318039,0.07963425666093826,0.08001268655061722,0.0801757425069809,0.08129040896892548,0.08226197212934494,0.0886346846818924,0.08964687585830688,0.0918712392449379,0.09214502573013306,0.09305866062641144,0.09560178965330124,0.0958552435040474,0.09646500647068024,0.09707296639680862,0.1042875126004219,0.11046752333641052,0.11069362610578537,0.11177384108304977,0.11978942900896072,0.12032918632030487,0.12638385593891144,0.12951558828353882,0.13723298907279968,0.14155274629592896,0.15001176297664642,0.1518763303756714,0.17085976898670197,0.1732146143913269,0.17990277707576752,0.1843005120754242,0.20387233793735504,0.21833544969558716,0.2236199527978897],\"yaxis\":\"y\",\"type\":\"scattergl\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Sorted neurons\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Loss change\"}},\"legend\":{\"title\":{\"text\":\"variable\"},\"tracegroupgap\":0},\"title\":{\"text\":\"Loss change from ablating MLP5 neuron\"},\"width\":800,\"showlegend\":true},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('5ade98a8-a9b1-4140-aa9d-92af4f3dcacb');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sorted_means, indices = torch.sort(diffs.mean(1))\n",
    "sorted_means = sorted_means.tolist()\n",
    "haystack_utils.line(sorted_means, xlabel=\"Sorted neurons\", ylabel=\"Loss change\", title=\"Loss change from ablating MLP5 neuron\") # xticks=indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>                            <div id=\"71b86ffd-1981-4b81-998e-88f2a979e2e5\" class=\"plotly-graph-div\" style=\"height:525px; width:1000px;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"71b86ffd-1981-4b81-998e-88f2a979e2e5\")) {                    Plotly.newPlot(                        \"71b86ffd-1981-4b81-998e-88f2a979e2e5\",                        [{\"error_y\":{\"array\":[1.071713930672391],\"type\":\"data\",\"visible\":true},\"name\":\"Original\",\"x\":[\"Original\"],\"y\":[1.4656818426772953],\"type\":\"bar\"},{\"error_y\":{\"array\":[1.6277532307648335],\"type\":\"data\",\"visible\":true},\"name\":\"Ablated\",\"x\":[\"Ablated\"],\"y\":[3.6773443269729613],\"type\":\"bar\"},{\"error_y\":{\"array\":[1.0914516236086484],\"type\":\"data\",\"visible\":true},\"name\":\"MLP5 path patched\",\"x\":[\"MLP5 path patched\"],\"y\":[2.100534976348281],\"type\":\"bar\"},{\"error_y\":{\"array\":[1.4953536570889405],\"type\":\"data\",\"visible\":true},\"name\":\"MLP5 path patched + Top 10 MLP5 neurons ablated\",\"x\":[\"Top MLP5 removed\"],\"y\":[4.143007079362869],\"type\":\"bar\"},{\"error_y\":{\"array\":[0.4531983964419732],\"type\":\"data\",\"visible\":true},\"name\":\"MLP5 path patched + Bottom 10 MLP5 neurons ablated\",\"x\":[\"Bottom MLP5 removed\"],\"y\":[0.48426404791884126],\"type\":\"bar\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"title\":{\"text\":\"Average last token loss when removing top \\u002f bottom neurons from path patching\"},\"xaxis\":{\"title\":{\"text\":\"\"}},\"yaxis\":{\"title\":{\"text\":\"Loss\"}},\"barmode\":\"group\",\"width\":1000,\"showlegend\":true},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('71b86ffd-1981-4b81-998e-88f2a979e2e5');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check loss change when ablating top / bottom neurons\n",
    "top_neurons_count = 10\n",
    "top_neurons = indices[-top_neurons_count:]\n",
    "bottom_neurons = indices[:top_neurons_count]\n",
    "\n",
    "with model.hooks(deactivate_neurons_fwd_hooks):\n",
    "    ablated_loss, ablated_cache = model.run_with_cache(prompts, return_type=\"loss\")\n",
    "\n",
    "ablate_top_neurons_hook = get_ablate_neurons_hook(top_neurons, ablated_cache)\n",
    "ablate_bottom_neurons_hook = get_ablate_neurons_hook(bottom_neurons, ablated_cache)\n",
    "\n",
    "original_loss, ablated_loss, _, all_MLP5_loss = haystack_utils.get_direct_effect(prompts, model, pos=-1, context_ablation_hooks=deactivate_neurons_fwd_hooks, context_activation_hooks=activate_neurons_fwd_hooks)\n",
    "_, _, _, top_MLP5_ablated_loss = haystack_utils.get_direct_effect(prompts, model, pos=-1, context_ablation_hooks=deactivate_neurons_fwd_hooks, context_activation_hooks=activate_neurons_fwd_hooks+ablate_top_neurons_hook)\n",
    "_, _, _, bottom_MLP5_ablated_loss = haystack_utils.get_direct_effect(prompts, model, pos=-1, context_ablation_hooks=deactivate_neurons_fwd_hooks, context_activation_hooks=activate_neurons_fwd_hooks+ablate_bottom_neurons_hook)\n",
    "\n",
    "names = [\"Original\", \"Ablated\", \"MLP5 path patched\", f\"MLP5 path patched + Top {top_neurons_count} MLP5 neurons ablated\", f\"MLP5 path patched + Bottom {top_neurons_count} MLP5 neurons ablated\"]\n",
    "short_names = [\"Original\", \"Ablated\", \"MLP5 path patched\", f\"Top MLP5 removed\", f\"Bottom MLP5 removed\"]\n",
    "\n",
    "values = [original_loss.tolist(), ablated_loss.tolist(), all_MLP5_loss.tolist(), top_MLP5_ablated_loss.tolist(), bottom_MLP5_ablated_loss.tolist()]\n",
    "haystack_utils.plot_barplot(values, names, short_names=short_names, ylabel=\"Loss\", title=f\"Average last token loss when removing top / bottom neurons from path patching\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigate top / bottom neuron boosts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Patch in MLP5 neurons, ablated bottom neurons, compare with patched MLP5 neurons without ablating bottom neurons\n",
    "\n",
    "with model.hooks(deactivate_neurons_fwd_hooks):\n",
    "    ablated_logits, ablated_cache = model.run_with_cache(prompts)\n",
    "\n",
    "ablate_top_neurons_hook = get_ablate_neurons_hook(top_neurons, ablated_cache)\n",
    "ablate_bottom_neurons_hook = get_ablate_neurons_hook(bottom_neurons, ablated_cache)\n",
    "\n",
    "original_logits, ablated_logprobs, _, all_MLP5_logprobs = haystack_utils.get_direct_effect(prompts, model, pos=-2, context_ablation_hooks=deactivate_neurons_fwd_hooks, context_activation_hooks=activate_neurons_fwd_hooks, return_type='logprobs')\n",
    "_, _, _, top_MLP5_ablated_logprobs = haystack_utils.get_direct_effect(prompts, model, pos=-2, context_ablation_hooks=deactivate_neurons_fwd_hooks, context_activation_hooks=activate_neurons_fwd_hooks+ablate_top_neurons_hook, return_type='logprobs')\n",
    "_, _, _, bottom_MLP5_ablated_logprobs = haystack_utils.get_direct_effect(prompts, model, pos=-2, context_ablation_hooks=deactivate_neurons_fwd_hooks, context_activation_hooks=activate_neurons_fwd_hooks+ablate_bottom_neurons_hook, return_type='logprobs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_difference_neurons(baseline_logprobs, ablated_logprops, positive=True, logprob_threshold=-7, k=50):\n",
    "    neuron_logprob_difference = (baseline_logprobs - ablated_logprops).mean(0)\n",
    "    neuron_logprob_difference[baseline_logprobs.mean(0) < logprob_threshold] = 0\n",
    "    if positive:\n",
    "        non_zero_count = (neuron_logprob_difference > 0).sum()\n",
    "    else:\n",
    "        non_zero_count = (neuron_logprob_difference < 0).sum()\n",
    "    top_logprob_difference, top_neurons = haystack_utils.top_k_with_exclude(neuron_logprob_difference, min(non_zero_count, k), all_ignore, largest=positive)\n",
    "    return top_logprob_difference, top_neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boosted tokens\n",
    "bottom_neuron_pos_difference_logprobs, bottom_pos_indices = get_top_difference_neurons(all_MLP5_logprobs, bottom_MLP5_ablated_logprobs, positive=True)\n",
    "top_neuron_pos_difference_logprobs, top_pos_indices = get_top_difference_neurons(all_MLP5_logprobs, top_MLP5_ablated_logprobs, positive=True)\n",
    "# Deboosted tokens\n",
    "bottom_neuron_neg_difference_logprobs, bottom_neg_indices = get_top_difference_neurons(all_MLP5_logprobs, bottom_MLP5_ablated_logprobs, positive=False)\n",
    "top_neuron_neg_difference_logprobs, top_neg_indices = get_top_difference_neurons(all_MLP5_logprobs, top_MLP5_ablated_logprobs, positive=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acac6cb7520a44b7b1db5a9bc208054b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='Tokens:', options=('Boosted', 'Deboosted'), value='Boosted'), Drop…"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def plot_data(boosted_deboosted, top_bottom):\n",
    "    if boosted_deboosted == 'Boosted' and top_bottom == 'Top':\n",
    "        logprobs = top_neuron_pos_difference_logprobs\n",
    "        indices = top_pos_indices\n",
    "        title_change = 'increase'\n",
    "    elif boosted_deboosted == 'Boosted' and top_bottom == 'Bottom':\n",
    "        logprobs = bottom_neuron_pos_difference_logprobs\n",
    "        indices = bottom_pos_indices\n",
    "        title_change = 'increase'\n",
    "    elif boosted_deboosted == 'Deboosted' and top_bottom == 'Top':\n",
    "        logprobs = top_neuron_neg_difference_logprobs\n",
    "        indices = top_neg_indices\n",
    "        title_change = 'decrease'\n",
    "    else:  # 'Deboosted' and 'Bottom'\n",
    "        logprobs = bottom_neuron_neg_difference_logprobs\n",
    "        indices = bottom_neg_indices\n",
    "        title_change = 'decrease'\n",
    "\n",
    "    xlabel = boosted_deboosted + \" tokens\"\n",
    "    ylabel = \"full_logprob - ablated_logprop\"\n",
    "    title = 'Logprob ' + title_change + ' from ' + top_bottom.lower() + ' neurons'\n",
    "    xticks = [model.to_str_tokens([i])[0] for i in indices]\n",
    "\n",
    "    haystack_utils.line(logprobs.cpu().numpy(), xlabel=xlabel, ylabel=ylabel, title=title, xticks=xticks, show_legend=False)\n",
    "\n",
    "\n",
    "boosted_deboosted = widgets.Dropdown(options=['Boosted', 'Deboosted'], value='Boosted', description='Tokens:')\n",
    "top_bottom = widgets.Dropdown(options=['Top', 'Bottom'], value='Top', description='Neurons:')\n",
    "widgets.interactive(plot_data, boosted_deboosted=boosted_deboosted, top_bottom=top_bottom)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decompose neuron effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get logprob change for individual neuron\n",
    "\n",
    "def get_individual_neuron_logprob_effect(sorted_indices, neuron_pos=0, top=True, positive=True, plot=True):\n",
    "    if top:\n",
    "        neurons = sorted_indices[-(neuron_pos+1)]\n",
    "    else:\n",
    "        neurons = sorted_indices[neuron_pos]\n",
    "    neurons_hook = get_ablate_neurons_hook([neurons], ablated_cache)\n",
    "\n",
    "    original_logits, ablated_logprobs, _, all_MLP5_logprobs = haystack_utils.get_direct_effect(prompts, model, pos=-2, context_ablation_hooks=deactivate_neurons_fwd_hooks, context_activation_hooks=activate_neurons_fwd_hooks, return_type='logprobs')\n",
    "    _, _, _, neuron_ablated_logprobs = haystack_utils.get_direct_effect(prompts, model, pos=-2, context_ablation_hooks=deactivate_neurons_fwd_hooks, context_activation_hooks=activate_neurons_fwd_hooks+neurons_hook, return_type='logprobs')\n",
    "    neuron_difference_logprobs, difference_indices = \\\n",
    "        get_top_difference_neurons(all_MLP5_logprobs, neuron_ablated_logprobs, positive=positive)\n",
    "    \n",
    "    if not plot:\n",
    "        return neuron_difference_logprobs, difference_indices\n",
    "    \n",
    "    if positive:\n",
    "        xlabel = 'Boosted tokens'\n",
    "        title = 'Logprob increase from neuron ' + str(neurons.item())\n",
    "    else: \n",
    "        xlabel = 'Deboosted tokens'\n",
    "        title = 'Logprob decrease from neuron ' + str(neurons.item())\n",
    "    if top:\n",
    "        title += f' (top {neuron_pos+1} neuron)'\n",
    "    else:\n",
    "        title += f' (bottom {neuron_pos+1} neuron)'\n",
    "\n",
    "    xticks = [model.to_str_tokens([i])[0] for i in difference_indices]\n",
    "    haystack_utils.line(neuron_difference_logprobs.cpu().numpy(), xlabel=xlabel, ylabel=\"full_logprob - ablated_logprop\", title=title, xticks=xticks, show_legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3abbccf115e4b83a7b7e16638bd60c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='Neuron Pos:', max=20), Dropdown(description='Neurons:', …"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a function to call your function with widget inputs\n",
    "def plot_individual_neuron(neuron_pos, top_bottom, pos_neg):\n",
    "    top_bool = True if top_bottom == 'Top' else False\n",
    "    positive_bool = True if pos_neg == 'Positive' else False\n",
    "    get_individual_neuron_logprob_effect(indices, neuron_pos=neuron_pos, top=top_bool, positive=positive_bool)\n",
    "\n",
    "# Define widgets\n",
    "neuron_pos_slider = widgets.IntSlider(min=0, max=20, step=1, value=0, description='Neuron Pos:')\n",
    "top_bottom_dropdown = widgets.Dropdown(options=['Top', 'Bottom'], value='Top', description='Neurons:')\n",
    "pos_neg_dropdown = widgets.Dropdown(options=['Positive', 'Negative'], value='Positive', description='Pos/Neg:')\n",
    "\n",
    "# Use interactive function to bind the widgets to the plotting function\n",
    "widgets.interactive(plot_individual_neuron, neuron_pos=neuron_pos_slider, top_bottom=top_bottom_dropdown, pos_neg=pos_neg_dropdown)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum individual weighted positive / negative effects\n",
    "def summed_neuron_differences(num_neurons=10, top=True, positive=True, k=50):\n",
    "    summed_neuron_diffs = torch.zeros(model.cfg.d_vocab).cuda()\n",
    "    for neuron_pos in range(num_neurons):\n",
    "        # It would probably be more principle to do our filtering on the summed diffs instead of per neuron\n",
    "        neuron_diff, token_indices = get_individual_neuron_logprob_effect(indices, neuron_pos=neuron_pos, top=top, positive=positive, plot=False)\n",
    "        summed_neuron_diffs[token_indices] += neuron_diff\n",
    "    if positive:\n",
    "        non_zero_count = (summed_neuron_diffs > 0).sum()\n",
    "    else:\n",
    "        non_zero_count = (summed_neuron_diffs < 0).sum()\n",
    "    top_logprob_difference, top_tokens = haystack_utils.top_k_with_exclude(summed_neuron_diffs, min(non_zero_count, k), all_ignore, largest=positive)\n",
    "    return top_logprob_difference, top_tokens\n",
    "\n",
    "def plot_summed_neuron_differences(num_neurons=10, top=True, positive=True):\n",
    "    top_logprob_difference, top_logprob_tokens = summed_neuron_differences(num_neurons=num_neurons, top=top, positive=positive)\n",
    "    xticks = [model.to_str_tokens([i])[0] for i in top_logprob_tokens]\n",
    "    \n",
    "    if positive:\n",
    "        title = \"Summed individual boosts by\"\n",
    "        xlabel = \"Boosted tokens\"\n",
    "    else:\n",
    "        title = \"Summed individual deboosts by\"\n",
    "        xlabel = \"Deboosted tokens\"\n",
    "    if top:\n",
    "        title += f\" top {num_neurons} neurons\"\n",
    "    else:\n",
    "        title += f\" bottom {num_neurons} neurons\"\n",
    "    haystack_utils.line(top_logprob_difference.cpu().numpy(), xlabel=xlabel, ylabel=\"full_logprob - ablated_logprop\", title=title, xticks=xticks, show_legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ab3a3c108ad4a308dbc4cf442bdbecc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=10, description='Num Neurons:', max=20, min=1), Dropdown(description='Ne…"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a function to call your function with widget inputs\n",
    "def plot_summed_neurons_widget(num_neurons, top_bottom, pos_neg):\n",
    "    top_bool = True if top_bottom == 'Top' else False\n",
    "    positive_bool = True if pos_neg == 'Positive' else False\n",
    "    plot_summed_neuron_differences(num_neurons=num_neurons, top=top_bool, positive=positive_bool)\n",
    "\n",
    "# Define widgets\n",
    "num_neuron_slider = widgets.IntSlider(min=1, max=20, step=1, value=10, description='Num Neurons:')\n",
    "top_bottom_dropdown = widgets.Dropdown(options=['Top', 'Bottom'], value='Top', description='Neurons:')\n",
    "pos_neg_dropdown = widgets.Dropdown(options=['Positive', 'Negative'], value='Positive', description='Pos/Neg:')\n",
    "\n",
    "# Use interactive function to bind the widgets to the plotting function\n",
    "widgets.interactive(plot_summed_neurons_widget, num_neurons=num_neuron_slider, top_bottom=top_bottom_dropdown, pos_neg=pos_neg_dropdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss is logprob(correct_token_logit) = logit(correct_token_logit) - LogSumExp(all_token_logits)\n",
    "Loss = -logprob\n",
    "\n",
    "LogSumExp approximates a maximum function. If the neuron engages in destructive interference of a high logit for a non-answer token, then the exp(logit) for the token will be lower and so the LogSumExp will be more similar to logit(correct_token_logit) so the loss will be lower. So a lower logsumexp(all vocab) is good.\n",
    "\n",
    "If the neuron engages in destructive interference of a low logit for a non-answer token, then the exp(logit) for the the token won't really change and so the logsumexp and the loss will both be the same.\n",
    "\n",
    "For a single neuron:\n",
    "1. For each \"gen\" prompt, zero centre the logits and record the logit of \"gen\". Calculate the mean \"gen\" logit.\n",
    "2. For each \"gen\" prompt, disable the neuron under test, zero centre the logits, edit the logit of \"gen\" to the mean with a hook, then get the logprob of \"gen\" and calculate logsumexp(all vocab) = logit(gen) - logprob(gen).\n",
    "2. For each \"gen\" prompt, enable the neuron, zero centre the logits, edit the logit of \"gen\" to the mean with a hook, then get the logprob of \"gen\" and calculate logsumexp(all vocab) = logit(gen) - logprob(gen).\n",
    "3. Take the difference in logsum exps. If it's positive, the neuron is reducing the loss via destructive interference by the difference.\n",
    "Can use the same procedure for sets of neurons, or for all neurons, to find high level effects of the context neuron\n",
    "\n",
    "Logprobs are logits with a constant subtracted and the constant is the same for every logit within a prompt.\n",
    "\n",
    "Taking the difference in terms with and without a neuron's effect via the context neuron:\n",
    "- If log sum exp increases, the neuron is boosting tokens on average. \n",
    "- If logit increases, the neuron is boosting the correct token\n",
    "\n",
    "\n",
    "Remove the neuron's effects on the gen logit. Take the mean on the prompt and position?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(40.5344, device='cuda:0')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Calculate the mean \"gen\" logit.\n",
    "gen_index = model.to_single_token('gen')\n",
    "gen_logits = []\n",
    "logits = model(prompts, return_type='logits') # batch pos vocab\n",
    "logits = logits - logits.mean(-1).unsqueeze(-1) # batch pos vocab, batch pos 1\n",
    "\n",
    "mean_gen_logit = logits[:, -2, gen_index].mean(0)\n",
    "mean_gen_logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import numpy as np\n",
    "# from transformer_lens import utils\n",
    "# \n",
    "# px.histogram(np.random.choice(logits[:, -1, gen_index].flatten().cpu().numpy(), 1000), nbins=100)\n",
    "# utils.test_prompt(\"\".join(model.to_str_tokens(prompts[0, :-1])), \"gen\", model, prepend_space_to_answer=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constructive interference diff, destructive interference diff\n",
      "0.123313 0.001592\n",
      "0.172376 0.002439\n",
      "0.123869 0.004327\n",
      "0.155212 0.004493\n",
      "0.263114 -0.002900\n",
      "1.314098 0.089305\n"
     ]
    }
   ],
   "source": [
    "def decompose_interference_diff(ablated_cache, disabled_neurons=[top_neurons[0]], mean=True\n",
    "                                ) -> tuple[float, float] | tuple[Float[Tensor, \"n_prompts\"], Float[Tensor, \"n_prompts\"]]:\n",
    "    '''\n",
    "    Finds the effect of the German context neuron ablation via a given set of MLP5 neurons on the logit of each final \n",
    "    token. Decomposes it into constructive and destructive interference. A positive constructive interference difference\n",
    "    means the neurons boost the logit of the correct token, a positive destructive interference difference means the\n",
    "    LogSumExp is getting closer to the correct token logit because neurons deboost the logits of the higher probability \n",
    "    incorrect tokens.\n",
    "\n",
    "    Loss = -logprob(correct_token_logit)\n",
    "    logprob(correct_token_logit) = correct_token_logit - LogSumExp(all_token_logits)\n",
    "\n",
    "    LogSumExp is a smooth maximum function, so it approximates the max of all logits. A neuron that destructively \n",
    "    interfers with a non-answer token with a high logit lowers the LogSumExp(all_token_logits) and thus the loss.'''\n",
    "    ablate_top_neuron_hook = get_ablate_neurons_hook(disabled_neurons, ablated_cache)\n",
    "\n",
    "    _, _, _, mlp5_enabled_logits = haystack_utils.get_direct_effect(prompts, model, pos=-2, \n",
    "                                                                    context_ablation_hooks=deactivate_neurons_fwd_hooks, \n",
    "                                                                    context_activation_hooks=activate_neurons_fwd_hooks, return_type='logits')\n",
    "    _, _, _, mlp5_enabled_top_neuron_ablated_logits = haystack_utils.get_direct_effect(prompts, model, pos=-2, \n",
    "                                                                                       context_ablation_hooks=deactivate_neurons_fwd_hooks, \n",
    "                                                                                       context_activation_hooks=activate_neurons_fwd_hooks+ablate_top_neuron_hook, return_type='logits')\n",
    "    \n",
    "    # Mean center logits to avoid picking up on constant boosts / deboosts\n",
    "    mlp5_enabled_logits = mlp5_enabled_logits - mlp5_enabled_logits.mean(-1).unsqueeze(-1)\n",
    "    mlp5_enabled_top_neuron_ablated_logits = mlp5_enabled_top_neuron_ablated_logits - mlp5_enabled_top_neuron_ablated_logits.mean(-1).unsqueeze(-1)\n",
    "    \n",
    "    # 1. Constructive interference difference\n",
    "    # This is the change in the correct answer token logit from ablating the neuron, positive is good\n",
    "    constructive_interference_diffs = mlp5_enabled_logits[:, gen_index] - mlp5_enabled_top_neuron_ablated_logits[:, gen_index]\n",
    "\n",
    "    # 2. Destructive interference difference\n",
    "    # This is the change in the LogSumExp of all logits from not ablating the neuron positive/increase in LogSumExp is bad \n",
    "    # even though it means that all tokens get deboosted more by a constant amount because the LogSumExp is guaranteed to be\n",
    "    # >= the correct token logit\n",
    "\n",
    "    # Set the gen logit to its mean value so the neuron's constructive interference doesn't affect the LogSumExp difference\n",
    "    mlp5_enabled_logits[:, gen_index] = mean_gen_logit\n",
    "    mlp5_enabled_top_neuron_ablated_logits[:, gen_index] = mean_gen_logit\n",
    "    \n",
    "    # Compute logsumexp\n",
    "    mlp5_enabled_log_sum_exp = mlp5_enabled_logits.exp().sum(-1).log()\n",
    "    mlp5_enabled_top_neuron_ablated_log_sum_exp = mlp5_enabled_top_neuron_ablated_logits.exp().sum(-1).log()\n",
    "\n",
    "    # Check for errors\n",
    "    assert torch.allclose(mlp5_enabled_log_sum_exp, mlp5_enabled_logits[:, gen_index] - mlp5_enabled_logits.log_softmax(-1)[:, gen_index])\n",
    "    assert torch.allclose(mlp5_enabled_top_neuron_ablated_log_sum_exp, mlp5_enabled_top_neuron_ablated_logits[:, gen_index] - mlp5_enabled_top_neuron_ablated_logits.log_softmax(-1)[:, gen_index])\n",
    "\n",
    "    # Difference in logsumexp\n",
    "    # Logsumexp of enabled should be higher than ablated if the neuron does something good\n",
    "    # Negative results are good - they mean that all tokens are deboosted more when the neuron is active\n",
    "    destructive_interference_diffs = mlp5_enabled_log_sum_exp - mlp5_enabled_top_neuron_ablated_log_sum_exp\n",
    "    # Convert the results so positive is good\n",
    "    destructive_interference_diffs *= -1\n",
    "\n",
    "    if mean:\n",
    "        return constructive_interference_diffs.mean().item(), destructive_interference_diffs.mean().item(), \n",
    "    return constructive_interference_diffs, destructive_interference_diffs,\n",
    "\n",
    "\n",
    "# Calculate neuron-wise loss change\n",
    "print(\"constructive interference diff, destructive interference diff\")\n",
    "for i in range(5):\n",
    "    constructive_diff, destructive_diff = decompose_interference_diff(ablated_cache, disabled_neurons=[top_neurons[i]], mean=True)\n",
    "    print(f'{constructive_diff:2f}', f'{destructive_diff:2f}')\n",
    "\n",
    "constructive_diff, destructive_diff = decompose_interference_diff(ablated_cache, top_neurons)\n",
    "print(f'{constructive_diff:2f}', f'{destructive_diff:2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neuron -> Token Logit Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0237,  0.0346, -0.0347,  0.0397,  0.0595,  0.0501, -0.0274,  0.1405,\n",
      "         0.0417, -0.0392], device='cuda:0')\n",
      "tensor([0.1634, 0.1461, 0.1405, 0.1399, 0.1349, 0.1207, 0.1196, 0.1161, 0.1156,\n",
      "        0.1139, 0.1130, 0.1128, 0.1120, 0.1104, 0.1096, 0.1070, 0.1056, 0.1048,\n",
      "        0.1042, 0.1028], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Our top neurons are selected by the difference in their boost in gen based on the context neuron\n",
    "# Many other neurons boost gen more per unit of activation\n",
    "\n",
    "cosine_sim = torch.nn.CosineSimilarity(dim=1)\n",
    "answer_residual_direction = model.tokens_to_residual_directions(\"gen\")\n",
    "neuron_weights = model.state_dict()['blocks.5.mlp.W_out'][top_neurons]\n",
    "\n",
    "cosine_sims = cosine_sim(neuron_weights, answer_residual_direction.unsqueeze(0))\n",
    "print(cosine_sims)\n",
    "\n",
    "cosine_sim = torch.nn.CosineSimilarity(dim=1)\n",
    "answer_residual_direction = model.tokens_to_residual_directions(\"gen\")\n",
    "neuron_weights = model.state_dict()['blocks.5.mlp.W_out']\n",
    "\n",
    "cosine_sims = cosine_sim(neuron_weights, answer_residual_direction.unsqueeze(0))\n",
    "top, indices = torch.topk(cosine_sims, 20)\n",
    "print(top)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "076679b2cd8646699e07870a6bc340b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8dc280a18ef4940b37b09aa1c07edc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8497acf208049ec9b8586102d9a0092",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "haystack_utils.clean_cache()\n",
    "english_activations_l5 = {}\n",
    "german_activations_l5 = {}\n",
    "all_activations_l5 = {}\n",
    "for layer in range(5, 6):\n",
    "    english_activations_l5 = get_mlp_activations(english_data, layer, model, mean=True)\n",
    "    german_activations_l5 = get_mlp_activations(german_data, layer, model, mean=True)\n",
    "    all_activations_l5 = get_mlp_activations(german_data[:50] + english_data[:150], layer, model, mean=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f4a233968014ed6987d959b27f5369f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93d3bf2430534efdbdfef807badd0532",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d4652aafc3e4fdd86877ac40b48f56d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import einops\n",
    "\n",
    "W_out = model.state_dict()['blocks.5.mlp.W_out']\n",
    "W_U = model.W_U\n",
    "avg_W_out_all = einops.einsum(all_activations_l5, W_out, 'd_mlp, d_mlp d_model -> d_mlp d_model')\n",
    "avg_mlp5_boosts_all = einops.einsum(avg_W_out_all, W_U, 'd_mlp d_model, d_model d_vocab -> d_mlp d_vocab')\n",
    "avg_W_out_enabled = einops.einsum(german_activations_l5, W_out, 'd_mlp, d_mlp d_model -> d_mlp d_model')\n",
    "avg_mlp5_boosts_enabled = einops.einsum(avg_W_out_enabled, W_U, 'd_mlp d_model, d_model d_vocab -> d_mlp d_vocab')\n",
    "avg_W_out_disabled = einops.einsum(english_activations_l5, W_out, 'd_mlp, d_mlp d_model -> d_mlp d_model')\n",
    "avg_mlp5_boosts_disabled = einops.einsum(avg_W_out_disabled, W_U, 'd_mlp d_model, d_model d_vocab -> d_mlp d_vocab')\n",
    "\n",
    "prompt_strs = [model.tokenizer.decode(prompts[i].tolist()) for i in range(prompts.shape[0])]\n",
    "\n",
    "gen_acts_l5_all = get_mlp_activations(prompt_strs, 5, model, mean=True, pos=-2)\n",
    "with model.hooks(deactivate_neurons_fwd_hooks):\n",
    "    gen_acts_l5_disabled = get_mlp_activations(prompt_strs, 5, model, mean=True, pos=-2)\n",
    "with model.hooks(activate_neurons_fwd_hooks):\n",
    "    gen_acts_l5_enabled = get_mlp_activations(prompt_strs, 5, model, mean=True, pos=-2)\n",
    "\n",
    "data = {\n",
    "    # 'neuron index': list(range(2048)),\n",
    "    'cos sim gen': cosine_sims.tolist(),\n",
    "    # 'avg act (enabled)': german_activations_l5.tolist(),\n",
    "    # 'avg act (disabled)': english_activations_l5.tolist(),\n",
    "    'avg act increase enabled': (german_activations_l5 - english_activations_l5).tolist(),\n",
    "    'avg act': all_activations_l5.tolist(),\n",
    "    'avg gen act': gen_acts_l5_all.tolist(),\n",
    "    'avg gen act enabled': gen_acts_l5_enabled.tolist(),\n",
    "    'avg gen act disabled': gen_acts_l5_disabled.tolist(),\n",
    "    'avg boost gen': avg_mlp5_boosts_all[:, gen_index].tolist(),\n",
    "    'avg boost enabled gen': avg_mlp5_boosts_enabled[:, gen_index].tolist(),\n",
    "    'avg boost disabled gen': avg_mlp5_boosts_disabled[:, gen_index].tolist()\n",
    "    # 'Loss change when ablated': \n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "# Line plot of each neuron's gen boost with dotted vertical lines where the German context neuron-boosted ones are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' and', ' creating', ' the', ' Sc', 'hen', 'gen', ' area', '.', ' From', ' 2012']\n",
      "[' Application', ' of', ' the', ' Sc', 'hen', 'gen', ' acquis', ' relating', ' to', ' the']\n",
      "[' Bulgaria', ' to', ' the', ' Sc', 'hen', 'gen', ' Area', ',', ' nor', ' the']\n",
      "[' can', ' enter', ' the', ' Sc', 'hen', 'gen', ' area', ' without', ' having', ' to']\n",
      "[' to', ' restrict', ' the', ' Sc', 'hen', 'gen', ' area', ',', ' compatible', ' with']\n"
     ]
    }
   ],
   "source": [
    "for prompt in english_data:\n",
    "    prompt_tokens = model.to_tokens(prompt)\n",
    "    if gen_index in prompt_tokens:\n",
    "        index = prompt_tokens[0].tolist().index(gen_index)\n",
    "        print(model.to_str_tokens(prompt_tokens[0, index-5:index+5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1107/2048 MLP5 neurons generically increase \"gen\", 54.05%\n",
      "461 of these fire more when context neuron enabled, 22.51%\n",
      "646 of these fire less when context neuron enabled, 31.54%\n",
      "\n",
      "279 gen neurons fire on average on gen token with context neuron enabled, 13.62%\n",
      "250 gen neurons fire on average on gen token with context neuron disabled, 12.21%\n",
      "51 gen neurons only fire on average with context neuron enabled, 2.49%\n"
     ]
    }
   ],
   "source": [
    "# Weight analysis\n",
    "gen_df = df[(df['cos sim gen'] > 0)]\n",
    "print(f\"{len(gen_df)}/{len(df)} MLP5 neurons generically increase \\\"gen\\\", {len(gen_df)/len(df)*100:.2f}%\")\n",
    "\n",
    "# Activation analysis\n",
    "# All on German prompts\n",
    "filtered_df = gen_df[(gen_df['avg act increase enabled'] > 0)]\n",
    "print(f\"{len(filtered_df)} of these fire more when context neuron enabled, {len(filtered_df)/len(df)*100:.2f}%\")\n",
    "filtered_df = gen_df[(gen_df['avg act increase enabled'] < 0)]\n",
    "print(f\"{len(filtered_df)} of these fire less when context neuron enabled, {len(filtered_df)/len(df)*100:.2f}%\")\n",
    "\n",
    "filtered_df = gen_df[(gen_df['avg gen act enabled'] >= 0)]\n",
    "print(f\"\\n{len(filtered_df)} gen neurons fire on average on gen token with context neuron enabled, {len(filtered_df)/len(df)*100:.2f}%\")\n",
    "filtered_df = gen_df[(gen_df['avg gen act disabled'] >= 0)]\n",
    "print(len(filtered_df), f\"gen neurons fire on average on gen token with context neuron disabled, {len(filtered_df)/len(df)*100:.2f}%\")\n",
    "\n",
    "\n",
    "filtered_df = gen_df[(gen_df['avg gen act disabled'] <= 0) & (gen_df['avg gen act enabled'] > 0)]\n",
    "print(len(filtered_df), f\"gen neurons only fire on average with context neuron enabled, {len(filtered_df)/len(df)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brainstorm notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to decompose an MLP5 neuron's effect into its boost to the correct logit and its deboost of other logits. I want to discover how these two effects change the log prob.\n",
    "\n",
    "metric like loss reduction vs. token boost\n",
    "\n",
    "~~run direct effect, patch each neuron in top 10 individually, get overall loss reduction from neuron controlled by context neuron (equivalent to logprob increase for correct token)~~\n",
    "decompose loss reduction into two parts:\n",
    "run direct effect, patch the correct token logit boost from the neuron (removing the neuron's other effects), get overall loss reduction from neuron (equivalent to logprob increase for correct token)\n",
    "destructive interference loss reduction = overall loss reduction - loss reduction from correct token logit boost (component of the logprob increase for correct token due to it deboosting incorrect token)\n",
    "\n",
    "Patch the correct token logit boost from the neuron (removing the neuron's other effects).\n",
    "1. Get baseline logprobs for a prompt\n",
    "2. Get difference in logits from activating the neuron under test. Can use get_direct_effects with return_type='logits'.\n",
    "3. Run the model with return_type='logits' and the neuron under test zero ablated. \n",
    "4. Add the correct token logit from step 1 to a copy of the output logits. Convert to logprobs\n",
    "5. Add the incorrect token logits from step 1 to a copy of the output logits. Convert to logprobs\n",
    "6. Compare A. lobprobs with correct answer token logit increase, B. logprobs with incorrect answer token logit increases, and C. baseline logprobs\n",
    "\n",
    "~~If the context neuron gives each neuron a flat boost then if we decompose the resulting flat boost to one MLP5 neuron into a boost to one logit vs. boosts to all other logits it will change the logprobs (first will increase answer probability and second will reduce answer probability). \n",
    "\n",
    "the two resulting log probs at the correct answer token won't add up to the original log probs (?). \n",
    "\n",
    "If the boost is flat the correct percentage decomposition is 1/50000 and 49999/50000? In practice/all other factors being equal\n",
    "\n",
    "New plan:\n",
    "\n",
    "Difference between baseline log prob and neuron log prob?\n",
    "Classify individual neurons by percentage constructive vs destructive by looking at their log probs and summing the incorrect token log probs\n",
    "\n",
    "Correct log prob difference\n",
    "Incorrect log prob difference (summed over every plausible token?)\n",
    "\n",
    "Largest boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the log prob for an incorrect token is significantly lower then that's where the extra probability density on the correct answer is coming from \n",
    "# Constructive interference increases correct token log prob and uniformly decreases other log probs\n",
    "# Destructive interference decreases specific other log probs and uniformly increases other log probs\n",
    " \n",
    "# Most neurons are a mixture of the above\n",
    "# Decompose neurons into what % of their effect is each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original_logits, ablated_logprobs, _, all_MLP5_logprobs = haystack_utils.get_direct_effect(prompts, model, pos=-2, context_ablation_hooks=deactivate_neurons_fwd_hooks, context_activation_hooks=activate_neurons_fwd_hooks, return_type='logprobs')\n",
    "# _, _, _, top_MLP5_ablated_logprobs = haystack_utils.get_direct_effect(prompts, model, pos=-2, context_ablation_hooks=deactivate_neurons_fwd_hooks, context_activation_hooks=activate_neurons_fwd_hooks+ablate_top_neurons_hook, return_type='logprobs')\n",
    "# _, _, _, bottom_MLP5_ablated_logprobs = haystack_utils.get_direct_effect(prompts, model, pos=-2, context_ablation_hooks=deactivate_neurons_fwd_hooks, context_activation_hooks=activate_neurons_fwd_hooks+ablate_bottom_neurons_hook, return_type='logprobs')\n",
    "\n",
    "# bottom_neuron_high_difference_logprobs = (all_MLP5_logprobs - bottom_MLP5_ablated_logprobs)\n",
    "# bottom_neuron_high_difference_logprobs[bottom_neuron_high_difference_logprobs < 0] = 0\n",
    "# bottom_neuron_high_difference_logprobs = bottom_neuron_high_difference_logprobs.mean(0)\n",
    "# bottom_neuron_high_difference_logprobs[all_MLP5_logprobs.mean(0) < -7] = 0\n",
    "\n",
    "# bottom_non_zero_count = (bottom_neuron_high_difference_logprobs > 0).sum()\n",
    "# bottom_neuron_high_difference_logprobs, bottom_indices = haystack_utils.top_k_with_exclude(bottom_neuron_high_difference_logprobs, min(bottom_non_zero_count, 50), all_ignore)\n",
    "# haystack_utils.line(bottom_neuron_high_difference_logprobs.cpu().numpy(), title='Largest positive difference in log probs for tokens when bottom neurons are not ablated', xticks=[model.to_str_tokens([i])[0] for i in bottom_indices])\n",
    "\n",
    "\n",
    "# top_neuron_high_difference_logprobs = (all_MLP5_logprobs - top_MLP5_ablated_logprobs)\n",
    "# top_neuron_high_difference_logprobs[top_neuron_high_difference_logprobs < 0] = 0\n",
    "# top_neuron_high_difference_logprobs = top_neuron_high_difference_logprobs.mean(0)\n",
    "# top_neuron_high_difference_logprobs[all_MLP5_logprobs.mean(0) < -7] = 0\n",
    "\n",
    "# top_non_zero_count = (top_neuron_high_difference_logprobs > 0).sum()\n",
    "# top_neuron_high_difference_logprobs, top_indices = haystack_utils.top_k_with_exclude(top_neuron_high_difference_logprobs, min(top_non_zero_count, 50), all_ignore)\n",
    "# haystack_utils.line(top_neuron_high_difference_logprobs.cpu().numpy(), title='Largest positive difference in log probs for tokens when bottom neurons are not ablated', xticks=[model.to_str_tokens([i])[0] for i in top_indices])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
