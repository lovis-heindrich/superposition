{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from transformer_lens import HookedTransformer, ActivationCache, utils, patching\n",
    "from jaxtyping import Float, Int, Bool\n",
    "from torch import Tensor\n",
    "from tqdm.auto import tqdm\n",
    "import plotly.io as pio\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "pio.renderers.default = \"notebook_connected\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.autograd.set_grad_enabled(False)\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "from haystack_utils import get_mlp_activations\n",
    "import haystack_utils\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model EleutherAI/pythia-70m into HookedTransformer\n",
      "data/german_europarl.json: Loaded 2000 examples with 152 to 2000 characters each.\n",
      "data/english_europarl.json: Loaded 2000 examples with 165 to 2000 characters each.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c06a2a6352543f690e06227c703df43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a703cf57f9a4851b00bd190f81176c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b10d95d9fb2f455982606658a1a53f26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc3dadee977b48b7a01462df9a92625e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bc54e42bc154aabac5799f5fa868111",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58bb3635164441ee84a90cb19c0ce825",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = HookedTransformer.from_pretrained(\"EleutherAI/pythia-70m\",\n",
    "    center_unembed=True,\n",
    "    center_writing_weights=True,\n",
    "    fold_ln=True,\n",
    "    device=device)\n",
    "\n",
    "german_data = haystack_utils.load_json_data(\"data/german_europarl.json\")\n",
    "english_data = haystack_utils.load_json_data(\"data/english_europarl.json\")\n",
    "\n",
    "\n",
    "english_activations = {}\n",
    "german_activations = {}\n",
    "for layer in range(3, 6):\n",
    "    english_activations[layer] = get_mlp_activations(english_data[:200], layer, model, mean=False)\n",
    "    german_activations[layer] = get_mlp_activations(german_data[:200], layer, model, mean=False)\n",
    "\n",
    "LAYER_TO_ABLATE = 3\n",
    "NEURONS_TO_ABLATE = [669]\n",
    "MEAN_ACTIVATION_ACTIVE = german_activations[LAYER_TO_ABLATE][:, NEURONS_TO_ABLATE].mean()\n",
    "MEAN_ACTIVATION_INACTIVE = english_activations[LAYER_TO_ABLATE][:, NEURONS_TO_ABLATE].mean()\n",
    "\n",
    "def deactivate_neurons_hook(value, hook):\n",
    "    value[:, :, NEURONS_TO_ABLATE] = MEAN_ACTIVATION_INACTIVE\n",
    "    return value\n",
    "deactivate_neurons_fwd_hooks=[(f'blocks.{LAYER_TO_ABLATE}.mlp.hook_post', deactivate_neurons_hook)]\n",
    "\n",
    "def activate_neurons_hook(value, hook):\n",
    "    value[:, :, NEURONS_TO_ABLATE] = MEAN_ACTIVATION_ACTIVE\n",
    "    return value\n",
    "activate_neurons_fwd_hooks=[(f'blocks.{LAYER_TO_ABLATE}.mlp.hook_post', activate_neurons_hook)]\n",
    "\n",
    "all_ignore, not_ignore = haystack_utils.get_weird_tokens(model, plot_norms=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find top common German tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b95f047240ad46f7b7869e6713b24c30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' der', 'en', ' die', ' und', 'ung', 'ä', ' in', ' den', ' des', 'ch', 'st', ' zu', 're', ' für', 'äsident', ' Pr', 'n', 'z', 'ischen', ' von', 'ü', 't', 'icht', 'in', 'ge', 'gen', 'te', ' ist', ' auf', 'ig', ' über', ' dass', ' im', 'f', ' er', 'es', ' das', 'men', 'g', 'ß', ' Europ', ' w', 'w', 'le', 'ten', ' eine', ' wir', ' ein', ' an', 'hen', 'ren', 'e', ' ich', 'ungen', ' W', ' Ver', ' B', ' dem', ' mit', ' dies', ' nicht', ' Z', 'h', ' z', 's', 'it', 'hr', ' es', ' zur', ' An', ' Herr', 'ich', 'heit', 'b', 'lich', 'l', ' ver', ' S', 'i', ' G', 'Der', ' V', 'der', 'u', 'ie', ' Ab', 'ungs', 'chte', 'chaft', 'igen', ' werden', 'uss', 'ord', 'em', ' Ber', 'ür', ' haben', 'et', ' um', ' Ich']\n"
     ]
    }
   ],
   "source": [
    "# Get top common german tokens excluding punctuation\n",
    "token_counts = torch.zeros(model.cfg.d_vocab).cuda()\n",
    "for example in tqdm(german_data):\n",
    "    tokens = model.to_tokens(example)\n",
    "    for token in tokens[0]:\n",
    "        token_counts[token.item()] += 1\n",
    "\n",
    "punctuation = [\"\\n\", \".\", \",\", \"!\", \"?\", \";\", \":\", \"-\", \"(\", \")\", \"[\", \"]\", \"{\", \"}\", \"<\", \">\", \"/\", \"\\\\\", \"\\\"\", \"'\"]\n",
    "leading_space_punctuation = [\" \" + char for char in punctuation]\n",
    "punctuation_tokens = model.to_tokens(punctuation + leading_space_punctuation + [' –', \" \", '  ', \"<|endoftext|>\"])[:, 1].flatten()\n",
    "token_counts[punctuation_tokens] = 0\n",
    "token_counts[all_ignore] = 0\n",
    "\n",
    "top_counts, top_tokens = torch.topk(token_counts, 100)\n",
    "print(model.to_str_tokens(top_tokens[:100]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of ngrams preceded by random prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_selection(tensor, n=12):\n",
    "    # Hacky replacement for np.random.choice\n",
    "    return tensor[torch.randperm(len(tensor))[:n]]\n",
    "\n",
    "def generate_random_prompts(end_string, n=50, length=12):\n",
    "    # Generate a batch of random prompts ending with a specific ngram\n",
    "    end_tokens = model.to_tokens(end_string).flatten()[1:]\n",
    "    prompts = []\n",
    "    for i in range(n):\n",
    "        prompt = get_random_selection(top_tokens[:max(50, length)], n=length).cuda()\n",
    "        prompt = torch.cat([prompt, end_tokens])\n",
    "        prompts.append(prompt)\n",
    "    prompts = torch.stack(prompts)\n",
    "    return prompts\n",
    "\n",
    "def replace_column(prompts: Int[Tensor, \"n_prompts n_tokens\"], token_index: int):\n",
    "    # Replaces a specific token position in a batch of prompts with random common German tokens\n",
    "    new_prompts = prompts.clone()\n",
    "    random_tokens = get_random_selection(top_tokens[:max(50, prompts.shape[0])], n=prompts.shape[0]).cuda()\n",
    "    new_prompts[:, token_index] = random_tokens\n",
    "    return new_prompts \n",
    "\n",
    "def loss_analysis(prompts: Tensor, title=\"\"):\n",
    "    # Loss plot for a batch of prompts\n",
    "    names = [\"Original\", \"Ablated\", \"MLP5 path patched\"]\n",
    "    original_loss, ablated_loss, _, only_activated_loss = \\\n",
    "        haystack_utils.get_direct_effect(prompts, model, pos=-1,\n",
    "                                        context_ablation_hooks=deactivate_neurons_fwd_hooks, \n",
    "                                        context_activation_hooks=activate_neurons_fwd_hooks, \n",
    "                                        )\n",
    "    haystack_utils.plot_barplot([original_loss.tolist(), ablated_loss.tolist(), only_activated_loss.tolist()], names, ylabel=\"Loss\", title=title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_analysis_random_prompts(end_string, n=50, length=12, replace_columns: list[int] | None = None):\n",
    "    # Loss plot for a batch of random prompts ending with a specific ngram and optionally replacing specific tokens\n",
    "    prompts = generate_random_prompts(end_string, n=n, length=length)\n",
    "    title=f\"Average last token loss on {length} random tokens ending in '{end_string}'\"\n",
    "    if replace_columns is not None:\n",
    "        replaced_tokens = model.to_str_tokens(prompts[0, replace_columns])\n",
    "        title += f\" replacing {replaced_tokens}\"\n",
    "        for column in replace_columns:\n",
    "            prompts = replace_column(prompts, column)\n",
    "    \n",
    "    loss_analysis(prompts, title=title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "# loss_analysis_random_prompts(\" Vorschlägen\", n=100, length=20)\n",
    "# loss_analysis_random_prompts(\" Vorschlägen\", n=100, length=20, replace_columns=[-2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0512b69a856416789606bd31ce46693",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Ngram: ', options=(' Vorschlägen', ' Vorschläge', ' häufig', ' schließt', ' beweglich'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b0f589098ca4dcaa63cc7d523af65b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SelectMultiple(description='Replace Columns:', index=(3,), options=('-2', '-3', '-4', 'None'), value=('None',)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c226fc1ceb4142f184385405bbdf92c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a dropdown menu widget\n",
    "dropdown = widgets.Dropdown(\n",
    "    options = [\" Vorschlägen\", \" Vorschläge\", \" häufig\", \" schließt\", \" beweglich\"], \n",
    "    value = \" Vorschlägen\",\n",
    "    description = 'Ngram: ',\n",
    ")\n",
    "\n",
    "replace_columns_dropdown = widgets.SelectMultiple(\n",
    "    options = ['-2', '-3', '-4', 'None'],\n",
    "    value = ['None'],  # default selected value\n",
    "    description = 'Replace Columns:',\n",
    ")\n",
    "\n",
    "# Create an output widget to hold the plot\n",
    "output = widgets.Output()\n",
    "\n",
    "# Define a function to call when the widget's value changes\n",
    "def update_plot(*args):\n",
    "    # Clear the old plot from the output\n",
    "    with output:\n",
    "        clear_output(wait=True)\n",
    "\n",
    "        if 'None' in replace_columns_dropdown.value:\n",
    "            replace_columns = None\n",
    "        else:\n",
    "            # If 'None' not selected, convert the selected values to integers\n",
    "            replace_columns = [int(val) for val in replace_columns_dropdown.value]\n",
    "        \n",
    "        # Call your function with the values from the widgets\n",
    "        loss_analysis_random_prompts(dropdown.value, n=100, length=20, replace_columns=replace_columns)\n",
    "\n",
    "# Set the function to be called when the widget's value changes\n",
    "dropdown.observe(update_plot, 'value')\n",
    "replace_columns_dropdown.observe(update_plot, 'value')\n",
    "\n",
    "# Display the widget and the output\n",
    "display(dropdown, replace_columns_dropdown, output)\n",
    "\n",
    "# Run once at startup\n",
    "update_plot()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify relevant neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 24])\n"
     ]
    }
   ],
   "source": [
    "prompts = generate_random_prompts(\" Vorschlägen\", n=100, length=20)\n",
    "print(prompts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eee9430a4fb54594b3d6c6c7944e16d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2048 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0009)\n"
     ]
    }
   ],
   "source": [
    "# Calculate neuron-wise loss change\n",
    "with model.hooks(deactivate_neurons_fwd_hooks):\n",
    "    _, ablated_cache = model.run_with_cache(prompts)\n",
    "\n",
    "def get_ablate_neurons_hook(neuron: int | list[int], ablated_cache):\n",
    "    def ablate_neurons_hook(value, hook):\n",
    "        value[:, :, neuron] = ablated_cache['blocks.5.mlp.hook_post'][:, :, neuron]\n",
    "        return value\n",
    "    return [('blocks.5.mlp.hook_post', ablate_neurons_hook)]\n",
    "\n",
    "diffs = torch.zeros(2048, prompts.shape[0])\n",
    "# Loss with path patched MLP5 neurons\n",
    "_, _, _, baseline_loss = haystack_utils.get_direct_effect(prompts, model, pos=-1, context_ablation_hooks=deactivate_neurons_fwd_hooks, context_activation_hooks=activate_neurons_fwd_hooks)\n",
    "for neuron in tqdm(range(2048)):\n",
    "    ablate_single_neuron_hook = get_ablate_neurons_hook(neuron, ablated_cache)\n",
    "    # Loss with path patched MLP5 neurons but a single neuron changed back to original ablated value\n",
    "    _, _, _, only_activated_loss = haystack_utils.get_direct_effect(prompts, model, pos=-1, context_ablation_hooks=deactivate_neurons_fwd_hooks, context_activation_hooks=activate_neurons_fwd_hooks+ablate_single_neuron_hook)\n",
    "    diffs[neuron] = only_activated_loss - baseline_loss\n",
    "\n",
    "print(diffs.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>                            <div id=\"cfa933b7-70d1-4451-8da8-37c32a3b7408\" class=\"plotly-graph-div\" style=\"height:525px; width:800px;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"cfa933b7-70d1-4451-8da8-37c32a3b7408\")) {                    Plotly.newPlot(                        \"cfa933b7-70d1-4451-8da8-37c32a3b7408\",                        [{\"hovertemplate\":\"variable=0\\u003cbr\\u003eindex=%{x}\\u003cbr\\u003evalue=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"0\",\"line\":{\"color\":\"#636efa\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"0\",\"showlegend\":true,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542,543,544,545,546,547,548,549,550,551,552,553,554,555,556,557,558,559,560,561,562,563,564,565,566,567,568,569,570,571,572,573,574,575,576,577,578,579,580,581,582,583,584,585,586,587,588,589,590,591,592,593,594,595,596,597,598,599,600,601,602,603,604,605,606,607,608,609,610,611,612,613,614,615,616,617,618,619,620,621,622,623,624,625,626,627,628,629,630,631,632,633,634,635,636,637,638,639,640,641,642,643,644,645,646,647,648,649,650,651,652,653,654,655,656,657,658,659,660,661,662,663,664,665,666,667,668,669,670,671,672,673,674,675,676,677,678,679,680,681,682,683,684,685,686,687,688,689,690,691,692,693,694,695,696,697,698,699,700,701,702,703,704,705,706,707,708,709,710,711,712,713,714,715,716,717,718,719,720,721,722,723,724,725,726,727,728,729,730,731,732,733,734,735,736,737,738,739,740,741,742,743,744,745,746,747,748,749,750,751,752,753,754,755,756,757,758,759,760,761,762,763,764,765,766,767,768,769,770,771,772,773,774,775,776,777,778,779,780,781,782,783,784,785,786,787,788,789,790,791,792,793,794,795,796,797,798,799,800,801,802,803,804,805,806,807,808,809,810,811,812,813,814,815,816,817,818,819,820,821,822,823,824,825,826,827,828,829,830,831,832,833,834,835,836,837,838,839,840,841,842,843,844,845,846,847,848,849,850,851,852,853,854,855,856,857,858,859,860,861,862,863,864,865,866,867,868,869,870,871,872,873,874,875,876,877,878,879,880,881,882,883,884,885,886,887,888,889,890,891,892,893,894,895,896,897,898,899,900,901,902,903,904,905,906,907,908,909,910,911,912,913,914,915,916,917,918,919,920,921,922,923,924,925,926,927,928,929,930,931,932,933,934,935,936,937,938,939,940,941,942,943,944,945,946,947,948,949,950,951,952,953,954,955,956,957,958,959,960,961,962,963,964,965,966,967,968,969,970,971,972,973,974,975,976,977,978,979,980,981,982,983,984,985,986,987,988,989,990,991,992,993,994,995,996,997,998,999,1000,1001,1002,1003,1004,1005,1006,1007,1008,1009,1010,1011,1012,1013,1014,1015,1016,1017,1018,1019,1020,1021,1022,1023,1024,1025,1026,1027,1028,1029,1030,1031,1032,1033,1034,1035,1036,1037,1038,1039,1040,1041,1042,1043,1044,1045,1046,1047,1048,1049,1050,1051,1052,1053,1054,1055,1056,1057,1058,1059,1060,1061,1062,1063,1064,1065,1066,1067,1068,1069,1070,1071,1072,1073,1074,1075,1076,1077,1078,1079,1080,1081,1082,1083,1084,1085,1086,1087,1088,1089,1090,1091,1092,1093,1094,1095,1096,1097,1098,1099,1100,1101,1102,1103,1104,1105,1106,1107,1108,1109,1110,1111,1112,1113,1114,1115,1116,1117,1118,1119,1120,1121,1122,1123,1124,1125,1126,1127,1128,1129,1130,1131,1132,1133,1134,1135,1136,1137,1138,1139,1140,1141,1142,1143,1144,1145,1146,1147,1148,1149,1150,1151,1152,1153,1154,1155,1156,1157,1158,1159,1160,1161,1162,1163,1164,1165,1166,1167,1168,1169,1170,1171,1172,1173,1174,1175,1176,1177,1178,1179,1180,1181,1182,1183,1184,1185,1186,1187,1188,1189,1190,1191,1192,1193,1194,1195,1196,1197,1198,1199,1200,1201,1202,1203,1204,1205,1206,1207,1208,1209,1210,1211,1212,1213,1214,1215,1216,1217,1218,1219,1220,1221,1222,1223,1224,1225,1226,1227,1228,1229,1230,1231,1232,1233,1234,1235,1236,1237,1238,1239,1240,1241,1242,1243,1244,1245,1246,1247,1248,1249,1250,1251,1252,1253,1254,1255,1256,1257,1258,1259,1260,1261,1262,1263,1264,1265,1266,1267,1268,1269,1270,1271,1272,1273,1274,1275,1276,1277,1278,1279,1280,1281,1282,1283,1284,1285,1286,1287,1288,1289,1290,1291,1292,1293,1294,1295,1296,1297,1298,1299,1300,1301,1302,1303,1304,1305,1306,1307,1308,1309,1310,1311,1312,1313,1314,1315,1316,1317,1318,1319,1320,1321,1322,1323,1324,1325,1326,1327,1328,1329,1330,1331,1332,1333,1334,1335,1336,1337,1338,1339,1340,1341,1342,1343,1344,1345,1346,1347,1348,1349,1350,1351,1352,1353,1354,1355,1356,1357,1358,1359,1360,1361,1362,1363,1364,1365,1366,1367,1368,1369,1370,1371,1372,1373,1374,1375,1376,1377,1378,1379,1380,1381,1382,1383,1384,1385,1386,1387,1388,1389,1390,1391,1392,1393,1394,1395,1396,1397,1398,1399,1400,1401,1402,1403,1404,1405,1406,1407,1408,1409,1410,1411,1412,1413,1414,1415,1416,1417,1418,1419,1420,1421,1422,1423,1424,1425,1426,1427,1428,1429,1430,1431,1432,1433,1434,1435,1436,1437,1438,1439,1440,1441,1442,1443,1444,1445,1446,1447,1448,1449,1450,1451,1452,1453,1454,1455,1456,1457,1458,1459,1460,1461,1462,1463,1464,1465,1466,1467,1468,1469,1470,1471,1472,1473,1474,1475,1476,1477,1478,1479,1480,1481,1482,1483,1484,1485,1486,1487,1488,1489,1490,1491,1492,1493,1494,1495,1496,1497,1498,1499,1500,1501,1502,1503,1504,1505,1506,1507,1508,1509,1510,1511,1512,1513,1514,1515,1516,1517,1518,1519,1520,1521,1522,1523,1524,1525,1526,1527,1528,1529,1530,1531,1532,1533,1534,1535,1536,1537,1538,1539,1540,1541,1542,1543,1544,1545,1546,1547,1548,1549,1550,1551,1552,1553,1554,1555,1556,1557,1558,1559,1560,1561,1562,1563,1564,1565,1566,1567,1568,1569,1570,1571,1572,1573,1574,1575,1576,1577,1578,1579,1580,1581,1582,1583,1584,1585,1586,1587,1588,1589,1590,1591,1592,1593,1594,1595,1596,1597,1598,1599,1600,1601,1602,1603,1604,1605,1606,1607,1608,1609,1610,1611,1612,1613,1614,1615,1616,1617,1618,1619,1620,1621,1622,1623,1624,1625,1626,1627,1628,1629,1630,1631,1632,1633,1634,1635,1636,1637,1638,1639,1640,1641,1642,1643,1644,1645,1646,1647,1648,1649,1650,1651,1652,1653,1654,1655,1656,1657,1658,1659,1660,1661,1662,1663,1664,1665,1666,1667,1668,1669,1670,1671,1672,1673,1674,1675,1676,1677,1678,1679,1680,1681,1682,1683,1684,1685,1686,1687,1688,1689,1690,1691,1692,1693,1694,1695,1696,1697,1698,1699,1700,1701,1702,1703,1704,1705,1706,1707,1708,1709,1710,1711,1712,1713,1714,1715,1716,1717,1718,1719,1720,1721,1722,1723,1724,1725,1726,1727,1728,1729,1730,1731,1732,1733,1734,1735,1736,1737,1738,1739,1740,1741,1742,1743,1744,1745,1746,1747,1748,1749,1750,1751,1752,1753,1754,1755,1756,1757,1758,1759,1760,1761,1762,1763,1764,1765,1766,1767,1768,1769,1770,1771,1772,1773,1774,1775,1776,1777,1778,1779,1780,1781,1782,1783,1784,1785,1786,1787,1788,1789,1790,1791,1792,1793,1794,1795,1796,1797,1798,1799,1800,1801,1802,1803,1804,1805,1806,1807,1808,1809,1810,1811,1812,1813,1814,1815,1816,1817,1818,1819,1820,1821,1822,1823,1824,1825,1826,1827,1828,1829,1830,1831,1832,1833,1834,1835,1836,1837,1838,1839,1840,1841,1842,1843,1844,1845,1846,1847,1848,1849,1850,1851,1852,1853,1854,1855,1856,1857,1858,1859,1860,1861,1862,1863,1864,1865,1866,1867,1868,1869,1870,1871,1872,1873,1874,1875,1876,1877,1878,1879,1880,1881,1882,1883,1884,1885,1886,1887,1888,1889,1890,1891,1892,1893,1894,1895,1896,1897,1898,1899,1900,1901,1902,1903,1904,1905,1906,1907,1908,1909,1910,1911,1912,1913,1914,1915,1916,1917,1918,1919,1920,1921,1922,1923,1924,1925,1926,1927,1928,1929,1930,1931,1932,1933,1934,1935,1936,1937,1938,1939,1940,1941,1942,1943,1944,1945,1946,1947,1948,1949,1950,1951,1952,1953,1954,1955,1956,1957,1958,1959,1960,1961,1962,1963,1964,1965,1966,1967,1968,1969,1970,1971,1972,1973,1974,1975,1976,1977,1978,1979,1980,1981,1982,1983,1984,1985,1986,1987,1988,1989,1990,1991,1992,1993,1994,1995,1996,1997,1998,1999,2000,2001,2002,2003,2004,2005,2006,2007,2008,2009,2010,2011,2012,2013,2014,2015,2016,2017,2018,2019,2020,2021,2022,2023,2024,2025,2026,2027,2028,2029,2030,2031,2032,2033,2034,2035,2036,2037,2038,2039,2040,2041,2042,2043,2044,2045,2046,2047],\"xaxis\":\"x\",\"y\":[-0.6110277771949768,-0.33639249205589294,-0.23184232413768768,-0.17237840592861176,-0.16838380694389343,-0.1618579626083374,-0.1499626785516739,-0.14876586198806763,-0.128236785531044,-0.12536658346652985,-0.12448274344205856,-0.11082892119884491,-0.10788564383983612,-0.10342591255903244,-0.10166553407907486,-0.09276355057954788,-0.09186997264623642,-0.08913391828536987,-0.08781980723142624,-0.087476447224617,-0.08697684109210968,-0.0850667953491211,-0.0819147601723671,-0.0783725455403328,-0.07459580153226852,-0.07441029697656631,-0.07315981388092041,-0.07256622612476349,-0.07209320366382599,-0.0711388811469078,-0.06985580176115036,-0.06983023881912231,-0.06711513549089432,-0.06657550483942032,-0.06648511439561844,-0.06081639230251312,-0.06074821576476097,-0.060730837285518646,-0.06032514572143555,-0.06018679216504097,-0.059910472482442856,-0.05785972252488136,-0.05770157650113106,-0.05761045590043068,-0.056803420186042786,-0.05649493262171745,-0.05542770400643349,-0.05454441159963608,-0.053697455674409866,-0.053318921476602554,-0.05223800241947174,-0.05090068653225899,-0.05000655725598335,-0.04899446666240692,-0.048255063593387604,-0.047483161091804504,-0.043808989226818085,-0.042905911803245544,-0.04274607077240944,-0.041829999536275864,-0.041748009622097015,-0.04171083867549896,-0.041660331189632416,-0.040367431938648224,-0.039160534739494324,-0.03902553766965866,-0.038888417184352875,-0.037637244910001755,-0.03724053502082825,-0.03709312900900841,-0.036934688687324524,-0.03686108812689781,-0.03665182739496231,-0.036551591008901596,-0.03638368472456932,-0.03608906641602516,-0.03593021631240845,-0.03570280596613884,-0.03507639095187187,-0.034863367676734924,-0.03352884575724602,-0.03252580016851425,-0.031665489077568054,-0.03154939040541649,-0.031302738934755325,-0.031107859686017036,-0.03096197545528412,-0.030483728274703026,-0.030338119715452194,-0.03023119457066059,-0.03009137697517872,-0.030073031783103943,-0.030053725466132164,-0.029920544475317,-0.029573319479823112,-0.02901836484670639,-0.028950106352567673,-0.028873585164546967,-0.02865203656256199,-0.028424344956874847,-0.02842049114406109,-0.028357649222016335,-0.028036464005708694,-0.027639009058475494,-0.027390573173761368,-0.027248291298747063,-0.027082180604338646,-0.027039969339966774,-0.026581816375255585,-0.026531200855970383,-0.025962214916944504,-0.025935931131243706,-0.025666477158665657,-0.025381775572896004,-0.025248564779758453,-0.024630935862660408,-0.02460155263543129,-0.024539776146411896,-0.024447375908493996,-0.023991238325834274,-0.023899517953395844,-0.023851538076996803,-0.02309569902718067,-0.02297651581466198,-0.02280152402818203,-0.022747598588466644,-0.022513756528496742,-0.022357475012540817,-0.02214977517724037,-0.021344292908906937,-0.02113509364426136,-0.02097337320446968,-0.02095402218401432,-0.02088048867881298,-0.020723232999444008,-0.020147468894720078,-0.020142359659075737,-0.020057683810591698,-0.019681520760059357,-0.019424546509981155,-0.019367720931768417,-0.019171040505170822,-0.019084803760051727,-0.018902206793427467,-0.018873924389481544,-0.01883484423160553,-0.018673501908779144,-0.01864764653146267,-0.018126215785741806,-0.018096428364515305,-0.01799304038286209,-0.01796082966029644,-0.017804842442274094,-0.01777757704257965,-0.017695901915431023,-0.017629927024245262,-0.017208006232976913,-0.016935400664806366,-0.016864042729139328,-0.016622429713606834,-0.016518540680408478,-0.016481027007102966,-0.016318747773766518,-0.016214143484830856,-0.01619708724319935,-0.016143176704645157,-0.01613847352564335,-0.015863435342907906,-0.01585734449326992,-0.015835998579859734,-0.01581769622862339,-0.015746237710118294,-0.01570659689605236,-0.015292266383767128,-0.015177054330706596,-0.015158346854150295,-0.014522472396492958,-0.014507847838103771,-0.014390634372830391,-0.013983828946948051,-0.013756471686065197,-0.01361510157585144,-0.013555396348237991,-0.013495358638465405,-0.013424878939986229,-0.013406063430011272,-0.013252134434878826,-0.013246127404272556,-0.013231132179498672,-0.01321814302355051,-0.013044256716966629,-0.013027758337557316,-0.013027298264205456,-0.013023794628679752,-0.013016318902373314,-0.012928548268973827,-0.012821298092603683,-0.01276999432593584,-0.012701260857284069,-0.012671643868088722,-0.012306916527450085,-0.012283915653824806,-0.012279875576496124,-0.012160254642367363,-0.012091007083654404,-0.012083364650607109,-0.01201761607080698,-0.011905626393854618,-0.011764551512897015,-0.011705807410180569,-0.011693690903484821,-0.011488310992717743,-0.01146386843174696,-0.011349625885486603,-0.011340371333062649,-0.011338043957948685,-0.011286672204732895,-0.01125375833362341,-0.011221427470445633,-0.011136902496218681,-0.011088751256465912,-0.010897341184318066,-0.010884207673370838,-0.010782051831483841,-0.010572142899036407,-0.010427255183458328,-0.01037436630576849,-0.010330935940146446,-0.010286842472851276,-0.010279416106641293,-0.010231287218630314,-0.010222259908914566,-0.01003488339483738,-0.009888570755720139,-0.009808734990656376,-0.009807119145989418,-0.009740384295582771,-0.009711273945868015,-0.009658528491854668,-0.009398728609085083,-0.009374583140015602,-0.009359530173242092,-0.009349498897790909,-0.009161438792943954,-0.009106211364269257,-0.008970528841018677,-0.008964646607637405,-0.008963722735643387,-0.008896232582628727,-0.008877472952008247,-0.008850100450217724,-0.008844545111060143,-0.008827784098684788,-0.008801707997918129,-0.008763250894844532,-0.008708082139492035,-0.008599016815423965,-0.008311381563544273,-0.008264272473752499,-0.008238423615694046,-0.008232258260250092,-0.008130570873618126,-0.008120210841298103,-0.008092207834124565,-0.008024713955819607,-0.007946802303195,-0.007934332825243473,-0.007891068235039711,-0.007888000458478928,-0.00785956159234047,-0.007705507334321737,-0.007693133316934109,-0.007645838428288698,-0.007629888132214546,-0.007628978695720434,-0.0075515746138989925,-0.007523140870034695,-0.007510763593018055,-0.007472245022654533,-0.007380706258118153,-0.007311439141631126,-0.007306767161935568,-0.0072922660037875175,-0.007274925708770752,-0.007247164845466614,-0.007220796309411526,-0.0072164274752140045,-0.007177421823143959,-0.007096877787262201,-0.007058213464915752,-0.007053699344396591,-0.007021171506494284,-0.0070181954652071,-0.007012279238551855,-0.007004818879067898,-0.006984387058764696,-0.0069778538309037685,-0.006951467599719763,-0.006785222329199314,-0.006721435580402613,-0.006719526834785938,-0.0066328467801213264,-0.00662073353305459,-0.006591436918824911,-0.006538397166877985,-0.006537296809256077,-0.006501481402665377,-0.0064800274558365345,-0.0064493510872125626,-0.006446910090744495,-0.006442300044000149,-0.006437754724174738,-0.0064298128709197044,-0.006368527188897133,-0.0063626025803387165,-0.006362506654113531,-0.006350735202431679,-0.006269610021263361,-0.006163181271404028,-0.0061592115089297295,-0.00615491159260273,-0.006117725279182196,-0.006110197398811579,-0.006079899147152901,-0.006073936354368925,-0.006060110405087471,-0.005978442262858152,-0.005939512047916651,-0.005936867091804743,-0.005918446928262711,-0.0059172916226089,-0.005891293287277222,-0.005875981412827969,-0.005868596024811268,-0.005849416367709637,-0.005808169953525066,-0.0058060516603291035,-0.005748296156525612,-0.005721311550587416,-0.005717984400689602,-0.005712010897696018,-0.005707012955099344,-0.005668291822075844,-0.0056527829729020596,-0.005631575360894203,-0.00561328511685133,-0.005566663574427366,-0.005564825609326363,-0.005492778029292822,-0.005461063235998154,-0.005448209121823311,-0.005434070713818073,-0.005413516890257597,-0.005397362168878317,-0.0053917597979307175,-0.0053282855078577995,-0.005305394995957613,-0.0052611567080020905,-0.005216449964791536,-0.005203840788453817,-0.005202163010835648,-0.005196862388402224,-0.005184344481676817,-0.005084437783807516,-0.00508101936429739,-0.005025753751397133,-0.005022849887609482,-0.005000678356736898,-0.004998263902962208,-0.004966654349118471,-0.004964745137840509,-0.004952881019562483,-0.00490608112886548,-0.004851404577493668,-0.004850432742387056,-0.004842759110033512,-0.0047876713797450066,-0.004734797403216362,-0.004733555018901825,-0.004730271641165018,-0.00471055181697011,-0.004700430203229189,-0.004660012200474739,-0.004624886438250542,-0.004593722987920046,-0.004576073493808508,-0.0045011271722614765,-0.004492622800171375,-0.004480751231312752,-0.004475962370634079,-0.0044494615867733955,-0.004424227401614189,-0.004379871301352978,-0.0043761697597801685,-0.004360038787126541,-0.004352058749645948,-0.004321377724409103,-0.004305964335799217,-0.00428040511906147,-0.004258701112121344,-0.004240519367158413,-0.004207979422062635,-0.004206954035907984,-0.004202962387353182,-0.004124288447201252,-0.004111864138394594,-0.0041106026619672775,-0.004092859104275703,-0.004018660169094801,-0.003982270136475563,-0.003971008583903313,-0.003949706442654133,-0.003932079765945673,-0.0039261155761778355,-0.003916149493306875,-0.003916038665920496,-0.003903924021869898,-0.0038930478040128946,-0.003829071531072259,-0.0038157214876264334,-0.0037704238202422857,-0.0037486853543668985,-0.0037459733430296183,-0.0037285825237631798,-0.003728117560967803,-0.0037240406963974237,-0.0037174206227064133,-0.003706138813868165,-0.003692404367029667,-0.0036898511461913586,-0.0036534187383949757,-0.0036472443025559187,-0.0036460133269429207,-0.0036387674044817686,-0.0036330721341073513,-0.0035671645309776068,-0.0035641680005937815,-0.0035496260970830917,-0.0035420479252934456,-0.003535559866577387,-0.003533089067786932,-0.003527737455442548,-0.0035183646250516176,-0.003513866337016225,-0.003453461453318596,-0.003418757114559412,-0.0034127505496144295,-0.003355323104187846,-0.003349282080307603,-0.003335033543407917,-0.003329463070258498,-0.0033085623290389776,-0.0032961564138531685,-0.0032731276005506516,-0.0032700428273528814,-0.003259691409766674,-0.003258290933445096,-0.0032469546422362328,-0.0032366884406656027,-0.0032340085599571466,-0.0032207558397203684,-0.0032081117387861013,-0.003202877938747406,-0.003191600553691387,-0.003173370147123933,-0.0031732888892292976,-0.0031404895707964897,-0.0031228074803948402,-0.003113222774118185,-0.003110741265118122,-0.003065185621380806,-0.0030613101553171873,-0.003057147841900587,-0.0030432483181357384,-0.003028334118425846,-0.0030235510785132647,-0.0030085488688200712,-0.003008490428328514,-0.0030048007611185312,-0.0029653930105268955,-0.0029484746046364307,-0.002938594901934266,-0.0029155209194868803,-0.0029094871133565903,-0.0028799623250961304,-0.002851900178939104,-0.002839419525116682,-0.002834745915606618,-0.0028161488007754087,-0.0028080346528440714,-0.0027969228103756905,-0.0027884577866643667,-0.00277834665030241,-0.002746135927736759,-0.0027458793483674526,-0.0027446916792541742,-0.002730234060436487,-0.002715010428801179,-0.0027095519471913576,-0.0027092110831290483,-0.0027053984813392162,-0.002701002871617675,-0.002673777285963297,-0.0026672023814171553,-0.002634789329022169,-0.0024992439430207014,-0.0024964110925793648,-0.0024936944246292114,-0.0024806184228509665,-0.002460553077980876,-0.0024577295407652855,-0.002456254092976451,-0.0024344485718756914,-0.0024176626466214657,-0.002413555048406124,-0.002375964540988207,-0.002363733248785138,-0.002334538148716092,-0.002334193093702197,-0.0023268256336450577,-0.002325975801795721,-0.0023060610983520746,-0.0022993606980890036,-0.0022991697769612074,-0.002291069831699133,-0.002287269337102771,-0.0022819573059678078,-0.0022778245620429516,-0.0022774639073759317,-0.002264568815007806,-0.002261796500533819,-0.0022397013381123543,-0.0022373576648533344,-0.0022210758179426193,-0.0022040260955691338,-0.002202412346377969,-0.0021757895592600107,-0.0021740139927715063,-0.0021632518619298935,-0.002162967575713992,-0.0021611922420561314,-0.0021581361070275307,-0.002152618719264865,-0.0021393403876572847,-0.0021269177086651325,-0.0021228324621915817,-0.0021190426778048277,-0.0021143234334886074,-0.0021114402916282415,-0.0021079317666590214,-0.002102107973769307,-0.002101094229146838,-0.0020997158717364073,-0.002075798576697707,-0.0020647041965276003,-0.002057552570477128,-0.0020501986145973206,-0.0020449883304536343,-0.0020447969436645508,-0.0020444924011826515,-0.00203744787722826,-0.0020328930113464594,-0.0020243097096681595,-0.0020218235440552235,-0.002020572777837515,-0.0020086588338017464,-0.002006521914154291,-0.0020056816283613443,-0.0019964987877756357,-0.001990985358133912,-0.001989843789488077,-0.001980792498216033,-0.0019771624356508255,-0.001975522143766284,-0.0019711179193109274,-0.001969629433006048,-0.0019663956481963396,-0.0019522941438481212,-0.0019427640363574028,-0.0019381747115403414,-0.0019089679699391127,-0.0018909887876361609,-0.001884689787402749,-0.0018764713313430548,-0.0018689221469685435,-0.0018560081953182817,-0.001853685942478478,-0.0018130914540961385,-0.0018054955871775746,-0.0017970545450225472,-0.001793371862731874,-0.0017887818394228816,-0.001751262228935957,-0.0017381578218191862,-0.0017267875373363495,-0.0017092719208449125,-0.0016964566893875599,-0.001686735195107758,-0.0016799086006358266,-0.0016608398873358965,-0.0016586300916969776,-0.0016546749975532293,-0.0016442490741610527,-0.0016032392159104347,-0.0016004618955776095,-0.0015964030753821135,-0.0015902668237686157,-0.001590064144693315,-0.001589653897099197,-0.0015686751576140523,-0.0015297202626243234,-0.0015283948741853237,-0.001525443745777011,-0.0015190389240160584,-0.0015085646882653236,-0.0014836686896160245,-0.001476240810006857,-0.0014757364988327026,-0.00146459077950567,-0.0014607468619942665,-0.0014583111042156816,-0.0014576860703527927,-0.0014528471510857344,-0.001441465923562646,-0.0014406803529709578,-0.0014339827466756105,-0.0014108854811638594,-0.00140958814881742,-0.0014040038222447038,-0.0014011316234245896,-0.0013967736158519983,-0.0013749313075095415,-0.0013637738302350044,-0.0013456774177029729,-0.0013439527247101068,-0.0013434523716568947,-0.0013288500485941768,-0.0013236583909019828,-0.0013114054454490542,-0.0012998575111851096,-0.0012955747079104185,-0.0012896894477307796,-0.001285781618207693,-0.0012820616830140352,-0.001238062046468258,-0.0012352627236396074,-0.0012336836662143469,-0.0012336112558841705,-0.0012271604500710964,-0.0012224711244925857,-0.0012187070678919554,-0.001215908327139914,-0.0012053379323333502,-0.0012052698293700814,-0.0012004956370219588,-0.0012002381263300776,-0.0012001075083389878,-0.0011845086701214314,-0.0011831417214125395,-0.0011768211843445897,-0.001172636286355555,-0.0011589901987463236,-0.0011541354469954967,-0.0011537829414010048,-0.0011512875789776444,-0.00114240194670856,-0.0011375839821994305,-0.0011373902671039104,-0.0011339769698679447,-0.001128596835769713,-0.001122480258345604,-0.001113880891352892,-0.0010992817115038633,-0.0010924675734713674,-0.0010913885198533535,-0.001091227401047945,-0.0010871178237721324,-0.0010852840496227145,-0.0010828047525137663,-0.0010680754203349352,-0.001065518008545041,-0.0010542185045778751,-0.0010532948654145002,-0.0010448690736666322,-0.0010297761764377356,-0.0010248442413285375,-0.0010230676271021366,-0.001018949318677187,-0.0010181524557992816,-0.0009984499774873257,-0.000996933551505208,-0.0009939871961250901,-0.000983856269158423,-0.0009828150505200028,-0.0009825958404690027,-0.0009814747609198093,-0.0009737901855260134,-0.0009731333702802658,-0.0009689845028333366,-0.0009645384270697832,-0.0009442850132472813,-0.0009355153306387365,-0.0009340189280919731,-0.0009166726376861334,-0.0009162130882032216,-0.0009151127887889743,-0.0009103950578719378,-0.0009055351256392896,-0.0008962716092355549,-0.0008941391133703291,-0.0008918247185647488,-0.0008916676160879433,-0.0008847208810038865,-0.0008682672632858157,-0.0008663763292133808,-0.00086596718756482,-0.0008610072545707226,-0.0008603340247645974,-0.0008496835944242775,-0.0008440754609182477,-0.0008421752718277276,-0.000820230518002063,-0.0008087183814495802,-0.0008076517260633409,-0.0008041522814892232,-0.0008039410458877683,-0.0008000805391930044,-0.0007983927498571575,-0.0007973101455718279,-0.000796268752310425,-0.0007908043335191905,-0.00078970892354846,-0.0007818849990144372,-0.0007789130322635174,-0.0007628852035850286,-0.0007597336079925299,-0.0007478626212105155,-0.000746442296076566,-0.0007418300956487656,-0.0007408061646856368,-0.00073863728903234,-0.0007385544013231993,-0.000729608756955713,-0.0007292101508937776,-0.0007186257862485945,-0.0007153459591791034,-0.0007140219677239656,-0.0007113370229490101,-0.0007035376038402319,-0.00070006592432037,-0.0006865830509923398,-0.0006793755455873907,-0.0006752352346666157,-0.0006743227131664753,-0.0006722389371134341,-0.0006620934000238776,-0.0006553668063133955,-0.0006514372071251273,-0.0006392952636815608,-0.0006345483707264066,-0.0006324183777906001,-0.0006315716309472919,-0.0006270935409702361,-0.0006260371883399785,-0.0006234674365259707,-0.0006220036884769797,-0.0006195117020979524,-0.0006140326149761677,-0.0006082343752495944,-0.0005925883306190372,-0.0005824335967190564,-0.0005771503783762455,-0.0005765393143519759,-0.0005744967493228614,-0.0005678377929143608,-0.0005597453564405441,-0.0005450857570394874,-0.0005443710833787918,-0.0005365465767681599,-0.0005331877619028091,-0.0005296864546835423,-0.0005286861560307443,-0.0005275047733448446,-0.0005266929510980844,-0.0005265231593511999,-0.0005264857900328934,-0.0005227510118857026,-0.0005221879109740257,-0.0005113867809996009,-0.0005087818135507405,-0.0005082073621451855,-0.0005051979678682983,-0.0005032978369854391,-0.0005019698874093592,-0.0005012962501496077,-0.0004954231553710997,-0.0004929445567540824,-0.0004905021050944924,-0.00047138758236542344,-0.0004657915560528636,-0.0004656077653635293,-0.00046241850941441953,-0.00045683395001105964,-0.00044324612827040255,-0.00043627669219858944,-0.00043123174691572785,-0.00043035749695263803,-0.00042965076863765717,-0.0004188370949123055,-0.0004173158959019929,-0.0004170775064267218,-0.0004088988061994314,-0.0004059230559505522,-0.0004053832672070712,-0.0004006566887255758,-0.00040029792580753565,-0.0003912986139766872,-0.00039076554821804166,-0.0003868576022796333,-0.0003852984227705747,-0.0003846398612949997,-0.0003794472722802311,-0.000376425392460078,-0.00037458018050529063,-0.0003741705440916121,-0.00037397167761810124,-0.0003705019480548799,-0.0003556272713467479,-0.0003548875101841986,-0.00035400313208810985,-0.00035351913538761437,-0.00035343997296877205,-0.0003517583245411515,-0.00035129510797560215,-0.00034843847970478237,-0.0003478537837509066,-0.00034758466063067317,-0.0003465617191977799,-0.000345368345733732,-0.0003407126641832292,-0.00033566297497600317,-0.0003332217747811228,-0.0003296658687759191,-0.0003294691850896925,-0.00032935268245637417,-0.00032830037525855005,-0.00032818157342262566,-0.00032360124168917537,-0.0003211084986105561,-0.00031670715543441474,-0.0003097396984230727,-0.0003092724073212594,-0.0003084851778112352,-0.00030516856350004673,-0.0003038879076484591,-0.00030167665681801736,-0.00028703935095109046,-0.0002856298233382404,-0.0002855447819456458,-0.0002815628831740469,-0.0002726614475250244,-0.0002722293429542333,-0.0002710535773076117,-0.0002680608886294067,-0.0002674837887752801,-0.00026220278232358396,-0.0002609809744171798,-0.0002544866583775729,-0.00025326068862341344,-0.0002527307369746268,-0.00023994150978978723,-0.0002386607666267082,-0.00023532044724561274,-0.0002348246780456975,-0.00023464720288757235,-0.00023424434766639024,-0.0002312675496796146,-0.0002235069841844961,-0.00022051561973057687,-0.00021942310559097677,-0.00021807361918035895,-0.000214922649320215,-0.00021440431009978056,-0.00021010104683227837,-0.0002075631491607055,-0.00020545721054077148,-0.00020467017020564526,-0.00020415228209458292,-0.00020283852063585073,-0.00019925006199628115,-0.00019898726895917207,-0.0001963080430869013,-0.0001949933939613402,-0.00019415914721321315,-0.00019200303358957171,-0.00019080564379692078,-0.00018741369422059506,-0.0001853691355790943,-0.00017854310863185674,-0.00017470066086389124,-0.0001677406980888918,-0.00016607638099230826,-0.0001654712832532823,-0.00016463488282170147,-0.00016340460570063442,-0.0001606801524758339,-0.0001606066944077611,-0.00016011115803848952,-0.0001569944288348779,-0.00015418138355016708,-0.0001527187996543944,-0.00014777023170609027,-0.00014663093315903097,-0.000145720798172988,-0.00014451969764195383,-0.00014084234135225415,-0.00014065667346585542,-0.0001400095206918195,-0.00013884727377444506,-0.00013672631757799536,-0.00013494223821908236,-0.00013292912626639009,-0.00013255112571641803,-0.0001234885276062414,-0.00012211267312522978,-0.00011694770364556462,-0.00011269096285104752,-0.00011108886246802285,-0.00011046655708923936,-0.00010900955385295674,-0.00010739728895714507,-0.00010614220082061365,-0.0001061266302713193,-0.00010478127660462633,-0.00010464411025168374,-0.00010288949852110818,-0.00010076843318529427,-0.00010000288602896035,-9.860112913884223e-05,-9.546734509058297e-05,-9.419322304893285e-05,-9.190201672026888e-05,-8.840952068567276e-05,-8.60520449350588e-05,-8.505005098413676e-05,-8.457888179691508e-05,-8.36836188682355e-05,-7.6988224464003e-05,-7.173199264798313e-05,-7.068156992318109e-05,-6.8892833951395e-05,-6.862547161290422e-05,-6.261423550313339e-05,-6.035517799318768e-05,-5.9402475017122924e-05,-5.933173088124022e-05,-5.914714347454719e-05,-5.713216887670569e-05,-5.645860073855147e-05,-5.4234900744631886e-05,-5.3244715672917664e-05,-5.301434430293739e-05,-5.123954178998247e-05,-4.8964695452013984e-05,-4.673313378589228e-05,-4.42680720880162e-05,-4.086501940037124e-05,-4.0134786104317755e-05,-3.966785880038515e-05,-3.896783891832456e-05,-3.838110569631681e-05,-3.790501432376914e-05,-3.7671812606276944e-05,-3.6831683246418834e-05,-3.497537181829102e-05,-3.4382566809654236e-05,-3.3957810956053436e-05,-3.391053542145528e-05,-3.311675027362071e-05,-3.2451265724375844e-05,-3.1349769415101036e-05,-3.079354792134836e-05,-2.9284581614774652e-05,-2.9210747015895322e-05,-2.8903707061544992e-05,-2.1675974494428374e-05,-2.118039810738992e-05,-2.0267292711650953e-05,-1.811355286918115e-05,-1.6944073649938218e-05,-1.6832613255246542e-05,-1.5204809642455075e-05,-1.4828219718765467e-05,-1.4128721886663698e-05,-1.2906044503324665e-05,-1.272290955967037e-05,-1.2329258424870204e-05,-1.153983157564653e-05,-1.141481061495142e-05,-9.367652637592983e-06,-9.293406947108451e-06,-8.992962648335379e-06,-8.860453817760572e-06,-6.732009296683827e-06,-6.624609341088217e-06,-5.591064564214321e-06,-4.005543814855628e-06,-3.7227198390610283e-06,-3.3394246656826e-06,-1.5909969306449057e-06,-6.302073529695917e-07,-4.0598214923193154e-07,-1.2334436405581073e-07,2.38418573772492e-09,6.066635478418902e-07,9.647384331401554e-07,2.665706006155233e-06,3.968551936850417e-06,4.362277650216129e-06,6.4001606006058864e-06,1.0096691767103039e-05,1.0576434760878328e-05,1.2349598364380654e-05,1.2724139196507167e-05,1.350946695310995e-05,1.4071651094127446e-05,1.4518089301418513e-05,1.59284463734366e-05,1.6013160347938538e-05,1.818783493945375e-05,1.8644854208105244e-05,2.1086298147565685e-05,2.3327096641878597e-05,2.656191645655781e-05,2.7821139156003483e-05,2.8296150048845448e-05,2.8309039407758974e-05,2.8358661438687705e-05,2.907928137574345e-05,3.2216459658229724e-05,3.266286148573272e-05,3.516338620102033e-05,3.5621971619548276e-05,3.948688390664756e-05,4.216909292154014e-05,4.229694604873657e-05,4.299521606299095e-05,4.7968813305487856e-05,5.049862011219375e-05,5.601983502856456e-05,5.676492946804501e-05,5.748409603256732e-05,5.870338645763695e-05,5.949974001850933e-05,5.97705329710152e-05,6.149798718979582e-05,6.157591997180134e-05,6.412394577637315e-05,6.546139775309712e-05,6.649553688475862e-05,6.842653965577483e-05,7.01641256455332e-05,7.067102706059813e-05,7.340267620747909e-05,7.557738717878237e-05,7.559668301837519e-05,7.568009459646419e-05,7.882118370616809e-05,7.910295971669257e-05,8.003376569831744e-05,8.087336755124852e-05,8.092694042716175e-05,8.34743696032092e-05,8.459728269372135e-05,8.487846935167909e-05,8.566625183448195e-05,8.609745418652892e-05,8.696712757227942e-05,8.8196546130348e-05,9.069807856576517e-05,9.152739949058741e-05,9.400181443197653e-05,9.623556979931891e-05,9.703211253508925e-05,9.957052679965273e-05,0.00010697450488805771,0.00010771986126201227,0.00010790161468321458,0.00011027548316633329,0.00012030765356030315,0.00012096159480279312,0.00012197707110317424,0.0001267695042770356,0.00013450108235701919,0.00013468562974594533,0.0001369798119412735,0.00013699691044166684,0.0001370368554489687,0.00014093529898673296,0.00014265913341660053,0.00014486491272691637,0.00014664918126072735,0.00014809519052505493,0.00015048957720864564,0.00015109259402379394,0.00015552177501376718,0.0001563202531542629,0.0001574745838297531,0.00015764907584525645,0.00016297608090098947,0.00016307317127939314,0.00016396743012592196,0.0001680684508755803,0.00017017696518450975,0.0001714979880489409,0.00017277531151194125,0.0001733448007144034,0.00017373048467561603,0.00017399684293195605,0.00018252909649163485,0.00018645264208316803,0.0001890636922325939,0.00018993523553945124,0.00019036434241570532,0.00019363783940207213,0.0001953491591848433,0.00019580383377615362,0.00019782278104685247,0.00019943781080655754,0.00020147695613559335,0.00020159162522759289,0.00020167026377748698,0.00020247917564120144,0.0002048569149337709,0.00020854399190284312,0.0002125094033544883,0.00021657056640833616,0.00021662753715645522,0.0002223556803073734,0.0002296364982612431,0.00023072134354151785,0.00023135729134082794,0.00023426610277965665,0.00023622297157999128,0.00024537817807868123,0.0002463643904775381,0.0002472901251167059,0.00025086686946451664,0.00025424937484785914,0.0002583970781415701,0.00025847076904028654,0.0002588391653262079,0.00026275086565874517,0.0002630542148835957,0.0002694408467505127,0.000276491540716961,0.000277077779173851,0.00027792638866230845,0.0002781991788651794,0.0002847513824235648,0.0002863622212316841,0.0002921824052464217,0.00030089859501458704,0.0003036009147763252,0.0003037644491996616,0.0003040024312213063,0.0003065616183448583,0.00030734186293557286,0.0003106666263192892,0.0003118574386462569,0.0003148510295432061,0.00031759633566252887,0.00031892952392809093,0.00032441309303976595,0.0003257014905102551,0.00032898681820370257,0.00034734889050014317,0.00034864916233345866,0.0003520563477650285,0.0003580876800697297,0.0003627474361564964,0.00036300826468504965,0.000366636726539582,0.0003676524793263525,0.00037183045060373843,0.00038384314393624663,0.00038812647107988596,0.000390317989513278,0.00039728268166072667,0.0004028821422252804,0.0004190189065411687,0.00042000075336545706,0.0004318764549680054,0.00043426884803920984,0.0004346670175436884,0.00044867920223623514,0.00045347216655500233,0.00045829027658328414,0.0004583359113894403,0.0004658466496039182,0.00046681013191118836,0.00046697689685970545,0.0004673335934057832,0.00048188449000008404,0.0004947531851939857,0.0004987001302652061,0.000499510788358748,0.000500100723002106,0.0005034295609220862,0.0005051843472756445,0.0005116602405905724,0.0005130536737851799,0.0005133118247613311,0.0005170903168618679,0.0005238091107457876,0.0005257385782897472,0.0005262857885099947,0.0005269635585136712,0.0005342201329767704,0.0005403294344432652,0.0005527077009901404,0.0005615688278339803,0.0005739373154938221,0.000578089791815728,0.0005854455521330237,0.0005897642113268375,0.0006115491851232946,0.000621960440184921,0.0006250017904676497,0.0006358006503432989,0.0006433505332097411,0.0006438292330130935,0.0006474766414612532,0.0006484361947514117,0.0006491323001682758,0.000650096102617681,0.0006519579910673201,0.0006555344443768263,0.0006643376545980573,0.0006677007768303156,0.0006692717433907092,0.0006762169068679214,0.0006785160512663424,0.0006806026212871075,0.0006811291095800698,0.0006904065376147628,0.0006999066681601107,0.0007028797408565879,0.0007194174686446786,0.0007234586519189179,0.0007241136627271771,0.0007254093652591109,0.0007383283809758723,0.0007387943333014846,0.0007474989979527891,0.0007481318898499012,0.0007508573471568525,0.0007534677279181778,0.0007600450771860778,0.0007623689016327262,0.0007835012511350214,0.0007844209903851151,0.0008057205122895539,0.0008088485919870436,0.0008090161718428135,0.0008383583044633269,0.0008439983939751983,0.0008467151201330125,0.0008555258973501623,0.0008578429115004838,0.0008600965375080705,0.0008686056826263666,0.0008704157080501318,0.0008746548555791378,0.0008805848774500191,0.0008961936691775918,0.0009107342339120805,0.0009141217451542616,0.000926725915633142,0.0009270919836126268,0.000932467810343951,0.0009484665933996439,0.0009557767189107835,0.0009576286538504064,0.0009712795726954937,0.0009858516277745366,0.000988674582913518,0.0009888894855976105,0.00099362398032099,0.0009950542589649558,0.0010025948286056519,0.001004969235509634,0.001030600629746914,0.001031125197187066,0.0010313965613022447,0.0010315609397366643,0.0010357407154515386,0.0010399757884442806,0.001049119047820568,0.0010537091875448823,0.001076839747838676,0.0010801932075992227,0.0010841748444363475,0.001092134858481586,0.0010945532703772187,0.0010977463098242879,0.0011007232824340463,0.0011026149149984121,0.0011059802491217852,0.001107429969124496,0.0011190302902832627,0.0011255439603701234,0.0011274656280875206,0.0011278423480689526,0.001129678450524807,0.0011353790760040283,0.00115467538125813,0.0011609357316046953,0.001161370426416397,0.0011693666456267238,0.0011716927401721478,0.0011793534504249692,0.0011813146993517876,0.0011817764025181532,0.001185193657875061,0.0011854799231514335,0.0011905853170901537,0.0011908833403140306,0.001196419820189476,0.001203143154270947,0.0012033949606120586,0.0012169132241979241,0.0012197433970868587,0.0012347542215138674,0.001235863077454269,0.0012367324670776725,0.0012423987500369549,0.00124481285456568,0.0012521813623607159,0.0012559260940179229,0.0012629079865291715,0.0012827259488403797,0.0012846944155171514,0.0012859298149123788,0.0013063097139820457,0.0013073303271085024,0.0013121213996782899,0.0013357250718399882,0.001355468761175871,0.0013741442235186696,0.0013836012221872807,0.0013873705174773932,0.0014150193892419338,0.0014179948484525084,0.0014189952053129673,0.0014190662186592817,0.0014193394454196095,0.0014230951201170683,0.001428332645446062,0.0014301587361842394,0.0014320856425911188,0.0014342692447826266,0.0014368847478181124,0.0014427873538807034,0.0014498436357825994,0.001450458774343133,0.001453849719837308,0.001468506408855319,0.0014709094539284706,0.001474624266847968,0.0014757828321307898,0.0014774433802813292,0.0014804936945438385,0.0014813828747719526,0.0014838173519819975,0.0014940554974600673,0.0015142002375796437,0.0015461425064131618,0.0015491620870307088,0.0015500521985813975,0.0015543147455900908,0.0015682813245803118,0.0015689465217292309,0.0016091382130980492,0.0016362853348255157,0.0016630259342491627,0.0016712960787117481,0.001686924952082336,0.0016877581365406513,0.0016954382881522179,0.0017195992404595017,0.0017297085141763091,0.001733119017444551,0.0017373515293002129,0.0017418349161744118,0.0017536015948280692,0.0017558207036927342,0.0017577160615473986,0.0017578706610947847,0.001760991057381034,0.001780372578650713,0.0017806824762374163,0.0017833367455750704,0.0017894292250275612,0.0018105001654475927,0.0018146156799048185,0.0018197823083028197,0.0018290793523192406,0.001840575598180294,0.0018592649139463902,0.0018818533280864358,0.0018921899609267712,0.0018944581970572472,0.0018949273508042097,0.0018991028191521764,0.0019017122685909271,0.001917084213346243,0.0019212620100006461,0.0019247946329414845,0.0019273337675258517,0.0019355980912223458,0.001950097386725247,0.0019507898250594735,0.0019578321371227503,0.0019595082849264145,0.001971334917470813,0.001973838312551379,0.0019825070630759,0.002011149190366268,0.0020126847084611654,0.0020333388820290565,0.002036861376836896,0.0020462339743971825,0.002046582056209445,0.002048956695944071,0.0020603672601282597,0.0020611430518329144,0.002065724227577448,0.0020685852505266666,0.0020695501007139683,0.002074671909213066,0.0020747347734868526,0.002081090584397316,0.002082953928038478,0.002101782476529479,0.002104806946590543,0.0021102542523294687,0.002113638911396265,0.002132636494934559,0.0021471362560987473,0.00215441407635808,0.0021621468476951122,0.0021661650389432907,0.0021892590448260307,0.0022028249222785234,0.002209234517067671,0.0022138329222798347,0.0022242129780352116,0.0022353625390678644,0.0022702645510435104,0.00228195171803236,0.0022822010796517134,0.002284546848386526,0.0022999600041657686,0.0023034056648612022,0.002303475048393011,0.002307422459125519,0.0023220747243613005,0.0023280985187739134,0.002338589634746313,0.002342010149732232,0.0023434229660779238,0.0023568428587168455,0.0023595213424414396,0.002366123255342245,0.0023848831187933683,0.0023898968938738108,0.0023918389342725277,0.0024379303213208914,0.002441019518300891,0.0024609363172203302,0.002477792324498296,0.002494890009984374,0.002497236244380474,0.0025086686946451664,0.0025482892524451017,0.002548876451328397,0.0025502415373921394,0.0025529598351567984,0.002577542094513774,0.002585366368293762,0.0025866380892693996,0.0025874334387481213,0.0025885975919663906,0.0026194918900728226,0.002621921245008707,0.0026328330859541893,0.0026363739743828773,0.0026448536664247513,0.002644954016432166,0.002675808034837246,0.00267905299551785,0.0026840122882276773,0.0026870244182646275,0.0026889541186392307,0.0027087789494544268,0.0027114737313240767,0.0027140495367348194,0.0027169950772076845,0.0027472504880279303,0.0027529941871762276,0.002769252983853221,0.0027698546182364225,0.002770400606095791,0.0027775559574365616,0.002786147641018033,0.002798284636810422,0.002805100055411458,0.0028155946638435125,0.0028322404250502586,0.002832360565662384,0.0028433215338736773,0.0028448165394365788,0.0028455958236008883,0.0028560541104525328,0.0028621512465178967,0.0028822903987020254,0.00288814096711576,0.0028923044446855783,0.002921747276559472,0.0029553512576967478,0.0029589885380119085,0.0029591978527605534,0.0029979245737195015,0.003009884851053357,0.003017449751496315,0.0030373558402061462,0.0030452448409050703,0.0030516628175973892,0.0030651085544377565,0.0030698003247380257,0.0031308066099882126,0.0031489126849919558,0.003169509582221508,0.0031883472111076117,0.003200393868610263,0.0032075613271445036,0.003211813513189554,0.0032700372394174337,0.003270748071372509,0.0032787013333290815,0.0032812259159982204,0.0032987145241349936,0.0033217305317521095,0.0033255619928240776,0.0033261990174651146,0.003334270091727376,0.0033706706017255783,0.0033889852929860353,0.0034128909464925528,0.0034703125711530447,0.0034786465112119913,0.0034812591038644314,0.0034907120279967785,0.0035002464428544044,0.0035018743947148323,0.003519123187288642,0.0035680034197866917,0.0035736497957259417,0.0035753189586102962,0.0035808426328003407,0.0035849916748702526,0.0035908017307519913,0.0036144189070910215,0.0036376065108925104,0.0036422950215637684,0.003642640309408307,0.003643923904746771,0.0036560939624905586,0.0036823288537561893,0.0036880942061543465,0.0037194443866610527,0.0037259170785546303,0.0037276390939950943,0.003757007187232375,0.0037764457520097494,0.0037768068723380566,0.003781686769798398,0.0038214579690247774,0.0038452106527984142,0.003881334327161312,0.0039102137088775635,0.003936617635190487,0.003968014847487211,0.003988528158515692,0.003992054611444473,0.004010888282209635,0.004018053878098726,0.00403106864541769,0.0040742866694927216,0.0041123200207948685,0.004144881386309862,0.00416949950158596,0.004210521001368761,0.004236153792589903,0.004238091874867678,0.004247210454195738,0.004259833600372076,0.004308809991925955,0.004323136992752552,0.004329333081841469,0.004361154977232218,0.00436370586976409,0.00443712854757905,0.004451950546354055,0.0044718580320477486,0.004480010364204645,0.00449384143576026,0.004510356578975916,0.004523934796452522,0.004601961001753807,0.00461519556120038,0.00467937346547842,0.004742673598229885,0.004758124239742756,0.004833906423300505,0.004842701368033886,0.004859992768615484,0.004874533507972956,0.004885760601609945,0.004923603497445583,0.004933519754558802,0.005013762507587671,0.005023196805268526,0.0050477744080126286,0.005056896712630987,0.005090123508125544,0.0051107220351696014,0.005113302264362574,0.005121631547808647,0.005123223178088665,0.005126745905727148,0.005163474008440971,0.005164070054888725,0.005180084612220526,0.005209990311414003,0.0052326335571706295,0.0052931103855371475,0.005293490830808878,0.0053036450408399105,0.005330649670213461,0.005356466863304377,0.0053734444081783295,0.005469209980219603,0.005471919663250446,0.005510894116014242,0.005516642238944769,0.00555171025916934,0.0055731358006596565,0.005573536269366741,0.005604377016425133,0.0056064859963953495,0.005607017315924168,0.005643184762448072,0.0057265437208116055,0.005729507189244032,0.005753300152719021,0.005787261761724949,0.0058164955116808414,0.005838397890329361,0.005894411355257034,0.005904235877096653,0.0060021295212209225,0.006021314300596714,0.0060297343879938126,0.006115984171628952,0.006130443885922432,0.00618400052189827,0.006202931981533766,0.006331599783152342,0.006346029229462147,0.006548614706844091,0.006556657142937183,0.006573303136974573,0.0065815444104373455,0.00658391835168004,0.006663701497018337,0.006730714812874794,0.006774412002414465,0.006776126567274332,0.006777232978492975,0.00678108399733901,0.006791761144995689,0.0068335141986608505,0.006838643457740545,0.006900012493133545,0.006906949449330568,0.006912944372743368,0.006933682132512331,0.006944464985281229,0.006946631707251072,0.007027873769402504,0.007035308051854372,0.0070526436902582645,0.0070606134831905365,0.007123004179447889,0.0071800388395786285,0.0072073740884661674,0.007331177592277527,0.007341022603213787,0.007403905503451824,0.007414479274302721,0.007471127435564995,0.007477058097720146,0.007480078376829624,0.007507860194891691,0.007552157621830702,0.007564164232462645,0.0075855315662920475,0.00761553505435586,0.007622588891535997,0.007627138867974281,0.00767618790268898,0.007698953151702881,0.0077379499562084675,0.007758952211588621,0.0077848793007433414,0.007799427956342697,0.007871486246585846,0.00794802326709032,0.007969573140144348,0.007999055087566376,0.008066771551966667,0.00807444378733635,0.008105695247650146,0.008116411045193672,0.00819020252674818,0.008220971561968327,0.008224663324654102,0.008290792815387249,0.008325494825839996,0.0083382036536932,0.008470208384096622,0.008517567999660969,0.0085466792806983,0.008567065000534058,0.00864166859537363,0.00869384128600359,0.008729626424610615,0.008731493726372719,0.00873181689530611,0.008790131658315659,0.0088056530803442,0.008858395740389824,0.008931418880820274,0.008933896198868752,0.008949857205152512,0.00904763862490654,0.009057022631168365,0.009073454886674881,0.009263335727155209,0.009326073341071606,0.009383435361087322,0.009439356625080109,0.009467181749641895,0.009495312348008156,0.009513413533568382,0.009519398212432861,0.009641438722610474,0.009696027263998985,0.009731368161737919,0.009732332080602646,0.009747020900249481,0.009754452854394913,0.00978823285549879,0.009819176979362965,0.009894737042486668,0.009936962276697159,0.010026122443377972,0.010092904791235924,0.010242573916912079,0.010638212785124779,0.010748105123639107,0.010919565334916115,0.01093352772295475,0.010942407883703709,0.010980640538036823,0.01102420687675476,0.011049244552850723,0.011245626956224442,0.011337297968566418,0.011386001482605934,0.011389489285647869,0.01146003883332014,0.011480867862701416,0.011540863662958145,0.011553647927939892,0.01155910175293684,0.011643708683550358,0.011798007413744926,0.011807018890976906,0.01190956961363554,0.011948322877287865,0.011956905014812946,0.011961235664784908,0.012048466131091118,0.012167121283710003,0.012383881956338882,0.012509217485785484,0.012619677931070328,0.012676719576120377,0.012761042453348637,0.012775905430316925,0.01278306357562542,0.012932593002915382,0.012978979386389256,0.013017564080655575,0.013039571233093739,0.013257723301649094,0.0133290383964777,0.013369266875088215,0.013483558781445026,0.013525212183594704,0.01360584981739521,0.013660312630236149,0.013680008240044117,0.01369490660727024,0.013697348535060883,0.01381932757794857,0.013864394277334213,0.013899224810302258,0.013977255672216415,0.013998409733176231,0.014095020480453968,0.014110459946095943,0.014194196090102196,0.014249768108129501,0.014269376173615456,0.014523487538099289,0.014540906995534897,0.014590740203857422,0.01472204364836216,0.014815793372690678,0.014831151813268661,0.014849974773824215,0.014998784288764,0.015022295527160168,0.01506354846060276,0.015401685610413551,0.01541387289762497,0.015456395223736763,0.015661681070923805,0.015705794095993042,0.015783242881298065,0.015853237360715866,0.01605045422911644,0.016129307448863983,0.016415180638432503,0.016577817499637604,0.01659502647817135,0.01662861369550228,0.016967620700597763,0.016992762684822083,0.01704540103673935,0.01707017421722412,0.017070768401026726,0.017247000709176064,0.017775483429431915,0.01782255806028843,0.017959142103791237,0.018343662843108177,0.018363701179623604,0.018474623560905457,0.01861364208161831,0.018638914451003075,0.018664062023162842,0.018709218129515648,0.01888963021337986,0.01890879124403,0.01890888437628746,0.01898835226893425,0.0192006453871727,0.01926160603761673,0.01953326165676117,0.01954355277121067,0.01958015002310276,0.01978788711130619,0.019879113882780075,0.019908197224140167,0.019909897819161415,0.020015589892864227,0.02001846581697464,0.02008284255862236,0.020207220688462257,0.020492054522037506,0.020693296566605568,0.020839951932430267,0.0211007222533226,0.02148679457604885,0.02152886800467968,0.021698441356420517,0.021804682910442352,0.021859200671315193,0.02217615582048893,0.022438175976276398,0.022532973438501358,0.02269722893834114,0.023414630442857742,0.023463847115635872,0.023538269102573395,0.02356458641588688,0.024411363527178764,0.024504948407411575,0.02451395057141781,0.02505035139620304,0.025350181385874748,0.026565857231616974,0.026621486991643906,0.026870034635066986,0.027395468205213547,0.02742384374141693,0.027525506913661957,0.0275467187166214,0.027638912200927734,0.02777078002691269,0.02804437093436718,0.02807006984949112,0.028227422386407852,0.028297699987888336,0.02866838499903679,0.028698068112134933,0.029217863455414772,0.029403559863567352,0.029775753617286682,0.029946772381663322,0.03041757270693779,0.030644798651337624,0.030660433694720268,0.03149887174367905,0.031990714371204376,0.03242480754852295,0.033163443207740784,0.03336141258478165,0.033703550696372986,0.03407895937561989,0.03479931503534317,0.03517847880721092,0.03518176078796387,0.035294197499752045,0.03537329286336899,0.03540678322315216,0.03581367805600166,0.03591268137097359,0.03606123849749565,0.03610008582472801,0.03696383535861969,0.037362970411777496,0.03753235191106796,0.03795325383543968,0.0385417565703392,0.03895445540547371,0.04002553969621658,0.0406196229159832,0.041856322437524796,0.04214116185903549,0.04336283355951309,0.043914251029491425,0.044807009398937225,0.04494800418615341,0.045185502618551254,0.04580935463309288,0.045901164412498474,0.04592997953295708,0.046233054250478745,0.04661700874567032,0.04677695780992508,0.04689987003803253,0.04695719853043556,0.0472075454890728,0.047220878303050995,0.04782312363386154,0.048076972365379333,0.048421505838632584,0.04873325303196907,0.049068231135606766,0.04970590025186539,0.04988856241106987,0.05022108927369118,0.05161275342106819,0.053839072585105896,0.053952474147081375,0.054760340601205826,0.05547623336315155,0.05555517226457596,0.055664487183094025,0.05739634484052658,0.05844751372933388,0.05867721512913704,0.06050011143088341,0.06359193474054337,0.06387250870466232,0.0654304251074791,0.06957817077636719,0.07050304859876633,0.07118788361549377,0.07165145874023438,0.07221929728984833,0.07372750341892242,0.07407938688993454,0.07412266731262207,0.07442957907915115,0.07509838789701462,0.07833680510520935,0.08278428018093109,0.08481713384389877,0.08511195331811905,0.085227832198143,0.0887957215309143,0.08879797160625458,0.08968100696802139,0.08977105468511581,0.09339826554059982,0.09390205144882202,0.09451556950807571,0.09515146911144257,0.09564457833766937,0.09735836088657379,0.0981149896979332,0.0997634306550026,0.10241515189409256,0.107045978307724,0.11444055289030075,0.12591558694839478,0.12663774192333221,0.12721878290176392,0.1316806972026825,0.13430272042751312,0.14050361514091492,0.14388766884803772,0.1517070233821869,0.1538819670677185,0.1727585345506668,0.17793476581573486,0.1787061244249344,0.18481853604316711,0.19680988788604736,0.23780593276023865,0.2527439594268799],\"yaxis\":\"y\",\"type\":\"scattergl\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Sorted neurons\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Loss change\"}},\"legend\":{\"title\":{\"text\":\"variable\"},\"tracegroupgap\":0},\"title\":{\"text\":\"Loss change from ablating MLP5 neuron\"},\"width\":800},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('cfa933b7-70d1-4451-8da8-37c32a3b7408');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sorted_means, indices = torch.sort(diffs.mean(1))\n",
    "sorted_means = sorted_means.tolist()\n",
    "haystack_utils.line(sorted_means, xlabel=\"Sorted neurons\", ylabel=\"Loss change\", title=\"Loss change from ablating MLP5 neuron\") # xticks=indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>                            <div id=\"7c782fa9-c5e7-4ff4-9ea4-6e297581d2e1\" class=\"plotly-graph-div\" style=\"height:525px; width:1000px;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"7c782fa9-c5e7-4ff4-9ea4-6e297581d2e1\")) {                    Plotly.newPlot(                        \"7c782fa9-c5e7-4ff4-9ea4-6e297581d2e1\",                        [{\"error_y\":{\"array\":[1.4805813294256063],\"type\":\"data\",\"visible\":true},\"name\":\"Original\",\"x\":[\"Original\"],\"y\":[1.7782519582565874],\"type\":\"bar\"},{\"error_y\":{\"array\":[2.284745338145314],\"type\":\"data\",\"visible\":true},\"name\":\"Ablated\",\"x\":[\"Ablated\"],\"y\":[4.157241359651088],\"type\":\"bar\"},{\"error_y\":{\"array\":[1.528455629397],\"type\":\"data\",\"visible\":true},\"name\":\"MLP5 path patched\",\"x\":[\"MLP5 path patched\"],\"y\":[2.3592610958963633],\"type\":\"bar\"},{\"error_y\":{\"array\":[2.0124886025072652],\"type\":\"data\",\"visible\":true},\"name\":\"MLP5 path patched + Top 10 MLP5 neurons ablated\",\"x\":[\"Top MLP5 removed\"],\"y\":[4.444820071905851],\"type\":\"bar\"},{\"error_y\":{\"array\":[0.8305999902091944],\"type\":\"data\",\"visible\":true},\"name\":\"MLP5 path patched + Bottom 10 MLP5 neurons ablated\",\"x\":[\"Bottom MLP5 removed\"],\"y\":[0.6735373494308442],\"type\":\"bar\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"title\":{\"text\":\"Average last token loss when removing top \\u002f bottom neurons from path patching\"},\"xaxis\":{\"title\":{\"text\":\"\"}},\"yaxis\":{\"title\":{\"text\":\"Loss\"}},\"barmode\":\"group\",\"width\":1000},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('7c782fa9-c5e7-4ff4-9ea4-6e297581d2e1');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check loss change when ablating top / bottom neurons\n",
    "\n",
    "top_neurons_count = 10\n",
    "top_neurons = indices[-top_neurons_count:]\n",
    "bottom_neurons = indices[:top_neurons_count]\n",
    "\n",
    "with model.hooks(deactivate_neurons_fwd_hooks):\n",
    "    ablated_loss, ablated_cache = model.run_with_cache(prompts, return_type=\"loss\")\n",
    "\n",
    "ablate_top_neurons_hook = get_ablate_neurons_hook(top_neurons, ablated_cache)\n",
    "ablate_bottom_neurons_hook = get_ablate_neurons_hook(bottom_neurons, ablated_cache)\n",
    "\n",
    "original_loss, ablated_loss, _, all_MLP5_loss = haystack_utils.get_direct_effect(prompts, model, pos=-1, context_ablation_hooks=deactivate_neurons_fwd_hooks, context_activation_hooks=activate_neurons_fwd_hooks)\n",
    "_, _, _, top_MLP5_ablated_loss = haystack_utils.get_direct_effect(prompts, model, pos=-1, context_ablation_hooks=deactivate_neurons_fwd_hooks, context_activation_hooks=activate_neurons_fwd_hooks+ablate_top_neurons_hook)\n",
    "_, _, _, bottom_MLP5_ablated_loss = haystack_utils.get_direct_effect(prompts, model, pos=-1, context_ablation_hooks=deactivate_neurons_fwd_hooks, context_activation_hooks=activate_neurons_fwd_hooks+ablate_bottom_neurons_hook)\n",
    "\n",
    "names = [\"Original\", \"Ablated\", \"MLP5 path patched\", f\"MLP5 path patched + Top {top_neurons_count} MLP5 neurons ablated\", f\"MLP5 path patched + Bottom {top_neurons_count} MLP5 neurons ablated\"]\n",
    "short_names = [\"Original\", \"Ablated\", \"MLP5 path patched\", f\"Top MLP5 removed\", f\"Bottom MLP5 removed\"]\n",
    "\n",
    "values = [original_loss.tolist(), ablated_loss.tolist(), all_MLP5_loss.tolist(), top_MLP5_ablated_loss.tolist(), bottom_MLP5_ablated_loss.tolist()]\n",
    "haystack_utils.plot_barplot(values, names, short_names=short_names, ylabel=\"Loss\", title=f\"Average last token loss when removing top / bottom neurons from path patching\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigate top / bottom neuron boosts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Patch in MLP5 neurons, ablated bottom neurons, compare with patched MLP5 neurons without ablating bottom neurons\n",
    "\n",
    "with model.hooks(deactivate_neurons_fwd_hooks):\n",
    "    ablated_logits, ablated_cache = model.run_with_cache(prompts)\n",
    "\n",
    "ablate_top_neurons_hook = get_ablate_neurons_hook(top_neurons, ablated_cache)\n",
    "ablate_bottom_neurons_hook = get_ablate_neurons_hook(bottom_neurons, ablated_cache)\n",
    "\n",
    "original_logits, ablated_logprobs, _, all_MLP5_logprobs = haystack_utils.get_direct_effect(prompts, model, pos=-2, context_ablation_hooks=deactivate_neurons_fwd_hooks, context_activation_hooks=activate_neurons_fwd_hooks, return_type='logprobs')\n",
    "_, _, _, top_MLP5_ablated_logprobs = haystack_utils.get_direct_effect(prompts, model, pos=-2, context_ablation_hooks=deactivate_neurons_fwd_hooks, context_activation_hooks=activate_neurons_fwd_hooks+ablate_top_neurons_hook, return_type='logprobs')\n",
    "_, _, _, bottom_MLP5_ablated_logprobs = haystack_utils.get_direct_effect(prompts, model, pos=-2, context_ablation_hooks=deactivate_neurons_fwd_hooks, context_activation_hooks=activate_neurons_fwd_hooks+ablate_bottom_neurons_hook, return_type='logprobs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_difference_neurons(baseline_logprobs, ablated_logprops, positive=True, logprob_threshold=-7, k=50):\n",
    "    neuron_logprob_difference = (baseline_logprobs - ablated_logprops).mean(0)\n",
    "    neuron_logprob_difference[baseline_logprobs.mean(0) < logprob_threshold] = 0\n",
    "    if positive:\n",
    "        non_zero_count = (neuron_logprob_difference > 0).sum()\n",
    "    else:\n",
    "        non_zero_count = (neuron_logprob_difference < 0).sum()\n",
    "    top_logprob_difference, top_neurons = haystack_utils.top_k_with_exclude(neuron_logprob_difference, min(non_zero_count, k), all_ignore, largest=positive)\n",
    "    return top_logprob_difference, top_neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boosted tokens\n",
    "bottom_neuron_pos_difference_logprobs, bottom_pos_indices = get_top_difference_neurons(all_MLP5_logprobs, bottom_MLP5_ablated_logprobs, positive=True)\n",
    "top_neuron_pos_difference_logprobs, top_pos_indices = get_top_difference_neurons(all_MLP5_logprobs, top_MLP5_ablated_logprobs, positive=True)\n",
    "# Deboosted tokens\n",
    "bottom_neuron_neg_difference_logprobs, bottom_neg_indices = get_top_difference_neurons(all_MLP5_logprobs, bottom_MLP5_ablated_logprobs, positive=False)\n",
    "top_neuron_neg_difference_logprobs, top_neg_indices = get_top_difference_neurons(all_MLP5_logprobs, top_MLP5_ablated_logprobs, positive=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7504c3139ce240af8a118e67ec9add4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='Tokens:', options=('Boosted', 'Deboosted'), value='Boosted'), Drop…"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def plot_data(boosted_deboosted, top_bottom):\n",
    "    if boosted_deboosted == 'Boosted' and top_bottom == 'Top':\n",
    "        logprobs = top_neuron_pos_difference_logprobs\n",
    "        indices = top_pos_indices\n",
    "        title_change = 'increase'\n",
    "    elif boosted_deboosted == 'Boosted' and top_bottom == 'Bottom':\n",
    "        logprobs = bottom_neuron_pos_difference_logprobs\n",
    "        indices = bottom_pos_indices\n",
    "        title_change = 'increase'\n",
    "    elif boosted_deboosted == 'Deboosted' and top_bottom == 'Top':\n",
    "        logprobs = top_neuron_neg_difference_logprobs\n",
    "        indices = top_neg_indices\n",
    "        title_change = 'decrease'\n",
    "    else:  # 'Deboosted' and 'Bottom'\n",
    "        logprobs = bottom_neuron_neg_difference_logprobs\n",
    "        indices = bottom_neg_indices\n",
    "        title_change = 'decrease'\n",
    "\n",
    "    xlabel = boosted_deboosted + \" tokens\"\n",
    "    ylabel = \"full_logprob - ablated_logprop\"\n",
    "    title = 'Logprob ' + title_change + ' from ' + top_bottom.lower() + ' neurons'\n",
    "    xticks = [model.to_str_tokens([i])[0] for i in indices]\n",
    "\n",
    "    haystack_utils.line(logprobs.cpu().numpy(), xlabel=xlabel, ylabel=ylabel, title=title, xticks=xticks, show_legend=False)\n",
    "\n",
    "\n",
    "boosted_deboosted = widgets.Dropdown(options=['Boosted', 'Deboosted'], value='Boosted', description='Tokens:')\n",
    "top_bottom = widgets.Dropdown(options=['Top', 'Bottom'], value='Top', description='Neurons:')\n",
    "widgets.interactive(plot_data, boosted_deboosted=boosted_deboosted, top_bottom=top_bottom)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decompose neuron effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get logprob change for individual neuron\n",
    "\n",
    "def get_individual_neuron_logprob_effect(sorted_indices, neuron_pos=0, top=True, positive=True, plot=True):\n",
    "    if top:\n",
    "        neurons = sorted_indices[-(neuron_pos+1)]\n",
    "    else:\n",
    "        neurons = sorted_indices[neuron_pos]\n",
    "    neurons_hook = get_ablate_neurons_hook([neurons], ablated_cache)\n",
    "\n",
    "    original_logits, ablated_logprobs, _, all_MLP5_logprobs = haystack_utils.get_direct_effect(prompts, model, pos=-2, context_ablation_hooks=deactivate_neurons_fwd_hooks, context_activation_hooks=activate_neurons_fwd_hooks, return_type='logprobs')\n",
    "    _, _, _, neuron_ablated_logprobs = haystack_utils.get_direct_effect(prompts, model, pos=-2, context_ablation_hooks=deactivate_neurons_fwd_hooks, context_activation_hooks=activate_neurons_fwd_hooks+neurons_hook, return_type='logprobs')\n",
    "    neuron_difference_logprobs, difference_indices = \\\n",
    "        get_top_difference_neurons(all_MLP5_logprobs, neuron_ablated_logprobs, positive=positive)\n",
    "    \n",
    "    if not plot:\n",
    "        return neuron_difference_logprobs, difference_indices\n",
    "    \n",
    "    if positive:\n",
    "        xlabel = 'Boosted tokens'\n",
    "        title = 'Logprob increase from neuron ' + str(neurons.item())\n",
    "    else: \n",
    "        xlabel = 'Deboosted tokens'\n",
    "        title = 'Logprob decrease from neuron ' + str(neurons.item())\n",
    "    if top:\n",
    "        title += f' (top {neuron_pos+1} neuron)'\n",
    "    else:\n",
    "        title += f' (bottom {neuron_pos+1} neuron)'\n",
    "\n",
    "    xticks = [model.to_str_tokens([i])[0] for i in difference_indices]\n",
    "    haystack_utils.line(neuron_difference_logprobs.cpu().numpy(), xlabel=xlabel, ylabel=\"full_logprob - ablated_logprop\", title=title, xticks=xticks, show_legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9caf1aa2418f404e97912db3e46cbfae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='Neuron Pos:', max=20), Dropdown(description='Neurons:', …"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a function to call your function with widget inputs\n",
    "def plot_individual_neuron(neuron_pos, top_bottom, pos_neg):\n",
    "    top_bool = True if top_bottom == 'Top' else False\n",
    "    positive_bool = True if pos_neg == 'Positive' else False\n",
    "    get_individual_neuron_logprob_effect(indices, neuron_pos=neuron_pos, top=top_bool, positive=positive_bool)\n",
    "\n",
    "# Define widgets\n",
    "neuron_pos_slider = widgets.IntSlider(min=0, max=20, step=1, value=0, description='Neuron Pos:')\n",
    "top_bottom_dropdown = widgets.Dropdown(options=['Top', 'Bottom'], value='Top', description='Neurons:')\n",
    "pos_neg_dropdown = widgets.Dropdown(options=['Positive', 'Negative'], value='Positive', description='Pos/Neg:')\n",
    "\n",
    "# Use interactive function to bind the widgets to the plotting function\n",
    "widgets.interactive(plot_individual_neuron, neuron_pos=neuron_pos_slider, top_bottom=top_bottom_dropdown, pos_neg=pos_neg_dropdown)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m     top_logprob_difference, top_tokens \u001b[39m=\u001b[39m haystack_utils\u001b[39m.\u001b[39mtop_k_with_exclude(summed_neuron_diffs, \u001b[39mmin\u001b[39m(non_zero_count, k), all_ignore, largest\u001b[39m=\u001b[39mpositive)\n\u001b[1;32m     16\u001b[0m     \u001b[39mreturn\u001b[39;00m top_logprob_difference, top_tokens\n\u001b[0;32m---> 18\u001b[0m top_logprob_difference, top_topkens \u001b[39m=\u001b[39m summed_neuron_differences(top\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, positive\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     19\u001b[0m xticks \u001b[39m=\u001b[39m [model\u001b[39m.\u001b[39mto_str_tokens([i])[\u001b[39m0\u001b[39m] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m top_tokens]\n\u001b[1;32m     20\u001b[0m haystack_utils\u001b[39m.\u001b[39mline(top_logprob_difference\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy(), xlabel\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mToken\u001b[39m\u001b[39m\"\u001b[39m, ylabel\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mfull_logprob - ablated_logprop\u001b[39m\u001b[39m\"\u001b[39m, title\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIndividual positive changes summed\u001b[39m\u001b[39m\"\u001b[39m, xticks\u001b[39m=\u001b[39mxticks, show_legend\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[29], line 4\u001b[0m, in \u001b[0;36msummed_neuron_differences\u001b[0;34m(num_neurons, top, positive, k)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msummed_neuron_differences\u001b[39m(num_neurons\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m, top\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, positive\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, k\u001b[39m=\u001b[39m\u001b[39m50\u001b[39m):\n\u001b[0;32m----> 4\u001b[0m     summed_neuron_diffs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mzeros(model\u001b[39m.\u001b[39;49mcfg\u001b[39m.\u001b[39;49md_mlp)\u001b[39m.\u001b[39;49mcuda()\n\u001b[1;32m      5\u001b[0m     \u001b[39mfor\u001b[39;00m neuron_pos \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_neurons):\n\u001b[1;32m      6\u001b[0m         \u001b[39m# It would probably be more principle to do our filtering on the summed diffs instead of per neuron\u001b[39;00m\n\u001b[1;32m      7\u001b[0m         neuron_diff, token_indices \u001b[39m=\u001b[39m get_individual_neuron_logprob_effect(indices, neuron_pos\u001b[39m=\u001b[39mneuron_pos, top\u001b[39m=\u001b[39mtop, positive\u001b[39m=\u001b[39mpositive, plot\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "# Sum individual weighted positive / negative effects\n",
    "\n",
    "def summed_neuron_differences(num_neurons=10, top=True, positive=True, k=50):\n",
    "    summed_neuron_diffs = torch.zeros(model.cfg.d_mlp).cuda()\n",
    "    for neuron_pos in range(num_neurons):\n",
    "        # It would probably be more principle to do our filtering on the summed diffs instead of per neuron\n",
    "        neuron_diff, token_indices = get_individual_neuron_logprob_effect(indices, neuron_pos=neuron_pos, top=top, positive=positive, plot=False)\n",
    "        summed_neuron_diffs[token_indices] += neuron_diff\n",
    "    print(summed_neuron_diffs.shape)\n",
    "    print((summed_neuron_diffs>0).sum())\n",
    "    if positive:\n",
    "        non_zero_count = (summed_neuron_diffs > 0).sum()\n",
    "    else:\n",
    "        non_zero_count = (summed_neuron_diffs < 0).sum()\n",
    "    top_logprob_difference, top_tokens = haystack_utils.top_k_with_exclude(summed_neuron_diffs, min(non_zero_count, k), all_ignore, largest=positive)\n",
    "    return top_logprob_difference, top_tokens\n",
    "\n",
    "top_logprob_difference, top_topkens = summed_neuron_differences(top=True, positive=True)\n",
    "xticks = [model.to_str_tokens([i])[0] for i in top_tokens]\n",
    "haystack_utils.line(top_logprob_difference.cpu().numpy(), xlabel=\"Token\", ylabel=\"full_logprob - ablated_logprop\", title=\"Individual positive changes summed\", xticks=xticks, show_legend=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brainstorm notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to decompose an MLP5 neuron's effect into its boost to the correct logit and its deboost of other logits. I want to discover how these two effects change the log prob.\n",
    "\n",
    "metric like loss reduction vs. token boost\n",
    "\n",
    "~~run direct effect, patch each neuron in top 10 individually, get overall loss reduction from neuron controlled by context neuron (equivalent to logprob increase for correct token)~~\n",
    "decompose loss reduction into two parts:\n",
    "run direct effect, patch the correct token logit boost from the neuron (removing the neuron's other effects), get overall loss reduction from neuron (equivalent to logprob increase for correct token)\n",
    "destructive interference loss reduction = overall loss reduction - loss reduction from correct token logit boost (component of the logprob increase for correct token due to it deboosting incorrect token)\n",
    "\n",
    "Patch the correct token logit boost from the neuron (removing the neuron's other effects).\n",
    "1. Get baseline logprobs for a prompt\n",
    "2. Get difference in logits from activating the neuron under test. Can use get_direct_effects with return_type='logits'.\n",
    "3. Run the model with return_type='logits' and the neuron under test zero ablated. \n",
    "4. Add the correct token logit from step 1 to a copy of the output logits. Convert to logprobs\n",
    "5. Add the incorrect token logits from step 1 to a copy of the output logits. Convert to logprobs\n",
    "6. Compare A. lobprobs with correct answer token logit increase, B. logprobs with incorrect answer token logit increases, and C. baseline logprobs\n",
    "\n",
    "~~If the context neuron gives each neuron a flat boost then if we decompose the resulting flat boost to one MLP5 neuron into a boost to one logit vs. boosts to all other logits it will change the logprobs (first will increase answer probability and second will reduce answer probability). \n",
    "\n",
    "the two resulting log probs at the correct answer token won't add up to the original log probs (?). \n",
    "\n",
    "If the boost is flat the correct percentage decomposition is 1/50000 and 49999/50000? In practice/all other factors being equal\n",
    "\n",
    "New plan:\n",
    "\n",
    "Difference between baseline log prob and neuron log prob?\n",
    "Classify individual neurons by percentage constructive vs destructive by looking at their log probs and summing the incorrect token log probs\n",
    "\n",
    "Correct log prob difference\n",
    "Incorrect log prob difference (summed over every plausible token?)\n",
    "\n",
    "Largest boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the log prob for an incorrect token is significantly lower then that's where the extra probability density on the correct answer is coming from \n",
    "# Constructive interference increases correct token log prob and uniformly decreases other log probs\n",
    "# Destructive interference decreases specific other log probs and uniformly increases other log probs\n",
    " \n",
    "# Most neurons are a mixture of the above\n",
    "# Decompose neurons into what % of their effect is each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original_logits, ablated_logprobs, _, all_MLP5_logprobs = haystack_utils.get_direct_effect(prompts, model, pos=-2, context_ablation_hooks=deactivate_neurons_fwd_hooks, context_activation_hooks=activate_neurons_fwd_hooks, return_type='logprobs')\n",
    "# _, _, _, top_MLP5_ablated_logprobs = haystack_utils.get_direct_effect(prompts, model, pos=-2, context_ablation_hooks=deactivate_neurons_fwd_hooks, context_activation_hooks=activate_neurons_fwd_hooks+ablate_top_neurons_hook, return_type='logprobs')\n",
    "# _, _, _, bottom_MLP5_ablated_logprobs = haystack_utils.get_direct_effect(prompts, model, pos=-2, context_ablation_hooks=deactivate_neurons_fwd_hooks, context_activation_hooks=activate_neurons_fwd_hooks+ablate_bottom_neurons_hook, return_type='logprobs')\n",
    "\n",
    "# bottom_neuron_high_difference_logprobs = (all_MLP5_logprobs - bottom_MLP5_ablated_logprobs)\n",
    "# bottom_neuron_high_difference_logprobs[bottom_neuron_high_difference_logprobs < 0] = 0\n",
    "# bottom_neuron_high_difference_logprobs = bottom_neuron_high_difference_logprobs.mean(0)\n",
    "# bottom_neuron_high_difference_logprobs[all_MLP5_logprobs.mean(0) < -7] = 0\n",
    "\n",
    "# bottom_non_zero_count = (bottom_neuron_high_difference_logprobs > 0).sum()\n",
    "# bottom_neuron_high_difference_logprobs, bottom_indices = haystack_utils.top_k_with_exclude(bottom_neuron_high_difference_logprobs, min(bottom_non_zero_count, 50), all_ignore)\n",
    "# haystack_utils.line(bottom_neuron_high_difference_logprobs.cpu().numpy(), title='Largest positive difference in log probs for tokens when bottom neurons are not ablated', xticks=[model.to_str_tokens([i])[0] for i in bottom_indices])\n",
    "\n",
    "\n",
    "# top_neuron_high_difference_logprobs = (all_MLP5_logprobs - top_MLP5_ablated_logprobs)\n",
    "# top_neuron_high_difference_logprobs[top_neuron_high_difference_logprobs < 0] = 0\n",
    "# top_neuron_high_difference_logprobs = top_neuron_high_difference_logprobs.mean(0)\n",
    "# top_neuron_high_difference_logprobs[all_MLP5_logprobs.mean(0) < -7] = 0\n",
    "\n",
    "# top_non_zero_count = (top_neuron_high_difference_logprobs > 0).sum()\n",
    "# top_neuron_high_difference_logprobs, top_indices = haystack_utils.top_k_with_exclude(top_neuron_high_difference_logprobs, min(top_non_zero_count, 50), all_ignore)\n",
    "# haystack_utils.line(top_neuron_high_difference_logprobs.cpu().numpy(), title='Largest positive difference in log probs for tokens when bottom neurons are not ablated', xticks=[model.to_str_tokens([i])[0] for i in top_indices])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
