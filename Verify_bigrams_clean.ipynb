{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from transformer_lens import HookedTransformer, ActivationCache, utils, patching\n",
    "from jaxtyping import Float, Int, Bool\n",
    "from torch import Tensor\n",
    "from tqdm.auto import tqdm\n",
    "import plotly.io as pio\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "pio.renderers.default = \"notebook_connected\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.autograd.set_grad_enabled(False)\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "from haystack_utils import get_mlp_activations\n",
    "import haystack_utils\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98533b0c09a74bd0baab2472dbea5b4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/567 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "156c87b0618a4abd8ae1bac283d5c7ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/166M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbb881b240c149dda83644a2de7e4c93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/396 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b13ffa60cbf347888475cd3bbf5fe5f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de7853bc24364c9fb25c68164c188094",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model EleutherAI/pythia-70m into HookedTransformer\n",
      "data/german_europarl.json: Loaded 2000 examples with 152 to 2000 characters each.\n",
      "data/english_europarl.json: Loaded 2000 examples with 165 to 2000 characters each.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24415915d2d448d19137e8cdb42bd727",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cb71f4b2aa64264aa4fb222f37081c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = HookedTransformer.from_pretrained(\"EleutherAI/pythia-70m\",\n",
    "    center_unembed=True,\n",
    "    center_writing_weights=True,\n",
    "    fold_ln=True,\n",
    "    device=device)\n",
    "\n",
    "german_data = haystack_utils.load_json_data(\"data/german_europarl.json\")[:200]\n",
    "english_data = haystack_utils.load_json_data(\"data/english_europarl.json\")[:200]\n",
    "\n",
    "\n",
    "english_activations = {}\n",
    "german_activations = {}\n",
    "for layer in range(3, 4):\n",
    "    english_activations[layer] = get_mlp_activations(english_data, layer, model, mean=False)\n",
    "    german_activations[layer] = get_mlp_activations(german_data, layer, model, mean=False)\n",
    "\n",
    "LAYER_TO_ABLATE = 3\n",
    "NEURONS_TO_ABLATE = [669]\n",
    "MEAN_ACTIVATION_ACTIVE = german_activations[LAYER_TO_ABLATE][:, NEURONS_TO_ABLATE].mean()\n",
    "MEAN_ACTIVATION_INACTIVE = english_activations[LAYER_TO_ABLATE][:, NEURONS_TO_ABLATE].mean()\n",
    "\n",
    "def deactivate_neurons_hook(value, hook):\n",
    "    value[:, :, NEURONS_TO_ABLATE] = MEAN_ACTIVATION_INACTIVE\n",
    "    return value\n",
    "deactivate_neurons_fwd_hooks=[(f'blocks.{LAYER_TO_ABLATE}.mlp.hook_post', deactivate_neurons_hook)]\n",
    "\n",
    "def activate_neurons_hook(value, hook):\n",
    "    value[:, :, NEURONS_TO_ABLATE] = MEAN_ACTIVATION_ACTIVE\n",
    "    return value\n",
    "activate_neurons_fwd_hooks=[(f'blocks.{LAYER_TO_ABLATE}.mlp.hook_post', activate_neurons_hook)]\n",
    "\n",
    "all_ignore, not_ignore = haystack_utils.get_weird_tokens(model, plot_norms=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find top common German tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a84f0beaf9eb448e80baee2acbb3bd8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' der', 'en', ' die', ' und', 'ung', 'ä', ' in', ' den', ' des', ' zu', 'ch', 'n', 'st', 're', 'z', ' von', ' für', 'äsident', ' Pr', 'ischen', 't', 'ü', 'icht', 'gen', ' ist', ' auf', ' dass', 'ge', 'ig', ' im', 'in', ' über', 'g', ' das', 'te', ' er', 'men', ' w', 'es', ' an', 'ß', ' wir', ' eine', 'f', ' W', 'hen', 'w', ' Europ', ' ich', 'ungen', 'ren', 'le', ' dem', 'ten', ' ein', 'e', ' Z', ' Ver', 'der', ' B', ' mit', ' dies', 'h', ' nicht', 'ungs', 's', ' G', ' z', 'it', ' Herr', ' es', 'l', ' S', 'ich', 'lich', ' An', 'heit', 'ie', ' Er', ' zur', ' V', ' ver', 'u', 'hr', 'chaft', 'Der', ' Ich', ' Ab', ' haben', 'i', 'ant', 'chte', ' mö', 'er', ' K', 'igen', ' Ber', 'ür', ' Fra', 'em']\n"
     ]
    }
   ],
   "source": [
    "# Get top common german tokens excluding punctuation\n",
    "token_counts = torch.zeros(model.cfg.d_vocab).cuda()\n",
    "for example in tqdm(german_data):\n",
    "    tokens = model.to_tokens(example)\n",
    "    for token in tokens[0]:\n",
    "        token_counts[token.item()] += 1\n",
    "\n",
    "punctuation = [\"\\n\", \".\", \",\", \"!\", \"?\", \";\", \":\", \"-\", \"(\", \")\", \"[\", \"]\", \"{\", \"}\", \"<\", \">\", \"/\", \"\\\\\", \"\\\"\", \"'\"]\n",
    "leading_space_punctuation = [\" \" + char for char in punctuation]\n",
    "punctuation_tokens = model.to_tokens(punctuation + leading_space_punctuation + [' –', \" \", '  ', \"<|endoftext|>\"])[:, 1].flatten()\n",
    "token_counts[punctuation_tokens] = 0\n",
    "token_counts[all_ignore] = 0\n",
    "\n",
    "top_counts, top_tokens = torch.topk(token_counts, 100)\n",
    "print(model.to_str_tokens(top_tokens[:100]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of ngrams preceded by random prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_selection(tensor, n=12):\n",
    "    # Hacky replacement for np.random.choice\n",
    "    return tensor[torch.randperm(len(tensor))[:n]]\n",
    "\n",
    "def generate_random_prompts(end_string, n=50, length=12):\n",
    "    # Generate a batch of random prompts ending with a specific ngram\n",
    "    end_tokens = model.to_tokens(end_string).flatten()[1:]\n",
    "    prompts = []\n",
    "    for i in range(n):\n",
    "        prompt = get_random_selection(top_tokens[:max(50, length)], n=length).cuda()\n",
    "        prompt = torch.cat([prompt, end_tokens])\n",
    "        prompts.append(prompt)\n",
    "    prompts = torch.stack(prompts)\n",
    "    return prompts\n",
    "\n",
    "def replace_column(prompts: Int[Tensor, \"n_prompts n_tokens\"], token_index: int):\n",
    "    # Replaces a specific token position in a batch of prompts with random common German tokens\n",
    "    new_prompts = prompts.clone()\n",
    "    random_tokens = get_random_selection(top_tokens[:max(50, prompts.shape[0])], n=prompts.shape[0]).cuda()\n",
    "    new_prompts[:, token_index] = random_tokens\n",
    "    return new_prompts \n",
    "\n",
    "def loss_analysis(prompts: Tensor, title=\"\"):\n",
    "    # Loss plot for a batch of prompts\n",
    "    names = [\"Original\", \"Ablated\", \"MLP5 path patched\"]\n",
    "    original_loss, ablated_loss, _, only_activated_loss = \\\n",
    "        haystack_utils.get_direct_effect(prompts, model, pos=-1,\n",
    "                                        context_ablation_hooks=deactivate_neurons_fwd_hooks, \n",
    "                                        context_activation_hooks=activate_neurons_fwd_hooks, \n",
    "                                        )\n",
    "    haystack_utils.plot_barplot([original_loss.tolist(), ablated_loss.tolist(), only_activated_loss.tolist()], names, ylabel=\"Loss\", title=title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_analysis_random_prompts(end_string, n=50, length=12, replace_columns: list[int] | None = None):\n",
    "    # Loss plot for a batch of random prompts ending with a specific ngram and optionally replacing specific tokens\n",
    "    prompts = generate_random_prompts(end_string, n=n, length=length)\n",
    "    title=f\"Average last token loss on {length} random tokens ending in '{end_string}'\"\n",
    "    if replace_columns is not None:\n",
    "        replaced_tokens = model.to_str_tokens(prompts[0, replace_columns])\n",
    "        title += f\" replacing {replaced_tokens}\"\n",
    "        for column in replace_columns:\n",
    "            prompts = replace_column(prompts, column)\n",
    "    \n",
    "    loss_analysis(prompts, title=title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "# loss_analysis_random_prompts(\" Vorschlägen\", n=100, length=20)\n",
    "# loss_analysis_random_prompts(\" Vorschlägen\", n=100, length=20, replace_columns=[-2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27d0ebf3e68b40ebaa3c7d4c3a1028e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Ngram: ', options=(' Vorschlägen', ' Vorschläge', ' häufig', ' schließt', ' beweglich'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93b53510e8734ab28ac7bd213c5d4419",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SelectMultiple(description='Replace Columns:', index=(3,), options=('-2', '-3', '-4', 'None'), value=('None',)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34135cb7d2e3448f87cddeba75d329fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a dropdown menu widget\n",
    "dropdown = widgets.Dropdown(\n",
    "    options = [\" Vorschlägen\", \" Vorschläge\", \" häufig\", \" schließt\", \" beweglich\"], \n",
    "    value = \" Vorschlägen\",\n",
    "    description = 'Ngram: ',\n",
    ")\n",
    "\n",
    "replace_columns_dropdown = widgets.SelectMultiple(\n",
    "    options = ['-2', '-3', '-4', 'None'],\n",
    "    value = ['None'],  # default selected value\n",
    "    description = 'Replace Columns:',\n",
    ")\n",
    "\n",
    "# Create an output widget to hold the plot\n",
    "output = widgets.Output()\n",
    "\n",
    "# Define a function to call when the widget's value changes\n",
    "def update_plot(*args):\n",
    "    # Clear the old plot from the output\n",
    "    with output:\n",
    "        clear_output(wait=True)\n",
    "\n",
    "        if 'None' in replace_columns_dropdown.value:\n",
    "            replace_columns = None\n",
    "        else:\n",
    "            # If 'None' not selected, convert the selected values to integers\n",
    "            replace_columns = [int(val) for val in replace_columns_dropdown.value]\n",
    "        \n",
    "        # Call your function with the values from the widgets\n",
    "        loss_analysis_random_prompts(dropdown.value, n=100, length=20, replace_columns=replace_columns)\n",
    "\n",
    "# Set the function to be called when the widget's value changes\n",
    "dropdown.observe(update_plot, 'value')\n",
    "replace_columns_dropdown.observe(update_plot, 'value')\n",
    "\n",
    "# Display the widget and the output\n",
    "display(dropdown, replace_columns_dropdown, output)\n",
    "\n",
    "# Run once at startup\n",
    "update_plot()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify relevant neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 24])\n"
     ]
    }
   ],
   "source": [
    "prompts = generate_random_prompts(\" Vorschlägen\", n=100, length=20)\n",
    "print(prompts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ead48ca5a5745dc88ec266ffa0a3f74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2048 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0007)\n"
     ]
    }
   ],
   "source": [
    "with model.hooks(deactivate_neurons_fwd_hooks):\n",
    "    _, ablated_cache = model.run_with_cache(prompts)\n",
    "\n",
    "def get_ablate_neurons_hook(neuron: int | list[int], ablated_cache, layer=5):\n",
    "    def ablate_neurons_hook(value, hook):\n",
    "        value[:, :, neuron] = ablated_cache[f'blocks.{layer}.mlp.hook_post'][:, :, neuron]\n",
    "        return value\n",
    "    return [(f'blocks.{layer}.mlp.hook_post', ablate_neurons_hook)]\n",
    "\n",
    "diffs = torch.zeros(2048, prompts.shape[0])\n",
    "# Loss with path patched MLP5 neurons\n",
    "_, _, _, baseline_loss = haystack_utils.get_direct_effect(prompts, model, pos=-1, context_ablation_hooks=deactivate_neurons_fwd_hooks, context_activation_hooks=activate_neurons_fwd_hooks)\n",
    "for neuron in tqdm(range(2048)):\n",
    "    ablate_single_neuron_hook = get_ablate_neurons_hook(neuron, ablated_cache)\n",
    "    # Loss with path patched MLP5 neurons but a single neuron changed back to original ablated value\n",
    "    _, _, _, only_activated_loss = haystack_utils.get_direct_effect(prompts, model, pos=-1, context_ablation_hooks=deactivate_neurons_fwd_hooks, context_activation_hooks=activate_neurons_fwd_hooks+ablate_single_neuron_hook)\n",
    "    diffs[neuron] = only_activated_loss - baseline_loss\n",
    "\n",
    "print(diffs.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>                            <div id=\"6daf8209-4670-4b56-bc91-71a77178afc5\" class=\"plotly-graph-div\" style=\"height:525px; width:800px;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"6daf8209-4670-4b56-bc91-71a77178afc5\")) {                    Plotly.newPlot(                        \"6daf8209-4670-4b56-bc91-71a77178afc5\",                        [{\"hovertemplate\":\"variable=0\\u003cbr\\u003eindex=%{x}\\u003cbr\\u003evalue=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"0\",\"line\":{\"color\":\"#636efa\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"0\",\"showlegend\":true,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542,543,544,545,546,547,548,549,550,551,552,553,554,555,556,557,558,559,560,561,562,563,564,565,566,567,568,569,570,571,572,573,574,575,576,577,578,579,580,581,582,583,584,585,586,587,588,589,590,591,592,593,594,595,596,597,598,599,600,601,602,603,604,605,606,607,608,609,610,611,612,613,614,615,616,617,618,619,620,621,622,623,624,625,626,627,628,629,630,631,632,633,634,635,636,637,638,639,640,641,642,643,644,645,646,647,648,649,650,651,652,653,654,655,656,657,658,659,660,661,662,663,664,665,666,667,668,669,670,671,672,673,674,675,676,677,678,679,680,681,682,683,684,685,686,687,688,689,690,691,692,693,694,695,696,697,698,699,700,701,702,703,704,705,706,707,708,709,710,711,712,713,714,715,716,717,718,719,720,721,722,723,724,725,726,727,728,729,730,731,732,733,734,735,736,737,738,739,740,741,742,743,744,745,746,747,748,749,750,751,752,753,754,755,756,757,758,759,760,761,762,763,764,765,766,767,768,769,770,771,772,773,774,775,776,777,778,779,780,781,782,783,784,785,786,787,788,789,790,791,792,793,794,795,796,797,798,799,800,801,802,803,804,805,806,807,808,809,810,811,812,813,814,815,816,817,818,819,820,821,822,823,824,825,826,827,828,829,830,831,832,833,834,835,836,837,838,839,840,841,842,843,844,845,846,847,848,849,850,851,852,853,854,855,856,857,858,859,860,861,862,863,864,865,866,867,868,869,870,871,872,873,874,875,876,877,878,879,880,881,882,883,884,885,886,887,888,889,890,891,892,893,894,895,896,897,898,899,900,901,902,903,904,905,906,907,908,909,910,911,912,913,914,915,916,917,918,919,920,921,922,923,924,925,926,927,928,929,930,931,932,933,934,935,936,937,938,939,940,941,942,943,944,945,946,947,948,949,950,951,952,953,954,955,956,957,958,959,960,961,962,963,964,965,966,967,968,969,970,971,972,973,974,975,976,977,978,979,980,981,982,983,984,985,986,987,988,989,990,991,992,993,994,995,996,997,998,999,1000,1001,1002,1003,1004,1005,1006,1007,1008,1009,1010,1011,1012,1013,1014,1015,1016,1017,1018,1019,1020,1021,1022,1023,1024,1025,1026,1027,1028,1029,1030,1031,1032,1033,1034,1035,1036,1037,1038,1039,1040,1041,1042,1043,1044,1045,1046,1047,1048,1049,1050,1051,1052,1053,1054,1055,1056,1057,1058,1059,1060,1061,1062,1063,1064,1065,1066,1067,1068,1069,1070,1071,1072,1073,1074,1075,1076,1077,1078,1079,1080,1081,1082,1083,1084,1085,1086,1087,1088,1089,1090,1091,1092,1093,1094,1095,1096,1097,1098,1099,1100,1101,1102,1103,1104,1105,1106,1107,1108,1109,1110,1111,1112,1113,1114,1115,1116,1117,1118,1119,1120,1121,1122,1123,1124,1125,1126,1127,1128,1129,1130,1131,1132,1133,1134,1135,1136,1137,1138,1139,1140,1141,1142,1143,1144,1145,1146,1147,1148,1149,1150,1151,1152,1153,1154,1155,1156,1157,1158,1159,1160,1161,1162,1163,1164,1165,1166,1167,1168,1169,1170,1171,1172,1173,1174,1175,1176,1177,1178,1179,1180,1181,1182,1183,1184,1185,1186,1187,1188,1189,1190,1191,1192,1193,1194,1195,1196,1197,1198,1199,1200,1201,1202,1203,1204,1205,1206,1207,1208,1209,1210,1211,1212,1213,1214,1215,1216,1217,1218,1219,1220,1221,1222,1223,1224,1225,1226,1227,1228,1229,1230,1231,1232,1233,1234,1235,1236,1237,1238,1239,1240,1241,1242,1243,1244,1245,1246,1247,1248,1249,1250,1251,1252,1253,1254,1255,1256,1257,1258,1259,1260,1261,1262,1263,1264,1265,1266,1267,1268,1269,1270,1271,1272,1273,1274,1275,1276,1277,1278,1279,1280,1281,1282,1283,1284,1285,1286,1287,1288,1289,1290,1291,1292,1293,1294,1295,1296,1297,1298,1299,1300,1301,1302,1303,1304,1305,1306,1307,1308,1309,1310,1311,1312,1313,1314,1315,1316,1317,1318,1319,1320,1321,1322,1323,1324,1325,1326,1327,1328,1329,1330,1331,1332,1333,1334,1335,1336,1337,1338,1339,1340,1341,1342,1343,1344,1345,1346,1347,1348,1349,1350,1351,1352,1353,1354,1355,1356,1357,1358,1359,1360,1361,1362,1363,1364,1365,1366,1367,1368,1369,1370,1371,1372,1373,1374,1375,1376,1377,1378,1379,1380,1381,1382,1383,1384,1385,1386,1387,1388,1389,1390,1391,1392,1393,1394,1395,1396,1397,1398,1399,1400,1401,1402,1403,1404,1405,1406,1407,1408,1409,1410,1411,1412,1413,1414,1415,1416,1417,1418,1419,1420,1421,1422,1423,1424,1425,1426,1427,1428,1429,1430,1431,1432,1433,1434,1435,1436,1437,1438,1439,1440,1441,1442,1443,1444,1445,1446,1447,1448,1449,1450,1451,1452,1453,1454,1455,1456,1457,1458,1459,1460,1461,1462,1463,1464,1465,1466,1467,1468,1469,1470,1471,1472,1473,1474,1475,1476,1477,1478,1479,1480,1481,1482,1483,1484,1485,1486,1487,1488,1489,1490,1491,1492,1493,1494,1495,1496,1497,1498,1499,1500,1501,1502,1503,1504,1505,1506,1507,1508,1509,1510,1511,1512,1513,1514,1515,1516,1517,1518,1519,1520,1521,1522,1523,1524,1525,1526,1527,1528,1529,1530,1531,1532,1533,1534,1535,1536,1537,1538,1539,1540,1541,1542,1543,1544,1545,1546,1547,1548,1549,1550,1551,1552,1553,1554,1555,1556,1557,1558,1559,1560,1561,1562,1563,1564,1565,1566,1567,1568,1569,1570,1571,1572,1573,1574,1575,1576,1577,1578,1579,1580,1581,1582,1583,1584,1585,1586,1587,1588,1589,1590,1591,1592,1593,1594,1595,1596,1597,1598,1599,1600,1601,1602,1603,1604,1605,1606,1607,1608,1609,1610,1611,1612,1613,1614,1615,1616,1617,1618,1619,1620,1621,1622,1623,1624,1625,1626,1627,1628,1629,1630,1631,1632,1633,1634,1635,1636,1637,1638,1639,1640,1641,1642,1643,1644,1645,1646,1647,1648,1649,1650,1651,1652,1653,1654,1655,1656,1657,1658,1659,1660,1661,1662,1663,1664,1665,1666,1667,1668,1669,1670,1671,1672,1673,1674,1675,1676,1677,1678,1679,1680,1681,1682,1683,1684,1685,1686,1687,1688,1689,1690,1691,1692,1693,1694,1695,1696,1697,1698,1699,1700,1701,1702,1703,1704,1705,1706,1707,1708,1709,1710,1711,1712,1713,1714,1715,1716,1717,1718,1719,1720,1721,1722,1723,1724,1725,1726,1727,1728,1729,1730,1731,1732,1733,1734,1735,1736,1737,1738,1739,1740,1741,1742,1743,1744,1745,1746,1747,1748,1749,1750,1751,1752,1753,1754,1755,1756,1757,1758,1759,1760,1761,1762,1763,1764,1765,1766,1767,1768,1769,1770,1771,1772,1773,1774,1775,1776,1777,1778,1779,1780,1781,1782,1783,1784,1785,1786,1787,1788,1789,1790,1791,1792,1793,1794,1795,1796,1797,1798,1799,1800,1801,1802,1803,1804,1805,1806,1807,1808,1809,1810,1811,1812,1813,1814,1815,1816,1817,1818,1819,1820,1821,1822,1823,1824,1825,1826,1827,1828,1829,1830,1831,1832,1833,1834,1835,1836,1837,1838,1839,1840,1841,1842,1843,1844,1845,1846,1847,1848,1849,1850,1851,1852,1853,1854,1855,1856,1857,1858,1859,1860,1861,1862,1863,1864,1865,1866,1867,1868,1869,1870,1871,1872,1873,1874,1875,1876,1877,1878,1879,1880,1881,1882,1883,1884,1885,1886,1887,1888,1889,1890,1891,1892,1893,1894,1895,1896,1897,1898,1899,1900,1901,1902,1903,1904,1905,1906,1907,1908,1909,1910,1911,1912,1913,1914,1915,1916,1917,1918,1919,1920,1921,1922,1923,1924,1925,1926,1927,1928,1929,1930,1931,1932,1933,1934,1935,1936,1937,1938,1939,1940,1941,1942,1943,1944,1945,1946,1947,1948,1949,1950,1951,1952,1953,1954,1955,1956,1957,1958,1959,1960,1961,1962,1963,1964,1965,1966,1967,1968,1969,1970,1971,1972,1973,1974,1975,1976,1977,1978,1979,1980,1981,1982,1983,1984,1985,1986,1987,1988,1989,1990,1991,1992,1993,1994,1995,1996,1997,1998,1999,2000,2001,2002,2003,2004,2005,2006,2007,2008,2009,2010,2011,2012,2013,2014,2015,2016,2017,2018,2019,2020,2021,2022,2023,2024,2025,2026,2027,2028,2029,2030,2031,2032,2033,2034,2035,2036,2037,2038,2039,2040,2041,2042,2043,2044,2045,2046,2047],\"xaxis\":\"x\",\"y\":[-0.6007359027862549,-0.31919851899147034,-0.2199327051639557,-0.1668737828731537,-0.16574989259243011,-0.16379956901073456,-0.14556460082530975,-0.12248487770557404,-0.12234074622392654,-0.11845769733190536,-0.11752720177173615,-0.11435630917549133,-0.09695679694414139,-0.09539821743965149,-0.09399348497390747,-0.09053568542003632,-0.08620841801166534,-0.08300551772117615,-0.08053115010261536,-0.079640693962574,-0.07877379655838013,-0.07771771401166916,-0.07692842185497284,-0.07416336983442307,-0.07392935454845428,-0.07300633192062378,-0.07179445773363113,-0.07157692313194275,-0.07006619870662689,-0.06916283816099167,-0.06812112033367157,-0.06524992734193802,-0.0650116428732872,-0.0644085630774498,-0.0636548101902008,-0.06332019716501236,-0.06272611767053604,-0.0614628791809082,-0.059745583683252335,-0.05949284881353378,-0.05672990158200264,-0.05651706084609032,-0.05535873398184776,-0.05517834052443504,-0.05363880470395088,-0.05345983803272247,-0.053124696016311646,-0.05285663530230522,-0.0527777299284935,-0.050734132528305054,-0.05063912272453308,-0.05044198036193848,-0.04921351745724678,-0.04838503524661064,-0.04684070497751236,-0.04423639923334122,-0.0430934801697731,-0.04134596884250641,-0.04094065725803375,-0.04066450893878937,-0.04062505066394806,-0.04032102972269058,-0.039742741733789444,-0.03911450132727623,-0.037816766649484634,-0.037759121507406235,-0.037584368139505386,-0.03735833242535591,-0.036888737231492996,-0.03662778437137604,-0.036146149039268494,-0.0357448011636734,-0.035469900816679,-0.035455573350191116,-0.03543217107653618,-0.034515511244535446,-0.034180935472249985,-0.03391788527369499,-0.03374595567584038,-0.033357542008161545,-0.03332696855068207,-0.03293667733669281,-0.03266962990164757,-0.032513801008462906,-0.03235214576125145,-0.03220116347074509,-0.031622398644685745,-0.03148785978555679,-0.03096175193786621,-0.03072150982916355,-0.03012658841907978,-0.030064783990383148,-0.029994726181030273,-0.02982107549905777,-0.029130103066563606,-0.029022924602031708,-0.028894007205963135,-0.02873307280242443,-0.028673963621258736,-0.028597116470336914,-0.02858247235417366,-0.028547868132591248,-0.028491156175732613,-0.028441840782761574,-0.02829298935830593,-0.02821173705160618,-0.02811424992978573,-0.028053082525730133,-0.027928819879889488,-0.027821134775877,-0.02771458588540554,-0.027644965797662735,-0.027434194460511208,-0.026864470914006233,-0.0253622904419899,-0.02530132606625557,-0.02494913525879383,-0.024767983704805374,-0.02475966513156891,-0.024644603952765465,-0.02446926385164261,-0.02435019053518772,-0.02388235740363598,-0.023319413885474205,-0.0232570618391037,-0.022621404379606247,-0.022491171956062317,-0.022248826920986176,-0.021731289103627205,-0.02123703993856907,-0.021068496629595757,-0.020772727206349373,-0.02068815380334854,-0.0206136554479599,-0.02036088891327381,-0.020198654383420944,-0.020093992352485657,-0.020024623721837997,-0.020008141174912453,-0.01955411024391651,-0.01907062716782093,-0.018999852240085602,-0.01895030215382576,-0.018470600247383118,-0.018392080441117287,-0.018184736371040344,-0.018137026578187943,-0.01807662658393383,-0.017774278298020363,-0.017750129103660583,-0.01763257011771202,-0.017622506245970726,-0.0175943560898304,-0.01737102121114731,-0.017368637025356293,-0.017313610762357712,-0.016766643151640892,-0.016537850722670555,-0.016490699723362923,-0.016467411071062088,-0.01645415462553501,-0.016268165782094002,-0.016152972355484962,-0.016053399071097374,-0.01602322608232498,-0.01600796915590763,-0.015971578657627106,-0.015876827761530876,-0.015775728970766068,-0.015775002539157867,-0.015616540797054768,-0.015260917134582996,-0.015187821350991726,-0.014779901131987572,-0.014544177800416946,-0.014225294813513756,-0.014225074090063572,-0.014207197353243828,-0.014078754000365734,-0.013812732882797718,-0.01377385575324297,-0.013654908165335655,-0.013643499463796616,-0.01359268557280302,-0.013557493686676025,-0.013416322879493237,-0.013054865412414074,-0.013006283901631832,-0.012979685328900814,-0.01283958088606596,-0.012692010030150414,-0.012675944715738297,-0.012460161000490189,-0.012429377064108849,-0.01232810877263546,-0.012294714339077473,-0.01223078928887844,-0.012192542664706707,-0.012035124003887177,-0.01202375628054142,-0.011888102628290653,-0.011874237097799778,-0.011848142370581627,-0.011774355545639992,-0.011750086210668087,-0.011735200881958008,-0.011718396097421646,-0.011576307006180286,-0.011518287472426891,-0.011439654044806957,-0.011330214329063892,-0.011179459281265736,-0.01109428983181715,-0.010903873480856419,-0.010839836671948433,-0.010832381434738636,-0.010822433978319168,-0.01076476089656353,-0.010719068348407745,-0.010705973021686077,-0.010623575188219547,-0.01041534636169672,-0.010185289196670055,-0.00995553843677044,-0.009784635156393051,-0.00977354310452938,-0.009754760190844536,-0.009753814898431301,-0.00969311036169529,-0.009615377523005009,-0.009607941843569279,-0.009603464975953102,-0.009475473314523697,-0.009465389885008335,-0.009454545564949512,-0.009176051244139671,-0.009160451591014862,-0.009154986590147018,-0.009142225608229637,-0.00912310741841793,-0.00909363105893135,-0.009074327535927296,-0.008949973620474339,-0.00890372321009636,-0.00881984457373619,-0.008794467896223068,-0.008760649710893631,-0.00867676641792059,-0.008663732558488846,-0.00864866841584444,-0.008645748719573021,-0.008589060045778751,-0.008490136824548244,-0.008381995372474194,-0.008269396610558033,-0.00824301689863205,-0.008159831166267395,-0.008135417476296425,-0.008105330169200897,-0.00810087937861681,-0.008096464909613132,-0.00808628462255001,-0.007919587194919586,-0.007902099750936031,-0.007896185852587223,-0.00789366103708744,-0.007872152142226696,-0.007820721715688705,-0.007732099387794733,-0.007673501502722502,-0.007650877349078655,-0.007609873544424772,-0.007591595873236656,-0.007449120283126831,-0.007444945629686117,-0.0073983389884233475,-0.00733135687187314,-0.007286909967660904,-0.007282228674739599,-0.007266777101904154,-0.007013409864157438,-0.007012976333498955,-0.006958913989365101,-0.006937407422810793,-0.006913952063769102,-0.006909416988492012,-0.006847459822893143,-0.006786942481994629,-0.006773378700017929,-0.006771587301045656,-0.006765326485037804,-0.006762223783880472,-0.006753851193934679,-0.006717056035995483,-0.006607407238334417,-0.006482780445367098,-0.006471399217844009,-0.006453051697462797,-0.006434381008148193,-0.0064329719170928,-0.006405808962881565,-0.006403250619769096,-0.006390118505805731,-0.006385740824043751,-0.006379866972565651,-0.006369216367602348,-0.006356352474540472,-0.006284547504037619,-0.006282567512243986,-0.006260260473936796,-0.006180488970130682,-0.006176414899528027,-0.006170265842229128,-0.006162392441183329,-0.006161361932754517,-0.006153232883661985,-0.0061158365570008755,-0.006077860482037067,-0.0060647535137832165,-0.006061099004000425,-0.00604213634505868,-0.006035700906068087,-0.006035636179149151,-0.006031947210431099,-0.0060012186877429485,-0.005981083028018475,-0.00596501212567091,-0.005943749099969864,-0.005926589947193861,-0.005877960938960314,-0.0058489530347287655,-0.005808146204799414,-0.005796713754534721,-0.005754828453063965,-0.005710591096431017,-0.005676567554473877,-0.005661860574036837,-0.00561642786487937,-0.005598424468189478,-0.005598200485110283,-0.005578292533755302,-0.005565099883824587,-0.005527326371520758,-0.005475402344018221,-0.005470395553857088,-0.00542861083522439,-0.005409251432865858,-0.005399869754910469,-0.00535158533602953,-0.005341504700481892,-0.005336179863661528,-0.005335159134119749,-0.005332665983587503,-0.005320476368069649,-0.005286436062306166,-0.005280701909214258,-0.005276587791740894,-0.005273712333291769,-0.0052713509649038315,-0.005270661320537329,-0.005267673637717962,-0.0052523124031722546,-0.005245096515864134,-0.005190285388380289,-0.005158380139619112,-0.0051192063838243484,-0.0050413054414093494,-0.005016216076910496,-0.005002371501177549,-0.004996562376618385,-0.004985634237527847,-0.004918539896607399,-0.004892498720437288,-0.0048909797333180904,-0.004854854196310043,-0.004845374263823032,-0.004826997872442007,-0.00477822357788682,-0.004756205715239048,-0.0047525144182145596,-0.004682099912315607,-0.004659310448914766,-0.004622182343155146,-0.004620669409632683,-0.0045710038393735886,-0.004565207753330469,-0.004535759799182415,-0.004532168619334698,-0.004520401358604431,-0.004519306123256683,-0.004412529524415731,-0.00440373457968235,-0.00433068023994565,-0.0043000211007893085,-0.004283522255718708,-0.004231903702020645,-0.004223988391458988,-0.004223971161991358,-0.004201687406748533,-0.004153392277657986,-0.004129132255911827,-0.004114353563636541,-0.004107117187231779,-0.004083823412656784,-0.0040800916031003,-0.004054819233715534,-0.004053934011608362,-0.004049547947943211,-0.00403974112123251,-0.003959126304835081,-0.0039441343396902084,-0.003939141519367695,-0.003935635555535555,-0.003909369930624962,-0.0038970918394625187,-0.003862756537273526,-0.0038452825974673033,-0.003818214638158679,-0.0038069547154009342,-0.003730716649442911,-0.0037272856570780277,-0.003690136596560478,-0.0036875582300126553,-0.0036502457223832607,-0.003642162773758173,-0.0035850806161761284,-0.003582518082112074,-0.0035723329056054354,-0.0035365407820791006,-0.003495270386338234,-0.003486690577119589,-0.003480467712506652,-0.0034754055086523294,-0.0034655993804335594,-0.0034650235902518034,-0.0034635746851563454,-0.0034458793234080076,-0.003424261696636677,-0.003419304732233286,-0.0034052380360662937,-0.003381876740604639,-0.003378896042704582,-0.0033730894792824984,-0.0033643951173871756,-0.0033443481661379337,-0.003343986812978983,-0.003329094033688307,-0.003326499368995428,-0.003319493494927883,-0.0033010123297572136,-0.003295005764812231,-0.003291700966656208,-0.0032861989457160234,-0.003265579929575324,-0.0032521861139684916,-0.003183749970048666,-0.0031780677381902933,-0.0031610876321792603,-0.003160609630867839,-0.0031584955286234617,-0.003156485501676798,-0.0031401794403791428,-0.00313630816526711,-0.003135967766866088,-0.0031189112924039364,-0.003105929819867015,-0.0031000834424048662,-0.0030979590956121683,-0.0030931660439819098,-0.0030652135610580444,-0.003063832875341177,-0.0030581336468458176,-0.0030358070507645607,-0.003000476397573948,-0.0029795081354677677,-0.002966900821775198,-0.0028974891174584627,-0.002865453250706196,-0.0028639736119657755,-0.0028202675748616457,-0.0028051198460161686,-0.00277344579808414,-0.002767174504697323,-0.0027663663495332003,-0.0027636440936475992,-0.0027455822564661503,-0.002742186188697815,-0.0027186148799955845,-0.002712347311899066,-0.002709559863433242,-0.0027036231476813555,-0.002695489441975951,-0.0026892595924437046,-0.002687122207134962,-0.00268496572971344,-0.002652909839525819,-0.0026348757091909647,-0.002602183725684881,-0.002601027488708496,-0.0025968733243644238,-0.0025952477008104324,-0.002583945170044899,-0.0025709709152579308,-0.002568400464951992,-0.0025667815934866667,-0.002565261209383607,-0.002550391247496009,-0.002522792899981141,-0.002521681832149625,-0.002512673381716013,-0.0025038858875632286,-0.002496779663488269,-0.002485870150849223,-0.0024546068161726,-0.0024539947044104338,-0.002442520810291171,-0.002416430739685893,-0.0024115280248224735,-0.0023715696297585964,-0.002361353486776352,-0.002353748306632042,-0.002343397121876478,-0.002317204140126705,-0.0023151226341724396,-0.0022967972327023745,-0.002290186006575823,-0.0022883983328938484,-0.0022849254310131073,-0.0022560772486031055,-0.002252878388389945,-0.0022451903205364943,-0.002239508554339409,-0.0022367876954376698,-0.002230187179520726,-0.0022173600737005472,-0.0021891372743993998,-0.0021839661058038473,-0.0021782643161714077,-0.0021748244762420654,-0.0021604320500046015,-0.0021600236650556326,-0.002158004092052579,-0.002152950270101428,-0.0021513737738132477,-0.0021507327910512686,-0.002146657556295395,-0.0021178063470870256,-0.002111156238242984,-0.0021065673790872097,-0.0020680041052401066,-0.002063396852463484,-0.0020298631861805916,-0.002027621027082205,-0.002026638947427273,-0.0020265786442905664,-0.0020225662738084793,-0.002014076104387641,-0.0020139943808317184,-0.0019877899903804064,-0.001980991568416357,-0.0019791265949606895,-0.001976248575374484,-0.0019663681741803885,-0.001962319714948535,-0.001958396751433611,-0.001954170875251293,-0.0019484034273773432,-0.0019430263200774789,-0.0019366545602679253,-0.001926745055243373,-0.001925501972436905,-0.0018934122053906322,-0.001891252351924777,-0.0018902163719758391,-0.0018791542388498783,-0.0018763653934001923,-0.0018753745825961232,-0.0018430552445352077,-0.0018316154601052403,-0.0018313457258045673,-0.0018138474551960826,-0.0018075049156323075,-0.0017927985172718763,-0.0017879122169688344,-0.0017730948748067021,-0.0017515071667730808,-0.0017450484447181225,-0.0017438600771129131,-0.0017306022346019745,-0.0017281699692830443,-0.0017209824873134494,-0.0017003583488985896,-0.0016887910896912217,-0.0016755493124946952,-0.0016690443735569715,-0.0016675646184012294,-0.0016204884741455317,-0.0016104711685329676,-0.0016104606911540031,-0.001604553428478539,-0.001601786119863391,-0.0015981219476088881,-0.00157636939547956,-0.001571904867887497,-0.0015698543284088373,-0.0015604211948812008,-0.0015460781287401915,-0.0015385088045150042,-0.0015277251368388534,-0.0015205342788249254,-0.0015166084049269557,-0.0015155668370425701,-0.0015148813836276531,-0.0015048468485474586,-0.001494383323006332,-0.0014742102939635515,-0.0014718063175678253,-0.0014603914460167289,-0.0014433528995141387,-0.0014433176256716251,-0.00144191924482584,-0.001440967433154583,-0.001435566577129066,-0.0014306276571005583,-0.0014081987319514155,-0.0013897557510063052,-0.0013883804203942418,-0.001379943685606122,-0.0013757603010162711,-0.0013685203157365322,-0.0013656052760779858,-0.0013571492163464427,-0.0013565312838181853,-0.0013465574011206627,-0.00134386180434376,-0.001336375717073679,-0.0013186638243496418,-0.0013182582333683968,-0.0013157647335901856,-0.0013144578551873565,-0.0013140657683834434,-0.0013058813055977225,-0.0013054825831204653,-0.0013035967713221908,-0.0013027957174926996,-0.001301027718000114,-0.0012924299808219075,-0.0012647181283682585,-0.0012475347612053156,-0.0012388058239594102,-0.0012211923021823168,-0.0012164245126768947,-0.001205463893711567,-0.0011980482377111912,-0.0011970496270805597,-0.0011862092651426792,-0.0011712205596268177,-0.0011707268422469497,-0.001169001217931509,-0.001166806323453784,-0.0011566291796043515,-0.0011351150460541248,-0.0011257438454777002,-0.0011176728876307607,-0.0011151628568768501,-0.0011145598255097866,-0.0011141615686938167,-0.0011140406131744385,-0.0011052540503442287,-0.0010976791381835938,-0.0010796680580824614,-0.0010743726743385196,-0.0010557891800999641,-0.0010554949985817075,-0.0010548945283517241,-0.0010544579708948731,-0.001053547952324152,-0.0010489050764590502,-0.0010439340258017182,-0.0010389287490397692,-0.0010268724290654063,-0.0010240456322208047,-0.0010161034297198057,-0.0010062514338642359,-0.0010035220766440034,-0.0010029483819380403,-0.000993056921288371,-0.0009909475920721889,-0.0009839626727625728,-0.0009785032598301768,-0.0009770754259079695,-0.0009663043892942369,-0.0009594674338586628,-0.0009466327610425651,-0.0009381853160448372,-0.000931169546674937,-0.000914763833861798,-0.0009019345161505044,-0.0008961732964962721,-0.0008939167601056397,-0.0008913682540878654,-0.0008814889006316662,-0.0008795513422228396,-0.0008753418806008995,-0.0008721162448637187,-0.0008631880627945065,-0.0008573874947614968,-0.0008512274944223464,-0.0008449234301224351,-0.0008406501729041338,-0.0008402336970902979,-0.0008388942223973572,-0.0008387601119466126,-0.000837941188365221,-0.0008352595614269376,-0.0008305689552798867,-0.0008304220391437411,-0.0008174382383003831,-0.0008127449546009302,-0.0008123324951156974,-0.0008068382740020752,-0.0008014975464902818,-0.0007981462986208498,-0.0007964992546476424,-0.0007763935718685389,-0.0007697902619838715,-0.0007666221354156733,-0.0007665245211683214,-0.0007629929459653795,-0.0007628819439560175,-0.0007583223050460219,-0.0007543504470959306,-0.0007541727973148227,-0.0007429890101775527,-0.00072545709554106,-0.0007230390328913927,-0.0007192904013209045,-0.0007151082390919328,-0.0007129035657271743,-0.0007119615329429507,-0.0007100297370925546,-0.0007073458982631564,-0.000705890532117337,-0.0007058790070004761,-0.0006969387759454548,-0.0006895740516483784,-0.0006796057568863034,-0.000675315095577389,-0.0006744572310708463,-0.0006735281785950065,-0.0006727144354954362,-0.0006684322725050151,-0.0006646555848419666,-0.0006642772350460291,-0.0006624227971769869,-0.0006561882910318673,-0.0006487928330898285,-0.0006481422460637987,-0.0006466121994890273,-0.0006406003376469016,-0.000639046891592443,-0.0006385479937307537,-0.0006321455584838986,-0.0006240287330001593,-0.0006112864357419312,-0.0006087164510972798,-0.0006083987536840141,-0.0005956970271654427,-0.0005906891892664135,-0.0005837632925249636,-0.000576092628762126,-0.0005746281240135431,-0.0005717948079109192,-0.0005617716815322638,-0.0005493196658790112,-0.0005468967719934881,-0.000539687869604677,-0.0005379645735956728,-0.0005356031470000744,-0.0005315327434800565,-0.0005301800556480885,-0.0005216108402237296,-0.0005202795728109777,-0.0005118428380228579,-0.0005118150729686022,-0.0004980105441063643,-0.0004926983965560794,-0.0004874581063631922,-0.0004855181323364377,-0.00048049449105747044,-0.0004768758954014629,-0.0004677932010963559,-0.0004669389163609594,-0.00046388685586862266,-0.0004542462411336601,-0.0004541469970718026,-0.00044799596071243286,-0.0004432125424500555,-0.00043777376413345337,-0.0004286305629648268,-0.00042623491026461124,-0.00041565284482203424,-0.00041372611303813756,-0.0004127632128074765,-0.0004117573844268918,-0.0004102006496395916,-0.00040957346209324896,-0.0003993961145170033,-0.0003970316029153764,-0.0003961172769777477,-0.00039511083741672337,-0.00039447814924642444,-0.0003917333378922194,-0.0003916938730981201,-0.0003783979918807745,-0.0003779636463150382,-0.0003773675998672843,-0.00036841363180428743,-0.0003652285085991025,-0.00036314845783635974,-0.00036217286833561957,-0.00036002681008540094,-0.0003599572228267789,-0.0003521849284879863,-0.00034971832064911723,-0.00034796656109392643,-0.000345203879987821,-0.00033819585223682225,-0.0003345885779708624,-0.00032730743987485766,-0.00032588481553830206,-0.0003243993269279599,-0.00032353296410292387,-0.0003207883273717016,-0.0003193870070390403,-0.0003188434347975999,-0.0003163963556289673,-0.0003113095590379089,-0.0003085002244915813,-0.0003051412058994174,-0.0002987058542203158,-0.0002947270986624062,-0.00029077232466079295,-0.0002884996065404266,-0.0002875213394872844,-0.00028657703660428524,-0.00028336449759081006,-0.0002813266182783991,-0.00027333616162650287,-0.00027090683579444885,-0.00026939957751892507,-0.00026903211255557835,-0.00026314781280234456,-0.00025836468557827175,-0.0002577830746304244,-0.0002569603966549039,-0.00025577173801138997,-0.00024747996940277517,-0.00024626791127957404,-0.00024601281620562077,-0.00024275362375192344,-0.00024049416242633015,-0.00023934825730975717,-0.00023859455541241914,-0.00023831606085877866,-0.00023640558356419206,-0.00023594200320076197,-0.00022995755716692656,-0.0002280354528920725,-0.0002217061846749857,-0.00021688654669560492,-0.00021581948385573924,-0.00021409451437648386,-0.00021392777853179723,-0.00021251484577078372,-0.00021097168792039156,-0.00020986184244975448,-0.0002070529735647142,-0.00020608186605386436,-0.00019929319387301803,-0.00019831315148621798,-0.0001977461506612599,-0.00019492134742904454,-0.0001914829044835642,-0.00019136637274641544,-0.00018578246817924082,-0.00018297613132745028,-0.0001782691542757675,-0.0001781830214895308,-0.00017809838755056262,-0.00017772063438314945,-0.00016901582421269268,-0.00016850724932737648,-0.00016711518401280046,-0.0001662020367803052,-0.00016337528359144926,-0.00015853151853661984,-0.00015780582907609642,-0.00015556484868284315,-0.00015443981101270765,-0.0001544076221762225,-0.0001540365774417296,-0.00015101581811904907,-0.00014878615911584347,-0.00014869168808218092,-0.0001484350796090439,-0.00014771580754313618,-0.0001458129263482988,-0.0001418055617250502,-0.00014149874914437532,-0.00013921484060119838,-0.00013581574603449553,-0.00013540357758756727,-0.0001338569854851812,-0.0001319731818512082,-0.00012938260624650866,-0.00012831910862587392,-0.00012631475692614913,-0.00012461096048355103,-0.00011791005817940459,-0.00011219024599995464,-0.0001088237768271938,-0.00010801955795614049,-0.00010529696737648919,-0.00010466888488736004,-0.00010290980571880937,-0.00010137364006368443,-9.93932771962136e-05,-9.930938540492207e-05,-9.749204036779702e-05,-9.747132571646944e-05,-9.263723768526688e-05,-9.209766722051427e-05,-9.109541861107573e-05,-8.85610279510729e-05,-8.378506026929244e-05,-8.314475417137146e-05,-8.222684118663892e-05,-8.006736607057974e-05,-7.86848395364359e-05,-7.795929559506476e-05,-7.731080404482782e-05,-7.706836186116561e-05,-7.039219053694978e-05,-6.933808617759496e-05,-6.862208101665601e-05,-6.734535418218002e-05,-6.612866854993626e-05,-6.426170148188248e-05,-6.313994526863098e-05,-6.28846901236102e-05,-5.7534278312232345e-05,-5.598083225777373e-05,-5.399420842877589e-05,-5.383417010307312e-05,-5.127251279191114e-05,-4.9754828069126233e-05,-4.8408805014332756e-05,-4.8245638026855886e-05,-4.47890161012765e-05,-4.348769653006457e-05,-4.199177055852488e-05,-4.009142503491603e-05,-3.9595961425220594e-05,-3.835111783700995e-05,-3.822162761935033e-05,-3.7107318348716944e-05,-3.699675289681181e-05,-3.616854519350454e-05,-3.427639603614807e-05,-3.214821117580868e-05,-3.067687066504732e-05,-2.888470953621436e-05,-2.8843731342931278e-05,-2.83379849861376e-05,-2.8217733415658586e-05,-2.6269555746694095e-05,-2.4099499569274485e-05,-2.3042857719701715e-05,-2.107173168042209e-05,-2.0062625480932184e-05,-1.8637776520336047e-05,-1.84038290171884e-05,-1.7243772163055837e-05,-1.6377716747228988e-05,-1.5716850612079725e-05,-1.5611201888532378e-05,-1.4457702491199598e-05,-1.429289568477543e-05,-1.3812184079142753e-05,-1.2605488336703274e-05,-1.2085289199603721e-05,-1.030892144626705e-05,-1.0191500223299954e-05,-8.994340532808565e-06,-7.313788046303671e-06,-7.188171366578899e-06,-1.7970800172406598e-06,-1.7823278994910652e-06,-1.0724365893111099e-06,-6.079673653403006e-08,1.0728836485895954e-08,4.5821070671081543e-07,1.204311843139294e-06,3.0788778531132266e-06,4.273205831850646e-06,4.425794031703845e-06,5.572289410338271e-06,5.738138952438021e-06,7.0804358074383345e-06,8.399039870710112e-06,8.716731827007607e-06,8.746832463657483e-06,9.404569937032647e-06,1.0437965102028102e-05,1.0544359611230902e-05,1.2073070138285402e-05,1.2689232789853122e-05,1.3353526810533367e-05,1.4491080946754664e-05,1.6385614799219184e-05,1.8405764421913773e-05,1.8459559214534238e-05,1.947060263773892e-05,1.9727498511201702e-05,2.0915716959279962e-05,2.2253692804952152e-05,2.2307634935714304e-05,2.3471564418287016e-05,2.3664682885282673e-05,2.4246275643236004e-05,2.45013834501151e-05,2.6016385163529776e-05,2.7541518647922203e-05,2.7547330319066532e-05,2.773821324808523e-05,2.7973950636805966e-05,2.8717815439449623e-05,3.06709116557613e-05,3.088325320277363e-05,3.094405110459775e-05,3.194198143319227e-05,3.234326868550852e-05,3.253027898608707e-05,3.506123903207481e-05,3.51256130670663e-05,3.566890882211737e-05,3.608122642617673e-05,3.665968688437715e-05,4.3552219722187147e-05,4.451662243809551e-05,4.474610250326805e-05,4.627332236850634e-05,4.70140585093759e-05,4.853844802710228e-05,4.905626337858848e-05,5.026266080676578e-05,5.18833112437278e-05,5.422651884146035e-05,5.5686086852801964e-05,6.499976007035002e-05,6.617486360482872e-05,6.752416811650619e-05,6.846264295745641e-05,7.016480230959132e-05,7.036671013338491e-05,7.1469992690254e-05,7.290616485988721e-05,7.333293615374714e-05,7.432386337313801e-05,7.72437488194555e-05,7.858872413635254e-05,7.908940460765734e-05,8.013814658625051e-05,8.392333984375e-05,8.498490205965936e-05,8.556887769373134e-05,8.900969987735152e-05,9.109139500651509e-05,9.301647514803335e-05,9.350776963401586e-05,9.470537042943761e-05,9.68694657785818e-05,9.782671986613423e-05,9.920060983859003e-05,9.920105367200449e-05,9.966656216420233e-05,0.00010108813876286149,0.00010129779548151419,0.00010674506484065205,0.00010978653881466016,0.00011243432527408004,0.00011722699127858505,0.00011811465083155781,0.00012275681365281343,0.0001236051321029663,0.00013305187167134136,0.00013380273594520986,0.00013383761688601226,0.0001347346551483497,0.0001404881477355957,0.00014086499868426472,0.00014127939357422292,0.00014714330609422177,0.0001486401306465268,0.0001487608242314309,0.0001488746638642624,0.0001516962074674666,0.00015318795340135694,0.0001546049170428887,0.00015819161490071565,0.00015889435599092394,0.000159076604177244,0.00015932128007989377,0.0001600417454028502,0.0001609179307706654,0.00016524926468264312,0.00016556904301978648,0.00016986906121019274,0.00017614082025829703,0.0001775728160282597,0.00018101751629728824,0.0001820383913582191,0.00018369957979302853,0.00018758415535558015,0.00019215613428968936,0.00019393696857150644,0.00019509806588757783,0.00019539833010639995,0.00019549757416825742,0.00019944057567045093,0.00020166054309811443,0.00020393937302287668,0.0002073800569633022,0.00021242201910354197,0.00021359608217608184,0.00021395027579274029,0.00021846368326805532,0.00022033005370758474,0.00022418885782826692,0.000229024444706738,0.00022996052575763315,0.00023232221428770572,0.00023648068599868566,0.0002365250838920474,0.00023987636086530983,0.00024326294078491628,0.00024800613755360246,0.00024820491671562195,0.00024986654170788825,0.0002518297696951777,0.00025888398522511125,0.00025925456429831684,0.00026202143635600805,0.0002629731607157737,0.00026670380611903965,0.0002697198069654405,0.000272552075330168,0.0002755562891252339,0.00027972369571216404,0.0002826456620823592,0.0002841106033883989,0.0002845278359018266,0.0002886353468056768,0.0002951240458060056,0.0002993615053128451,0.0002998873533215374,0.0003035767294932157,0.00030528916977345943,0.00030532164964824915,0.0003076632274314761,0.00030895264353603125,0.0003165279340464622,0.00031791775836609304,0.0003206725523341447,0.00032140687108039856,0.0003243732498958707,0.0003245122788939625,0.00032520442618988454,0.0003298722149338573,0.0003336106310598552,0.000336634082486853,0.0003377246903255582,0.00034402310848236084,0.000347424007486552,0.0003523539053276181,0.00035786614171229303,0.0003589738917071372,0.0003606140671763569,0.00036127641214989126,0.0003634415625128895,0.0003657318593468517,0.0003683862159959972,0.0003705793642438948,0.00038314296398311853,0.0003844818565994501,0.00038855551974847913,0.0003903631877619773,0.0003960438189096749,0.00039993508835323155,0.00040421963785775006,0.0004054559685755521,0.00040694564813748,0.00041353152482770383,0.0004139697412028909,0.00041976614738814533,0.00042776629561558366,0.0004370072565507144,0.0004390063986647874,0.00044576614163815975,0.00044788242666982114,0.0004479314375203103,0.00045437575317919254,0.0004614856734406203,0.0004643948341254145,0.0004668198525905609,0.0004700474382843822,0.0004733249661512673,0.0004788206424564123,0.00048722937935963273,0.0004882423672825098,0.0004929095739498734,0.0004977668868377805,0.0005004909471608698,0.0005065602017566562,0.0005088757025077939,0.0005104836891405284,0.0005146649200469255,0.0005287929088808596,0.0005360955256037414,0.0005378677160479128,0.0005448213196359575,0.000553489604499191,0.0005568121559917927,0.0005576731055043638,0.0005582491867244244,0.0005583085003308952,0.0005598920397460461,0.0005631233798339963,0.0005699129542335868,0.0005817365599796176,0.0005940322298556566,0.0005942381685599685,0.0005984216695651412,0.0006013917736709118,0.0006147484527900815,0.0006158011965453625,0.0006204973324202001,0.0006236116751097143,0.000644091225694865,0.0006476895650848746,0.0006489428924396634,0.0006509642116725445,0.0006532014813274145,0.0006604322697967291,0.0006654555909335613,0.000665942148771137,0.0006659877253696322,0.000677960051689297,0.0007116250926628709,0.000718706869520247,0.0007257285760715604,0.0007330724620260298,0.0007434881990775466,0.0007468664553016424,0.0007514160824939609,0.0007699075504206121,0.0007734294049441814,0.0007744951290078461,0.0007887154933996499,0.000793666229583323,0.0008005545823834836,0.0008066028240136802,0.0008156216354109347,0.0008292552665807307,0.0008304587099701166,0.0008335858583450317,0.0008368114940822124,0.0008388145361095667,0.000840191962197423,0.0008508127648383379,0.0008511154446750879,0.0008611795492470264,0.0008785842219367623,0.0008823880343697965,0.0008919457904994488,0.0008921811240725219,0.0008997092954814434,0.0009011873626150191,0.0009017364936880767,0.0009031604276970029,0.0009111194522120059,0.0009255278273485601,0.000931400980334729,0.0009321217075921595,0.0009559215395711362,0.000958784541580826,0.0009762939880602062,0.000978234806098044,0.000989609630778432,0.000991695560514927,0.0010090272407978773,0.0010172517504543066,0.0010317941196262836,0.0010331619996577501,0.001040707342326641,0.0010512549197301269,0.001064790179952979,0.0010648956522345543,0.0010651713237166405,0.0010668560862541199,0.001067231991328299,0.0010709549533203244,0.001081067486666143,0.001084637944586575,0.0010860031470656395,0.0010912026045843959,0.0010955723701044917,0.0011079540709033608,0.0011138702975586057,0.0011167152551934123,0.0011177979176864028,0.0011273138225078583,0.0011302585480734706,0.0011408993741497397,0.0011460566893219948,0.0011505017755553126,0.0011525946902111173,0.0011552111245691776,0.0011559991398826241,0.001165988389402628,0.0011728338431566954,0.0011859292862936854,0.0011978086549788713,0.0011989388149231672,0.0012076165294274688,0.0012103156186640263,0.0012129240203648806,0.0012138986494392157,0.0012210028944537044,0.0012280935188755393,0.0012318296357989311,0.00123990373685956,0.001245734398253262,0.0012490525841712952,0.0012540959287434816,0.0012545010540634394,0.001254684291779995,0.0012639600317925215,0.0012778905220329762,0.0012785904109477997,0.0012909338111057878,0.0012960892636328936,0.001299348776228726,0.001300646923482418,0.0013165121199563146,0.001331449137069285,0.0013367305509746075,0.0013381864409893751,0.001345447264611721,0.0013549859868362546,0.0013585032429546118,0.0013651188928633928,0.0013719856506213546,0.0013841954059898853,0.001389584969729185,0.0013953919988125563,0.0013957375194877386,0.0013965073740109801,0.00140523596201092,0.001411590026691556,0.0014132416108623147,0.0014181293081492186,0.0014237880241125822,0.0014318750472739339,0.0014374781167134643,0.0014410216826945543,0.0014504253631457686,0.0014507508603855968,0.0014537898823618889,0.0014539407566189766,0.0014604197349399328,0.0014889755984768271,0.001531273708678782,0.001541884965263307,0.0015471273800358176,0.0015497368294745684,0.001558205345645547,0.0015613749856129289,0.001565461279824376,0.0015719995135441422,0.0015727957943454385,0.0015835101949051023,0.0015921428566798568,0.0015955936396494508,0.0016138517530635,0.0016177630750462413,0.0016264371806755662,0.0016299554845318198,0.001641012728214264,0.001650449587032199,0.0016532604349777102,0.0016533839516341686,0.0016591899329796433,0.0016593943582847714,0.0016685526352375746,0.0016717667458578944,0.0016731690848246217,0.001675153849646449,0.0016797161661088467,0.001688585034571588,0.0016913354629650712,0.0016931849531829357,0.0016971684526652098,0.00171540561132133,0.001727874856442213,0.0017364302184432745,0.0017377383774146438,0.0017423101235181093,0.0017694070702418685,0.001771316397935152,0.0017777313478291035,0.0017868219874799252,0.0018425763119012117,0.00184440182056278,0.0018445263849571347,0.0018476517871022224,0.0018651224672794342,0.0018670446006581187,0.0018699002685025334,0.0018706565024331212,0.0018726279959082603,0.00187555025331676,0.0018900383729487658,0.0018906790064647794,0.0018943905597552657,0.0019214755157008767,0.0019221173133701086,0.00193317502271384,0.0019401217577978969,0.001942484057508409,0.0019472830463200808,0.001948421006090939,0.0019485600059852004,0.0019585350528359413,0.001961766742169857,0.001961951144039631,0.001974544720724225,0.001994058024138212,0.001995238708332181,0.002012664685025811,0.002020682906731963,0.002034222474321723,0.002036391757428646,0.0020547397434711456,0.002064076252281666,0.0020755778532475233,0.002077140612527728,0.0020974143408238888,0.002098034368827939,0.0021186969242990017,0.0021226524841040373,0.0021267456468194723,0.002127427374944091,0.0021294436883181334,0.0021308748982846737,0.002136458409950137,0.002148904837667942,0.0021555342245846987,0.002169078215956688,0.0021990241948515177,0.002205055905506015,0.0022259571123868227,0.0022272667847573757,0.002235367661342025,0.002243479248136282,0.002245331648737192,0.0022465719375759363,0.002252942882478237,0.0022651071194559336,0.002265507820993662,0.0022665325086563826,0.0022680768743157387,0.0022863619960844517,0.0022940782364457846,0.002295687096193433,0.0023051858879625797,0.002311460208147764,0.002327111316844821,0.0023313299752771854,0.002334489254280925,0.002349215094000101,0.0023551727645099163,0.002358972327783704,0.0023645744659006596,0.0023673553951084614,0.0023861341178417206,0.002401388017460704,0.002402388956397772,0.0024248443078249693,0.0024272436276078224,0.0024306478444486856,0.0024420765694230795,0.0024632150307297707,0.0024888692423701286,0.002494111889973283,0.0025089187547564507,0.002513394458219409,0.002531610429286957,0.0025660640094429255,0.002598236547783017,0.002602384192869067,0.0026168907061219215,0.002636837773025036,0.002666339511051774,0.002673214767128229,0.0026747926604002714,0.0026860793586820364,0.0027033828664571047,0.0027186244260519743,0.002720374846830964,0.0027204900979995728,0.00273775402456522,0.0027378881350159645,0.0027536069974303246,0.0027552759274840355,0.0027615141589194536,0.002762825461104512,0.0027844863943755627,0.0028017449658364058,0.0028126609977334738,0.002892232034355402,0.002910953713580966,0.002912584226578474,0.0029352232813835144,0.0029451220761984587,0.002948152832686901,0.0029546564910560846,0.002994805807247758,0.003001350676640868,0.0030033416114747524,0.0030205887742340565,0.0030216097366064787,0.0030261902138590813,0.003031509229913354,0.003056570887565613,0.00311095779761672,0.0031119126360863447,0.003131269011646509,0.003152316203340888,0.003191114868968725,0.0031969451811164618,0.0032122558914124966,0.003230435773730278,0.003236338496208191,0.0032459963113069534,0.003247929736971855,0.0032702821772545576,0.0032879526261240244,0.003314332338050008,0.003328140592202544,0.0033480944111943245,0.0033521619625389576,0.003352919826284051,0.0033667630050331354,0.003369983984157443,0.003372526727616787,0.0034173177555203438,0.003422647016122937,0.003470802213996649,0.0034756576642394066,0.003563400125131011,0.003569179680198431,0.003584408201277256,0.0035926690325140953,0.003673489671200514,0.0036783183459192514,0.0037127393297851086,0.003715094644576311,0.003716969396919012,0.003754660487174988,0.0037572418805211782,0.003761123400181532,0.0037854760885238647,0.0038052715826779604,0.003817732911556959,0.0038199753034859896,0.003836201038211584,0.00383774284273386,0.0038515455089509487,0.003915558569133282,0.003937344532459974,0.0039779990911483765,0.004003379959613085,0.004020386375486851,0.0040486715734004974,0.004077906720340252,0.004088615067303181,0.004089741501957178,0.004118802957236767,0.004146728664636612,0.004186002071946859,0.004215310327708721,0.004290435463190079,0.004296071361750364,0.004325162619352341,0.0043894327245652676,0.004389562178403139,0.004396435804665089,0.004421336576342583,0.004436648916453123,0.0044583166018128395,0.004464533645659685,0.004472062457352877,0.004505672957748175,0.004510342609137297,0.004515267442911863,0.004518870264291763,0.0045597436837852,0.0045720115303993225,0.0046063982881605625,0.004680062644183636,0.004690459929406643,0.0047046164982020855,0.004723763093352318,0.004745230544358492,0.004772723186761141,0.0047990321181714535,0.00479922816157341,0.004843974485993385,0.00484800199046731,0.004860293120145798,0.004915524739772081,0.004919642582535744,0.004938480444252491,0.004996596369892359,0.005010709632188082,0.005012491252273321,0.005019504576921463,0.005110027268528938,0.005119210109114647,0.005156300496309996,0.0051565770991146564,0.0051751770079135895,0.005176201928406954,0.005203993525356054,0.00520983524620533,0.005210683215409517,0.0052381944842636585,0.005294403992593288,0.00533577986061573,0.005384030286222696,0.005388057790696621,0.0053901528008282185,0.005445918533951044,0.005500533618032932,0.005503856111317873,0.005531632341444492,0.0055375900119543076,0.005580418277531862,0.005582996644079685,0.005610709078609943,0.005655215587466955,0.005669374484568834,0.0056916107423603535,0.00572981545701623,0.005734637845307589,0.00576559966430068,0.0057676867581903934,0.005836943164467812,0.005876818206161261,0.005890330299735069,0.005948830861598253,0.005962503608316183,0.005973111372441053,0.005989476572722197,0.00601095100864768,0.006013857200741768,0.006020695436745882,0.006066435482352972,0.006114855408668518,0.006134030409157276,0.00621056417003274,0.006217521615326405,0.006224663928151131,0.006243327632546425,0.006254048552364111,0.006335064768791199,0.006364564411342144,0.006377392914146185,0.006387119181454182,0.00639871321618557,0.0063995360396802425,0.006414430215954781,0.006433768663555384,0.006483627948909998,0.006529086735099554,0.006543783936649561,0.006567005999386311,0.006592032965272665,0.006639787927269936,0.006654451601207256,0.006657897029072046,0.00671624718233943,0.006791794206947088,0.006818985566496849,0.0068959821946918964,0.006914619356393814,0.0069293249398469925,0.006939440965652466,0.006962573155760765,0.007015770301222801,0.00702576944604516,0.00706855021417141,0.007207234390079975,0.00726676918566227,0.007280778139829636,0.007298725191503763,0.007339117117226124,0.007357817143201828,0.007368721533566713,0.0073881554417312145,0.007393181789666414,0.007444996386766434,0.007459302432835102,0.0074787382036447525,0.007497199345380068,0.007541353814303875,0.007566886022686958,0.007567224092781544,0.007728023920208216,0.007791259791702032,0.007942060008645058,0.007968940772116184,0.007997601293027401,0.008002330549061298,0.00801110826432705,0.008023339323699474,0.008122598752379417,0.008124993182718754,0.008133693598210812,0.008247229270637035,0.008259737864136696,0.00834251195192337,0.008352555334568024,0.008352970704436302,0.008371527306735516,0.008483809418976307,0.00853393692523241,0.00855483952909708,0.008648086339235306,0.008664018474519253,0.00866732932627201,0.008686273358762264,0.008741326630115509,0.008855024352669716,0.008868691511452198,0.008931506425142288,0.008958568796515465,0.009001458063721657,0.009125463664531708,0.00916739460080862,0.009200976230204105,0.009202093817293644,0.009211632423102856,0.009318344295024872,0.009338254109025002,0.009550770744681358,0.009572366252541542,0.009599341079592705,0.009685290977358818,0.009699713438749313,0.009773391298949718,0.009777852334082127,0.009788365103304386,0.009788479655981064,0.009848513640463352,0.009909364394843578,0.009966603480279446,0.009997721761465073,0.01001433003693819,0.010015334002673626,0.010107708163559437,0.01010800525546074,0.010303890332579613,0.010433979332447052,0.010629598051309586,0.010672024451196194,0.010821227915585041,0.01086549274623394,0.010926899500191212,0.010963796637952328,0.011000069789588451,0.011023181490600109,0.011040938086807728,0.011103710159659386,0.011204625479876995,0.011477118358016014,0.011512055061757565,0.011541929095983505,0.011550748720765114,0.011642233468592167,0.011642334051430225,0.011735553853213787,0.011737782508134842,0.011753600090742111,0.011778485961258411,0.011792117729783058,0.011799910105764866,0.011846858076751232,0.011941627599298954,0.01196388527750969,0.01199153158813715,0.012033763341605663,0.01215564738959074,0.012179145589470863,0.012194489128887653,0.012212259694933891,0.012345992028713226,0.012354862876236439,0.012447063811123371,0.012590985745191574,0.012606042437255383,0.01271760556846857,0.01274906750768423,0.012812035158276558,0.012859017588198185,0.012979025952517986,0.012996410019695759,0.013098749332129955,0.013231447897851467,0.013249073177576065,0.013488962315022945,0.013551418669521809,0.01357231754809618,0.013600731268525124,0.013610738329589367,0.013616145588457584,0.013704650104045868,0.013860556297004223,0.013972240500152111,0.014126963913440704,0.014130396768450737,0.014205330982804298,0.01428595744073391,0.014359336346387863,0.014383265748620033,0.014442240819334984,0.014464220963418484,0.014567619189620018,0.01457661297172308,0.014636280946433544,0.01467939279973507,0.014716998673975468,0.014904992654919624,0.015114681795239449,0.015275082550942898,0.01572117581963539,0.015750747174024582,0.015823259949684143,0.016192300245165825,0.016193732619285583,0.016465596854686737,0.0166054405272007,0.01676001586019993,0.016792627051472664,0.01708688773214817,0.017137855291366577,0.017160145565867424,0.017295585945248604,0.017820198088884354,0.01785101369023323,0.017998764291405678,0.018019167706370354,0.01803683303296566,0.01804259978234768,0.01819533295929432,0.01831207424402237,0.01865539513528347,0.018809836357831955,0.018961988389492035,0.019004806876182556,0.019029712304472923,0.01906653121113777,0.019376786425709724,0.01978321373462677,0.01994207873940468,0.019970053806900978,0.020052004605531693,0.020177016034722328,0.020187053829431534,0.0202815979719162,0.02054745703935623,0.020740970969200134,0.020777855068445206,0.021027619019150734,0.021207105368375778,0.0213596411049366,0.021481703966856003,0.021516360342502594,0.02173933945596218,0.021871007978916168,0.021947991102933884,0.02197927050292492,0.02228497341275215,0.0222996324300766,0.022358665242791176,0.022367868572473526,0.0225746538490057,0.022741639986634254,0.022885741665959358,0.02295912429690361,0.023056404665112495,0.023116374388337135,0.023202940821647644,0.02364460751414299,0.02391974627971649,0.024273831397294998,0.02431202121078968,0.024746878072619438,0.02478884719312191,0.02513764798641205,0.025454740971326828,0.02553250826895237,0.02572201006114483,0.02613152004778385,0.026937223970890045,0.026945780962705612,0.02702167257666588,0.0271335169672966,0.027207503095269203,0.027268914505839348,0.02793874777853489,0.028075361624360085,0.02873639389872551,0.028840918093919754,0.029027828946709633,0.02930142730474472,0.029478132724761963,0.03058832883834839,0.031197547912597656,0.03177075460553169,0.03199594095349312,0.0322825089097023,0.03258322551846504,0.03278735652565956,0.032845400273799896,0.03316153213381767,0.03341851755976677,0.03350179269909859,0.03367899730801582,0.034050725400447845,0.03456001356244087,0.0345631018280983,0.03470484912395477,0.03492540121078491,0.03525513783097267,0.035324469208717346,0.03599904105067253,0.03820287436246872,0.03867832571268082,0.03898129612207413,0.03905819356441498,0.04047693312168121,0.040882304310798645,0.04151494801044464,0.04163059592247009,0.04170989617705345,0.04206171631813049,0.04219318553805351,0.04287867993116379,0.043233733624219894,0.04332456737756729,0.04354337602853775,0.044462524354457855,0.044500112533569336,0.04467841237783432,0.045203618705272675,0.04528912156820297,0.04530610144138336,0.04666190221905708,0.04674627259373665,0.04764746502041817,0.04806244373321533,0.049151454120874405,0.049284256994724274,0.04931177198886871,0.04932958632707596,0.05029499903321266,0.051056601107120514,0.05172402039170265,0.05206909030675888,0.05279859900474548,0.05302537977695465,0.055693574249744415,0.05642136558890343,0.05654767155647278,0.05803866684436798,0.059016164392232895,0.05914458632469177,0.0618138462305069,0.06522826850414276,0.06534166634082794,0.06539841741323471,0.0669441744685173,0.06725715100765228,0.06983456760644913,0.06986010819673538,0.07138819992542267,0.07193724811077118,0.0724119320511818,0.07263099402189255,0.07427096366882324,0.07642047852277756,0.07726147770881653,0.07826638221740723,0.07868833839893341,0.08249428868293762,0.08423547446727753,0.08543038368225098,0.08707037568092346,0.08915073424577713,0.0900479406118393,0.0901796892285347,0.09295517951250076,0.09313410520553589,0.09487263858318329,0.0954207330942154,0.09606379270553589,0.10503819584846497,0.10607321560382843,0.10835164040327072,0.11579176783561707,0.11678256839513779,0.1212545707821846,0.12188544869422913,0.12267381697893143,0.13407091796398163,0.1345394253730774,0.14621824026107788,0.14730437099933624,0.16145943105220795,0.16174036264419556,0.1681281477212906,0.17259807884693146,0.20582892000675201,0.21438899636268616,0.22457243502140045],\"yaxis\":\"y\",\"type\":\"scattergl\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Sorted neurons\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Loss change\"}},\"legend\":{\"title\":{\"text\":\"variable\"},\"tracegroupgap\":0},\"title\":{\"text\":\"Loss change from ablating MLP5 neuron\"},\"width\":800},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('6daf8209-4670-4b56-bc91-71a77178afc5');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sorted_means, indices = torch.sort(diffs.mean(1))\n",
    "sorted_means = sorted_means.tolist()\n",
    "haystack_utils.line(sorted_means, xlabel=\"Sorted neurons\", ylabel=\"Loss change\", title=\"Loss change from ablating MLP5 neuron\") # xticks=indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>                            <div id=\"0fee271a-1b07-4aba-85b1-90160ab44176\" class=\"plotly-graph-div\" style=\"height:525px; width:1000px;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"0fee271a-1b07-4aba-85b1-90160ab44176\")) {                    Plotly.newPlot(                        \"0fee271a-1b07-4aba-85b1-90160ab44176\",                        [{\"error_y\":{\"array\":[1.2357071647709184],\"type\":\"data\",\"visible\":true},\"name\":\"Original\",\"x\":[\"Original\"],\"y\":[1.4855889752600342],\"type\":\"bar\"},{\"error_y\":{\"array\":[1.993160814998365],\"type\":\"data\",\"visible\":true},\"name\":\"Ablated\",\"x\":[\"Ablated\"],\"y\":[3.7526948595047],\"type\":\"bar\"},{\"error_y\":{\"array\":[1.2828599892732686],\"type\":\"data\",\"visible\":true},\"name\":\"MLP5 path patched\",\"x\":[\"MLP5 path patched\"],\"y\":[2.1706071588397027],\"type\":\"bar\"},{\"error_y\":{\"array\":[1.6424129301250119],\"type\":\"data\",\"visible\":true},\"name\":\"MLP5 path patched + Top 10 MLP5 neurons ablated\",\"x\":[\"Top MLP5 removed\"],\"y\":[4.144659381508827],\"type\":\"bar\"},{\"error_y\":{\"array\":[0.7617194745327451],\"type\":\"data\",\"visible\":true},\"name\":\"MLP5 path patched + Bottom 10 MLP5 neurons ablated\",\"x\":[\"Bottom MLP5 removed\"],\"y\":[0.5833573736576363],\"type\":\"bar\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"title\":{\"text\":\"Average last token loss when removing top \\u002f bottom neurons from path patching\"},\"xaxis\":{\"title\":{\"text\":\"\"}},\"yaxis\":{\"title\":{\"text\":\"Loss\"}},\"barmode\":\"group\",\"width\":1000},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('0fee271a-1b07-4aba-85b1-90160ab44176');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check loss change when ablating top / bottom neurons\n",
    "top_neurons_count = 10\n",
    "top_neurons = indices[-top_neurons_count:]\n",
    "bottom_neurons = indices[:top_neurons_count]\n",
    "\n",
    "with model.hooks(deactivate_neurons_fwd_hooks):\n",
    "    ablated_loss, ablated_cache = model.run_with_cache(prompts, return_type=\"loss\")\n",
    "\n",
    "ablate_top_neurons_hook = get_ablate_neurons_hook(top_neurons, ablated_cache)\n",
    "ablate_bottom_neurons_hook = get_ablate_neurons_hook(bottom_neurons, ablated_cache)\n",
    "\n",
    "original_loss, ablated_loss, _, all_MLP5_loss = haystack_utils.get_direct_effect(prompts, model, pos=-1, context_ablation_hooks=deactivate_neurons_fwd_hooks, context_activation_hooks=activate_neurons_fwd_hooks)\n",
    "_, _, _, top_MLP5_ablated_loss = haystack_utils.get_direct_effect(prompts, model, pos=-1, context_ablation_hooks=deactivate_neurons_fwd_hooks, context_activation_hooks=activate_neurons_fwd_hooks+ablate_top_neurons_hook)\n",
    "_, _, _, bottom_MLP5_ablated_loss = haystack_utils.get_direct_effect(prompts, model, pos=-1, context_ablation_hooks=deactivate_neurons_fwd_hooks, context_activation_hooks=activate_neurons_fwd_hooks+ablate_bottom_neurons_hook)\n",
    "\n",
    "names = [\"Original\", \"Ablated\", \"MLP5 path patched\", f\"MLP5 path patched + Top {top_neurons_count} MLP5 neurons ablated\", f\"MLP5 path patched + Bottom {top_neurons_count} MLP5 neurons ablated\"]\n",
    "short_names = [\"Original\", \"Ablated\", \"MLP5 path patched\", f\"Top MLP5 removed\", f\"Bottom MLP5 removed\"]\n",
    "\n",
    "values = [original_loss.tolist(), ablated_loss.tolist(), all_MLP5_loss.tolist(), top_MLP5_ablated_loss.tolist(), bottom_MLP5_ablated_loss.tolist()]\n",
    "haystack_utils.plot_barplot(values, names, short_names=short_names, ylabel=\"Loss\", title=f\"Average last token loss when removing top / bottom neurons from path patching\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigate top / bottom neuron boosts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Patch in MLP5 neurons, ablated bottom neurons, compare with patched MLP5 neurons without ablating bottom neurons\n",
    "\n",
    "with model.hooks(deactivate_neurons_fwd_hooks):\n",
    "    ablated_logits, ablated_cache = model.run_with_cache(prompts)\n",
    "\n",
    "ablate_top_neurons_hook = get_ablate_neurons_hook(top_neurons, ablated_cache)\n",
    "ablate_bottom_neurons_hook = get_ablate_neurons_hook(bottom_neurons, ablated_cache)\n",
    "\n",
    "original_logits, ablated_logprobs, _, all_MLP5_logprobs = haystack_utils.get_direct_effect(prompts, model, pos=-2, context_ablation_hooks=deactivate_neurons_fwd_hooks, context_activation_hooks=activate_neurons_fwd_hooks, return_type='logprobs')\n",
    "_, _, _, top_MLP5_ablated_logprobs = haystack_utils.get_direct_effect(prompts, model, pos=-2, context_ablation_hooks=deactivate_neurons_fwd_hooks, context_activation_hooks=activate_neurons_fwd_hooks+ablate_top_neurons_hook, return_type='logprobs')\n",
    "_, _, _, bottom_MLP5_ablated_logprobs = haystack_utils.get_direct_effect(prompts, model, pos=-2, context_ablation_hooks=deactivate_neurons_fwd_hooks, context_activation_hooks=activate_neurons_fwd_hooks+ablate_bottom_neurons_hook, return_type='logprobs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_difference_neurons(baseline_logprobs, ablated_logprops, positive=True, logprob_threshold=-7, k=50):\n",
    "    neuron_logprob_difference = (baseline_logprobs - ablated_logprops).mean(0)\n",
    "    neuron_logprob_difference[baseline_logprobs.mean(0) < logprob_threshold] = 0\n",
    "    if positive:\n",
    "        non_zero_count = (neuron_logprob_difference > 0).sum()\n",
    "    else:\n",
    "        non_zero_count = (neuron_logprob_difference < 0).sum()\n",
    "    top_logprob_difference, top_neurons = haystack_utils.top_k_with_exclude(neuron_logprob_difference, min(non_zero_count, k), all_ignore, largest=positive)\n",
    "    return top_logprob_difference, top_neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boosted tokens\n",
    "bottom_neuron_pos_difference_logprobs, bottom_pos_indices = get_top_difference_neurons(all_MLP5_logprobs, bottom_MLP5_ablated_logprobs, positive=True)\n",
    "top_neuron_pos_difference_logprobs, top_pos_indices = get_top_difference_neurons(all_MLP5_logprobs, top_MLP5_ablated_logprobs, positive=True)\n",
    "# Deboosted tokens\n",
    "bottom_neuron_neg_difference_logprobs, bottom_neg_indices = get_top_difference_neurons(all_MLP5_logprobs, bottom_MLP5_ablated_logprobs, positive=False)\n",
    "top_neuron_neg_difference_logprobs, top_neg_indices = get_top_difference_neurons(all_MLP5_logprobs, top_MLP5_ablated_logprobs, positive=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "610a5dcb07814534b27084693d4eb198",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='Tokens:', options=('Boosted', 'Deboosted'), value='Boosted'), Drop…"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def plot_data(boosted_deboosted, top_bottom):\n",
    "    if boosted_deboosted == 'Boosted' and top_bottom == 'Top':\n",
    "        logprobs = top_neuron_pos_difference_logprobs\n",
    "        indices = top_pos_indices\n",
    "        title_change = 'increase'\n",
    "    elif boosted_deboosted == 'Boosted' and top_bottom == 'Bottom':\n",
    "        logprobs = bottom_neuron_pos_difference_logprobs\n",
    "        indices = bottom_pos_indices\n",
    "        title_change = 'increase'\n",
    "    elif boosted_deboosted == 'Deboosted' and top_bottom == 'Top':\n",
    "        logprobs = top_neuron_neg_difference_logprobs\n",
    "        indices = top_neg_indices\n",
    "        title_change = 'decrease'\n",
    "    else:  # 'Deboosted' and 'Bottom'\n",
    "        logprobs = bottom_neuron_neg_difference_logprobs\n",
    "        indices = bottom_neg_indices\n",
    "        title_change = 'decrease'\n",
    "\n",
    "    xlabel = boosted_deboosted + \" tokens\"\n",
    "    ylabel = \"full_logprob - ablated_logprop\"\n",
    "    title = 'Logprob ' + title_change + ' from ' + top_bottom.lower() + ' neurons'\n",
    "    xticks = [model.to_str_tokens([i])[0] for i in indices]\n",
    "\n",
    "    haystack_utils.line(logprobs.cpu().numpy(), xlabel=xlabel, ylabel=ylabel, title=title, xticks=xticks, show_legend=False)\n",
    "\n",
    "\n",
    "boosted_deboosted = widgets.Dropdown(options=['Boosted', 'Deboosted'], value='Boosted', description='Tokens:')\n",
    "top_bottom = widgets.Dropdown(options=['Top', 'Bottom'], value='Top', description='Neurons:')\n",
    "widgets.interactive(plot_data, boosted_deboosted=boosted_deboosted, top_bottom=top_bottom)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decompose neuron effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get logprob change for individual neuron\n",
    "\n",
    "def get_individual_neuron_logprob_effect(sorted_indices, neuron_pos=0, top=True, positive=True, plot=True):\n",
    "    if top:\n",
    "        neurons = sorted_indices[-(neuron_pos+1)]\n",
    "    else:\n",
    "        neurons = sorted_indices[neuron_pos]\n",
    "    neurons_hook = get_ablate_neurons_hook([neurons], ablated_cache)\n",
    "\n",
    "    original_logits, ablated_logprobs, _, all_MLP5_logprobs = haystack_utils.get_direct_effect(prompts, model, pos=-2, context_ablation_hooks=deactivate_neurons_fwd_hooks, context_activation_hooks=activate_neurons_fwd_hooks, return_type='logprobs')\n",
    "    _, _, _, neuron_ablated_logprobs = haystack_utils.get_direct_effect(prompts, model, pos=-2, context_ablation_hooks=deactivate_neurons_fwd_hooks, context_activation_hooks=activate_neurons_fwd_hooks+neurons_hook, return_type='logprobs')\n",
    "    neuron_difference_logprobs, difference_indices = \\\n",
    "        get_top_difference_neurons(all_MLP5_logprobs, neuron_ablated_logprobs, positive=positive)\n",
    "    \n",
    "    if not plot:\n",
    "        return neuron_difference_logprobs, difference_indices\n",
    "    \n",
    "    if positive:\n",
    "        xlabel = 'Boosted tokens'\n",
    "        title = 'Logprob increase from neuron ' + str(neurons.item())\n",
    "    else: \n",
    "        xlabel = 'Deboosted tokens'\n",
    "        title = 'Logprob decrease from neuron ' + str(neurons.item())\n",
    "    if top:\n",
    "        title += f' (top {neuron_pos+1} neuron)'\n",
    "    else:\n",
    "        title += f' (bottom {neuron_pos+1} neuron)'\n",
    "\n",
    "    xticks = [model.to_str_tokens([i])[0] for i in difference_indices]\n",
    "    haystack_utils.line(neuron_difference_logprobs.cpu().numpy(), xlabel=xlabel, ylabel=\"full_logprob - ablated_logprop\", title=title, xticks=xticks, show_legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b957437de2be4599a7161938bbf70974",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='Neuron Pos:', max=20), Dropdown(description='Neurons:', …"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a function to call your function with widget inputs\n",
    "def plot_individual_neuron(neuron_pos, top_bottom, pos_neg):\n",
    "    top_bool = True if top_bottom == 'Top' else False\n",
    "    positive_bool = True if pos_neg == 'Positive' else False\n",
    "    get_individual_neuron_logprob_effect(indices, neuron_pos=neuron_pos, top=top_bool, positive=positive_bool)\n",
    "\n",
    "# Define widgets\n",
    "neuron_pos_slider = widgets.IntSlider(min=0, max=20, step=1, value=0, description='Neuron Pos:')\n",
    "top_bottom_dropdown = widgets.Dropdown(options=['Top', 'Bottom'], value='Top', description='Neurons:')\n",
    "pos_neg_dropdown = widgets.Dropdown(options=['Positive', 'Negative'], value='Positive', description='Pos/Neg:')\n",
    "\n",
    "# Use interactive function to bind the widgets to the plotting function\n",
    "widgets.interactive(plot_individual_neuron, neuron_pos=neuron_pos_slider, top_bottom=top_bottom_dropdown, pos_neg=pos_neg_dropdown)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum individual weighted positive / negative effects\n",
    "def summed_neuron_differences(num_neurons=10, top=True, positive=True, k=50):\n",
    "    summed_neuron_diffs = torch.zeros(model.cfg.d_vocab).cuda()\n",
    "    for neuron_pos in range(num_neurons):\n",
    "        # It would probably be more principle to do our filtering on the summed diffs instead of per neuron\n",
    "        neuron_diff, token_indices = get_individual_neuron_logprob_effect(indices, neuron_pos=neuron_pos, top=top, positive=positive, plot=False)\n",
    "        summed_neuron_diffs[token_indices] += neuron_diff\n",
    "    if positive:\n",
    "        non_zero_count = (summed_neuron_diffs > 0).sum()\n",
    "    else:\n",
    "        non_zero_count = (summed_neuron_diffs < 0).sum()\n",
    "    top_logprob_difference, top_tokens = haystack_utils.top_k_with_exclude(summed_neuron_diffs, min(non_zero_count, k), all_ignore, largest=positive)\n",
    "    return top_logprob_difference, top_tokens\n",
    "\n",
    "def plot_summed_neuron_differences(num_neurons=10, top=True, positive=True):\n",
    "    top_logprob_difference, top_logprob_tokens = summed_neuron_differences(num_neurons=num_neurons, top=top, positive=positive)\n",
    "    xticks = [model.to_str_tokens([i])[0] for i in top_logprob_tokens]\n",
    "    \n",
    "    if positive:\n",
    "        title = \"Summed individual boosts by\"\n",
    "        xlabel = \"Boosted tokens\"\n",
    "    else:\n",
    "        title = \"Summed individual deboosts by\"\n",
    "        xlabel = \"Deboosted tokens\"\n",
    "    if top:\n",
    "        title += f\" top {num_neurons} neurons\"\n",
    "    else:\n",
    "        title += f\" bottom {num_neurons} neurons\"\n",
    "    haystack_utils.line(top_logprob_difference.cpu().numpy(), xlabel=xlabel, ylabel=\"full_logprob - ablated_logprop\", title=title, xticks=xticks, show_legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "090f8b32f6bb41fbb0a39b74c70c3075",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=10, description='Num Neurons:', max=20, min=1), Dropdown(description='Ne…"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a function to call your function with widget inputs\n",
    "def plot_summed_neurons_widget(num_neurons, top_bottom, pos_neg):\n",
    "    top_bool = True if top_bottom == 'Top' else False\n",
    "    positive_bool = True if pos_neg == 'Positive' else False\n",
    "    plot_summed_neuron_differences(num_neurons=num_neurons, top=top_bool, positive=positive_bool)\n",
    "\n",
    "# Define widgets\n",
    "num_neuron_slider = widgets.IntSlider(min=1, max=20, step=1, value=10, description='Num Neurons:')\n",
    "top_bottom_dropdown = widgets.Dropdown(options=['Top', 'Bottom'], value='Top', description='Neurons:')\n",
    "pos_neg_dropdown = widgets.Dropdown(options=['Positive', 'Negative'], value='Positive', description='Pos/Neg:')\n",
    "\n",
    "# Use interactive function to bind the widgets to the plotting function\n",
    "widgets.interactive(plot_summed_neurons_widget, num_neurons=num_neuron_slider, top_bottom=top_bottom_dropdown, pos_neg=pos_neg_dropdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss is logprob(correct_token_logit) = logit(correct_token_logit) - LogSumExp(all_token_logits)\n",
    "Loss = -logprob\n",
    "\n",
    "LogSumExp approximates a maximum function. If the neuron engages in destructive interference of a high logit for a non-answer token, then the exp(logit) for the token will be lower and so the LogSumExp will be more similar to logit(correct_token_logit) so the loss will be lower. So a lower logsumexp(all vocab) is good.\n",
    "\n",
    "If the neuron engages in destructive interference of a low logit for a non-answer token, then the exp(logit) for the the token won't really change and so the logsumexp and the loss will both be the same.\n",
    "\n",
    "For a single neuron:\n",
    "1. For each \"gen\" prompt, zero centre the logits and record the logit of \"gen\". Calculate the mean \"gen\" logit.\n",
    "2. For each \"gen\" prompt, disable the neuron under test, zero centre the logits, edit the logit of \"gen\" to the mean with a hook, then get the logprob of \"gen\" and calculate logsumexp(all vocab) = logit(gen) - logprob(gen).\n",
    "2. For each \"gen\" prompt, enable the neuron, zero centre the logits, edit the logit of \"gen\" to the mean with a hook, then get the logprob of \"gen\" and calculate logsumexp(all vocab) = logit(gen) - logprob(gen).\n",
    "3. Take the difference in logsum exps. If it's positive, the neuron is reducing the loss via destructive interference by the difference.\n",
    "Can use the same procedure for sets of neurons, or for all neurons, to find high level effects of the context neuron\n",
    "\n",
    "Logprobs are logits with a constant subtracted and the constant is the same for every logit within a prompt.\n",
    "\n",
    "Taking the difference in terms with and without a neuron's effect via the context neuron:\n",
    "- If log sum exp increases, the neuron is boosting tokens on average. \n",
    "- If logit increases, the neuron is boosting the correct token\n",
    "\n",
    "\n",
    "Remove the neuron's effects on the gen logit. Take the mean on the prompt and position?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(11.2822, device='cuda:0')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Calculate the mean \"gen\" logit.\n",
    "gen_index = model.to_single_token('gen')\n",
    "gen_logits = []\n",
    "logits = model(prompts, return_type='logits') # batch pos vocab\n",
    "logits = logits - logits.mean(-1).unsqueeze(-1) # batch pos vocab, batch pos 1\n",
    "\n",
    "mean_gen_logit = logits[:, -1, gen_index].mean(0)\n",
    "mean_gen_logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import numpy as np\n",
    "# from transformer_lens import utils\n",
    "# \n",
    "# px.histogram(np.random.choice(logits[:, -1, gen_index].flatten().cpu().numpy(), 1000), nbins=100)\n",
    "# utils.test_prompt(\"\".join(model.to_str_tokens(prompts[0, :-1])), \"gen\", model, prepend_space_to_answer=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constructive interference diff, destructive interference diff\n",
      "torch.Size([100]) torch.Size([])\n",
      "0.038536 -0.028058\n",
      "torch.Size([100]) torch.Size([])\n",
      "-0.000311 -0.000128\n",
      "torch.Size([100]) torch.Size([])\n",
      "0.028768 0.006270\n",
      "torch.Size([100]) torch.Size([])\n",
      "0.000000 0.000000\n",
      "torch.Size([100]) torch.Size([])\n",
      "0.026021 -0.003454\n",
      "torch.Size([100]) torch.Size([])\n",
      "0.337268 -0.055345\n"
     ]
    }
   ],
   "source": [
    "def decompose_interference_diff(ablated_cache, disabled_neurons=[top_neurons[0]], mean=True\n",
    "                                ) -> tuple[float, float] | tuple[Float[Tensor, \"n_prompts\"], Float[Tensor, \"n_prompts\"]]:\n",
    "    '''\n",
    "    Finds the effect of the German context neuron ablation via a given set of MLP5 neurons on the logit of each final \n",
    "    token. Decomposes it into constructive and destructive interference. A positive constructive interference difference\n",
    "    means the neurons boost the logit of the correct token, a positive destructive interference difference means the\n",
    "    neurons deboost the logits of the incorrect tokens.\n",
    "\n",
    "    Loss = -logprob(correct_token_logit)\n",
    "    logprob(correct_token_logit) = correct_token_logit - LogSumExp(all_token_logits)\n",
    "\n",
    "    LogSumExp is a smooth maximum function, so it approximates the max of all logits. A neuron that destructively \n",
    "    interfers with a non-answer token with a high logit lowers the LogSumExp(all_token_logits) and thus the loss.'''\n",
    "    ablate_top_neuron_hook = get_ablate_neurons_hook(disabled_neurons, ablated_cache)\n",
    "\n",
    "    _, _, _, mlp5_enabled_logits = haystack_utils.get_direct_effect(prompts, model, pos=-1, \n",
    "                                                                    context_ablation_hooks=deactivate_neurons_fwd_hooks, \n",
    "                                                                    context_activation_hooks=activate_neurons_fwd_hooks, return_type='logits')\n",
    "    _, _, _, mlp5_enabled_top_neuron_ablated_logits = haystack_utils.get_direct_effect(prompts, model, pos=-1, \n",
    "                                                                                       context_ablation_hooks=deactivate_neurons_fwd_hooks, \n",
    "                                                                                       context_activation_hooks=activate_neurons_fwd_hooks+ablate_top_neuron_hook, return_type='logits')\n",
    "    \n",
    "    # Mean center logits to avoid picking up on constant boosts / deboosts\n",
    "    mlp5_enabled_logits = mlp5_enabled_logits - mlp5_enabled_logits.mean(-1).unsqueeze(-1)\n",
    "    mlp5_enabled_top_neuron_ablated_logits = mlp5_enabled_top_neuron_ablated_logits - mlp5_enabled_top_neuron_ablated_logits.mean(-1).unsqueeze(-1)\n",
    "    \n",
    "    # 1. Constructive interference difference\n",
    "    # This is the change in the correct answer token logit from ablating the neuron, positive is good\n",
    "    constructive_interference_diffs = mlp5_enabled_logits[:, gen_index] - mlp5_enabled_top_neuron_ablated_logits[:, gen_index]\n",
    "\n",
    "    # 2. Destructive interference difference\n",
    "    # This is the change in the LogSumExp of all logits from not ablating the neuron, \n",
    "    # Lucia: positive/increase in LogSumExp is bad\n",
    "    # Lovis: I don't think so - positive logsumexp means that all tokens get deboosted which is good\n",
    "\n",
    "    # Set the gen logit to its mean value so the neuron's constructive interference doesn't affect the LogSumExp difference\n",
    "    mlp5_enabled_logits[:, gen_index] = mean_gen_logit\n",
    "    mlp5_enabled_top_neuron_ablated_logits[:, gen_index] = mean_gen_logit\n",
    "    \n",
    "    # Compute logsumexp\n",
    "    mlp5_enabled_log_sum_exp = mlp5_enabled_logits.exp().sum(-1).log()\n",
    "    mlp5_enabled_top_neuron_ablated_log_sum_exp = mlp5_enabled_top_neuron_ablated_logits.exp().sum(-1).log()\n",
    "\n",
    "    # Check for errors\n",
    "    assert torch.allclose(mlp5_enabled_log_sum_exp, mlp5_enabled_logits[:, gen_index] - mlp5_enabled_logits.log_softmax(-1)[:, gen_index])\n",
    "    assert torch.allclose(mlp5_enabled_top_neuron_ablated_log_sum_exp, mlp5_enabled_top_neuron_ablated_logits[:, gen_index] - mlp5_enabled_top_neuron_ablated_logits.log_softmax(-1)[:, gen_index])\n",
    "\n",
    "    # Difference in logsumexp\n",
    "    # Logsumexp of enabled should be higher than ablated if the neuron does something good\n",
    "    # Positive results are good - they mean that all tokens are deboosted more when the neuron is active\n",
    "    destructive_interference_diffs = mlp5_enabled_log_sum_exp - mlp5_enabled_top_neuron_ablated_log_sum_exp\n",
    "    # 3. Flip the diff's sign to ease interpretation, now positive is good for both constructive and destructive interference\n",
    "    #destructive_interference_diffs *= -1\n",
    "\n",
    "    if mean:\n",
    "        return constructive_interference_diffs.mean().item(), destructive_interference_diffs.mean().item(), \n",
    "    return constructive_interference_diffs, destructive_interference_diffs,\n",
    "\n",
    "\n",
    "# Calculate neuron-wise loss change\n",
    "print(\"constructive interference diff, destructive interference diff\")\n",
    "for i in range(5):\n",
    "    constructive_diff, destructive_diff = decompose_interference_diff(ablated_cache, disabled_neurons=[top_neurons[i]], mean=True)\n",
    "    print(f'{constructive_diff:2f}', f'{destructive_diff:2f}')\n",
    "\n",
    "constructive_diff, destructive_diff = decompose_interference_diff(ablated_cache, top_neurons)\n",
    "print(f'{constructive_diff:2f}', f'{destructive_diff:2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neuron -> Token Logit Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.0346, -0.0237, -0.0347, -0.0274,  0.0397,  0.0595,  0.0501,  0.1405,\n",
      "         0.0417, -0.0392], device='cuda:0')\n",
      "tensor([0.1634, 0.1461, 0.1405, 0.1399, 0.1349, 0.1207, 0.1196, 0.1161, 0.1156,\n",
      "        0.1139, 0.1130, 0.1128, 0.1120, 0.1104, 0.1096, 0.1070, 0.1056, 0.1048,\n",
      "        0.1042, 0.1028], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Our top neurons are selected by the difference in their boost in gen based on the context neuron\n",
    "# Many other neurons boost gen more per unit of activation\n",
    "\n",
    "cosine_sim = torch.nn.CosineSimilarity(dim=1)\n",
    "answer_residual_direction = model.tokens_to_residual_directions(\"gen\")\n",
    "neuron_weights = model.state_dict()['blocks.5.mlp.W_out'][top_neurons]\n",
    "\n",
    "cosine_sims = cosine_sim(neuron_weights, answer_residual_direction.unsqueeze(0))\n",
    "print(cosine_sims)\n",
    "\n",
    "cosine_sim = torch.nn.CosineSimilarity(dim=1)\n",
    "answer_residual_direction = model.tokens_to_residual_directions(\"gen\")\n",
    "neuron_weights = model.state_dict()['blocks.5.mlp.W_out']\n",
    "\n",
    "cosine_sims = cosine_sim(neuron_weights, answer_residual_direction.unsqueeze(0))\n",
    "top, indices = torch.topk(cosine_sims, 20)\n",
    "print(top)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "198e93849982411faa0dcf8e5ef8ac7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf179ff7fb764634a07e15129fc40b11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "666223a1c4de42aa80fd674d2a7fa1fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "haystack_utils.clean_cache()\n",
    "english_activations_l5 = {}\n",
    "german_activations_l5 = {}\n",
    "all_activations_l5 = {}\n",
    "for layer in range(5, 6):\n",
    "    english_activations_l5[layer] = get_mlp_activations(english_data, layer, model, mean=True)\n",
    "    german_activations_l5[layer] = get_mlp_activations(german_data, layer, model, mean=True)\n",
    "    all_activations_l5[layer] = get_mlp_activations(german_data[:50] + english_data[:150], layer, model, mean=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'unsqueeze'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 28\u001b[0m\n\u001b[1;32m     26\u001b[0m W_out \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mstate_dict()[\u001b[39m'\u001b[39m\u001b[39mblocks.5.mlp.W_out\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     27\u001b[0m W_U \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mW_U\n\u001b[0;32m---> 28\u001b[0m \u001b[39mprint\u001b[39m(W_out\u001b[39m.\u001b[39mshape, W_U\u001b[39m.\u001b[39mshape, all_activations_l5\u001b[39m.\u001b[39;49munsqueeze(\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mshape)\n\u001b[1;32m     30\u001b[0m average_boost \u001b[39m=\u001b[39m W_out \u001b[39m*\u001b[39m all_activations_l5\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m) \u001b[39m*\u001b[39m W_U\n\u001b[1;32m     31\u001b[0m \u001b[39mprint\u001b[39m(average_boost\u001b[39m.\u001b[39mshape) \u001b[39m# d_mlp d_vocab\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'unsqueeze'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# token based measures:\n",
    "# index\n",
    "# next index\n",
    "\n",
    "# neuron based measures:\n",
    "# index\n",
    "# cosine sim with gen\n",
    "# average activation when context neuron enabled\n",
    "# average activation when context neuron disabled\n",
    "# 5 if cosine sim > 0 else -5\n",
    "# loss change when neuron ablated\n",
    "\n",
    "# boost_gen_acts = cosine_sims\n",
    "# boost_gen_acts[cosine_sims > 0] = 5.0\n",
    "# boost_gen_acts[cosine_sims <= 0] = -5.0\n",
    "\n",
    "# def get_deactivate_neuron_hook(neuron):\n",
    "#     def deactivate_neurons_hook(value, hook):\n",
    "#         value[:, :, neuron] = MEAN_ACTIVATION_INACTIVE\n",
    "#         return value\n",
    "# deactivate_neurons_fwd_hooks=[(f'blocks.{LAYER_TO_ABLATE}.mlp.hook_post', deactivate_neurons_hook)]\n",
    "# loss_diffs = []\n",
    "# for i in range(2048):\n",
    "\n",
    "W_out = model.state_dict()['blocks.5.mlp.W_out']\n",
    "W_U = model.W_U\n",
    "print(W_out.shape, W_U.shape, all_activations_l5.unsqueeze(1).shape)\n",
    "\n",
    "average_boost = W_out * all_activations_l5.unsqueeze(1) * W_U\n",
    "print(average_boost.shape) # d_mlp d_vocab\n",
    "\n",
    "# 'Average gen boost': [i zip(all_activations_l5,\n",
    "\n",
    "# mean activation\n",
    "data = {\n",
    "    'Neuron index': list(range(2048)),\n",
    "    'Cosine similarity with \\\"gen\\\"': cosine_sims.tolist(),\n",
    "    'Average act - context neuron enabled': german_activations_l5.tolist(),\n",
    "    'Average act - context neuron disabled': english_activations_l5.tolist(),\n",
    "    'Average act': all_activations_l5.tolist()\n",
    "    # 'Boost \\\"gen\\\" act': boost_gen_acts.tolist(),\n",
    "    # 'Loss change when ablated': \n",
    "}\n",
    "\n",
    "# Create the dataframe\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Print the dataframe\n",
    "print(df)\n",
    "\n",
    "# More complete picture of neurons that boost gen.\n",
    "# Line plot of each neuron's gen boost with dotted vertical lines where the German context neuron-boosted ones are"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brainstorm notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to decompose an MLP5 neuron's effect into its boost to the correct logit and its deboost of other logits. I want to discover how these two effects change the log prob.\n",
    "\n",
    "metric like loss reduction vs. token boost\n",
    "\n",
    "~~run direct effect, patch each neuron in top 10 individually, get overall loss reduction from neuron controlled by context neuron (equivalent to logprob increase for correct token)~~\n",
    "decompose loss reduction into two parts:\n",
    "run direct effect, patch the correct token logit boost from the neuron (removing the neuron's other effects), get overall loss reduction from neuron (equivalent to logprob increase for correct token)\n",
    "destructive interference loss reduction = overall loss reduction - loss reduction from correct token logit boost (component of the logprob increase for correct token due to it deboosting incorrect token)\n",
    "\n",
    "Patch the correct token logit boost from the neuron (removing the neuron's other effects).\n",
    "1. Get baseline logprobs for a prompt\n",
    "2. Get difference in logits from activating the neuron under test. Can use get_direct_effects with return_type='logits'.\n",
    "3. Run the model with return_type='logits' and the neuron under test zero ablated. \n",
    "4. Add the correct token logit from step 1 to a copy of the output logits. Convert to logprobs\n",
    "5. Add the incorrect token logits from step 1 to a copy of the output logits. Convert to logprobs\n",
    "6. Compare A. lobprobs with correct answer token logit increase, B. logprobs with incorrect answer token logit increases, and C. baseline logprobs\n",
    "\n",
    "~~If the context neuron gives each neuron a flat boost then if we decompose the resulting flat boost to one MLP5 neuron into a boost to one logit vs. boosts to all other logits it will change the logprobs (first will increase answer probability and second will reduce answer probability). \n",
    "\n",
    "the two resulting log probs at the correct answer token won't add up to the original log probs (?). \n",
    "\n",
    "If the boost is flat the correct percentage decomposition is 1/50000 and 49999/50000? In practice/all other factors being equal\n",
    "\n",
    "New plan:\n",
    "\n",
    "Difference between baseline log prob and neuron log prob?\n",
    "Classify individual neurons by percentage constructive vs destructive by looking at their log probs and summing the incorrect token log probs\n",
    "\n",
    "Correct log prob difference\n",
    "Incorrect log prob difference (summed over every plausible token?)\n",
    "\n",
    "Largest boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the log prob for an incorrect token is significantly lower then that's where the extra probability density on the correct answer is coming from \n",
    "# Constructive interference increases correct token log prob and uniformly decreases other log probs\n",
    "# Destructive interference decreases specific other log probs and uniformly increases other log probs\n",
    " \n",
    "# Most neurons are a mixture of the above\n",
    "# Decompose neurons into what % of their effect is each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original_logits, ablated_logprobs, _, all_MLP5_logprobs = haystack_utils.get_direct_effect(prompts, model, pos=-2, context_ablation_hooks=deactivate_neurons_fwd_hooks, context_activation_hooks=activate_neurons_fwd_hooks, return_type='logprobs')\n",
    "# _, _, _, top_MLP5_ablated_logprobs = haystack_utils.get_direct_effect(prompts, model, pos=-2, context_ablation_hooks=deactivate_neurons_fwd_hooks, context_activation_hooks=activate_neurons_fwd_hooks+ablate_top_neurons_hook, return_type='logprobs')\n",
    "# _, _, _, bottom_MLP5_ablated_logprobs = haystack_utils.get_direct_effect(prompts, model, pos=-2, context_ablation_hooks=deactivate_neurons_fwd_hooks, context_activation_hooks=activate_neurons_fwd_hooks+ablate_bottom_neurons_hook, return_type='logprobs')\n",
    "\n",
    "# bottom_neuron_high_difference_logprobs = (all_MLP5_logprobs - bottom_MLP5_ablated_logprobs)\n",
    "# bottom_neuron_high_difference_logprobs[bottom_neuron_high_difference_logprobs < 0] = 0\n",
    "# bottom_neuron_high_difference_logprobs = bottom_neuron_high_difference_logprobs.mean(0)\n",
    "# bottom_neuron_high_difference_logprobs[all_MLP5_logprobs.mean(0) < -7] = 0\n",
    "\n",
    "# bottom_non_zero_count = (bottom_neuron_high_difference_logprobs > 0).sum()\n",
    "# bottom_neuron_high_difference_logprobs, bottom_indices = haystack_utils.top_k_with_exclude(bottom_neuron_high_difference_logprobs, min(bottom_non_zero_count, 50), all_ignore)\n",
    "# haystack_utils.line(bottom_neuron_high_difference_logprobs.cpu().numpy(), title='Largest positive difference in log probs for tokens when bottom neurons are not ablated', xticks=[model.to_str_tokens([i])[0] for i in bottom_indices])\n",
    "\n",
    "\n",
    "# top_neuron_high_difference_logprobs = (all_MLP5_logprobs - top_MLP5_ablated_logprobs)\n",
    "# top_neuron_high_difference_logprobs[top_neuron_high_difference_logprobs < 0] = 0\n",
    "# top_neuron_high_difference_logprobs = top_neuron_high_difference_logprobs.mean(0)\n",
    "# top_neuron_high_difference_logprobs[all_MLP5_logprobs.mean(0) < -7] = 0\n",
    "\n",
    "# top_non_zero_count = (top_neuron_high_difference_logprobs > 0).sum()\n",
    "# top_neuron_high_difference_logprobs, top_indices = haystack_utils.top_k_with_exclude(top_neuron_high_difference_logprobs, min(top_non_zero_count, 50), all_ignore)\n",
    "# haystack_utils.line(top_neuron_high_difference_logprobs.cpu().numpy(), title='Largest positive difference in log probs for tokens when bottom neurons are not ablated', xticks=[model.to_str_tokens([i])[0] for i in top_indices])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
