{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: nltk in /Users/luciaquirke/.pyenv/versions/3.10.0/lib/python3.10/site-packages (3.8.1)\n",
      "Requirement already satisfied: click in /Users/luciaquirke/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: tqdm in /Users/luciaquirke/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from nltk) (4.66.1)\n",
      "Requirement already satisfied: joblib in /Users/luciaquirke/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/luciaquirke/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from nltk) (2023.8.8)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.3; however, version 23.2.1 is available.\n",
      "You should consider upgrading via the '/Users/luciaquirke/.pyenv/versions/3.10.0/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install nltk\n",
    "\n",
    "import random\n",
    "import argparse\n",
    "import json\n",
    "import pickle\n",
    "import os\n",
    "import gzip\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformer_lens import HookedTransformer, utils\n",
    "from datasets import load_dataset\n",
    "from fancy_einsum import einsum\n",
    "from tqdm.auto import tqdm\n",
    "import plotly.io as pio\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML\n",
    "from datasets import load_dataset\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from nltk import ngrams\n",
    "\n",
    "import neel.utils as nutils\n",
    "from neel_plotly import *\n",
    "import haystack_utils\n",
    "import probing_utils\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "def set_seeds():\n",
    "    SEED = 42\n",
    "    torch.manual_seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    random.seed(SEED)\n",
    "\n",
    "    pio.renderers.default = \"notebook_connected+notebook\"\n",
    "    # torch.autograd.set_grad_enabled(False)\n",
    "    # torch.set_grad_enabled(False)\n",
    "\n",
    "    NUM_CHECKPOINTS = 143\n",
    "\n",
    "\n",
    "def get_model(model_name: str, checkpoint: int) -> HookedTransformer:\n",
    "    model = HookedTransformer.from_pretrained(\n",
    "        model_name,\n",
    "        checkpoint_index=checkpoint,\n",
    "        center_unembed=True,\n",
    "        center_writing_weights=True,\n",
    "        fold_ln=True,\n",
    "        device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def preload_models() -> int:\n",
    "    \"\"\"Preload models into cache so we can iterate over them quickly and return the model checkpoint count.\"\"\"\n",
    "    i = 0\n",
    "    try:\n",
    "        with tqdm(total=None) as pbar:\n",
    "            while True:\n",
    "                get_model(i)\n",
    "                i += 1\n",
    "                pbar.update(1)\n",
    "\n",
    "    except IndexError:\n",
    "        return i\n",
    "\n",
    "\n",
    "def load_language_data() -> dict:\n",
    "    \"\"\"\n",
    "    Returns: dictionary keyed by language code, containing 200 lines of each language included in the Europarl dataset.\n",
    "    \"\"\"\n",
    "    lang_data = {}\n",
    "    lang_data[\"en\"] = haystack_utils.load_json_data(\"data/english_europarl.json\")[:200]\n",
    "\n",
    "    europarl_data_dir = Path(\"data/europarl/\")\n",
    "    for file in os.listdir(europarl_data_dir):\n",
    "        if file.endswith(\".txt\"):\n",
    "            lang = file.split(\"_\")[0]\n",
    "            lang_data[lang] = haystack_utils.load_txt_data(europarl_data_dir.joinpath(file))\n",
    "\n",
    "    for lang in lang_data.keys():\n",
    "        print(lang, len(lang_data[lang]))\n",
    "    return lang_data\n",
    "\n",
    "\n",
    "def get_common_ngrams(\n",
    "    model: HookedTransformer, prompts: list[str], n: int, top_k=100\n",
    ") -> list[str]:\n",
    "    \"\"\"\n",
    "    n: n-gram length\n",
    "    top_k: number of n-grams to return\n",
    "\n",
    "    Returns: List of most common n-grams in prompts sorted by frequency\n",
    "    \"\"\"\n",
    "    all_ngrams = []\n",
    "    for prompt in tqdm(prompts):\n",
    "        str_tokens = model.to_str_tokens(prompt)\n",
    "        all_ngrams.extend(ngrams(str_tokens, n))\n",
    "    # Filter n-grams which contain punctuation\n",
    "    all_ngrams = [\n",
    "        x\n",
    "        for x in all_ngrams\n",
    "        if all(\n",
    "            [\n",
    "                y.strip() not in [\"\\n\", \"-\", \"(\", \")\", \".\", \",\", \";\", \"!\", \"?\", \"\"]\n",
    "                for y in x\n",
    "            ]\n",
    "        )\n",
    "    ]\n",
    "    return Counter(all_ngrams).most_common(top_k)\n",
    "\n",
    "\n",
    "def train_probe(\n",
    "    positive_data: torch.Tensor, negative_data: torch.Tensor\n",
    ") -> tuple[float, float]:\n",
    "    labels = np.concatenate([np.ones(len(positive_data)), np.zeros(len(negative_data))])\n",
    "    data = np.concatenate([positive_data.cpu().numpy(), negative_data.cpu().numpy()])\n",
    "    scaler = preprocessing.StandardScaler().fit(data)\n",
    "    data = scaler.transform(data)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(\n",
    "        data, labels, test_size=0.2, random_state=SEED\n",
    "    )\n",
    "    probe = probing_utils.get_probe(x_train, y_train, max_iter=2000)\n",
    "    f1, mcc = probing_utils.get_probe_score(probe, x_test, y_test)\n",
    "    return f1, mcc\n",
    "\n",
    "\n",
    "def get_mlp_activations(\n",
    "    prompts: list[str], layer: int, model: HookedTransformer\n",
    ") -> torch.Tensor:\n",
    "    act_label = f\"blocks.{layer}.mlp.hook_post\"\n",
    "\n",
    "    acts = []\n",
    "    for prompt in prompts:\n",
    "        with model.hooks([(act_label, save_activation)]):\n",
    "            model(prompt)\n",
    "            act = model.hook_dict[act_label].ctx[\"activation\"][:, 10:400, :]\n",
    "        act = einops.rearrange(act, \"batch pos n_neurons -> (batch pos) n_neurons\")\n",
    "        acts.append(act)\n",
    "    acts = torch.concat(acts, dim=0)\n",
    "    return acts[:k]\n",
    "\n",
    "\n",
    "def zero_ablate_hook(value, hook):\n",
    "    value[:, :, :] = 0\n",
    "    return value\n",
    "\n",
    "\n",
    "def get_layer_probe_performance(\n",
    "    model: HookedTransformer,\n",
    "    checkpoint: int,\n",
    "    layer: int,\n",
    "    german_data: np.array,\n",
    "    non_german_data: np.array,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Probe performance for each neuron.\"\"\"\n",
    "\n",
    "    german_activations = get_mlp_activations(german_data[:30], layer, model)[:10_000]\n",
    "    non_german_activations = get_mlp_activations(non_german_data[:30], layer, model)[\n",
    "        :10_000\n",
    "    ]\n",
    "\n",
    "    mean_german_activations = german_activations.mean(0).cpu().numpy()\n",
    "    mean_non_german_activations = non_german_activations.mean(0).cpu().numpy()\n",
    "\n",
    "    f1s = []\n",
    "    mccs = []\n",
    "    for neuron in range(model.cfg.d_mlp):\n",
    "        f1, mcc = train_probe(\n",
    "            german_activations[:, neuron].unsqueeze(-1),\n",
    "            non_german_activations[:, neuron].unsqueeze(-1),\n",
    "        )\n",
    "        f1s.append(f1)\n",
    "        mccs.append(mcc)\n",
    "\n",
    "    checkpoint_neuron_labels = [\n",
    "        f\"C{checkpoint}L{layer}N{i}\" for i in range(model.cfg.d_mlp)\n",
    "    ]\n",
    "    neuron_labels = [f\"L{layer}N{i}\" for i in range(model.cfg.d_mlp)]\n",
    "    neuron_indices = [i for i in range(model.cfg.d_mlp)]\n",
    "\n",
    "    layer_df = pd.DataFrame(\n",
    "        {\n",
    "            \"Label\": checkpoint_neuron_labels,\n",
    "            \"NeuronLabel\": neuron_labels,\n",
    "            \"Neuron\": neuron_indices,\n",
    "            \"F1\": f1s,\n",
    "            \"MCC\": mccs,\n",
    "            \"MeanGermanActivation\": mean_german_activations,\n",
    "            \"MeanNonGermanActivation\": mean_non_german_activations,\n",
    "            \"Checkpoint\": checkpoint * len(checkpoint_neuron_labels),\n",
    "            \"Layer\": layer * len(checkpoint_neuron_labels),\n",
    "        }\n",
    "    )\n",
    "    return layer_df\n",
    "\n",
    "\n",
    "def get_layer_ablation_loss(\n",
    "    model: HookedTransformer, german_data: list, checkpoint: int, layer: int\n",
    "):\n",
    "    loss_data = []\n",
    "\n",
    "    for prompt in german_data[:100]:\n",
    "        loss = model(prompt, return_type=\"loss\").item()\n",
    "        with model.hooks([(f\"blocks.{layer}.mlp.hook_post\", zero_ablate_hook)]):\n",
    "            ablated_loss = model(prompt, return_type=\"loss\").item()\n",
    "        loss_difference = ablated_loss - loss\n",
    "        loss_data.append([checkpoint, layer, loss_difference, loss, ablated_loss])\n",
    "\n",
    "    layer_df = pd.DataFrame(\n",
    "        loss_data,\n",
    "        columns=[\n",
    "            \"Checkpoint\",\n",
    "            \"Layer\",\n",
    "            \"LossDifference\",\n",
    "            \"OriginalLoss\",\n",
    "            \"AblatedLoss\",\n",
    "        ],\n",
    "    )\n",
    "    return layer_df\n",
    "\n",
    "\n",
    "def get_language_losses(\n",
    "    model: HookedTransformer, checkpoint: int, lang_data: dict\n",
    ") -> pd.DataFrame:\n",
    "    data = []\n",
    "    for lang in lang_data.keys():\n",
    "        losses = []\n",
    "        for prompt in lang_data[lang]:\n",
    "            loss = model(prompt, return_type=\"loss\").item()\n",
    "            losses.append(loss)\n",
    "        data.append([checkpoint, lang, np.mean(losses)])\n",
    "\n",
    "    return pd.DataFrame(data, columns=[\"Checkpoint\", \"Language\", \"Loss\"], index=[0])\n",
    "\n",
    "\n",
    "def get_ngram_losses(\n",
    "    model: HookedTransformer,\n",
    "    checkpoint: int,\n",
    "    ngrams: list[str],\n",
    "    common_tokens: list[str],\n",
    ") -> pd.DataFrame:\n",
    "    data = []\n",
    "    for ngram in ngrams:\n",
    "        prompts = haystack_utils.generate_random_prompts(\n",
    "            ngram, model, common_tokens, 100, 20\n",
    "        )\n",
    "        loss = eval_prompts(prompts, model)\n",
    "        with model.hooks(deactivate_neurons_fwd_hooks):\n",
    "            ablated_loss = eval_prompts(prompts, model)\n",
    "        data.append([loss, ablated_loss, ablated_loss - loss, checkpoint, ngram])\n",
    "\n",
    "    df = pd.DataFrame(\n",
    "        data,\n",
    "        columns=[\"OriginalLoss\", \"AblatedLoss\", \"LossIncrease\", \"Checkpoint\", \"Ngram\"],\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "def run_probe_analysis(\n",
    "    model_name: str,\n",
    "    num_checkpoints: int,\n",
    "    lang_data: dict,\n",
    "    output_dir: str,\n",
    ") -> None:\n",
    "    \"\"\"Collect several dataframes covering whole layer ablation losses, ngram loss, language losses, and neuron probe performance.\"\"\"\n",
    "    model = get_model(model_name, 0)\n",
    "    n_layers = model.cfg.n_layers\n",
    "\n",
    "    german_data = lang_data[\"de\"]\n",
    "    non_german_data = np.concatenate([lang_data[lang] for lang in lang_data.keys() if lang != \"de\"])\n",
    "    np.random.shuffle(non_german_data)\n",
    "    non_german_data = non_german_data[:200].tolist()\n",
    "\n",
    "    all_ignore, _ = haystack_utils.get_weird_tokens(model, plot_norms=False)\n",
    "    common_tokens = haystack_utils.get_common_tokens(\n",
    "        german_data, model, all_ignore, k=100\n",
    "    )\n",
    "    top_german_trigrams = get_common_ngrams(model, lang_data[\"de\"], 3, 200)\n",
    "\n",
    "    random_trigram_indices = np.random.choice(\n",
    "        range(len(top_trigrams)), 20, replace=False\n",
    "    )\n",
    "    random_trigrams = [\"\".join(top_trigrams[i][0]) for i in random_trigram_indices]\n",
    "\n",
    "    probe_dfs = []\n",
    "    layer_ablation_dfs = []\n",
    "    lang_loss_dfs = []\n",
    "    ngram_loss_dfs = []\n",
    "    with tqdm(total=checkpoints * n_layers) as pbar:\n",
    "        for checkpoint in range(num_checkpoints):\n",
    "            model = get_model(checkpoint)\n",
    "            for layer in range(n_layers):\n",
    "                partial_probe_df = get_layer_probe_performance(\n",
    "                    model, checkpoint, layer, german_data, non_german_data\n",
    "                )\n",
    "                probe_dfs.append(partial_probe_df)\n",
    "\n",
    "                partial_layer_ablation_df = get_layer_ablation_loss(\n",
    "                    model, german_data, checkpoint, layer\n",
    "                )\n",
    "\n",
    "                layer_ablation_dfs.append(partial_layer_ablation_df)\n",
    "                lang_loss_dfs.append(get_language_losses(model, checkpoint, lang_data))\n",
    "                ngram_loss_dfs.append(\n",
    "                    get_ngram_losses(model, checkpoint, random_trigrams, common_tokens)\n",
    "                )\n",
    "\n",
    "                # Save progress to allow for checkpointing the analysis\n",
    "                with open(\n",
    "                    output_dir + model_name + \"_checkpoint_features.pkl.gz\", \"wb\"\n",
    "                ) as f:\n",
    "                    pickle.dump(\n",
    "                        {\n",
    "                            \"probe\": probe_dfs,\n",
    "                            \"layer_ablation\": layer_ablation_dfs,\n",
    "                            \"lang_loss\": lang_loss_dfs,\n",
    "                        },\n",
    "                        f,\n",
    "                    )\n",
    "\n",
    "                pbar.update(1)\n",
    "\n",
    "    # Open the pickle file\n",
    "    with open(output_dir + model_name + \"_checkpoint_features.pkl.gz\", \"rb\") as f:\n",
    "        data = pickle.load(f)\n",
    "\n",
    "    # Concatenate the dataframes\n",
    "    data = {df_name: pd.concat(dfs) for dfs_name, dfs in data.items()}\n",
    "\n",
    "    # Compress with gzip using high compression and save\n",
    "    with gzip.open(\n",
    "        output_dir + model_name + \"_checkpoint_features.pkl.gz\", \"wb\", compresslevel=9\n",
    "    ) as f_out:\n",
    "        pickle.dump(data, f_out)\n",
    "\n",
    "\n",
    "def analyze_model_checkpoints(model_name: str, output_dir: str) -> None:\n",
    "    set_seeds()\n",
    "\n",
    "    # Will take about 50GB of disk space for Pythia 70M models\n",
    "    # num_checkpoints = preload_models()\n",
    "    num_checkpoints = 2\n",
    "\n",
    "    # Load probe data\n",
    "    lang_data = load_language_data()\n",
    "\n",
    "    run_probe_analysis(\n",
    "        model_name, num_checkpoints, lang_data, output_dir\n",
    "    )\n",
    "\n",
    "\n",
    "def process_data(output_dir: str, model_name: str) -> None:\n",
    "    def load_probe_analysis():\n",
    "        with gzip.open(\n",
    "            output_dir + model_name + \"_checkpoint_neurons.pkl.gz\", \"rb\"\n",
    "        ) as f:\n",
    "            data = pickle.load(f)\n",
    "        return data\n",
    "\n",
    "    probe_df = load_probe_analysis()\n",
    "\n",
    "    checkpoints = []\n",
    "    top_probe = []\n",
    "    for checkpoint in probe_df[\"Checkpoint\"].unique():\n",
    "        checkpoint_df = probe_df[probe_df[\"Checkpoint\"] == checkpoint]\n",
    "        top_probe.append(checkpoint_df[\"MCC\"].max())\n",
    "        checkpoints.append(checkpoint)\n",
    "    fig = px.line(\n",
    "        x=checkpoints,\n",
    "        y=top_probe,\n",
    "        title=\"Top Probe MCC by Checkpoint\",\n",
    "        width=800,\n",
    "        height=400,\n",
    "    )\n",
    "    fig.write_image(output_dir + \"top_mcc_by_checkpoint.png\")\n",
    "\n",
    "    accurate_neurons = probe_df[\n",
    "        (probe_df[\"MCC\"] > 0.85)\n",
    "        & (probe_df[\"MeanGermanActivation\"] > probe_df[\"MeanNonGermanActivation\"])\n",
    "    ][[\"NeuronLabel\", \"MCC\"]].copy()\n",
    "    accurate_neurons = accurate_neurons.sort_values(by=\"MCC\", ascending=False)\n",
    "    print(\n",
    "        len(accurate_neurons[\"NeuronLabel\"].unique()),\n",
    "        \"neurons with an MCC > 0.85 for German text recognition at any point during training.\",\n",
    "    )\n",
    "\n",
    "    good_neurons = accurate_neurons[\"NeuronLabel\"].unique()[:50]\n",
    "\n",
    "    def get_mean_non_german(df, neuron, layer, checkpoint):\n",
    "        label = f\"C{checkpoint}L{layer}N{neuron}\"\n",
    "        df = df[df[\"Label\"] == label][\"MeanNonGermanActivation\"].item()\n",
    "        return df\n",
    "\n",
    "    def get_mean_german(df, neuron, layer, checkpoint):\n",
    "        label = f\"C{checkpoint}L{layer}N{neuron}\"\n",
    "        df = df[df[\"Label\"] == label][\"MeanGermanActivation\"].item()\n",
    "        return df\n",
    "\n",
    "    get_mean_non_german(probe_df, 669, 3, 140)\n",
    "\n",
    "    random_neurons = probe_df[\n",
    "        (probe_df[\"Layer\"].isin(layer_vals)) & (probe_df[\"Neuron\"].isin(neuron_vals))\n",
    "    ]\n",
    "    random_neurons = random_neurons[\"NeuronLabel\"].unique()\n",
    "\n",
    "    fig = px.line(\n",
    "        probe_df[\n",
    "            probe_df[\"NeuronLabel\"].isin(good_neurons)\n",
    "            | probe_df[\"NeuronLabel\"].isin(bad_neurons)\n",
    "        ],\n",
    "        x=\"Checkpoint\",\n",
    "        y=\"MCC\",\n",
    "        color=\"NeuronLabel\",\n",
    "        title=\"Neurons with max MCC >= 0.85\",\n",
    "    )\n",
    "    fig.write_image(output_dir + \"high_mcc_neurons.png\")\n",
    "\n",
    "    context_neuron_df = probe_df[probe_df[\"NeuronLabel\"] == \"L3N669\"]\n",
    "    fig = px.line(\n",
    "        context_neuron_df,\n",
    "        x=\"Checkpoint\",\n",
    "        y=[\"MeanGermanActivation\", \"MeanEnglishActivation\"],\n",
    "    )\n",
    "    fig.write_image(output_dir + \"mean_activations.png\")\n",
    "\n",
    "    with gzip.open(\n",
    "        output_dir + model_name + \"_checkpoint_layer_ablations.pkl.gz\", \"rb\"\n",
    "    ) as f:\n",
    "        layer_ablation_df = pickle.load(f)\n",
    "\n",
    "    fig = px.line(\n",
    "        layer_ablation_df.groupby([\"Checkpoint\", \"Layer\"]).mean().reset_index(),\n",
    "        x=\"Checkpoint\",\n",
    "        y=\"LossDifference\",\n",
    "        color=\"Layer\",\n",
    "        title=\"Loss difference for zero-ablating MLP layers on German data\",\n",
    "        width=900,\n",
    "    )\n",
    "    fig.write_image(output_dir + \"layer_ablation_losses.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/english_europarl.json: Loaded 2000 examples with 165 to 2000 characters each.\n",
      "data/europarl/it_200_samples.txt: Loaded 200 examples with 502 to 2676 characters each.\n",
      "data/europarl/fi_200_samples.txt: Loaded 200 examples with 503 to 2394 characters each.\n",
      "data/europarl/hu_200_samples.txt: Loaded 200 examples with 504 to 2208 characters each.\n",
      "data/europarl/de_200_samples.txt: Loaded 200 examples with 501 to 2566 characters each.\n",
      "data/europarl/el_200_samples.txt: Loaded 200 examples with 502 to 3253 characters each.\n",
      "data/europarl/nl_200_samples.txt: Loaded 200 examples with 502 to 2423 characters each.\n",
      "data/europarl/bg_200_samples.txt: Loaded 200 examples with 501 to 2211 characters each.\n",
      "data/europarl/ro_200_samples.txt: Loaded 200 examples with 502 to 2154 characters each.\n",
      "data/europarl/pl_200_samples.txt: Loaded 200 examples with 501 to 2162 characters each.\n",
      "data/europarl/cs_200_samples.txt: Loaded 200 examples with 501 to 1954 characters each.\n",
      "data/europarl/lt_200_samples.txt: Loaded 200 examples with 502 to 2078 characters each.\n",
      "data/europarl/fr_200_samples.txt: Loaded 200 examples with 501 to 2605 characters each.\n",
      "data/europarl/sk_200_samples.txt: Loaded 200 examples with 503 to 2021 characters each.\n",
      "data/europarl/sv_200_samples.txt: Loaded 200 examples with 504 to 2290 characters each.\n",
      "data/europarl/sl_200_samples.txt: Loaded 200 examples with 502 to 1882 characters each.\n",
      "data/europarl/lv_200_samples.txt: Loaded 200 examples with 502 to 1850 characters each.\n",
      "data/europarl/da_200_samples.txt: Loaded 200 examples with 502 to 2333 characters each.\n",
      "data/europarl/pt_200_samples.txt: Loaded 200 examples with 502 to 2562 characters each.\n",
      "data/europarl/es_200_samples.txt: Loaded 200 examples with 501 to 2508 characters each.\n",
      "data/europarl/et_200_samples.txt: Loaded 200 examples with 503 to 1887 characters each.\n",
      "en 200\n",
      "it 200\n",
      "fi 200\n",
      "hu 200\n",
      "de 200\n",
      "el 200\n",
      "nl 200\n",
      "bg 200\n",
      "ro 200\n",
      "pl 200\n",
      "cs 200\n",
      "lt 200\n",
      "fr 200\n",
      "sk 200\n",
      "sv 200\n",
      "sl 200\n",
      "lv 200\n",
      "da 200\n",
      "pt 200\n",
      "es 200\n",
      "et 200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n",
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model EleutherAI/pythia-70m into HookedTransformer\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/luciaquirke/code/superposition/temp.ipynb Cell 2\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/luciaquirke/code/superposition/temp.ipynb#W1sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m save_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(Path(\u001b[39m\"\u001b[39m\u001b[39mfeature_formation/\u001b[39m\u001b[39m\"\u001b[39m), \u001b[39m\"\u001b[39m\u001b[39mEleutherAI/pythia-70m\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/luciaquirke/code/superposition/temp.ipynb#W1sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m os\u001b[39m.\u001b[39mmakedirs(save_path, exist_ok\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/luciaquirke/code/superposition/temp.ipynb#W1sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m analyze_model_checkpoints(\u001b[39m\"\u001b[39;49m\u001b[39mEleutherAI/pythia-70m\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mfeature_formation/\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "\u001b[1;32m/Users/luciaquirke/code/superposition/temp.ipynb Cell 2\u001b[0m line \u001b[0;36m3\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/luciaquirke/code/superposition/temp.ipynb#W1sZmlsZQ%3D%3D?line=353'>354</a>\u001b[0m \u001b[39m# Load probe data\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/luciaquirke/code/superposition/temp.ipynb#W1sZmlsZQ%3D%3D?line=354'>355</a>\u001b[0m lang_data \u001b[39m=\u001b[39m load_language_data()\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/luciaquirke/code/superposition/temp.ipynb#W1sZmlsZQ%3D%3D?line=356'>357</a>\u001b[0m run_probe_analysis(\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/luciaquirke/code/superposition/temp.ipynb#W1sZmlsZQ%3D%3D?line=357'>358</a>\u001b[0m     model_name, num_checkpoints, lang_data, output_dir\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/luciaquirke/code/superposition/temp.ipynb#W1sZmlsZQ%3D%3D?line=358'>359</a>\u001b[0m )\n",
      "\u001b[1;32m/Users/luciaquirke/code/superposition/temp.ipynb Cell 2\u001b[0m line \u001b[0;36m2\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/luciaquirke/code/superposition/temp.ipynb#W1sZmlsZQ%3D%3D?line=281'>282</a>\u001b[0m non_german_data \u001b[39m=\u001b[39m non_german_data[:\u001b[39m200\u001b[39m]\u001b[39m.\u001b[39mtolist()\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/luciaquirke/code/superposition/temp.ipynb#W1sZmlsZQ%3D%3D?line=283'>284</a>\u001b[0m all_ignore, _ \u001b[39m=\u001b[39m haystack_utils\u001b[39m.\u001b[39mget_weird_tokens(model, plot_norms\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/luciaquirke/code/superposition/temp.ipynb#W1sZmlsZQ%3D%3D?line=284'>285</a>\u001b[0m common_tokens \u001b[39m=\u001b[39m haystack_utils\u001b[39m.\u001b[39;49mget_common_tokens(\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/luciaquirke/code/superposition/temp.ipynb#W1sZmlsZQ%3D%3D?line=285'>286</a>\u001b[0m     german_data, model, all_ignore, k\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/luciaquirke/code/superposition/temp.ipynb#W1sZmlsZQ%3D%3D?line=286'>287</a>\u001b[0m )\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/luciaquirke/code/superposition/temp.ipynb#W1sZmlsZQ%3D%3D?line=287'>288</a>\u001b[0m top_german_trigrams \u001b[39m=\u001b[39m get_common_ngrams(model, lang_data[\u001b[39m\"\u001b[39m\u001b[39mde\u001b[39m\u001b[39m\"\u001b[39m], \u001b[39m3\u001b[39m, \u001b[39m200\u001b[39m)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/luciaquirke/code/superposition/temp.ipynb#W1sZmlsZQ%3D%3D?line=289'>290</a>\u001b[0m random_trigram_indices \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mchoice(\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/luciaquirke/code/superposition/temp.ipynb#W1sZmlsZQ%3D%3D?line=290'>291</a>\u001b[0m     \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(top_trigrams)), \u001b[39m20\u001b[39m, replace\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/luciaquirke/code/superposition/temp.ipynb#W1sZmlsZQ%3D%3D?line=291'>292</a>\u001b[0m )\n",
      "File \u001b[0;32m~/code/superposition/haystack_utils.py:1410\u001b[0m, in \u001b[0;36mget_common_tokens\u001b[0;34m(data, model, ignore_tokens, k, return_counts, return_unsorted_counts)\u001b[0m\n\u001b[1;32m   1408\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_common_tokens\u001b[39m(data, model, ignore_tokens, k\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m, return_counts\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, return_unsorted_counts\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m   1409\u001b[0m     \u001b[39m# Get top common german tokens excluding punctuation\u001b[39;00m\n\u001b[0;32m-> 1410\u001b[0m     token_counts \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mzeros(model\u001b[39m.\u001b[39;49mcfg\u001b[39m.\u001b[39;49md_vocab)\u001b[39m.\u001b[39;49mcuda()\n\u001b[1;32m   1411\u001b[0m     \u001b[39mfor\u001b[39;00m example \u001b[39min\u001b[39;00m tqdm(data):\n\u001b[1;32m   1412\u001b[0m         tokens \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mto_tokens(example)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.0/lib/python3.10/site-packages/torch/cuda/__init__.py:239\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    236\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    237\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mmultiprocessing, you must use the \u001b[39m\u001b[39m'\u001b[39m\u001b[39mspawn\u001b[39m\u001b[39m'\u001b[39m\u001b[39m start method\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    238\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(torch\u001b[39m.\u001b[39m_C, \u001b[39m'\u001b[39m\u001b[39m_cuda_getDeviceCount\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m--> 239\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mTorch not compiled with CUDA enabled\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    240\u001b[0m \u001b[39mif\u001b[39;00m _cudart \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    241\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\n\u001b[1;32m    242\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "save_path = os.path.join(Path(\"feature_formation/\"), \"EleutherAI/pythia-70m\")\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "analyze_model_checkpoints(\"EleutherAI/pythia-70m\", \"feature_formation/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--model MODEL] [--output_dir OUTPUT_DIR]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: --ip=127.0.0.1 --stdin=9013 --control=9011 --hb=9010 --Session.signature_scheme=\"hmac-sha256\" --Session.key=b\"985c17ea-0829-4277-bdfa-4ad8ea449a9d\" --shell=9012 --transport=\"tcp\" --iopub=9014 --f=/Users/luciaquirke/Library/Jupyter/runtime/kernel-v2-41152SD5xfpSf0VlJ.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/luciaquirke/.pyenv/versions/3.10.0/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3534: UserWarning:\n",
      "\n",
      "To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(\n",
    "        formatter_class=argparse.ArgumentDefaultsHelpFormatter\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--model\",\n",
    "        default=\"EleutherAI/pythia-70m\",\n",
    "        help=\"Name of model from TransformerLens\",\n",
    "    )\n",
    "    parser.add_argument(\"--output_dir\", default=\"feature_formation\")\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    save_path = os.path.join(args.output_dir, args.model)\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "    analyze_model_checkpoints(args.model, args.output_dir)\n",
    "\n",
    "    \n",
    "    save_file = os.path.join(save_path, f\"{args.feature_dataset}_neurons.csv\")\n",
    "    results.to_csv(save_file, index=False)\n",
    "\n",
    "    save_image = os.path.join(save_path, \"images\")\n",
    "    os.makedirs(save_image, exist_ok=True)\n",
    "    # process_data(args.output_dir, args.model, save_image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
