{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /Users/luciaquirke/.pyenv/versions/3.10.0/lib/python3.10/site-packages (3.8.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/luciaquirke/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from nltk) (2023.8.8)\n",
      "Requirement already satisfied: joblib in /Users/luciaquirke/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: tqdm in /Users/luciaquirke/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from nltk) (4.66.1)\n",
      "Requirement already satisfied: click in /Users/luciaquirke/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from nltk) (8.1.7)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.3; however, version 23.2.1 is available.\n",
      "You should consider upgrading via the '/Users/luciaquirke/.pyenv/versions/3.10.0/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install nltk\n",
    "\n",
    "import random\n",
    "import argparse\n",
    "import json\n",
    "import pickle\n",
    "import os\n",
    "import gzip\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformer_lens import HookedTransformer, utils\n",
    "from datasets import load_dataset\n",
    "from fancy_einsum import einsum\n",
    "from tqdm.auto import tqdm\n",
    "import plotly.io as pio\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML\n",
    "from datasets import load_dataset\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from nltk import ngrams\n",
    "\n",
    "import neel.utils as nutils\n",
    "from neel_plotly import *\n",
    "import haystack_utils\n",
    "import probing_utils\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "def set_seeds():\n",
    "    SEED = 42\n",
    "    torch.manual_seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    random.seed(SEED)\n",
    "\n",
    "    pio.renderers.default = \"notebook_connected+notebook\"\n",
    "    # torch.autograd.set_grad_enabled(False)\n",
    "    # torch.set_grad_enabled(False)\n",
    "\n",
    "    NUM_CHECKPOINTS = 143\n",
    "\n",
    "\n",
    "def get_model(checkpoint: int) -> HookedTransformer:\n",
    "    model = HookedTransformer.from_pretrained(\n",
    "        \"EleutherAI/pythia-70m\",\n",
    "        checkpoint_index=checkpoint,\n",
    "        center_unembed=True,\n",
    "        center_writing_weights=True,\n",
    "        fold_ln=True,\n",
    "        device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def preload_models() -> int:\n",
    "    \"\"\"Preload models into cache so we can iterate over them quickly and return the model checkpoint count.\"\"\"\n",
    "    i = 0\n",
    "    try:\n",
    "        with tqdm(total=None) as pbar:\n",
    "            while True:\n",
    "                get_model(i)\n",
    "                i += 1\n",
    "                pbar.update(1)\n",
    "\n",
    "    except IndexError:\n",
    "        return i\n",
    "\n",
    "\n",
    "def load_language_data() -> dict:\n",
    "    \"\"\"\n",
    "    Returns: dictionary keyed by language code, containing 200 lines of each language included in the Europarl dataset.\n",
    "    \"\"\"\n",
    "    lang_data = {}\n",
    "    lang_data[\"en\"] = haystack_utils.load_json_data(\"data/english_europarl.json\")[:200]\n",
    "\n",
    "    europarl_data_dir = Path(\"data/europarl/\")\n",
    "    for file in os.listdir(europarl_data_dir):\n",
    "        if file.endswith(\".txt\"):\n",
    "            lang = file.split(\"_\")[0]\n",
    "            lang_data[lang] = haystack_utils.load_txt_data(europarl_data_dir.joinpath(file))\n",
    "\n",
    "    for lang in lang_data.keys():\n",
    "        print(lang, len(lang_data[lang]))\n",
    "    return lang_data\n",
    "\n",
    "\n",
    "def get_common_ngrams(\n",
    "    model: HookedTransformer, prompts: list[str], n: int, top_k=100\n",
    ") -> list[str]:\n",
    "    \"\"\"\n",
    "    n: n-gram length\n",
    "    top_k: number of n-grams to return\n",
    "\n",
    "    Returns: List of most common n-grams in prompts sorted by frequency\n",
    "    \"\"\"\n",
    "    all_ngrams = []\n",
    "    for prompt in tqdm(prompts):\n",
    "        str_tokens = model.to_str_tokens(prompt)\n",
    "        all_ngrams.extend(ngrams(str_tokens, n))\n",
    "    # Filter n-grams which contain punctuation\n",
    "    all_ngrams = [\n",
    "        x\n",
    "        for x in all_ngrams\n",
    "        if all(\n",
    "            [\n",
    "                y.strip() not in [\"\\n\", \"-\", \"(\", \")\", \".\", \",\", \";\", \"!\", \"?\", \"\"]\n",
    "                for y in x\n",
    "            ]\n",
    "        )\n",
    "    ]\n",
    "    return Counter(all_ngrams).most_common(top_k)\n",
    "\n",
    "\n",
    "def train_probe(\n",
    "    positive_data: torch.Tensor, negative_data: torch.Tensor\n",
    ") -> tuple[float, float]:\n",
    "    labels = np.concatenate([np.ones(len(positive_data)), np.zeros(len(negative_data))])\n",
    "    data = np.concatenate([positive_data.cpu().numpy(), negative_data.cpu().numpy()])\n",
    "    scaler = preprocessing.StandardScaler().fit(data)\n",
    "    data = scaler.transform(data)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(\n",
    "        data, labels, test_size=0.2, random_state=SEED\n",
    "    )\n",
    "    probe = probing_utils.get_probe(x_train, y_train, max_iter=2000)\n",
    "    f1, mcc = probing_utils.get_probe_score(probe, x_test, y_test)\n",
    "    return f1, mcc\n",
    "\n",
    "\n",
    "def get_mlp_activations(\n",
    "    prompts: list[str], layer: int, model: HookedTransformer\n",
    ") -> torch.Tensor:\n",
    "    act_label = f\"blocks.{layer}.mlp.hook_post\"\n",
    "\n",
    "    acts = []\n",
    "    for prompt in prompts:\n",
    "        with model.hooks([(act_label, save_activation)]):\n",
    "            model(prompt)\n",
    "            act = model.hook_dict[act_label].ctx[\"activation\"][:, 10:400, :]\n",
    "        act = einops.rearrange(act, \"batch pos n_neurons -> (batch pos) n_neurons\")\n",
    "        acts.append(act)\n",
    "    acts = torch.concat(acts, dim=0)\n",
    "    return acts[:k]\n",
    "\n",
    "\n",
    "def zero_ablate_hook(value, hook):\n",
    "    value[:, :, :] = 0\n",
    "    return value\n",
    "\n",
    "\n",
    "def get_layer_probe_performance(\n",
    "    model: HookedTransformer,\n",
    "    checkpoint: int,\n",
    "    layer: int,\n",
    "    german_data: np.array,\n",
    "    non_german_data: np.array,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Probe performance for each neuron.\"\"\"\n",
    "\n",
    "    german_activations = get_mlp_activations(german_data[:30], layer, model)[:10_000]\n",
    "    non_german_activations = get_mlp_activations(non_german_data[:30], layer, model)[\n",
    "        :10_000\n",
    "    ]\n",
    "\n",
    "    mean_german_activations = german_activations.mean(0).cpu().numpy()\n",
    "    mean_non_german_activations = non_german_activations.mean(0).cpu().numpy()\n",
    "\n",
    "    f1s = []\n",
    "    mccs = []\n",
    "    for neuron in range(model.cfg.d_mlp):\n",
    "        f1, mcc = train_probe(\n",
    "            german_activations[:, neuron].unsqueeze(-1),\n",
    "            non_german_activations[:, neuron].unsqueeze(-1),\n",
    "        )\n",
    "        f1s.append(f1)\n",
    "        mccs.append(mcc)\n",
    "\n",
    "    checkpoint_neuron_labels = [\n",
    "        f\"C{checkpoint}L{layer}N{i}\" for i in range(model.cfg.d_mlp)\n",
    "    ]\n",
    "    neuron_labels = [f\"L{layer}N{i}\" for i in range(model.cfg.d_mlp)]\n",
    "    neuron_indices = [i for i in range(model.cfg.d_mlp)]\n",
    "\n",
    "    layer_df = pd.DataFrame(\n",
    "        {\n",
    "            \"Label\": checkpoint_neuron_labels,\n",
    "            \"NeuronLabel\": neuron_labels,\n",
    "            \"Neuron\": neuron_indices,\n",
    "            \"F1\": f1s,\n",
    "            \"MCC\": mccs,\n",
    "            \"MeanGermanActivation\": mean_german_activations,\n",
    "            \"MeanNonGermanActivation\": mean_non_german_activations,\n",
    "            \"Checkpoint\": checkpoint * len(checkpoint_neuron_labels),\n",
    "            \"Layer\": layer * len(checkpoint_neuron_labels),\n",
    "        }\n",
    "    )\n",
    "    return layer_df\n",
    "\n",
    "\n",
    "def get_layer_ablation_loss(\n",
    "    model: HookedTransformer, german_data: list, checkpoint: int, layer: int\n",
    "):\n",
    "    loss_data = []\n",
    "\n",
    "    for prompt in german_data[:100]:\n",
    "        loss = model(prompt, return_type=\"loss\").item()\n",
    "        with model.hooks([(f\"blocks.{layer}.mlp.hook_post\", zero_ablate_hook)]):\n",
    "            ablated_loss = model(prompt, return_type=\"loss\").item()\n",
    "        loss_difference = ablated_loss - loss\n",
    "        loss_data.append([checkpoint, layer, loss_difference, loss, ablated_loss])\n",
    "\n",
    "    layer_df = pd.DataFrame(\n",
    "        loss_data,\n",
    "        columns=[\n",
    "            \"Checkpoint\",\n",
    "            \"Layer\",\n",
    "            \"LossDifference\",\n",
    "            \"OriginalLoss\",\n",
    "            \"AblatedLoss\",\n",
    "        ],\n",
    "    )\n",
    "    return layer_df\n",
    "\n",
    "\n",
    "def get_language_losses(\n",
    "    model: HookedTransformer, checkpoint: int, lang_data: dict\n",
    ") -> pd.DataFrame:\n",
    "    data = []\n",
    "    for lang in lang_data.keys():\n",
    "        losses = []\n",
    "        for prompt in lang_data[lang]:\n",
    "            loss = model(prompt, return_type=\"loss\").item()\n",
    "            losses.append(loss)\n",
    "        data.append([checkpoint, lang, np.mean(losses)])\n",
    "\n",
    "    return pd.DataFrame(data, columns=[\"Checkpoint\", \"Language\", \"Loss\"], index=[0])\n",
    "\n",
    "\n",
    "def get_ngram_losses(\n",
    "    model: HookedTransformer,\n",
    "    checkpoint: int,\n",
    "    ngrams: list[str],\n",
    "    common_tokens: list[str],\n",
    ") -> pd.DataFrame:\n",
    "    data = []\n",
    "    for ngram in ngrams:\n",
    "        prompts = haystack_utils.generate_random_prompts(\n",
    "            ngram, model, common_tokens, 100, 20\n",
    "        )\n",
    "        loss = eval_prompts(prompts, model)\n",
    "        with model.hooks(deactivate_neurons_fwd_hooks):\n",
    "            ablated_loss = eval_prompts(prompts, model)\n",
    "        data.append([loss, ablated_loss, ablated_loss - loss, checkpoint, ngram])\n",
    "\n",
    "    df = pd.DataFrame(\n",
    "        data,\n",
    "        columns=[\"OriginalLoss\", \"AblatedLoss\", \"LossIncrease\", \"Checkpoint\", \"Ngram\"],\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "def run_probe_analysis(\n",
    "    model_name: str,\n",
    "    num_checkpoints: int,\n",
    "    lang_data: dict,\n",
    "    top_german_trigrams: list[str],\n",
    "    output_dir: str,\n",
    ") -> None:\n",
    "    \"\"\"Collect several dataframes covering whole layer ablation losses, ngram loss, language losses, and neuron probe performance.\"\"\"\n",
    "    model = get_model(model_name, 0)\n",
    "    n_layers = model.cfg.n_layers\n",
    "\n",
    "    german_data = lang_data[\"de\"]\n",
    "    non_german_data = np.random.shuffle(\n",
    "        np.concatenate([lang_data[lang] for lang in lang_data.keys() if lang != \"de\"])\n",
    "    ).tolist()\n",
    "\n",
    "    all_ignore, _ = haystack_utils.get_weird_tokens(model, plot_norms=False)\n",
    "    common_tokens = haystack_utils.get_common_tokens(\n",
    "        german_data, model, all_ignore, k=100\n",
    "    )\n",
    "    top_german_trigrams = get_common_ngrams(model, lang_data[\"de\"], 3, 200)\n",
    "\n",
    "    random_trigram_indices = np.random.choice(\n",
    "        range(len(top_trigrams)), 20, replace=False\n",
    "    )\n",
    "    random_trigrams = [\"\".join(top_trigrams[i][0]) for i in random_trigram_indices]\n",
    "\n",
    "    probe_dfs = []\n",
    "    layer_ablation_dfs = []\n",
    "    lang_loss_dfs = []\n",
    "    ngram_loss_dfs = []\n",
    "    with tqdm(total=checkpoints * n_layers) as pbar:\n",
    "        for checkpoint in range(num_checkpoints):\n",
    "            model = get_model(checkpoint)\n",
    "            for layer in range(n_layers):\n",
    "                partial_probe_df = get_layer_probe_performance(\n",
    "                    model, checkpoint, layer, german_data, non_german_data\n",
    "                )\n",
    "                probe_dfs.append(partial_probe_df)\n",
    "\n",
    "                partial_layer_ablation_df = get_layer_ablation_loss(\n",
    "                    model, german_data, checkpoint, layer\n",
    "                )\n",
    "\n",
    "                layer_ablation_dfs.append(partial_layer_ablation_df)\n",
    "                lang_loss_dfs.append(get_language_losses(model, checkpoint, lang_data))\n",
    "                ngram_loss_dfs.append(\n",
    "                    get_ngram_losses(model, checkpoint, random_trigrams, common_tokens)\n",
    "                )\n",
    "\n",
    "                # Save progress to allow for checkpointing the analysis\n",
    "                with open(\n",
    "                    output_dir + model_name + \"_checkpoint_features.pkl.gz\", \"wb\"\n",
    "                ) as f:\n",
    "                    pickle.dump(\n",
    "                        {\n",
    "                            \"probe\": probe_dfs,\n",
    "                            \"layer_ablation\": layer_ablation_dfs,\n",
    "                            \"lang_loss\": lang_loss_dfs,\n",
    "                        },\n",
    "                        f,\n",
    "                    )\n",
    "\n",
    "                pbar.update(1)\n",
    "\n",
    "    # Open the pickle file\n",
    "    with open(output_dir + model_name + \"_checkpoint_features.pkl.gz\", \"rb\") as f:\n",
    "        data = pickle.load(f)\n",
    "\n",
    "    # Concatenate the dataframes\n",
    "    data = {df_name: pd.concat(dfs) for dfs_name, dfs in data.items()}\n",
    "\n",
    "    # Compress with gzip using high compression and save\n",
    "    with gzip.open(\n",
    "        output_dir + model_name + \"_checkpoint_features.pkl.gz\", \"wb\", compresslevel=9\n",
    "    ) as f_out:\n",
    "        pickle.dump(data, f_out)\n",
    "\n",
    "\n",
    "def analyze_model_checkpoints(model_name: str, output_dir: str) -> None:\n",
    "    set_seeds()\n",
    "\n",
    "    # Will take about 50GB of disk space for Pythia 70M models\n",
    "    num_checkpoints = preload_models()\n",
    "\n",
    "    # Load probe data\n",
    "    lang_data = load_language_data()\n",
    "\n",
    "    run_probe_analysis(\n",
    "        model_name, num_checkpoints, lang_data, top_german_trigrams, output_dir\n",
    "    )\n",
    "\n",
    "\n",
    "def process_data(output_dir: str, model_name: str) -> None:\n",
    "    def load_probe_analysis():\n",
    "        with gzip.open(\n",
    "            output_dir + model_name + \"_checkpoint_neurons.pkl.gz\", \"rb\"\n",
    "        ) as f:\n",
    "            data = pickle.load(f)\n",
    "        return data\n",
    "\n",
    "    probe_df = load_probe_analysis()\n",
    "\n",
    "    checkpoints = []\n",
    "    top_probe = []\n",
    "    for checkpoint in probe_df[\"Checkpoint\"].unique():\n",
    "        checkpoint_df = probe_df[probe_df[\"Checkpoint\"] == checkpoint]\n",
    "        top_probe.append(checkpoint_df[\"MCC\"].max())\n",
    "        checkpoints.append(checkpoint)\n",
    "    fig = px.line(\n",
    "        x=checkpoints,\n",
    "        y=top_probe,\n",
    "        title=\"Top Probe MCC by Checkpoint\",\n",
    "        width=800,\n",
    "        height=400,\n",
    "    )\n",
    "    fig.write_image(output_dir + \"top_mcc_by_checkpoint.png\")\n",
    "\n",
    "    accurate_neurons = probe_df[\n",
    "        (probe_df[\"MCC\"] > 0.85)\n",
    "        & (probe_df[\"MeanGermanActivation\"] > probe_df[\"MeanNonGermanActivation\"])\n",
    "    ][[\"NeuronLabel\", \"MCC\"]].copy()\n",
    "    accurate_neurons = accurate_neurons.sort_values(by=\"MCC\", ascending=False)\n",
    "    print(\n",
    "        len(accurate_neurons[\"NeuronLabel\"].unique()),\n",
    "        \"neurons with an MCC > 0.85 for German text recognition at any point during training.\",\n",
    "    )\n",
    "\n",
    "    good_neurons = accurate_neurons[\"NeuronLabel\"].unique()[:50]\n",
    "\n",
    "    def get_mean_non_german(df, neuron, layer, checkpoint):\n",
    "        label = f\"C{checkpoint}L{layer}N{neuron}\"\n",
    "        df = df[df[\"Label\"] == label][\"MeanNonGermanActivation\"].item()\n",
    "        return df\n",
    "\n",
    "    def get_mean_german(df, neuron, layer, checkpoint):\n",
    "        label = f\"C{checkpoint}L{layer}N{neuron}\"\n",
    "        df = df[df[\"Label\"] == label][\"MeanGermanActivation\"].item()\n",
    "        return df\n",
    "\n",
    "    get_mean_non_german(probe_df, 669, 3, 140)\n",
    "\n",
    "    random_neurons = probe_df[\n",
    "        (probe_df[\"Layer\"].isin(layer_vals)) & (probe_df[\"Neuron\"].isin(neuron_vals))\n",
    "    ]\n",
    "    random_neurons = random_neurons[\"NeuronLabel\"].unique()\n",
    "\n",
    "    fig = px.line(\n",
    "        probe_df[\n",
    "            probe_df[\"NeuronLabel\"].isin(good_neurons)\n",
    "            | probe_df[\"NeuronLabel\"].isin(bad_neurons)\n",
    "        ],\n",
    "        x=\"Checkpoint\",\n",
    "        y=\"MCC\",\n",
    "        color=\"NeuronLabel\",\n",
    "        title=\"Neurons with max MCC >= 0.85\",\n",
    "    )\n",
    "    fig.write_image(output_dir + \"high_mcc_neurons.png\")\n",
    "\n",
    "    context_neuron_df = probe_df[probe_df[\"NeuronLabel\"] == \"L3N669\"]\n",
    "    fig = px.line(\n",
    "        context_neuron_df,\n",
    "        x=\"Checkpoint\",\n",
    "        y=[\"MeanGermanActivation\", \"MeanEnglishActivation\"],\n",
    "    )\n",
    "    fig.write_image(output_dir + \"mean_activations.png\")\n",
    "\n",
    "    with gzip.open(\n",
    "        output_dir + model_name + \"_checkpoint_layer_ablations.pkl.gz\", \"rb\"\n",
    "    ) as f:\n",
    "        layer_ablation_df = pickle.load(f)\n",
    "\n",
    "    fig = px.line(\n",
    "        layer_ablation_df.groupby([\"Checkpoint\", \"Layer\"]).mean().reset_index(),\n",
    "        x=\"Checkpoint\",\n",
    "        y=\"LossDifference\",\n",
    "        color=\"Layer\",\n",
    "        title=\"Loss difference for zero-ablating MLP layers on German data\",\n",
    "        width=900,\n",
    "    )\n",
    "    fig.write_image(output_dir + \"layer_ablation_losses.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd48f3a595ff45efbeda77ecbf29f2c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "470f580c247a45b29a2f8530eafc8285",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/166M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model EleutherAI/pythia-70m into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b50edc51ef9246d4a43aacf0f4808eda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/166M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/luciaquirke/code/superposition/temp.ipynb Cell 2\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/luciaquirke/code/superposition/temp.ipynb#W2sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m save_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(Path(\u001b[39m\"\u001b[39m\u001b[39mfeature_formation/\u001b[39m\u001b[39m\"\u001b[39m), \u001b[39m\"\u001b[39m\u001b[39mEleutherAI/pythia-70m\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/luciaquirke/code/superposition/temp.ipynb#W2sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m os\u001b[39m.\u001b[39mmakedirs(save_path, exist_ok\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/luciaquirke/code/superposition/temp.ipynb#W2sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m analyze_model_checkpoints(\u001b[39m\"\u001b[39;49m\u001b[39mEleutherAI/pythia-70m\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mfeature_formation/\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "\u001b[1;32m/Users/luciaquirke/code/superposition/temp.ipynb Cell 2\u001b[0m line \u001b[0;36m3\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/luciaquirke/code/superposition/temp.ipynb#W2sZmlsZQ%3D%3D?line=348'>349</a>\u001b[0m set_seeds()\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/luciaquirke/code/superposition/temp.ipynb#W2sZmlsZQ%3D%3D?line=350'>351</a>\u001b[0m \u001b[39m# Will take about 50GB of disk space for Pythia 70M models\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/luciaquirke/code/superposition/temp.ipynb#W2sZmlsZQ%3D%3D?line=351'>352</a>\u001b[0m num_checkpoints \u001b[39m=\u001b[39m preload_models()\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/luciaquirke/code/superposition/temp.ipynb#W2sZmlsZQ%3D%3D?line=353'>354</a>\u001b[0m \u001b[39m# Load probe data\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/luciaquirke/code/superposition/temp.ipynb#W2sZmlsZQ%3D%3D?line=354'>355</a>\u001b[0m lang_data \u001b[39m=\u001b[39m load_language_data()\n",
      "\u001b[1;32m/Users/luciaquirke/code/superposition/temp.ipynb Cell 2\u001b[0m line \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/luciaquirke/code/superposition/temp.ipynb#W2sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m \u001b[39mwith\u001b[39;00m tqdm(total\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m) \u001b[39mas\u001b[39;00m pbar:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/luciaquirke/code/superposition/temp.ipynb#W2sZmlsZQ%3D%3D?line=67'>68</a>\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/luciaquirke/code/superposition/temp.ipynb#W2sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m         get_model(i)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/luciaquirke/code/superposition/temp.ipynb#W2sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m         i \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/luciaquirke/code/superposition/temp.ipynb#W2sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m         pbar\u001b[39m.\u001b[39mupdate(\u001b[39m1\u001b[39m)\n",
      "\u001b[1;32m/Users/luciaquirke/code/superposition/temp.ipynb Cell 2\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/luciaquirke/code/superposition/temp.ipynb#W2sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_model\u001b[39m(checkpoint: \u001b[39mint\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m HookedTransformer:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/luciaquirke/code/superposition/temp.ipynb#W2sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m     model \u001b[39m=\u001b[39m HookedTransformer\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/luciaquirke/code/superposition/temp.ipynb#W2sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mEleutherAI/pythia-70m\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/luciaquirke/code/superposition/temp.ipynb#W2sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m         checkpoint_index\u001b[39m=\u001b[39;49mcheckpoint,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/luciaquirke/code/superposition/temp.ipynb#W2sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m         center_unembed\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/luciaquirke/code/superposition/temp.ipynb#W2sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m         center_writing_weights\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/luciaquirke/code/superposition/temp.ipynb#W2sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m         fold_ln\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/luciaquirke/code/superposition/temp.ipynb#W2sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m         device\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcuda\u001b[39;49m\u001b[39m\"\u001b[39;49m \u001b[39mif\u001b[39;49;00m torch\u001b[39m.\u001b[39;49mcuda\u001b[39m.\u001b[39;49mis_available() \u001b[39melse\u001b[39;49;00m \u001b[39m\"\u001b[39;49m\u001b[39mcpu\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/luciaquirke/code/superposition/temp.ipynb#W2sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/luciaquirke/code/superposition/temp.ipynb#W2sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.0/lib/python3.10/site-packages/transformer_lens/HookedTransformer.py:997\u001b[0m, in \u001b[0;36mHookedTransformer.from_pretrained\u001b[0;34m(cls, model_name, fold_ln, center_writing_weights, center_unembed, refactor_factored_attn_matrices, checkpoint_index, checkpoint_value, hf_model, device, n_devices, tokenizer, move_to_device, fold_value_biases, default_prepend_bos, default_padding_side, **from_pretrained_kwargs)\u001b[0m\n\u001b[1;32m    993\u001b[0m         center_writing_weights \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    995\u001b[0m \u001b[39m# Get the state dict of the model (ie a mapping of parameter names to tensors), processed to match the\u001b[39;00m\n\u001b[1;32m    996\u001b[0m \u001b[39m# HookedTransformer parameter names.\u001b[39;00m\n\u001b[0;32m--> 997\u001b[0m state_dict \u001b[39m=\u001b[39m loading\u001b[39m.\u001b[39;49mget_pretrained_state_dict(\n\u001b[1;32m    998\u001b[0m     official_model_name, cfg, hf_model, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfrom_pretrained_kwargs\n\u001b[1;32m    999\u001b[0m )\n\u001b[1;32m   1001\u001b[0m \u001b[39m# Create the HookedTransformer object\u001b[39;00m\n\u001b[1;32m   1002\u001b[0m model \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m(\n\u001b[1;32m   1003\u001b[0m     cfg,\n\u001b[1;32m   1004\u001b[0m     tokenizer,\n\u001b[1;32m   1005\u001b[0m     move_to_device\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m   1006\u001b[0m     default_padding_side\u001b[39m=\u001b[39mdefault_padding_side,\n\u001b[1;32m   1007\u001b[0m )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.0/lib/python3.10/site-packages/transformer_lens/loading_from_pretrained.py:978\u001b[0m, in \u001b[0;36mget_pretrained_state_dict\u001b[0;34m(official_model_name, cfg, hf_model, **kwargs)\u001b[0m\n\u001b[1;32m    972\u001b[0m     hf_model \u001b[39m=\u001b[39m AutoModelForCausalLM\u001b[39m.\u001b[39mfrom_pretrained(\n\u001b[1;32m    973\u001b[0m         official_model_name,\n\u001b[1;32m    974\u001b[0m         revision\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcheckpoint-\u001b[39m\u001b[39m{\u001b[39;00mcfg\u001b[39m.\u001b[39mcheckpoint_value\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    975\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m    976\u001b[0m     )\n\u001b[1;32m    977\u001b[0m \u001b[39melif\u001b[39;00m official_model_name\u001b[39m.\u001b[39mstartswith(\u001b[39m\"\u001b[39m\u001b[39mEleutherAI/pythia\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 978\u001b[0m     hf_model \u001b[39m=\u001b[39m AutoModelForCausalLM\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m    979\u001b[0m         official_model_name,\n\u001b[1;32m    980\u001b[0m         revision\u001b[39m=\u001b[39;49m\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mstep\u001b[39;49m\u001b[39m{\u001b[39;49;00mcfg\u001b[39m.\u001b[39;49mcheckpoint_value\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    981\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    982\u001b[0m     )\n\u001b[1;32m    983\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    984\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    985\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCheckpoints for model \u001b[39m\u001b[39m{\u001b[39;00mofficial_model_name\u001b[39m}\u001b[39;00m\u001b[39m are not supported\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    986\u001b[0m     )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.0/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:563\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mtype\u001b[39m(config) \u001b[39min\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    562\u001b[0m     model_class \u001b[39m=\u001b[39m _get_model_class(config, \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 563\u001b[0m     \u001b[39mreturn\u001b[39;00m model_class\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m    564\u001b[0m         pretrained_model_name_or_path, \u001b[39m*\u001b[39;49mmodel_args, config\u001b[39m=\u001b[39;49mconfig, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mhub_kwargs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    565\u001b[0m     )\n\u001b[1;32m    566\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    567\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnrecognized configuration class \u001b[39m\u001b[39m{\u001b[39;00mconfig\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m for this kind of AutoModel: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    568\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModel type should be one of \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(c\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m \u001b[39m\u001b[39mfor\u001b[39;00m\u001b[39m \u001b[39mc\u001b[39m \u001b[39m\u001b[39min\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.0/lib/python3.10/site-packages/transformers/modeling_utils.py:2793\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2790\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2791\u001b[0m         \u001b[39m# This repo has no safetensors file of any kind, we switch to PyTorch.\u001b[39;00m\n\u001b[1;32m   2792\u001b[0m         filename \u001b[39m=\u001b[39m _add_variant(WEIGHTS_NAME, variant)\n\u001b[0;32m-> 2793\u001b[0m         resolved_archive_file \u001b[39m=\u001b[39m cached_file(\n\u001b[1;32m   2794\u001b[0m             pretrained_model_name_or_path, filename, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcached_file_kwargs\n\u001b[1;32m   2795\u001b[0m         )\n\u001b[1;32m   2796\u001b[0m \u001b[39mif\u001b[39;00m resolved_archive_file \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m filename \u001b[39m==\u001b[39m _add_variant(WEIGHTS_NAME, variant):\n\u001b[1;32m   2797\u001b[0m     \u001b[39m# Maybe the checkpoint is sharded, we try to grab the index name in this case.\u001b[39;00m\n\u001b[1;32m   2798\u001b[0m     resolved_archive_file \u001b[39m=\u001b[39m cached_file(\n\u001b[1;32m   2799\u001b[0m         pretrained_model_name_or_path,\n\u001b[1;32m   2800\u001b[0m         _add_variant(WEIGHTS_INDEX_NAME, variant),\n\u001b[1;32m   2801\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcached_file_kwargs,\n\u001b[1;32m   2802\u001b[0m     )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.0/lib/python3.10/site-packages/transformers/utils/hub.py:429\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    426\u001b[0m user_agent \u001b[39m=\u001b[39m http_user_agent(user_agent)\n\u001b[1;32m    427\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    428\u001b[0m     \u001b[39m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 429\u001b[0m     resolved_file \u001b[39m=\u001b[39m hf_hub_download(\n\u001b[1;32m    430\u001b[0m         path_or_repo_id,\n\u001b[1;32m    431\u001b[0m         filename,\n\u001b[1;32m    432\u001b[0m         subfolder\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m \u001b[39mif\u001b[39;49;00m \u001b[39mlen\u001b[39;49m(subfolder) \u001b[39m==\u001b[39;49m \u001b[39m0\u001b[39;49m \u001b[39melse\u001b[39;49;00m subfolder,\n\u001b[1;32m    433\u001b[0m         repo_type\u001b[39m=\u001b[39;49mrepo_type,\n\u001b[1;32m    434\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m    435\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m    436\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[1;32m    437\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m    438\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m    439\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[1;32m    440\u001b[0m         token\u001b[39m=\u001b[39;49mtoken,\n\u001b[1;32m    441\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m    442\u001b[0m     )\n\u001b[1;32m    443\u001b[0m \u001b[39mexcept\u001b[39;00m GatedRepoError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    444\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    445\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mYou are trying to access a gated repo.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mMake sure to request access at \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    446\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mhttps://huggingface.co/\u001b[39m\u001b[39m{\u001b[39;00mpath_or_repo_id\u001b[39m}\u001b[39;00m\u001b[39m and pass a token having permission to this repo either \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    447\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mby logging in with `huggingface-cli login` or by passing `token=<your_token>`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    448\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.0/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[39mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    116\u001b[0m     kwargs \u001b[39m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, has_token\u001b[39m=\u001b[39mhas_token, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[0;32m--> 118\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.0/lib/python3.10/site-packages/huggingface_hub/file_download.py:1364\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout)\u001b[0m\n\u001b[1;32m   1361\u001b[0m \u001b[39mwith\u001b[39;00m temp_file_manager() \u001b[39mas\u001b[39;00m temp_file:\n\u001b[1;32m   1362\u001b[0m     logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mdownloading \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m to \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, url, temp_file\u001b[39m.\u001b[39mname)\n\u001b[0;32m-> 1364\u001b[0m     http_get(\n\u001b[1;32m   1365\u001b[0m         url_to_download,\n\u001b[1;32m   1366\u001b[0m         temp_file,\n\u001b[1;32m   1367\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m   1368\u001b[0m         resume_size\u001b[39m=\u001b[39;49mresume_size,\n\u001b[1;32m   1369\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m   1370\u001b[0m         expected_size\u001b[39m=\u001b[39;49mexpected_size,\n\u001b[1;32m   1371\u001b[0m     )\n\u001b[1;32m   1373\u001b[0m \u001b[39mif\u001b[39;00m local_dir \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1374\u001b[0m     logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mStoring \u001b[39m\u001b[39m{\u001b[39;00murl\u001b[39m}\u001b[39;00m\u001b[39m in cache at \u001b[39m\u001b[39m{\u001b[39;00mblob_path\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.0/lib/python3.10/site-packages/huggingface_hub/file_download.py:541\u001b[0m, in \u001b[0;36mhttp_get\u001b[0;34m(url, temp_file, proxies, resume_size, headers, timeout, max_retries, expected_size)\u001b[0m\n\u001b[1;32m    531\u001b[0m     displayed_name \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m(â€¦)\u001b[39m\u001b[39m{\u001b[39;00mdisplayed_name[\u001b[39m-\u001b[39m\u001b[39m20\u001b[39m:]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    533\u001b[0m progress \u001b[39m=\u001b[39m tqdm(\n\u001b[1;32m    534\u001b[0m     unit\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mB\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    535\u001b[0m     unit_scale\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    539\u001b[0m     disable\u001b[39m=\u001b[39m\u001b[39mbool\u001b[39m(logger\u001b[39m.\u001b[39mgetEffectiveLevel() \u001b[39m==\u001b[39m logging\u001b[39m.\u001b[39mNOTSET),\n\u001b[1;32m    540\u001b[0m )\n\u001b[0;32m--> 541\u001b[0m \u001b[39mfor\u001b[39;00m chunk \u001b[39min\u001b[39;00m r\u001b[39m.\u001b[39miter_content(chunk_size\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m \u001b[39m*\u001b[39m \u001b[39m1024\u001b[39m \u001b[39m*\u001b[39m \u001b[39m1024\u001b[39m):\n\u001b[1;32m    542\u001b[0m     \u001b[39mif\u001b[39;00m chunk:  \u001b[39m# filter out keep-alive new chunks\u001b[39;00m\n\u001b[1;32m    543\u001b[0m         progress\u001b[39m.\u001b[39mupdate(\u001b[39mlen\u001b[39m(chunk))\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.0/lib/python3.10/site-packages/requests/models.py:816\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    814\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw, \u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    815\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 816\u001b[0m         \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw\u001b[39m.\u001b[39mstream(chunk_size, decode_content\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    817\u001b[0m     \u001b[39mexcept\u001b[39;00m ProtocolError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    818\u001b[0m         \u001b[39mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.0/lib/python3.10/site-packages/urllib3/response.py:940\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m    938\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    939\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m is_fp_closed(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp) \u001b[39mor\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_decoded_buffer) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 940\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(amt\u001b[39m=\u001b[39;49mamt, decode_content\u001b[39m=\u001b[39;49mdecode_content)\n\u001b[1;32m    942\u001b[0m         \u001b[39mif\u001b[39;00m data:\n\u001b[1;32m    943\u001b[0m             \u001b[39myield\u001b[39;00m data\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.0/lib/python3.10/site-packages/urllib3/response.py:879\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    876\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_decoded_buffer) \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m amt:\n\u001b[1;32m    877\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_decoded_buffer\u001b[39m.\u001b[39mget(amt)\n\u001b[0;32m--> 879\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_raw_read(amt)\n\u001b[1;32m    881\u001b[0m flush_decoder \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    882\u001b[0m \u001b[39mif\u001b[39;00m amt \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.0/lib/python3.10/site-packages/urllib3/response.py:814\u001b[0m, in \u001b[0;36mHTTPResponse._raw_read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    811\u001b[0m fp_closed \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp, \u001b[39m\"\u001b[39m\u001b[39mclosed\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    813\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_error_catcher():\n\u001b[0;32m--> 814\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fp_read(amt) \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m fp_closed \u001b[39melse\u001b[39;00m \u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    815\u001b[0m     \u001b[39mif\u001b[39;00m amt \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m amt \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m data:\n\u001b[1;32m    816\u001b[0m         \u001b[39m# Platform-specific: Buggy versions of Python.\u001b[39;00m\n\u001b[1;32m    817\u001b[0m         \u001b[39m# Close the connection when no data is returned\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    822\u001b[0m         \u001b[39m# not properly close the connection in all cases. There is\u001b[39;00m\n\u001b[1;32m    823\u001b[0m         \u001b[39m# no harm in redundantly calling close.\u001b[39;00m\n\u001b[1;32m    824\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.0/lib/python3.10/site-packages/urllib3/response.py:799\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    796\u001b[0m     \u001b[39mreturn\u001b[39;00m buffer\u001b[39m.\u001b[39mgetvalue()\n\u001b[1;32m    797\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    798\u001b[0m     \u001b[39m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[0;32m--> 799\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fp\u001b[39m.\u001b[39;49mread(amt) \u001b[39mif\u001b[39;00m amt \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp\u001b[39m.\u001b[39mread()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.0/lib/python3.10/http/client.py:464\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m amt \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength:\n\u001b[1;32m    462\u001b[0m     \u001b[39m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[1;32m    463\u001b[0m     amt \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength\n\u001b[0;32m--> 464\u001b[0m s \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp\u001b[39m.\u001b[39;49mread(amt)\n\u001b[1;32m    465\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m s \u001b[39mand\u001b[39;00m amt:\n\u001b[1;32m    466\u001b[0m     \u001b[39m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[1;32m    467\u001b[0m     \u001b[39m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[1;32m    468\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_close_conn()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.0/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[1;32m    706\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.0/lib/python3.10/ssl.py:1273\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1269\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1270\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1271\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[1;32m   1272\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[0;32m-> 1273\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(nbytes, buffer)\n\u001b[1;32m   1274\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1275\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.0/lib/python3.10/ssl.py:1129\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1127\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1128\u001b[0m     \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1129\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m, buffer)\n\u001b[1;32m   1130\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1131\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "save_path = os.path.join(Path(\"feature_formation/\"), \"EleutherAI/pythia-70m\")\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "analyze_model_checkpoints(\"EleutherAI/pythia-70m\", \"feature_formation/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--model MODEL] [--output_dir OUTPUT_DIR]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: --ip=127.0.0.1 --stdin=9013 --control=9011 --hb=9010 --Session.signature_scheme=\"hmac-sha256\" --Session.key=b\"985c17ea-0829-4277-bdfa-4ad8ea449a9d\" --shell=9012 --transport=\"tcp\" --iopub=9014 --f=/Users/luciaquirke/Library/Jupyter/runtime/kernel-v2-41152SD5xfpSf0VlJ.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/luciaquirke/.pyenv/versions/3.10.0/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3534: UserWarning:\n",
      "\n",
      "To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(\n",
    "        formatter_class=argparse.ArgumentDefaultsHelpFormatter\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--model\",\n",
    "        default=\"EleutherAI/pythia-70m\",\n",
    "        help=\"Name of model from TransformerLens\",\n",
    "    )\n",
    "    parser.add_argument(\"--output_dir\", default=\"feature_formation\")\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    save_path = os.path.join(args.output_dir, args.model)\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "    analyze_model_checkpoints(args.model, args.output_dir)\n",
    "\n",
    "    \n",
    "    save_file = os.path.join(save_path, f\"{args.feature_dataset}_neurons.csv\")\n",
    "    results.to_csv(save_file, index=False)\n",
    "\n",
    "    save_image = os.path.join(save_path, \"images\")\n",
    "    os.makedirs(save_image, exist_ok=True)\n",
    "    # process_data(args.output_dir, args.model, save_image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
