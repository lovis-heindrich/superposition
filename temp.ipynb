{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install nltk\n",
    "\n",
    "import random\n",
    "import argparse\n",
    "import pickle\n",
    "import os\n",
    "import gzip\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from einops import einops\n",
    "from transformer_lens import HookedTransformer\n",
    "from tqdm.auto import tqdm\n",
    "import plotly.io as pio\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from neel_plotly import *\n",
    "import haystack_utils\n",
    "import probing_utils\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "SEED = 42\n",
    "\n",
    "def set_seeds():\n",
    "    \n",
    "    torch.manual_seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    random.seed(SEED)\n",
    "\n",
    "    pio.renderers.default = \"notebook_connected+notebook\"\n",
    "    # torch.autograd.set_grad_enabled(False)\n",
    "    # torch.set_grad_enabled(False)\n",
    "\n",
    "    # NUM_CHECKPOINTS = 143\n",
    "\n",
    "\n",
    "def get_model(model_name: str, checkpoint: int) -> HookedTransformer:\n",
    "    model = HookedTransformer.from_pretrained(\n",
    "        model_name,\n",
    "        checkpoint_index=checkpoint,\n",
    "        center_unembed=True,\n",
    "        center_writing_weights=True,\n",
    "        fold_ln=True,\n",
    "        device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def preload_models(model_name: str) -> int:\n",
    "    \"\"\"Preload models into cache so we can iterate over them quickly and return the model checkpoint count.\"\"\"\n",
    "    i = 0\n",
    "    try:\n",
    "        with tqdm(total=None) as pbar:\n",
    "            while True:\n",
    "                get_model(model_name, i)\n",
    "                i += 1\n",
    "                pbar.update(1)\n",
    "\n",
    "    except IndexError:\n",
    "        return i\n",
    "\n",
    "\n",
    "def load_language_data() -> dict:\n",
    "    \"\"\"\n",
    "    Returns: dictionary keyed by language code, containing 200 lines of each language included in the Europarl dataset.\n",
    "    \"\"\"\n",
    "    lang_data = {}\n",
    "    lang_data[\"en\"] = haystack_utils.load_json_data(\"data/english_europarl.json\")[:200]\n",
    "\n",
    "    europarl_data_dir = Path(\"data/europarl/\")\n",
    "    for file in os.listdir(europarl_data_dir):\n",
    "        if file.endswith(\".txt\"):\n",
    "            lang = file.split(\"_\")[0]\n",
    "            lang_data[lang] = haystack_utils.load_txt_data(europarl_data_dir.joinpath(file))\n",
    "\n",
    "    for lang in lang_data.keys():\n",
    "        print(lang, len(lang_data[lang]))\n",
    "    return lang_data\n",
    "\n",
    "\n",
    "def train_probe(\n",
    "    positive_data: torch.Tensor, negative_data: torch.Tensor\n",
    ") -> tuple[float, float]:\n",
    "    labels = np.concatenate([np.ones(len(positive_data)), np.zeros(len(negative_data))])\n",
    "    data = np.concatenate([positive_data.cpu().numpy(), negative_data.cpu().numpy()])\n",
    "    scaler = preprocessing.StandardScaler().fit(data)\n",
    "    data = scaler.transform(data)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(\n",
    "        data, labels, test_size=0.2, random_state=SEED\n",
    "    )\n",
    "    probe = probing_utils.get_probe(x_train, y_train, max_iter=2000)\n",
    "    f1, mcc = probing_utils.get_probe_score(probe, x_test, y_test)\n",
    "    return f1, mcc\n",
    "\n",
    "\n",
    "def save_activation(value, hook):\n",
    "    hook.ctx['activation'] = value\n",
    "    return value\n",
    "\n",
    "\n",
    "def get_mlp_activations(\n",
    "    prompts: list[str], layer: int, model: HookedTransformer\n",
    ") -> torch.Tensor:\n",
    "    act_label = f\"blocks.{layer}.mlp.hook_post\"\n",
    "\n",
    "    acts = []\n",
    "    for prompt in prompts:\n",
    "        with model.hooks([(act_label, save_activation)]):\n",
    "            model(prompt)\n",
    "            act = model.hook_dict[act_label].ctx[\"activation\"][:, 10:400, :]\n",
    "        act = einops.rearrange(act, \"batch pos n_neurons -> (batch pos) n_neurons\")\n",
    "        acts.append(act)\n",
    "    acts = torch.concat(acts, dim=0)\n",
    "    return acts\n",
    "\n",
    "\n",
    "def zero_ablate_hook(value, hook):\n",
    "    value[:, :, :] = 0\n",
    "    return value\n",
    "\n",
    "\n",
    "def get_layer_probe_performance(\n",
    "    model: HookedTransformer,\n",
    "    checkpoint: int,\n",
    "    layer: int,\n",
    "    german_data: np.array,\n",
    "    non_german_data: np.array,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Probe performance for each neuron.\"\"\"\n",
    "\n",
    "    german_activations = get_mlp_activations(german_data[:30], layer, model)[:10_000]\n",
    "    non_german_activations = get_mlp_activations(non_german_data[:30], layer, model)[\n",
    "        :10_000\n",
    "    ]\n",
    "\n",
    "    mean_german_activations = german_activations.mean(0).cpu().numpy()\n",
    "    mean_non_german_activations = non_german_activations.mean(0).cpu().numpy()\n",
    "\n",
    "    f1s = []\n",
    "    mccs = []\n",
    "    for neuron in range(model.cfg.d_mlp):\n",
    "        f1, mcc = train_probe(\n",
    "            german_activations[:, neuron].unsqueeze(-1),\n",
    "            non_german_activations[:, neuron].unsqueeze(-1),\n",
    "        )\n",
    "        f1s.append(f1)\n",
    "        mccs.append(mcc)\n",
    "\n",
    "    checkpoint_neuron_labels = [\n",
    "        f\"C{checkpoint}L{layer}N{i}\" for i in range(model.cfg.d_mlp)\n",
    "    ]\n",
    "    neuron_labels = [f\"L{layer}N{i}\" for i in range(model.cfg.d_mlp)]\n",
    "    neuron_indices = [i for i in range(model.cfg.d_mlp)]\n",
    "\n",
    "    layer_df = pd.DataFrame(\n",
    "        {\n",
    "            \"Label\": checkpoint_neuron_labels,\n",
    "            \"NeuronLabel\": neuron_labels,\n",
    "            \"Neuron\": neuron_indices,\n",
    "            \"F1\": f1s,\n",
    "            \"MCC\": mccs,\n",
    "            \"MeanGermanActivation\": mean_german_activations,\n",
    "            \"MeanNonGermanActivation\": mean_non_german_activations,\n",
    "            \"Checkpoint\": [checkpoint] * len(checkpoint_neuron_labels),\n",
    "            \"Layer\": [layer] * len(checkpoint_neuron_labels),\n",
    "        }\n",
    "    )\n",
    "    return layer_df\n",
    "\n",
    "\n",
    "def get_layer_ablation_loss(\n",
    "    model: HookedTransformer, german_data: list, checkpoint: int, layer: int\n",
    "):\n",
    "    loss_data = []\n",
    "\n",
    "    for prompt in german_data[:100]:\n",
    "        loss = model(prompt, return_type=\"loss\").item()\n",
    "        with model.hooks([(f\"blocks.{layer}.mlp.hook_post\", zero_ablate_hook)]):\n",
    "            ablated_loss = model(prompt, return_type=\"loss\").item()\n",
    "        loss_difference = ablated_loss - loss\n",
    "        loss_data.append([checkpoint, layer, loss_difference, loss, ablated_loss])\n",
    "\n",
    "    layer_df = pd.DataFrame(\n",
    "        loss_data,\n",
    "        columns=[\n",
    "            \"Checkpoint\",\n",
    "            \"Layer\",\n",
    "            \"LossDifference\",\n",
    "            \"OriginalLoss\",\n",
    "            \"AblatedLoss\",\n",
    "        ],\n",
    "    )\n",
    "    return layer_df\n",
    "\n",
    "\n",
    "def get_language_losses(\n",
    "    model: HookedTransformer, checkpoint: int, lang_data: dict\n",
    ") -> pd.DataFrame:\n",
    "    data = []\n",
    "    for lang in lang_data.keys():\n",
    "        losses = []\n",
    "        for prompt in lang_data[lang]:\n",
    "            loss = model(prompt, return_type=\"loss\").item()\n",
    "            losses.append(loss)\n",
    "        data.append([checkpoint, lang, np.mean(losses)])\n",
    "\n",
    "    return pd.DataFrame(data, columns=[\"Checkpoint\", \"Language\", \"Loss\"])\n",
    "\n",
    "\n",
    "def run_probe_analysis(\n",
    "    model_name: str,\n",
    "    num_checkpoints: int,\n",
    "    lang_data: dict,\n",
    "    output_dir: Path,\n",
    ") -> None:\n",
    "    \"\"\"Collect several dataframes covering whole layer ablation losses, ngram loss, language losses, and neuron probe performance.\"\"\"\n",
    "    model = get_model(model_name, 0)\n",
    "    n_layers = model.cfg.n_layers\n",
    "\n",
    "    german_data = lang_data[\"de\"]\n",
    "    non_german_data = np.concatenate([lang_data[lang] for lang in lang_data.keys() if lang != \"de\"])\n",
    "    np.random.shuffle(non_german_data)\n",
    "    non_german_data = non_german_data[:200].tolist()\n",
    "\n",
    "    probe_dfs = []\n",
    "    layer_ablation_dfs = []\n",
    "    lang_loss_dfs = []\n",
    "    with tqdm(total=num_checkpoints * n_layers) as pbar:\n",
    "        for checkpoint in range(num_checkpoints):\n",
    "            model = get_model(model_name, checkpoint)\n",
    "            for layer in range(n_layers):\n",
    "                partial_probe_df = get_layer_probe_performance(\n",
    "                    model, checkpoint, layer, german_data, non_german_data\n",
    "                )\n",
    "                probe_dfs.append(partial_probe_df)\n",
    "\n",
    "                partial_layer_ablation_df = get_layer_ablation_loss(\n",
    "                    model, german_data, checkpoint, layer\n",
    "                )\n",
    "\n",
    "                layer_ablation_dfs.append(partial_layer_ablation_df)\n",
    "                lang_loss_dfs.append(get_language_losses(model, checkpoint, lang_data))\n",
    "\n",
    "                # Save progress to allow for checkpointing the analysis\n",
    "                with open(\n",
    "                    output_dir.joinpath(model_name + \"_checkpoint_features.pkl.gz\"), \"wb\"\n",
    "                ) as f:\n",
    "                    pickle.dump(\n",
    "                        {\n",
    "                            \"probe\": probe_dfs,\n",
    "                            \"layer_ablation\": layer_ablation_dfs,\n",
    "                            \"lang_loss\": lang_loss_dfs,\n",
    "                        },\n",
    "                        f,\n",
    "                    )\n",
    "\n",
    "                pbar.update(1)\n",
    "\n",
    "    # Open the pickle file\n",
    "    with open(output_dir.joinpath(model_name + \"_checkpoint_features.pkl.gz\"), \"rb\") as f:\n",
    "        data = pickle.load(f)\n",
    "\n",
    "    # Concatenate the dataframes\n",
    "    data = {dfs_name: pd.concat(dfs) for dfs_name, dfs in data.items()}\n",
    "\n",
    "    # Compress with gzip using high compression and save\n",
    "    with gzip.open(\n",
    "        output_dir.joinpath(model_name + \"_checkpoint_features.pkl.gz\"), \"wb\", compresslevel=9\n",
    "    ) as f_out:\n",
    "        pickle.dump(data, f_out)\n",
    "\n",
    "\n",
    "def analyze_model_checkpoints(model_name: str, output_dir: Path) -> None:\n",
    "    set_seeds()\n",
    "\n",
    "    # Will take about 50GB of disk space for Pythia 70M models\n",
    "    num_checkpoints = preload_models(model_name)\n",
    "\n",
    "    # Load probe data\n",
    "    lang_data = load_language_data()\n",
    "\n",
    "    run_probe_analysis(\n",
    "        model_name, num_checkpoints, lang_data, Path(output_dir)\n",
    "    )\n",
    "\n",
    "\n",
    "def process_data(model_name: str, output_dir: Path, image_dir: Path) -> None:\n",
    "    model = get_model(model_name, 0)\n",
    "    with gzip.open(\n",
    "            output_dir.joinpath(model_name + \"_checkpoint_features.pkl.gz\"), \"rb\"\n",
    "        ) as f:\n",
    "        data = pickle.load(f)\n",
    "\n",
    "    probe_df = data['probe']\n",
    "\n",
    "    checkpoints = []\n",
    "    top_probe = []\n",
    "    for checkpoint in probe_df[\"Checkpoint\"].unique():\n",
    "        checkpoint_df = probe_df[probe_df[\"Checkpoint\"] == checkpoint]\n",
    "        top_probe.append(checkpoint_df[\"MCC\"].max())\n",
    "        checkpoints.append(checkpoint)\n",
    "    fig = px.line(\n",
    "        x=checkpoints,\n",
    "        y=top_probe,\n",
    "        title=\"Top Probe MCC by Checkpoint\",\n",
    "        width=800,\n",
    "        height=400,\n",
    "    )\n",
    "    fig.write_image(image_dir.joinpath(\"top_mcc_by_checkpoint.png\"))\n",
    "\n",
    "    accurate_neurons = probe_df[\n",
    "        (probe_df[\"MCC\"] > 0.85)\n",
    "        & (probe_df[\"MeanGermanActivation\"] > probe_df[\"MeanNonGermanActivation\"])\n",
    "    ][[\"NeuronLabel\", \"MCC\"]].copy()\n",
    "    accurate_neurons = accurate_neurons.sort_values(by=\"MCC\", ascending=False)\n",
    "    print(\n",
    "        len(accurate_neurons[\"NeuronLabel\"].unique()),\n",
    "        \"neurons with an MCC > 0.85 for German text recognition at any point during training.\",\n",
    "    )\n",
    "\n",
    "    good_neurons = accurate_neurons[\"NeuronLabel\"].unique()[:50]\n",
    "\n",
    "    # Melt the DataFrame\n",
    "    probe_df_melt = probe_df[probe_df[\"NeuronLabel\"].isin(good_neurons)].melt(id_vars=['Checkpoint'], var_name='NeuronLabel', value_vars=\"F1\", value_name='F1 score')\n",
    "    probe_df_melt['F1 score'] = pd.to_numeric(probe_df_melt['F1 score'], errors='coerce')\n",
    "\n",
    "    # Calculate percentiles at each x-coordinate\n",
    "    percentiles = [0.25, 0.5, 0.75]\n",
    "    \n",
    "    grouped = probe_df_melt.groupby('Checkpoint')['F1 score'].describe(percentiles=percentiles).reset_index()\n",
    "    # Plot\n",
    "    fig = go.Figure()\n",
    "\n",
    "    fig.add_trace(go.Scatter(x=grouped['Checkpoint'], y=grouped['25%'], fill=None, mode='lines', line_color='rgba(0,100,80,0.2)', showlegend=False))\n",
    "    fig.add_trace(go.Scatter(x=grouped['Checkpoint'], y=grouped['75%'], fill='tonexty', fillcolor='rgba(0,100,80,0.2)', line_color='rgba(0,100,80,0.2)', name=\"25th-75th percentile\"))\n",
    "    fig.add_trace(go.Scatter(x=grouped['Checkpoint'], y=grouped['50%'], mode='lines', line=dict(color='rgb(0,100,80)', width=2), name=\"Median\"))\n",
    "\n",
    "    fig.update_layout(title=\"F1 score of top neurons over time\", xaxis_title=\"Checkpoint\", yaxis_title=\"F1 score\")\n",
    "\n",
    "    fig.write_image(image_dir.joinpath(\"top_f1s_with_quartiles.png\"))\n",
    "\n",
    "    def get_mean_non_german(df, neuron, layer, checkpoint):\n",
    "        label = f\"C{checkpoint}L{layer}N{neuron}\"\n",
    "        df = df[df[\"Label\"] == label][\"MeanNonGermanActivation\"].item()\n",
    "        return df\n",
    "\n",
    "    def get_mean_german(df, neuron, layer, checkpoint):\n",
    "        label = f\"C{checkpoint}L{layer}N{neuron}\"\n",
    "        df = df[df[\"Label\"] == label][\"MeanGermanActivation\"].item()\n",
    "        return df\n",
    "\n",
    "    get_mean_non_german(probe_df, 669, 3, 140)\n",
    "\n",
    "    layer_vals = np.random.randint(0, model.cfg.n_layers, good_neurons.size)\n",
    "    neuron_vals = np.random.randint(0, model.cfg.d_mlp, good_neurons.size)\n",
    "    random_neurons = probe_df[\n",
    "        (probe_df[\"Layer\"].isin(layer_vals)) & (probe_df[\"Neuron\"].isin(neuron_vals))\n",
    "    ]\n",
    "    random_neurons = random_neurons[\"NeuronLabel\"].unique()\n",
    "\n",
    "    fig = px.line(\n",
    "        probe_df[probe_df[\"NeuronLabel\"].isin(good_neurons)],\n",
    "        x=\"Checkpoint\",\n",
    "        y=\"MCC\",\n",
    "        color=\"NeuronLabel\",\n",
    "        title=\"Neurons with max MCC >= 0.85\",\n",
    "    )\n",
    "    fig.write_image(image_dir.joinpath(\"high_mcc_neurons.png\"))\n",
    "\n",
    "    context_neuron_df = probe_df[probe_df[\"NeuronLabel\"] == \"L3N669\"]\n",
    "    fig = px.line(\n",
    "        context_neuron_df,\n",
    "        x=\"Checkpoint\",\n",
    "        y=[\"MeanGermanActivation\", \"MeanEnglishActivation\"],\n",
    "    )\n",
    "    fig.write_image(image_dir.joinpath(\"mean_activations.png\"))\n",
    "\n",
    "\n",
    "    layer_ablation_df = data['layer_ablation']\n",
    "    fig = px.line(\n",
    "        layer_ablation_df.groupby([\"Checkpoint\", \"Layer\"]).mean().reset_index(),\n",
    "        x=\"Checkpoint\",\n",
    "        y=\"LossDifference\",\n",
    "        color=\"Layer\",\n",
    "        title=\"Loss difference for zero-ablating MLP layers on German data\",\n",
    "        width=900,\n",
    "    )\n",
    "    fig.write_image(image_dir.joinpath(\"layer_ablation_losses.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--model MODEL] [--output_dir OUTPUT_DIR]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: --ip=127.0.0.1 --stdin=9008 --control=9006 --hb=9005 --Session.signature_scheme=\"hmac-sha256\" --Session.key=b\"adc69bbf-d442-4503-8fcb-693a0a4dbf77\" --shell=9007 --transport=\"tcp\" --iopub=9009 --f=/root/.local/share/jupyter/runtime/kernel-v2-5895NAsgnTZ12voI.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3513: UserWarning:\n",
      "\n",
      "To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(\n",
    "        formatter_class=argparse.ArgumentDefaultsHelpFormatter\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--model\",\n",
    "        default=\"EleutherAI/pythia-70m\",\n",
    "        help=\"Name of model from TransformerLens\",\n",
    "    )\n",
    "    parser.add_argument(\"--output_dir\", default=\"feature_formation\")\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    save_path = os.path.join(args.output_dir, args.model)\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "    analyze_model_checkpoints(args.model, args.output_dir)\n",
    "\n",
    "    save_image_path = os.path.join(save_path, \"images\")\n",
    "    os.makedirs(save_image_path, exist_ok=True)\n",
    "    \n",
    "    process_data(args.model, save_path, save_image_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
