{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from transformer_lens import HookedTransformer\n",
    "from jaxtyping import Float, Int, Bool\n",
    "from torch import Tensor\n",
    "from tqdm.auto import tqdm\n",
    "import plotly.io as pio\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px \n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from IPython.display import display, HTML\n",
    "from datasets import load_dataset\n",
    "from collections import Counter\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "pio.renderers.default = \"notebook_connected+notebook\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.autograd.set_grad_enabled(False)\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "from haystack_utils import get_mlp_activations\n",
    "import haystack_utils\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 148.00 MiB (GPU 0; 47.54 GiB total capacity; 831.65 MiB already allocated; 137.12 MiB free; 850.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m haystack_utils\u001b[39m.\u001b[39mclean_cache()\n\u001b[0;32m----> 2\u001b[0m model \u001b[39m=\u001b[39m HookedTransformer\u001b[39m.\u001b[39;49mfrom_pretrained(\u001b[39m\"\u001b[39;49m\u001b[39mEleutherAI/pythia-160m\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      3\u001b[0m     center_unembed\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m      4\u001b[0m     center_writing_weights\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m      5\u001b[0m     fold_ln\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m      6\u001b[0m     device\u001b[39m=\u001b[39;49mdevice)\n\u001b[1;32m      8\u001b[0m german_data \u001b[39m=\u001b[39m haystack_utils\u001b[39m.\u001b[39mload_json_data(\u001b[39m\"\u001b[39m\u001b[39mdata/german_europarl.json\u001b[39m\u001b[39m\"\u001b[39m)[:\u001b[39m200\u001b[39m]\n\u001b[1;32m      9\u001b[0m english_data \u001b[39m=\u001b[39m haystack_utils\u001b[39m.\u001b[39mload_json_data(\u001b[39m\"\u001b[39m\u001b[39mdata/english_europarl.json\u001b[39m\u001b[39m\"\u001b[39m)[:\u001b[39m200\u001b[39m]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformer_lens/HookedTransformer.py:940\u001b[0m, in \u001b[0;36mHookedTransformer.from_pretrained\u001b[0;34m(cls, model_name, fold_ln, center_writing_weights, center_unembed, refactor_factored_attn_matrices, checkpoint_index, checkpoint_value, hf_model, device, n_devices, tokenizer, move_to_device, fold_value_biases, **from_pretrained_kwargs)\u001b[0m\n\u001b[1;32m    930\u001b[0m model\u001b[39m.\u001b[39mload_and_process_state_dict(\n\u001b[1;32m    931\u001b[0m     state_dict,\n\u001b[1;32m    932\u001b[0m     fold_ln\u001b[39m=\u001b[39mfold_ln,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    936\u001b[0m     refactor_factored_attn_matrices\u001b[39m=\u001b[39mrefactor_factored_attn_matrices,\n\u001b[1;32m    937\u001b[0m )\n\u001b[1;32m    939\u001b[0m \u001b[39mif\u001b[39;00m move_to_device:\n\u001b[0;32m--> 940\u001b[0m     model\u001b[39m.\u001b[39;49mmove_model_modules_to_device()\n\u001b[1;32m    942\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mLoaded pretrained model \u001b[39m\u001b[39m{\u001b[39;00mmodel_name\u001b[39m}\u001b[39;00m\u001b[39m into HookedTransformer\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    944\u001b[0m \u001b[39mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformer_lens/HookedTransformer.py:797\u001b[0m, in \u001b[0;36mHookedTransformer.move_model_modules_to_device\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    793\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mln_final\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    794\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_final\u001b[39m.\u001b[39mto(\n\u001b[1;32m    795\u001b[0m         devices\u001b[39m.\u001b[39mget_device_for_block_index(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcfg\u001b[39m.\u001b[39mn_layers \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcfg)\n\u001b[1;32m    796\u001b[0m     )\n\u001b[0;32m--> 797\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49munembed\u001b[39m.\u001b[39;49mto(\n\u001b[1;32m    798\u001b[0m     devices\u001b[39m.\u001b[39;49mget_device_for_block_index(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcfg\u001b[39m.\u001b[39;49mn_layers \u001b[39m-\u001b[39;49m \u001b[39m1\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcfg)\n\u001b[1;32m    799\u001b[0m )\n\u001b[1;32m    800\u001b[0m \u001b[39mfor\u001b[39;00m i, block \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblocks):\n\u001b[1;32m    801\u001b[0m     block\u001b[39m.\u001b[39mto(devices\u001b[39m.\u001b[39mget_device_for_block_index(i, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcfg))\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1145\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1141\u001b[0m         \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1142\u001b[0m                     non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1143\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1145\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply(convert)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:820\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    816\u001b[0m \u001b[39m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    817\u001b[0m \u001b[39m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    818\u001b[0m \u001b[39m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    819\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 820\u001b[0m     param_applied \u001b[39m=\u001b[39m fn(param)\n\u001b[1;32m    821\u001b[0m should_use_set_data \u001b[39m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    822\u001b[0m \u001b[39mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1143\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1140\u001b[0m \u001b[39mif\u001b[39;00m convert_to_format \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m t\u001b[39m.\u001b[39mdim() \u001b[39min\u001b[39;00m (\u001b[39m4\u001b[39m, \u001b[39m5\u001b[39m):\n\u001b[1;32m   1141\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1142\u001b[0m                 non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1143\u001b[0m \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39;49mto(device, dtype \u001b[39mif\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_floating_point() \u001b[39mor\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_complex() \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m, non_blocking)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 148.00 MiB (GPU 0; 47.54 GiB total capacity; 831.65 MiB already allocated; 137.12 MiB free; 850.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "haystack_utils.clean_cache()\n",
    "model = HookedTransformer.from_pretrained(\"EleutherAI/pythia-160m\",\n",
    "    center_unembed=True,\n",
    "    center_writing_weights=True,\n",
    "    fold_ln=True,\n",
    "    device=device)\n",
    "\n",
    "german_data = haystack_utils.load_json_data(\"data/german_europarl.json\")[:200]\n",
    "english_data = haystack_utils.load_json_data(\"data/english_europarl.json\")[:200]\n",
    "\n",
    "all_ignore, not_ignore = haystack_utils.get_weird_tokens(model, plot_norms=False)\n",
    "common_tokens = haystack_utils.get_common_tokens(german_data, model, all_ignore, k=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a9c395b9390415f9a40ff99dbf4af17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 44.00 MiB (GPU 0; 47.54 GiB total capacity; 933.43 MiB already allocated; 31.12 MiB free; 956.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m german_activations \u001b[39m=\u001b[39m {}\n\u001b[1;32m     16\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m \u001b[39mset\u001b[39m([layer \u001b[39mfor\u001b[39;00m layer, \u001b[39m*\u001b[39m_ \u001b[39min\u001b[39;00m german_neurons_with_f1]):\n\u001b[0;32m---> 17\u001b[0m     english_activations[layer] \u001b[39m=\u001b[39m get_mlp_activations(english_data, layer, model, mean\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m     18\u001b[0m     german_activations[layer] \u001b[39m=\u001b[39m get_mlp_activations(german_data, layer, model, mean\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m     20\u001b[0m \u001b[39mfor\u001b[39;00m item \u001b[39min\u001b[39;00m german_neurons_with_f1:\n",
      "File \u001b[0;32m~/Documents/haystack_utils.py:73\u001b[0m, in \u001b[0;36mget_mlp_activations\u001b[0;34m(prompts, layer, model, num_prompts, context_crop_start, context_crop_end, mean, hook_pre, pos)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(num_prompts)):\n\u001b[1;32m     72\u001b[0m     tokens \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mto_tokens(prompts[i])\n\u001b[0;32m---> 73\u001b[0m     _, cache \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mrun_with_cache(tokens)\n\u001b[1;32m     74\u001b[0m     act \u001b[39m=\u001b[39m cache[act_label][:, context_crop_start:context_crop_end, :]\n\u001b[1;32m     75\u001b[0m     \u001b[39mif\u001b[39;00m pos \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformer_lens/HookedTransformer.py:440\u001b[0m, in \u001b[0;36mHookedTransformer.run_with_cache\u001b[0;34m(self, return_cache_object, remove_batch_dim, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun_with_cache\u001b[39m(\n\u001b[1;32m    425\u001b[0m     \u001b[39mself\u001b[39m, \u001b[39m*\u001b[39mmodel_args, return_cache_object\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, remove_batch_dim\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[1;32m    426\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    433\u001b[0m     Union[ActivationCache, Dict[\u001b[39mstr\u001b[39m, torch\u001b[39m.\u001b[39mTensor]],\n\u001b[1;32m    434\u001b[0m ]:\n\u001b[1;32m    435\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    436\u001b[0m \u001b[39m    Wrapper around run_with_cache in HookedRootModule. If return_cache_object is True, this will return an\u001b[39;00m\n\u001b[1;32m    437\u001b[0m \u001b[39m    ActivationCache object, with a bunch of useful HookedTransformer specific methods, otherwise it will return a\u001b[39;00m\n\u001b[1;32m    438\u001b[0m \u001b[39m    dictionary of activations as in HookedRootModule.\u001b[39;00m\n\u001b[1;32m    439\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 440\u001b[0m     out, cache_dict \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mrun_with_cache(\n\u001b[1;32m    441\u001b[0m         \u001b[39m*\u001b[39;49mmodel_args, remove_batch_dim\u001b[39m=\u001b[39;49mremove_batch_dim, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    442\u001b[0m     )\n\u001b[1;32m    443\u001b[0m     \u001b[39mif\u001b[39;00m return_cache_object:\n\u001b[1;32m    444\u001b[0m         cache \u001b[39m=\u001b[39m ActivationCache(\n\u001b[1;32m    445\u001b[0m             cache_dict, \u001b[39mself\u001b[39m, has_batch_dim\u001b[39m=\u001b[39m\u001b[39mnot\u001b[39;00m remove_batch_dim\n\u001b[1;32m    446\u001b[0m         )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformer_lens/hook_points.py:459\u001b[0m, in \u001b[0;36mHookedRootModule.run_with_cache\u001b[0;34m(self, names_filter, device, remove_batch_dim, incl_bwd, reset_hooks_end, clear_contexts, *model_args, **model_kwargs)\u001b[0m\n\u001b[1;32m    449\u001b[0m cache_dict, fwd, bwd \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_caching_hooks(\n\u001b[1;32m    450\u001b[0m     names_filter, incl_bwd, device, remove_batch_dim\u001b[39m=\u001b[39mremove_batch_dim\n\u001b[1;32m    451\u001b[0m )\n\u001b[1;32m    453\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhooks(\n\u001b[1;32m    454\u001b[0m     fwd_hooks\u001b[39m=\u001b[39mfwd,\n\u001b[1;32m    455\u001b[0m     bwd_hooks\u001b[39m=\u001b[39mbwd,\n\u001b[1;32m    456\u001b[0m     reset_hooks_end\u001b[39m=\u001b[39mreset_hooks_end,\n\u001b[1;32m    457\u001b[0m     clear_contexts\u001b[39m=\u001b[39mclear_contexts,\n\u001b[1;32m    458\u001b[0m ):\n\u001b[0;32m--> 459\u001b[0m     model_out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(\u001b[39m*\u001b[39;49mmodel_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs)\n\u001b[1;32m    460\u001b[0m     \u001b[39mif\u001b[39;00m incl_bwd:\n\u001b[1;32m    461\u001b[0m         model_out\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformer_lens/HookedTransformer.py:386\u001b[0m, in \u001b[0;36mHookedTransformer.forward\u001b[0;34m(self, input, return_type, loss_per_token, prepend_bos, stop_at_layer, past_kv_cache)\u001b[0m\n\u001b[1;32m    384\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    385\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 386\u001b[0m     logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49munembed(residual)  \u001b[39m# [batch, pos, d_vocab]\u001b[39;00m\n\u001b[1;32m    387\u001b[0m     \u001b[39mif\u001b[39;00m return_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mlogits\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    388\u001b[0m         \u001b[39mreturn\u001b[39;00m logits\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformer_lens/components.py:58\u001b[0m, in \u001b[0;36mUnembed.forward\u001b[0;34m(self, residual)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m     55\u001b[0m     \u001b[39mself\u001b[39m, residual: Float[torch\u001b[39m.\u001b[39mTensor, \u001b[39m\"\u001b[39m\u001b[39mbatch pos d_model\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m     56\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Float[torch\u001b[39m.\u001b[39mTensor, \u001b[39m\"\u001b[39m\u001b[39mbatch pos d_vocab_out\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[1;32m     57\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m---> 58\u001b[0m         einsum(\n\u001b[1;32m     59\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mbatch pos d_model, d_model vocab -> batch pos vocab\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     60\u001b[0m             residual,\n\u001b[1;32m     61\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mW_U,\n\u001b[1;32m     62\u001b[0m         )\n\u001b[1;32m     63\u001b[0m         \u001b[39m+\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mb_U\n\u001b[1;32m     64\u001b[0m     )\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 0; 47.54 GiB total capacity; 933.43 MiB already allocated; 31.12 MiB free; 956.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "german_neurons_with_f1 = [\n",
    "    [5, 2649, 1.0],\n",
    "    [8,\t2994, 1.0],\n",
    "    [11, 2911, 0.99],\n",
    "    [10, 1129, 0.97],\n",
    "    [6, 1838, 0.65],\n",
    "    [7, 1594, 0.65],\n",
    "    [11, 1819, 0.61],\n",
    "    [11, 2014, 0.56],\n",
    "    [10, 753, 0.54],\n",
    "    [11, 205, 0.48],\n",
    "]\n",
    "\n",
    "english_activations = {}\n",
    "german_activations = {}\n",
    "for layer in set([layer for layer, *_ in german_neurons_with_f1]):\n",
    "    english_activations[layer] = get_mlp_activations(english_data, layer, model, mean=False)\n",
    "    german_activations[layer] = get_mlp_activations(german_data, layer, model, mean=False)\n",
    "\n",
    "for item in german_neurons_with_f1:\n",
    "    layer, neuron, f1 = item\n",
    "    item.append(german_activations[layer][:, neuron].mean(0))\n",
    "    item.append(english_activations[layer][:, neuron].mean(0))\n",
    "\n",
    "def get_neuron_hook(layer, neuron, act_value):\n",
    "    def neuron_hook(value, hook):\n",
    "        value[:, :, neuron] = act_value\n",
    "        return value\n",
    "    return (f'blocks.{layer}.mlp.hook_post', neuron_hook)\n",
    "\n",
    "deactivate_context_hooks = [get_neuron_hook(5, 2649, english_activations[5][:, 2649].mean()), get_neuron_hook(8, 2994, english_activations[8][:, 2994].mean())]\n",
    "activate_context_hooks = [get_neuron_hook(5, 2649, german_activations[5][:, 2649].mean()), get_neuron_hook(8, 2994,  german_activations[8][:, 2994].mean())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_effects_german(prompt, index):\n",
    "        \"\"\"Customised to L5 and L8 context neurons\"\"\"\n",
    "        downstream_components = [(f\"blocks.{layer}.hook_{component}_out\") for layer in [6, 7, 9, 10, 11] for component in ['mlp', 'attn']]\n",
    "     \n",
    "        original, ablated, direct_effect, _ = haystack_utils.get_direct_effect(\n",
    "                prompt, model, pos=index, context_ablation_hooks=deactivate_context_hooks, context_activation_hooks=activate_context_hooks,\n",
    "                deactivated_components=tuple(downstream_components), activated_components=(\"blocks.5.hook_mlp_out\", \"blocks.8.hook_mlp_out\",))\n",
    "        \n",
    "        data = [original, ablated, direct_effect]\n",
    "        for layer in [9, 10, 11]:\n",
    "                _, _, _, activated_component_loss = haystack_utils.get_direct_effect(\n",
    "                        prompt, model, pos=index, context_ablation_hooks=deactivate_context_hooks, context_activation_hooks=activate_context_hooks,\n",
    "                        deactivated_components=tuple(component for component in downstream_components if component != f\"blocks.{layer}.hook_mlp_out\"),\n",
    "                        activated_components=(f\"blocks.{layer}.hook_mlp_out\",))\n",
    "                data.append(activated_component_loss)\n",
    "        return data\n",
    "\n",
    "def attn_effects_german(prompt, index):\n",
    "        \"\"\"Customised to L5 and L8 context neurons\"\"\"\n",
    "        downstream_components = [(f\"blocks.{layer}.hook_{component}_out\") for layer in [6, 7, 9, 10, 11] for component in ['mlp', 'attn']]\n",
    "\n",
    "        data = []\n",
    "        for layer in [9, 10, 11]:\n",
    "                _, _, _, activated_component_loss = haystack_utils.get_direct_effect(\n",
    "                        prompt, model, pos=index, context_ablation_hooks=deactivate_context_hooks, context_activation_hooks=activate_context_hooks,\n",
    "                        deactivated_components=tuple(component for component in downstream_components if component != f\"blocks.{layer}.hook_mlp_out\"),\n",
    "                        activated_components=(f\"blocks.{layer}.hook_attn_out\",))\n",
    "                data.append(activated_component_loss)\n",
    "        return data\n",
    "\n",
    "def component_analysis(end_strings: list[str] | str):\n",
    "    if isinstance(end_strings, str):\n",
    "        end_strings = [end_strings]\n",
    "    for end_string in end_strings:\n",
    "        print(model.to_str_tokens(end_string))\n",
    "        random_prompts = haystack_utils.generate_random_prompts(end_string, model, common_tokens, 400, length=20)\n",
    "        data = mlp_effects_german(random_prompts, -1)\n",
    "\n",
    "        haystack_utils.plot_barplot([[item.cpu().flatten().mean().item()] for item in data],\n",
    "                                        names=['original', 'ablated', 'direct effect'] + [f'{i}{j}' for j in [9, 10, 11] for i in [\"MLP\"]], # + [\"MLP9 + MLP11\"]\n",
    "                                        title=f'Loss increases from ablating various MLP components for end string \\\"{end_string}\\\"')\n",
    "        \n",
    "def interest_measure(original_loss, ablated_loss, context_and_activated_loss, only_activated_loss):\n",
    "    \"\"\"Per-token measure, mixture of overall loss increase and loss increase from ablating MLP11\"\"\"\n",
    "    loss_diff = (ablated_loss - original_loss) # Loss increase from context neuron\n",
    "    mlp_11_power = (only_activated_loss - original_loss) # Loss increase from MLP11\n",
    "    mlp_11_power[mlp_11_power < 0] = 0\n",
    "    combined = 0.5 * loss_diff - mlp_11_power\n",
    "    combined[original_loss > 6] = 0\n",
    "    combined[original_loss > ablated_loss] = 0\n",
    "    return combined\n",
    "\n",
    "def print_prompt(prompt: str):\n",
    "    \"\"\"Red/blue scale showing the interest measure for each token\"\"\"\n",
    "    str_token_prompt = model.to_str_tokens(model.to_tokens(prompt))\n",
    "    original_loss, ablated_loss, context_and_activated_loss, only_activated_loss = haystack_utils.get_direct_effect(\n",
    "        prompt, model, pos=None, context_ablation_hooks=deactivate_context_hooks, context_activation_hooks=activate_context_hooks,\n",
    "        deactivated_components =(\"blocks.9.hook_attn_out\", \"blocks.10.hook_attn_out\", \"blocks.11.hook_attn_out\", \"blocks.9.hook_mlp_out\", \"blocks.10.hook_mlp_out\"),\n",
    "        activated_components = (\"blocks.11.hook_mlp_out\",))\n",
    "\n",
    "    pos_wise_diff = interest_measure(original_loss, ablated_loss, context_and_activated_loss, only_activated_loss).flatten().cpu().tolist()\n",
    "\n",
    "    loss_list = [loss.flatten().cpu().tolist() for loss in [original_loss, ablated_loss, context_and_activated_loss, only_activated_loss]]\n",
    "    loss_names = [\"original_loss\", \"ablated_loss\", \"context_and_activated_loss\", \"only_activated_loss\"]\n",
    "    haystack_utils.clean_print_strings_as_html(str_token_prompt[1:], pos_wise_diff, additional_measures=loss_list, additional_measure_names=loss_names)\n",
    "\n",
    "def get_mlp11_decrease_measure(losses: list[tuple[Float[Tensor, \"pos\"], Float[Tensor, \"pos\"], Float[Tensor, \"pos\"], Float[Tensor, \"pos\"]]]):\n",
    "    \"\"\"Token with max interest measure\"\"\"\n",
    "    measure = []\n",
    "    for original_loss, ablated_loss, context_and_activated_loss, only_activated_loss in losses:\n",
    "        combined = interest_measure(original_loss, ablated_loss, context_and_activated_loss, only_activated_loss)\n",
    "        measure.append(combined.max().item())\n",
    "    return measure\n",
    "\n",
    "def left_pad(prompts, model):\n",
    "    tokens = model.to_tokens(prompts)\n",
    "    target_length = tokens.shape[1]\n",
    "\n",
    "    results = []\n",
    "    for prompt in prompts:\n",
    "        tokens = model.to_tokens(prompt)[0]\n",
    "        padded_tokens = torch.cat([torch.zeros((target_length - tokens.shape[0],), dtype=int).cuda(), tokens])\n",
    "        results.append(padded_tokens)\n",
    "\n",
    "    return torch.stack(results)\n",
    "\n",
    "def print_counter(token: str, data: list[str], next_tokens_count = 1):\n",
    "    counter = Counter()\n",
    "    token_index = model.to_single_token(token)\n",
    "    for prompt in data:\n",
    "        tokens = model.to_tokens(prompt)[0]\n",
    "        try: \n",
    "            index = tokens.tolist().index(token_index)\n",
    "        except:\n",
    "            continue\n",
    "        if index + next_tokens_count < len(tokens):\n",
    "            next_tokens = tokens[index : index + next_tokens_count + 1]\n",
    "            next_tokens_str = \"\".join(model.to_str_tokens(next_tokens))\n",
    "            counter.update([next_tokens_str])\n",
    "    print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_language_logprob_diffs(prompt, index, german_token, french_token):\n",
    "        \"\"\"Customised to L5 and L8 context neurons. \n",
    "        Reverses the German context activation hooks for use on French text - the ablation hook activates the German context\"\"\"\n",
    "        downstream_components = [(f\"blocks.{layer}.hook_{component}_out\") for layer in [6, 7, 9, 10, 11] for component in ['mlp', 'attn']]\n",
    "     \n",
    "        original_logprobs, ablated_logprobs, direct_effect_logprobs, _ = haystack_utils.get_direct_effect(\n",
    "                prompt, model, pos=index, context_ablation_hooks=activate_context_hooks, context_activation_hooks=[],\n",
    "                deactivated_components=tuple(downstream_components), activated_components=(\"blocks.5.hook_mlp_out\", \"blocks.8.hook_mlp_out\",),\n",
    "                return_type='logprobs')\n",
    "        \n",
    "        data = [original_logprobs, ablated_logprobs, direct_effect_logprobs]\n",
    "        for layer in [9, 10, 11]:\n",
    "                _, _, _, activated_component_loss = haystack_utils.get_direct_effect(\n",
    "                        prompt, model, pos=index, context_ablation_hooks=activate_context_hooks, context_activation_hooks=[],\n",
    "                        deactivated_components=tuple(component for component in downstream_components if component != f\"blocks.{layer}.hook_mlp_out\"),\n",
    "                        activated_components=(f\"blocks.{layer}.hook_mlp_out\",), return_type='logprobs')\n",
    "                data.append(activated_component_loss)\n",
    "\n",
    "        data = [item[:, french_token] - item[:, german_token] for item in data]\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ord -> n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "component_analysis([' legen'])\n",
    "component_analysis([' die Bräsidentin'])\n",
    "component_analysis([' Anerkennung'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
