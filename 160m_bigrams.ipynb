{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from transformer_lens import HookedTransformer\n",
    "from jaxtyping import Float, Int, Bool\n",
    "from torch import Tensor\n",
    "from tqdm.auto import tqdm\n",
    "import plotly.io as pio\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px \n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from IPython.display import display, HTML\n",
    "from datasets import load_dataset\n",
    "from collections import Counter\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "pio.renderers.default = \"notebook_connected+notebook\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.autograd.set_grad_enabled(False)\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "from haystack_utils import get_mlp_activations\n",
    "import haystack_utils\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model EleutherAI/pythia-160m into HookedTransformer\n",
      "data/german_europarl.json: Loaded 2000 examples with 152 to 2000 characters each.\n",
      "data/english_europarl.json: Loaded 2000 examples with 165 to 2000 characters each.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa0d896acb3f43999bedb44bc7fce0cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "haystack_utils.clean_cache()\n",
    "model = HookedTransformer.from_pretrained(\"EleutherAI/pythia-160m\",\n",
    "    center_unembed=True,\n",
    "    center_writing_weights=True,\n",
    "    fold_ln=True,\n",
    "    device=device)\n",
    "\n",
    "german_data = haystack_utils.load_json_data(\"data/german_europarl.json\")[:200]\n",
    "english_data = haystack_utils.load_json_data(\"data/english_europarl.json\")[:200]\n",
    "\n",
    "all_ignore, not_ignore = haystack_utils.get_weird_tokens(model, plot_norms=False)\n",
    "common_tokens = haystack_utils.get_common_tokens(german_data, model, all_ignore, k=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fa240311e1e4c3e84a19434abc6c733",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2f49b4f14864793916d61a0a73ee02b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91b1f3b6cd9449e1a88ffface9c351e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcdc7bf870dd4be980be1962862bf756",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acade12b164942cfa4bdc9aef9ea0afe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdf219c9da3c44249798c12bf30b2568",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab69e255ed6a4a66a5f1166360a38cb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "131cf13e807a421aa7fa909a6130756a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a243c99d23c4721a02616c432ee7687",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "026ee464e7f94638b7214a6f466cbebe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "german_neurons_with_f1 = [\n",
    "    [5, 2649, 1.0],\n",
    "    [8,\t2994, 1.0],\n",
    "    [11, 2911, 0.99],\n",
    "    [10, 1129, 0.97],\n",
    "    [6, 1838, 0.65],\n",
    "    [7, 1594, 0.65],\n",
    "    [11, 1819, 0.61],\n",
    "    [11, 2014, 0.56],\n",
    "    [10, 753, 0.54],\n",
    "    [11, 205, 0.48],\n",
    "]\n",
    "\n",
    "english_activations = {}\n",
    "german_activations = {}\n",
    "for layer in set([layer for layer, *_ in german_neurons_with_f1]):\n",
    "    english_activations[layer] = get_mlp_activations(english_data, layer, model, mean=False)\n",
    "    german_activations[layer] = get_mlp_activations(german_data, layer, model, mean=False)\n",
    "\n",
    "for item in german_neurons_with_f1:\n",
    "    layer, neuron, f1 = item\n",
    "    item.append(german_activations[layer][:, neuron].mean(0))\n",
    "    item.append(english_activations[layer][:, neuron].mean(0))\n",
    "\n",
    "def get_neuron_hook(layer, neuron, act_value):\n",
    "    def neuron_hook(value, hook):\n",
    "        value[:, :, neuron] = act_value\n",
    "        return value\n",
    "    return (f'blocks.{layer}.mlp.hook_post', neuron_hook)\n",
    "\n",
    "deactivate_context_hooks = [get_neuron_hook(5, 2649, english_activations[5][:, 2649].mean()), get_neuron_hook(8, 2994, english_activations[8][:, 2994].mean())]\n",
    "activate_context_hooks = [get_neuron_hook(5, 2649, german_activations[5][:, 2649].mean()), get_neuron_hook(8, 2994,  german_activations[8][:, 2994].mean())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_effects_german(prompt, index):\n",
    "        \"\"\"Customised to L5 and L8 context neurons\"\"\"\n",
    "        downstream_components = [(f\"blocks.{layer}.hook_{component}_out\") for layer in [6, 7, 9, 10, 11] for component in ['mlp', 'attn']]\n",
    "     \n",
    "        original, ablated, direct_effect, _ = haystack_utils.get_direct_effect(\n",
    "                prompt, model, pos=index, context_ablation_hooks=deactivate_context_hooks, context_activation_hooks=activate_context_hooks,\n",
    "                deactivated_components=tuple(downstream_components), activated_components=(\"blocks.5.hook_mlp_out\", \"blocks.8.hook_mlp_out\",))\n",
    "        \n",
    "        data = [original, ablated, direct_effect]\n",
    "        for layer in [9, 10, 11]:\n",
    "                _, _, _, activated_component_loss = haystack_utils.get_direct_effect(\n",
    "                        prompt, model, pos=index, context_ablation_hooks=deactivate_context_hooks, context_activation_hooks=activate_context_hooks,\n",
    "                        deactivated_components=tuple(component for component in downstream_components if component != f\"blocks.{layer}.hook_mlp_out\"),\n",
    "                        activated_components=(f\"blocks.{layer}.hook_mlp_out\",))\n",
    "                data.append(activated_component_loss)\n",
    "        return data\n",
    "\n",
    "def attn_effects_german(prompt, index):\n",
    "        \"\"\"Customised to L5 and L8 context neurons\"\"\"\n",
    "        downstream_components = [(f\"blocks.{layer}.hook_{component}_out\") for layer in [6, 7, 9, 10, 11] for component in ['mlp', 'attn']]\n",
    "\n",
    "        data = []\n",
    "        for layer in [9, 10, 11]:\n",
    "                _, _, _, activated_component_loss = haystack_utils.get_direct_effect(\n",
    "                        prompt, model, pos=index, context_ablation_hooks=deactivate_context_hooks, context_activation_hooks=activate_context_hooks,\n",
    "                        deactivated_components=tuple(component for component in downstream_components if component != f\"blocks.{layer}.hook_mlp_out\"),\n",
    "                        activated_components=(f\"blocks.{layer}.hook_attn_out\",))\n",
    "                data.append(activated_component_loss)\n",
    "        return data\n",
    "\n",
    "def component_analysis(end_strings: list[str] | str):\n",
    "    if isinstance(end_strings, str):\n",
    "        end_strings = [end_strings]\n",
    "    for end_string in end_strings:\n",
    "        print(model.to_str_tokens(end_string))\n",
    "        random_prompts = haystack_utils.generate_random_prompts(end_string, model, common_tokens, 400, length=20)\n",
    "        data = mlp_effects_german(random_prompts, -1)\n",
    "\n",
    "        haystack_utils.plot_barplot([[item.cpu().flatten().mean().item()] for item in data],\n",
    "                                        names=['original', 'ablated', 'direct effect'] + [f'{i}{j}' for j in [9, 10, 11] for i in [\"MLP\"]], # + [\"MLP9 + MLP11\"]\n",
    "                                        title=f'Loss increases from ablating various MLP components for end string \\\"{end_string}\\\"')\n",
    "        \n",
    "def interest_measure(original_loss, ablated_loss, context_and_activated_loss, only_activated_loss):\n",
    "    \"\"\"Per-token measure, mixture of overall loss increase and loss increase from ablating MLP11\"\"\"\n",
    "    loss_diff = (ablated_loss - original_loss) # Loss increase from context neuron\n",
    "    mlp_11_power = (only_activated_loss - original_loss) # Loss increase from MLP11\n",
    "    mlp_11_power[mlp_11_power < 0] = 0\n",
    "    combined = 0.5 * loss_diff - mlp_11_power\n",
    "    combined[original_loss > 6] = 0\n",
    "    combined[original_loss > ablated_loss] = 0\n",
    "    return combined\n",
    "\n",
    "def print_prompt(prompt: str):\n",
    "    \"\"\"Red/blue scale showing the interest measure for each token\"\"\"\n",
    "    str_token_prompt = model.to_str_tokens(model.to_tokens(prompt))\n",
    "    original_loss, ablated_loss, context_and_activated_loss, only_activated_loss = haystack_utils.get_direct_effect(\n",
    "        prompt, model, pos=None, context_ablation_hooks=deactivate_context_hooks, context_activation_hooks=activate_context_hooks,\n",
    "        deactivated_components =(\"blocks.9.hook_attn_out\", \"blocks.10.hook_attn_out\", \"blocks.11.hook_attn_out\", \"blocks.9.hook_mlp_out\", \"blocks.10.hook_mlp_out\"),\n",
    "        activated_components = (\"blocks.11.hook_mlp_out\",))\n",
    "\n",
    "    pos_wise_diff = interest_measure(original_loss, ablated_loss, context_and_activated_loss, only_activated_loss).flatten().cpu().tolist()\n",
    "\n",
    "    loss_list = [loss.flatten().cpu().tolist() for loss in [original_loss, ablated_loss, context_and_activated_loss, only_activated_loss]]\n",
    "    loss_names = [\"original_loss\", \"ablated_loss\", \"context_and_activated_loss\", \"only_activated_loss\"]\n",
    "    haystack_utils.clean_print_strings_as_html(str_token_prompt[1:], pos_wise_diff, additional_measures=loss_list, additional_measure_names=loss_names)\n",
    "\n",
    "def get_mlp11_decrease_measure(losses: list[tuple[Float[Tensor, \"pos\"], Float[Tensor, \"pos\"], Float[Tensor, \"pos\"], Float[Tensor, \"pos\"]]]):\n",
    "    \"\"\"Token with max interest measure\"\"\"\n",
    "    measure = []\n",
    "    for original_loss, ablated_loss, context_and_activated_loss, only_activated_loss in losses:\n",
    "        combined = interest_measure(original_loss, ablated_loss, context_and_activated_loss, only_activated_loss)\n",
    "        measure.append(combined.max().item())\n",
    "    return measure\n",
    "\n",
    "def left_pad(prompts, model):\n",
    "    tokens = model.to_tokens(prompts)\n",
    "    target_length = tokens.shape[1]\n",
    "\n",
    "    results = []\n",
    "    for prompt in prompts:\n",
    "        tokens = model.to_tokens(prompt)[0]\n",
    "        padded_tokens = torch.cat([torch.zeros((target_length - tokens.shape[0],), dtype=int).cuda(), tokens])\n",
    "        results.append(padded_tokens)\n",
    "\n",
    "    return torch.stack(results)\n",
    "\n",
    "def print_counter(token: str, data: list[str], next_tokens_count = 1):\n",
    "    counter = Counter()\n",
    "    token_index = model.to_single_token(token)\n",
    "    for prompt in data:\n",
    "        tokens = model.to_tokens(prompt)[0]\n",
    "        try: \n",
    "            index = tokens.tolist().index(token_index)\n",
    "        except:\n",
    "            continue\n",
    "        if index + next_tokens_count < len(tokens):\n",
    "            next_tokens = tokens[index : index + next_tokens_count + 1]\n",
    "            next_tokens_str = \"\".join(model.to_str_tokens(next_tokens))\n",
    "            counter.update([next_tokens_str])\n",
    "    print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_language_logprob_diffs(prompt, index, german_token, french_token):\n",
    "        \"\"\"Customised to L5 and L8 context neurons. \n",
    "        Reverses the German context activation hooks for use on French text - the ablation hook activates the German context\"\"\"\n",
    "        downstream_components = [(f\"blocks.{layer}.hook_{component}_out\") for layer in [6, 7, 9, 10, 11] for component in ['mlp', 'attn']]\n",
    "     \n",
    "        original_logprobs, ablated_logprobs, direct_effect_logprobs, _ = haystack_utils.get_direct_effect(\n",
    "                prompt, model, pos=index, context_ablation_hooks=activate_context_hooks, context_activation_hooks=[],\n",
    "                deactivated_components=tuple(downstream_components), activated_components=(\"blocks.5.hook_mlp_out\", \"blocks.8.hook_mlp_out\",),\n",
    "                return_type='logprobs')\n",
    "        \n",
    "        data = [original_logprobs, ablated_logprobs, direct_effect_logprobs]\n",
    "        for layer in [9, 10, 11]:\n",
    "                _, _, _, activated_component_loss = haystack_utils.get_direct_effect(\n",
    "                        prompt, model, pos=index, context_ablation_hooks=activate_context_hooks, context_activation_hooks=[],\n",
    "                        deactivated_components=tuple(component for component in downstream_components if component != f\"blocks.{layer}.hook_mlp_out\"),\n",
    "                        activated_components=(f\"blocks.{layer}.hook_mlp_out\",), return_type='logprobs')\n",
    "                data.append(activated_component_loss)\n",
    "\n",
    "        data = [item[:, french_token] - item[:, german_token] for item in data]\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ord -> n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "component_analysis([' legen'])\n",
    "component_analysis([' die Br√§sidentin'])\n",
    "component_analysis([' Anerkennung'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
