{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'HookPoints' from 'transformer_lens.hook_points' (/opt/conda/lib/python3.10/site-packages/transformer_lens/hook_points.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[116], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformer_lens\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mhook_points\u001b[39;00m \u001b[39mimport\u001b[39;00m HookPoints\n\u001b[1;32m     16\u001b[0m pio\u001b[39m.\u001b[39mrenderers\u001b[39m.\u001b[39mdefault \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mnotebook_connected\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     17\u001b[0m device \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available() \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'HookPoints' from 'transformer_lens.hook_points' (/opt/conda/lib/python3.10/site-packages/transformer_lens/hook_points.py)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from transformer_lens import HookedTransformer, ActivationCache, utils, patching\n",
    "from jaxtyping import Float, Int, Bool\n",
    "from torch import Tensor\n",
    "from tqdm.auto import tqdm\n",
    "import plotly.io as pio\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "from einops import einsum\n",
    "import plotly.express as px\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "pio.renderers.default = \"notebook_connected\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.autograd.set_grad_enabled(False)\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "from haystack_utils import get_mlp_activations\n",
    "import haystack_utils\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "haystack_utils.clean_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model EleutherAI/pythia-70m into HookedTransformer\n",
      "data/german_europarl.json: Loaded 2000 examples with 152 to 2000 characters each.\n",
      "data/english_europarl.json: Loaded 2000 examples with 165 to 2000 characters each.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e43b4eaf1dba46e19f6d103c3576e9d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7c8169261f646c7a314dbca3c137c79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = HookedTransformer.from_pretrained(\"EleutherAI/pythia-70m\",\n",
    "    center_unembed=True,\n",
    "    center_writing_weights=True,\n",
    "    fold_ln=True,\n",
    "    device=device)\n",
    "\n",
    "german_data = haystack_utils.load_json_data(\"data/german_europarl.json\")[:200]\n",
    "english_data = haystack_utils.load_json_data(\"data/english_europarl.json\")[:200]\n",
    "\n",
    "\n",
    "english_activations = {}\n",
    "german_activations = {}\n",
    "for layer in range(3, 4):\n",
    "    english_activations[layer] = get_mlp_activations(english_data, layer, model, mean=False)\n",
    "    german_activations[layer] = get_mlp_activations(german_data, layer, model, mean=False)\n",
    "\n",
    "LAYER_TO_ABLATE = 3\n",
    "NEURONS_TO_ABLATE = [669]\n",
    "MEAN_ACTIVATION_ACTIVE = german_activations[LAYER_TO_ABLATE][:, NEURONS_TO_ABLATE].mean()\n",
    "MEAN_ACTIVATION_INACTIVE = english_activations[LAYER_TO_ABLATE][:, NEURONS_TO_ABLATE].mean()\n",
    "\n",
    "def deactivate_neurons_hook(value, hook):\n",
    "    value[:, :, NEURONS_TO_ABLATE] = MEAN_ACTIVATION_INACTIVE\n",
    "    return value\n",
    "deactivate_neurons_fwd_hooks=[(f'blocks.{LAYER_TO_ABLATE}.mlp.hook_post', deactivate_neurons_hook)]\n",
    "\n",
    "def activate_neurons_hook(value, hook):\n",
    "    value[:, :, NEURONS_TO_ABLATE] = MEAN_ACTIVATION_ACTIVE\n",
    "    return value\n",
    "activate_neurons_fwd_hooks=[(f'blocks.{LAYER_TO_ABLATE}.mlp.hook_post', activate_neurons_hook)]\n",
    "\n",
    "all_ignore, not_ignore = haystack_utils.get_weird_tokens(model, plot_norms=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95180d77198146a2ba27c4682c5fd059",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "common_tokens = haystack_utils.get_common_tokens(german_data, model, all_ignore, k=50)\n",
    "prompts = haystack_utils.generate_random_prompts(\" Vorschlägen\", model, common_tokens, 200, length=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find important prev token heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 20])\n",
      "[' W', ' zu', 'ge', ' auf', 'ge', ' in', 'icht', 'ß', ' W', 'ig', ' in', 'g', ' er', 'gen', ' W', ' das', 'in', ' er', 'ch', 'in']\n"
     ]
    }
   ],
   "source": [
    "random_prompts = haystack_utils.generate_random_prompts(\" Vorschlägen\", model, common_tokens, 100, length=20)[:, :-4]\n",
    "print(random_prompts.shape)\n",
    "print(model.to_str_tokens(random_prompts[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 8, 64])\n"
     ]
    }
   ],
   "source": [
    "loss, cache = model.run_with_cache(random_prompts)\n",
    "mean_attention_activations = [] \n",
    "for layer in range(6):\n",
    "    activation = cache[f'blocks.{layer}.attn.hook_z'].mean((0, 1))\n",
    "    mean_attention_activations.append(activation)\n",
    "mean_attention_activations = torch.stack(mean_attention_activations)\n",
    "print(mean_attention_activations.shape) # layer head pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ablate_attention_hook(layer, head, mean_activation, pos=-2):\n",
    "    def ablate_attention_head(value, hook):\n",
    "        value[:, pos, head, :] = mean_activation[layer, head, :]\n",
    "        return value\n",
    "\n",
    "    return (f'blocks.{layer}.attn.hook_z', ablate_attention_head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "pos = -2\n",
    "for layer in range(6):\n",
    "    for head in range(8):\n",
    "        ablate_head_hook = [get_ablate_attention_hook(layer, head, mean_attention_activations, pos=pos)]\n",
    "        with model.hooks(fwd_hooks=ablate_head_hook):\n",
    "            loss = model(prompts, return_type=\"loss\", loss_per_token=True)[:, -1].tolist()\n",
    "            losses.append(loss)\n",
    "\n",
    "original_loss = model(prompts, return_type=\"loss\", loss_per_token=True)[:, -1].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>                            <div id=\"ae229892-07bb-4b60-8ec9-6573342b6b31\" class=\"plotly-graph-div\" style=\"height:525px; width:1000px;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"ae229892-07bb-4b60-8ec9-6573342b6b31\")) {                    Plotly.newPlot(                        \"ae229892-07bb-4b60-8ec9-6573342b6b31\",                        [{\"error_y\":{\"array\":[1.389686286571527],\"type\":\"data\",\"visible\":true},\"name\":\"Original\",\"x\":[\"Original\"],\"y\":[1.2341186654334888],\"type\":\"bar\"},{\"error_y\":{\"array\":[1.4266802799475096],\"type\":\"data\",\"visible\":true},\"name\":\"L0H0\",\"x\":[\"L0H0\"],\"y\":[1.2599682064028457],\"type\":\"bar\"},{\"error_y\":{\"array\":[0.8423655501893332],\"type\":\"data\",\"visible\":true},\"name\":\"L0H1\",\"x\":[\"L0H1\"],\"y\":[0.4074728715546371],\"type\":\"bar\"},{\"error_y\":{\"array\":[1.3898424575533228],\"type\":\"data\",\"visible\":true},\"name\":\"L0H2\",\"x\":[\"L0H2\"],\"y\":[1.2635257250489667],\"type\":\"bar\"},{\"error_y\":{\"array\":[1.4188715895325548],\"type\":\"data\",\"visible\":true},\"name\":\"L0H3\",\"x\":[\"L0H3\"],\"y\":[1.304431827662047],\"type\":\"bar\"},{\"error_y\":{\"array\":[1.3371331507079178],\"type\":\"data\",\"visible\":true},\"name\":\"L0H4\",\"x\":[\"L0H4\"],\"y\":[1.1150665025622584],\"type\":\"bar\"},{\"error_y\":{\"array\":[1.6314575419312782],\"type\":\"data\",\"visible\":true},\"name\":\"L0H5\",\"x\":[\"L0H5\"],\"y\":[1.596686280677095],\"type\":\"bar\"},{\"error_y\":{\"array\":[1.2997091043399374],\"type\":\"data\",\"visible\":true},\"name\":\"L0H6\",\"x\":[\"L0H6\"],\"y\":[1.0846073029248509],\"type\":\"bar\"},{\"error_y\":{\"array\":[1.5671957779906371],\"type\":\"data\",\"visible\":true},\"name\":\"L0H7\",\"x\":[\"L0H7\"],\"y\":[4.206957339346409],\"type\":\"bar\"},{\"error_y\":{\"array\":[1.5230599401477032],\"type\":\"data\",\"visible\":true},\"name\":\"L1H0\",\"x\":[\"L1H0\"],\"y\":[1.4993834144086577],\"type\":\"bar\"},{\"error_y\":{\"array\":[1.7588717702186547],\"type\":\"data\",\"visible\":true},\"name\":\"L1H1\",\"x\":[\"L1H1\"],\"y\":[1.6593253288173582],\"type\":\"bar\"},{\"error_y\":{\"array\":[1.3629289357214938],\"type\":\"data\",\"visible\":true},\"name\":\"L1H2\",\"x\":[\"L1H2\"],\"y\":[1.2479408311471343],\"type\":\"bar\"},{\"error_y\":{\"array\":[1.3758239921884658],\"type\":\"data\",\"visible\":true},\"name\":\"L1H3\",\"x\":[\"L1H3\"],\"y\":[1.2860371819604188],\"type\":\"bar\"},{\"error_y\":{\"array\":[1.2620671225428008],\"type\":\"data\",\"visible\":true},\"name\":\"L1H4\",\"x\":[\"L1H4\"],\"y\":[1.00163998910808],\"type\":\"bar\"},{\"error_y\":{\"array\":[1.3338449083722803],\"type\":\"data\",\"visible\":true},\"name\":\"L1H5\",\"x\":[\"L1H5\"],\"y\":[1.2564453458599747],\"type\":\"bar\"},{\"error_y\":{\"array\":[1.006348730829279],\"type\":\"data\",\"visible\":true},\"name\":\"L1H6\",\"x\":[\"L1H6\"],\"y\":[0.7137381230283063],\"type\":\"bar\"},{\"error_y\":{\"array\":[1.760982613897661],\"type\":\"data\",\"visible\":true},\"name\":\"L1H7\",\"x\":[\"L1H7\"],\"y\":[1.653069267309038],\"type\":\"bar\"},{\"error_y\":{\"array\":[1.3035721779907603],\"type\":\"data\",\"visible\":true},\"name\":\"L2H0\",\"x\":[\"L2H0\"],\"y\":[1.1471748185018078],\"type\":\"bar\"},{\"error_y\":{\"array\":[1.7577759007360165],\"type\":\"data\",\"visible\":true},\"name\":\"L2H1\",\"x\":[\"L2H1\"],\"y\":[1.8114360427716747],\"type\":\"bar\"},{\"error_y\":{\"array\":[1.5570103667333954],\"type\":\"data\",\"visible\":true},\"name\":\"L2H2\",\"x\":[\"L2H2\"],\"y\":[1.446145947133191],\"type\":\"bar\"},{\"error_y\":{\"array\":[1.2114513862519112],\"type\":\"data\",\"visible\":true},\"name\":\"L2H3\",\"x\":[\"L2H3\"],\"y\":[0.9802001306251623],\"type\":\"bar\"},{\"error_y\":{\"array\":[1.221299390445437],\"type\":\"data\",\"visible\":true},\"name\":\"L2H4\",\"x\":[\"L2H4\"],\"y\":[1.0244618333037943],\"type\":\"bar\"},{\"error_y\":{\"array\":[1.5044634337411722],\"type\":\"data\",\"visible\":true},\"name\":\"L2H5\",\"x\":[\"L2H5\"],\"y\":[1.4264899332507048],\"type\":\"bar\"},{\"error_y\":{\"array\":[1.3559778298343839],\"type\":\"data\",\"visible\":true},\"name\":\"L2H6\",\"x\":[\"L2H6\"],\"y\":[1.1880089293560012],\"type\":\"bar\"},{\"error_y\":{\"array\":[1.3808113257039256],\"type\":\"data\",\"visible\":true},\"name\":\"L2H7\",\"x\":[\"L2H7\"],\"y\":[1.274248190918006],\"type\":\"bar\"},{\"error_y\":{\"array\":[1.3763201961352365],\"type\":\"data\",\"visible\":true},\"name\":\"L3H0\",\"x\":[\"L3H0\"],\"y\":[1.2269909818144515],\"type\":\"bar\"},{\"error_y\":{\"array\":[1.3508877361288043],\"type\":\"data\",\"visible\":true},\"name\":\"L3H1\",\"x\":[\"L3H1\"],\"y\":[1.2226880620303564],\"type\":\"bar\"},{\"error_y\":{\"array\":[1.2544366548965658],\"type\":\"data\",\"visible\":true},\"name\":\"L3H2\",\"x\":[\"L3H2\"],\"y\":[1.074137444656808],\"type\":\"bar\"},{\"error_y\":{\"array\":[1.1415137738645413],\"type\":\"data\",\"visible\":true},\"name\":\"L3H3\",\"x\":[\"L3H3\"],\"y\":[0.8997538722446188],\"type\":\"bar\"},{\"error_y\":{\"array\":[1.081906977359836],\"type\":\"data\",\"visible\":true},\"name\":\"L3H4\",\"x\":[\"L3H4\"],\"y\":[1.0616735996352509],\"type\":\"bar\"},{\"error_y\":{\"array\":[1.3579191424437287],\"type\":\"data\",\"visible\":true},\"name\":\"L3H5\",\"x\":[\"L3H5\"],\"y\":[1.2048634175909683],\"type\":\"bar\"},{\"error_y\":{\"array\":[1.371640232044208],\"type\":\"data\",\"visible\":true},\"name\":\"L3H6\",\"x\":[\"L3H6\"],\"y\":[1.2149533716856968],\"type\":\"bar\"},{\"error_y\":{\"array\":[1.4410429870446695],\"type\":\"data\",\"visible\":true},\"name\":\"L3H7\",\"x\":[\"L3H7\"],\"y\":[1.3418462017457933],\"type\":\"bar\"},{\"error_y\":{\"array\":[1.4114240243350025],\"type\":\"data\",\"visible\":true},\"name\":\"L4H0\",\"x\":[\"L4H0\"],\"y\":[1.3326594457495957],\"type\":\"bar\"},{\"error_y\":{\"array\":[1.4232271653560822],\"type\":\"data\",\"visible\":true},\"name\":\"L4H1\",\"x\":[\"L4H1\"],\"y\":[1.2645517759036737],\"type\":\"bar\"},{\"error_y\":{\"array\":[1.3888825628782957],\"type\":\"data\",\"visible\":true},\"name\":\"L4H2\",\"x\":[\"L4H2\"],\"y\":[1.2318722151452675],\"type\":\"bar\"},{\"error_y\":{\"array\":[1.385326658459898],\"type\":\"data\",\"visible\":true},\"name\":\"L4H3\",\"x\":[\"L4H3\"],\"y\":[1.2327178132580594],\"type\":\"bar\"},{\"error_y\":{\"array\":[1.3844949401319706],\"type\":\"data\",\"visible\":true},\"name\":\"L4H4\",\"x\":[\"L4H4\"],\"y\":[1.2211844168929382],\"type\":\"bar\"},{\"error_y\":{\"array\":[1.344919589804078],\"type\":\"data\",\"visible\":true},\"name\":\"L4H5\",\"x\":[\"L4H5\"],\"y\":[1.1767813656502404],\"type\":\"bar\"},{\"error_y\":{\"array\":[1.3905459045092565],\"type\":\"data\",\"visible\":true},\"name\":\"L4H6\",\"x\":[\"L4H6\"],\"y\":[1.2299251519446261],\"type\":\"bar\"},{\"error_y\":{\"array\":[1.3704189834433154],\"type\":\"data\",\"visible\":true},\"name\":\"L4H7\",\"x\":[\"L4H7\"],\"y\":[1.2192516693496145],\"type\":\"bar\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"title\":{\"text\":\"Loss for ablated attention heads for pos -2\"},\"xaxis\":{\"title\":{\"text\":\"\"}},\"yaxis\":{\"title\":{\"text\":\"\"}},\"barmode\":\"group\",\"width\":1000,\"showlegend\":false},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('ae229892-07bb-4b60-8ec9-6573342b6b31');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "names = [\"Original\"] + [f\"L{layer}H{head}\" for layer in range(5) for head in range(8)]\n",
    "all_losses = [original_loss] + losses\n",
    "\n",
    "haystack_utils.plot_barplot(all_losses, names, legend=False, title=f\"Loss for ablated attention heads for pos {pos}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If head boosts \"gen\" directly it is a skip bigram\n",
    "- Otherwise we don't know"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['izophren', ' fathers', ' fights', 'arman', 'aland', 'harma', 'apiro', 'ariat', 'etine', 'yr', ' lane', 'ability', 'ijn', 'enix', ' championships', 'agram', 'pires', 'omore', 'amond', 'icism']\n",
      "tensor(-2.2559, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "_, cache = model.run_with_cache(prompts)\n",
    "\n",
    "ablate_head_hook = [get_ablate_attention_hook(layer=0, head=7, mean_activation=mean_attention_activations, pos=-2)]\n",
    "with model.hooks(fwd_hooks=ablate_head_hook):\n",
    "    _, ablated_cache = model.run_with_cache(prompts)\n",
    "\n",
    "def resid_to_logprob(cache, layer=0, pos=-2):\n",
    "    resid = cache[f'blocks.{layer}.hook_resid_post']\n",
    "    resid = model.ln_final(resid)\n",
    "    logprobs = model.unembed(resid)[:, pos].log_softmax(-1).mean(0)\n",
    "    return logprobs\n",
    "\n",
    "original_logprob = resid_to_logprob(cache, 1)\n",
    "ablated_logprob = resid_to_logprob(ablated_cache, 1)\n",
    "\n",
    "diffs = original_logprob - ablated_logprob\n",
    "\n",
    "top_diff, top_tokens = diffs.topk(20)\n",
    "print(model.to_str_tokens(top_tokens))\n",
    "print(diffs[model.to_single_token(\"gen\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0: 0.05415569990873337\n",
      "Layer 1: 0.15168869495391846\n",
      "Layer 2: 0.09743906557559967\n",
      "Layer 3: 0.11201384663581848\n",
      "Layer 4: 0.11475804448127747\n",
      "Layer 5: 0.12647250294685364\n"
     ]
    }
   ],
   "source": [
    "# Residual stream head activation value on gen token\n",
    "gen_z = cache[f'blocks.0.attn.hook_z'][:, -2, 7, :]\n",
    "gen_res = einsum(gen_z, model.W_O[0, 7], \"batch d_head, d_head d_model -> batch d_model\")\n",
    "gen_res = gen_res.mean(0)\n",
    "\n",
    "# Dot products with mlp layers\n",
    "res = {}\n",
    "for layer in range(6):\n",
    "    mlp_weight = model.W_in[layer]\n",
    "    mlp_dot = einsum(gen_res, mlp_weight, \"d_model, d_model d_mlp -> d_mlp\")\n",
    "    res[layer] = mlp_dot.cpu().numpy()\n",
    "    print(f\"Layer {layer}: {mlp_dot.abs().mean()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>                            <div id=\"f86a8daa-770d-4b0a-97e4-79ebc6df7da2\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"f86a8daa-770d-4b0a-97e4-79ebc6df7da2\")) {                    Plotly.newPlot(                        \"f86a8daa-770d-4b0a-97e4-79ebc6df7da2\",                        [{\"hovertemplate\":\"variable=0\\u003cbr\\u003eindex=%{x}\\u003cbr\\u003evalue=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"0\",\"line\":{\"color\":\"#636efa\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"0\",\"showlegend\":true,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542,543,544,545,546,547,548,549,550,551,552,553,554,555,556,557,558,559,560,561,562,563,564,565,566,567,568,569,570,571,572,573,574,575,576,577,578,579,580,581,582,583,584,585,586,587,588,589,590,591,592,593,594,595,596,597,598,599,600,601,602,603,604,605,606,607,608,609,610,611,612,613,614,615,616,617,618,619,620,621,622,623,624,625,626,627,628,629,630,631,632,633,634,635,636,637,638,639,640,641,642,643,644,645,646,647,648,649,650,651,652,653,654,655,656,657,658,659,660,661,662,663,664,665,666,667,668,669,670,671,672,673,674,675,676,677,678,679,680,681,682,683,684,685,686,687,688,689,690,691,692,693,694,695,696,697,698,699,700,701,702,703,704,705,706,707,708,709,710,711,712,713,714,715,716,717,718,719,720,721,722,723,724,725,726,727,728,729,730,731,732,733,734,735,736,737,738,739,740,741,742,743,744,745,746,747,748,749,750,751,752,753,754,755,756,757,758,759,760,761,762,763,764,765,766,767,768,769,770,771,772,773,774,775,776,777,778,779,780,781,782,783,784,785,786,787,788,789,790,791,792,793,794,795,796,797,798,799,800,801,802,803,804,805,806,807,808,809,810,811,812,813,814,815,816,817,818,819,820,821,822,823,824,825,826,827,828,829,830,831,832,833,834,835,836,837,838,839,840,841,842,843,844,845,846,847,848,849,850,851,852,853,854,855,856,857,858,859,860,861,862,863,864,865,866,867,868,869,870,871,872,873,874,875,876,877,878,879,880,881,882,883,884,885,886,887,888,889,890,891,892,893,894,895,896,897,898,899,900,901,902,903,904,905,906,907,908,909,910,911,912,913,914,915,916,917,918,919,920,921,922,923,924,925,926,927,928,929,930,931,932,933,934,935,936,937,938,939,940,941,942,943,944,945,946,947,948,949,950,951,952,953,954,955,956,957,958,959,960,961,962,963,964,965,966,967,968,969,970,971,972,973,974,975,976,977,978,979,980,981,982,983,984,985,986,987,988,989,990,991,992,993,994,995,996,997,998,999,1000,1001,1002,1003,1004,1005,1006,1007,1008,1009,1010,1011,1012,1013,1014,1015,1016,1017,1018,1019,1020,1021,1022,1023,1024,1025,1026,1027,1028,1029,1030,1031,1032,1033,1034,1035,1036,1037,1038,1039,1040,1041,1042,1043,1044,1045,1046,1047,1048,1049,1050,1051,1052,1053,1054,1055,1056,1057,1058,1059,1060,1061,1062,1063,1064,1065,1066,1067,1068,1069,1070,1071,1072,1073,1074,1075,1076,1077,1078,1079,1080,1081,1082,1083,1084,1085,1086,1087,1088,1089,1090,1091,1092,1093,1094,1095,1096,1097,1098,1099,1100,1101,1102,1103,1104,1105,1106,1107,1108,1109,1110,1111,1112,1113,1114,1115,1116,1117,1118,1119,1120,1121,1122,1123,1124,1125,1126,1127,1128,1129,1130,1131,1132,1133,1134,1135,1136,1137,1138,1139,1140,1141,1142,1143,1144,1145,1146,1147,1148,1149,1150,1151,1152,1153,1154,1155,1156,1157,1158,1159,1160,1161,1162,1163,1164,1165,1166,1167,1168,1169,1170,1171,1172,1173,1174,1175,1176,1177,1178,1179,1180,1181,1182,1183,1184,1185,1186,1187,1188,1189,1190,1191,1192,1193,1194,1195,1196,1197,1198,1199,1200,1201,1202,1203,1204,1205,1206,1207,1208,1209,1210,1211,1212,1213,1214,1215,1216,1217,1218,1219,1220,1221,1222,1223,1224,1225,1226,1227,1228,1229,1230,1231,1232,1233,1234,1235,1236,1237,1238,1239,1240,1241,1242,1243,1244,1245,1246,1247,1248,1249,1250,1251,1252,1253,1254,1255,1256,1257,1258,1259,1260,1261,1262,1263,1264,1265,1266,1267,1268,1269,1270,1271,1272,1273,1274,1275,1276,1277,1278,1279,1280,1281,1282,1283,1284,1285,1286,1287,1288,1289,1290,1291,1292,1293,1294,1295,1296,1297,1298,1299,1300,1301,1302,1303,1304,1305,1306,1307,1308,1309,1310,1311,1312,1313,1314,1315,1316,1317,1318,1319,1320,1321,1322,1323,1324,1325,1326,1327,1328,1329,1330,1331,1332,1333,1334,1335,1336,1337,1338,1339,1340,1341,1342,1343,1344,1345,1346,1347,1348,1349,1350,1351,1352,1353,1354,1355,1356,1357,1358,1359,1360,1361,1362,1363,1364,1365,1366,1367,1368,1369,1370,1371,1372,1373,1374,1375,1376,1377,1378,1379,1380,1381,1382,1383,1384,1385,1386,1387,1388,1389,1390,1391,1392,1393,1394,1395,1396,1397,1398,1399,1400,1401,1402,1403,1404,1405,1406,1407,1408,1409,1410,1411,1412,1413,1414,1415,1416,1417,1418,1419,1420,1421,1422,1423,1424,1425,1426,1427,1428,1429,1430,1431,1432,1433,1434,1435,1436,1437,1438,1439,1440,1441,1442,1443,1444,1445,1446,1447,1448,1449,1450,1451,1452,1453,1454,1455,1456,1457,1458,1459,1460,1461,1462,1463,1464,1465,1466,1467,1468,1469,1470,1471,1472,1473,1474,1475,1476,1477,1478,1479,1480,1481,1482,1483,1484,1485,1486,1487,1488,1489,1490,1491,1492,1493,1494,1495,1496,1497,1498,1499,1500,1501,1502,1503,1504,1505,1506,1507,1508,1509,1510,1511,1512,1513,1514,1515,1516,1517,1518,1519,1520,1521,1522,1523,1524,1525,1526,1527,1528,1529,1530,1531,1532,1533,1534,1535,1536,1537,1538,1539,1540,1541,1542,1543,1544,1545,1546,1547,1548,1549,1550,1551,1552,1553,1554,1555,1556,1557,1558,1559,1560,1561,1562,1563,1564,1565,1566,1567,1568,1569,1570,1571,1572,1573,1574,1575,1576,1577,1578,1579,1580,1581,1582,1583,1584,1585,1586,1587,1588,1589,1590,1591,1592,1593,1594,1595,1596,1597,1598,1599,1600,1601,1602,1603,1604,1605,1606,1607,1608,1609,1610,1611,1612,1613,1614,1615,1616,1617,1618,1619,1620,1621,1622,1623,1624,1625,1626,1627,1628,1629,1630,1631,1632,1633,1634,1635,1636,1637,1638,1639,1640,1641,1642,1643,1644,1645,1646,1647,1648,1649,1650,1651,1652,1653,1654,1655,1656,1657,1658,1659,1660,1661,1662,1663,1664,1665,1666,1667,1668,1669,1670,1671,1672,1673,1674,1675,1676,1677,1678,1679,1680,1681,1682,1683,1684,1685,1686,1687,1688,1689,1690,1691,1692,1693,1694,1695,1696,1697,1698,1699,1700,1701,1702,1703,1704,1705,1706,1707,1708,1709,1710,1711,1712,1713,1714,1715,1716,1717,1718,1719,1720,1721,1722,1723,1724,1725,1726,1727,1728,1729,1730,1731,1732,1733,1734,1735,1736,1737,1738,1739,1740,1741,1742,1743,1744,1745,1746,1747,1748,1749,1750,1751,1752,1753,1754,1755,1756,1757,1758,1759,1760,1761,1762,1763,1764,1765,1766,1767,1768,1769,1770,1771,1772,1773,1774,1775,1776,1777,1778,1779,1780,1781,1782,1783,1784,1785,1786,1787,1788,1789,1790,1791,1792,1793,1794,1795,1796,1797,1798,1799,1800,1801,1802,1803,1804,1805,1806,1807,1808,1809,1810,1811,1812,1813,1814,1815,1816,1817,1818,1819,1820,1821,1822,1823,1824,1825,1826,1827,1828,1829,1830,1831,1832,1833,1834,1835,1836,1837,1838,1839,1840,1841,1842,1843,1844,1845,1846,1847,1848,1849,1850,1851,1852,1853,1854,1855,1856,1857,1858,1859,1860,1861,1862,1863,1864,1865,1866,1867,1868,1869,1870,1871,1872,1873,1874,1875,1876,1877,1878,1879,1880,1881,1882,1883,1884,1885,1886,1887,1888,1889,1890,1891,1892,1893,1894,1895,1896,1897,1898,1899,1900,1901,1902,1903,1904,1905,1906,1907,1908,1909,1910,1911,1912,1913,1914,1915,1916,1917,1918,1919,1920,1921,1922,1923,1924,1925,1926,1927,1928,1929,1930,1931,1932,1933,1934,1935,1936,1937,1938,1939,1940,1941,1942,1943,1944,1945,1946,1947,1948,1949,1950,1951,1952,1953,1954,1955,1956,1957,1958,1959,1960,1961,1962,1963,1964,1965,1966,1967,1968,1969,1970,1971,1972,1973,1974,1975,1976,1977,1978,1979,1980,1981,1982,1983,1984,1985,1986,1987,1988,1989,1990,1991,1992,1993,1994,1995,1996,1997,1998,1999,2000,2001,2002,2003,2004,2005,2006,2007,2008,2009,2010,2011,2012,2013,2014,2015,2016,2017,2018,2019,2020,2021,2022,2023,2024,2025,2026,2027,2028,2029,2030,2031,2032,2033,2034,2035,2036,2037,2038,2039,2040,2041,2042,2043,2044,2045,2046,2047],\"xaxis\":\"x\",\"y\":[-0.5405229330062866,-0.5173037648200989,-0.5080670118331909,-0.49490290880203247,-0.4681939482688904,-0.4680291414260864,-0.4635322093963623,-0.45484381914138794,-0.44785499572753906,-0.4401147663593292,-0.437616229057312,-0.4297390878200531,-0.428283154964447,-0.42600542306900024,-0.42427343130111694,-0.41254568099975586,-0.40470224618911743,-0.40280115604400635,-0.4001685678958893,-0.3990113139152527,-0.39697253704071045,-0.3965156078338623,-0.39469781517982483,-0.39240992069244385,-0.39214539527893066,-0.3919052183628082,-0.3910784125328064,-0.3902013301849365,-0.38991135358810425,-0.38957303762435913,-0.38417643308639526,-0.38404494524002075,-0.3838324546813965,-0.38027286529541016,-0.37825143337249756,-0.37800416350364685,-0.377160906791687,-0.3757255971431732,-0.3746562898159027,-0.37283653020858765,-0.3725469708442688,-0.37240076065063477,-0.3692054748535156,-0.36841827630996704,-0.3683137595653534,-0.36309969425201416,-0.3623444139957428,-0.3581483066082001,-0.35774359107017517,-0.35763055086135864,-0.35710662603378296,-0.3547055125236511,-0.3530154526233673,-0.3515549600124359,-0.34838491678237915,-0.34686461091041565,-0.34367311000823975,-0.3408380150794983,-0.33894187211990356,-0.3370410203933716,-0.3367692828178406,-0.33676761388778687,-0.3334522843360901,-0.3328133225440979,-0.3305683732032776,-0.326787531375885,-0.3261321783065796,-0.32588279247283936,-0.3240926265716553,-0.3215908408164978,-0.3206237554550171,-0.32045918703079224,-0.3195613622665405,-0.318328320980072,-0.3183203339576721,-0.3182834982872009,-0.3135281801223755,-0.31100308895111084,-0.310782253742218,-0.3101803660392761,-0.309705913066864,-0.3095306158065796,-0.30709606409072876,-0.3044959306716919,-0.30377405881881714,-0.30225783586502075,-0.3005898594856262,-0.29791396856307983,-0.29746678471565247,-0.2964242100715637,-0.29573071002960205,-0.29561442136764526,-0.29455190896987915,-0.2939985394477844,-0.29314950108528137,-0.2923666834831238,-0.2888947129249573,-0.2870263457298279,-0.2835904061794281,-0.28234371542930603,-0.28142163157463074,-0.27994972467422485,-0.2788827419281006,-0.2782817780971527,-0.2777061462402344,-0.2775324285030365,-0.2767895460128784,-0.27668851613998413,-0.27638691663742065,-0.27559754252433777,-0.2746701240539551,-0.2696039080619812,-0.2693752944469452,-0.2677896022796631,-0.26741623878479004,-0.2671588361263275,-0.2667973041534424,-0.26417040824890137,-0.263691246509552,-0.262455552816391,-0.26092538237571716,-0.259966105222702,-0.25959542393684387,-0.2588905096054077,-0.2580612599849701,-0.25786951184272766,-0.2575035095214844,-0.2573759853839874,-0.25698116421699524,-0.2563711404800415,-0.2531502842903137,-0.2529325485229492,-0.2518501579761505,-0.2511499524116516,-0.2501593828201294,-0.2500156760215759,-0.2495768666267395,-0.24934983253479004,-0.24885645508766174,-0.24819131195545197,-0.24717003107070923,-0.2469537854194641,-0.2463683933019638,-0.24618159234523773,-0.2459750473499298,-0.2453938126564026,-0.24529831111431122,-0.2444295883178711,-0.24360644817352295,-0.24291299283504486,-0.24268105626106262,-0.2412049025297165,-0.24111396074295044,-0.2403002679347992,-0.23987486958503723,-0.238519087433815,-0.23843243718147278,-0.23840957880020142,-0.238066166639328,-0.2376963347196579,-0.23622283339500427,-0.23603954911231995,-0.23559312522411346,-0.23556768894195557,-0.23546944558620453,-0.23414336144924164,-0.2339080572128296,-0.2335410714149475,-0.23335689306259155,-0.232989102602005,-0.23267516493797302,-0.23241083323955536,-0.23234902322292328,-0.23223596811294556,-0.23219645023345947,-0.23195436596870422,-0.23178787529468536,-0.23130521178245544,-0.22999729216098785,-0.22916735708713531,-0.22890111804008484,-0.2287619411945343,-0.22873520851135254,-0.22859537601470947,-0.22829586267471313,-0.22756949067115784,-0.22677859663963318,-0.22603820264339447,-0.22454345226287842,-0.22384697198867798,-0.2234300971031189,-0.22328799962997437,-0.22092995047569275,-0.22062495350837708,-0.22056511044502258,-0.2204730063676834,-0.21975499391555786,-0.2196739912033081,-0.21955499053001404,-0.21912679076194763,-0.21888421475887299,-0.2182542085647583,-0.2179436981678009,-0.21775959432125092,-0.2176266312599182,-0.21684280037879944,-0.21663817763328552,-0.21622949838638306,-0.2161659300327301,-0.2160152792930603,-0.21428579092025757,-0.21376174688339233,-0.2134128212928772,-0.21294398605823517,-0.21283192932605743,-0.21228420734405518,-0.21136295795440674,-0.21077010035514832,-0.20946303009986877,-0.20909421145915985,-0.20908008515834808,-0.2086724489927292,-0.2085505872964859,-0.20760948956012726,-0.20616483688354492,-0.20606905221939087,-0.20575755834579468,-0.2045293003320694,-0.20407620072364807,-0.20406533777713776,-0.20401990413665771,-0.20358341932296753,-0.20279747247695923,-0.2022145688533783,-0.20203113555908203,-0.20066912472248077,-0.20045870542526245,-0.19976463913917542,-0.19924107193946838,-0.198461651802063,-0.19811834394931793,-0.19737976789474487,-0.19726702570915222,-0.19713495671749115,-0.19712981581687927,-0.19670245051383972,-0.19631671905517578,-0.1960836499929428,-0.19595539569854736,-0.1952001452445984,-0.1951945424079895,-0.1948045790195465,-0.19238434731960297,-0.19209128618240356,-0.19158527255058289,-0.19144991040229797,-0.19079278409481049,-0.18930897116661072,-0.18882498145103455,-0.18863388895988464,-0.18807333707809448,-0.18802383542060852,-0.18786728382110596,-0.18773311376571655,-0.18769571185112,-0.18700334429740906,-0.18665730953216553,-0.18660181760787964,-0.18590745329856873,-0.18420301377773285,-0.18410629034042358,-0.18392539024353027,-0.18391253054141998,-0.18363967537879944,-0.18336930871009827,-0.1831386685371399,-0.18248465657234192,-0.18184734880924225,-0.18178360164165497,-0.18106965720653534,-0.1807200312614441,-0.18066608905792236,-0.18022772669792175,-0.18020880222320557,-0.17972183227539062,-0.17956647276878357,-0.17948605120182037,-0.17913302779197693,-0.17909929156303406,-0.17879876494407654,-0.178239107131958,-0.17793072760105133,-0.17786121368408203,-0.177438884973526,-0.17705368995666504,-0.17683258652687073,-0.1768006980419159,-0.17672789096832275,-0.1763288527727127,-0.17587488889694214,-0.17546167969703674,-0.17505069077014923,-0.17397543787956238,-0.17322714626789093,-0.17273172736167908,-0.17140772938728333,-0.17054523527622223,-0.17033521831035614,-0.16913872957229614,-0.16830869019031525,-0.1682755947113037,-0.16809049248695374,-0.1673748791217804,-0.1673162877559662,-0.16698604822158813,-0.16673479974269867,-0.16652728617191315,-0.16636241972446442,-0.1659531146287918,-0.16550174355506897,-0.1644255816936493,-0.16379092633724213,-0.1629026234149933,-0.16247886419296265,-0.16192062199115753,-0.1617940068244934,-0.16171899437904358,-0.1616755723953247,-0.16144368052482605,-0.1608273983001709,-0.16000434756278992,-0.15978550910949707,-0.1592884063720703,-0.15879052877426147,-0.15858499705791473,-0.1581965833902359,-0.15804362297058105,-0.1579314023256302,-0.15755406022071838,-0.1571691334247589,-0.15664345026016235,-0.15664193034172058,-0.1564798802137375,-0.1556115299463272,-0.15542346239089966,-0.1543474644422531,-0.15385188162326813,-0.15320420265197754,-0.15274396538734436,-0.15253901481628418,-0.15229864418506622,-0.15194527804851532,-0.15146486461162567,-0.15026259422302246,-0.1500169038772583,-0.149641215801239,-0.1491648256778717,-0.14915001392364502,-0.1491197943687439,-0.14861218631267548,-0.14850232005119324,-0.14774931967258453,-0.14735668897628784,-0.1473078727722168,-0.14709804952144623,-0.14707455039024353,-0.14679932594299316,-0.1465814709663391,-0.1461489051580429,-0.14562897384166718,-0.14491865038871765,-0.14467306435108185,-0.14438311755657196,-0.14409081637859344,-0.14373177289962769,-0.14313967525959015,-0.14282569289207458,-0.14278945326805115,-0.14142611622810364,-0.14100658893585205,-0.140392005443573,-0.1401616483926773,-0.13992178440093994,-0.13985496759414673,-0.1390114724636078,-0.13859865069389343,-0.13815468549728394,-0.13809853792190552,-0.13785046339035034,-0.13760438561439514,-0.13729551434516907,-0.13694411516189575,-0.13675138354301453,-0.1366211473941803,-0.13633739948272705,-0.13608607649803162,-0.13602206110954285,-0.13595068454742432,-0.13576370477676392,-0.13502921164035797,-0.13488182425498962,-0.13483701646327972,-0.13414791226387024,-0.13410980999469757,-0.13387148082256317,-0.13358362019062042,-0.13270919024944305,-0.13266560435295105,-0.13230186700820923,-0.1320229470729828,-0.1318361908197403,-0.13167434930801392,-0.13139882683753967,-0.13134348392486572,-0.13075648248195648,-0.1301780492067337,-0.12984144687652588,-0.12983977794647217,-0.12955276668071747,-0.12896934151649475,-0.12888042628765106,-0.1287747472524643,-0.12814472615718842,-0.12806233763694763,-0.12804007530212402,-0.12800267338752747,-0.12725472450256348,-0.1267653852701187,-0.12643632292747498,-0.1257500946521759,-0.12546974420547485,-0.1253279149532318,-0.12503716349601746,-0.12472888827323914,-0.12449979037046432,-0.12432088702917099,-0.12371563911437988,-0.12352491915225983,-0.12343896180391312,-0.12338841706514359,-0.12298056483268738,-0.12281502783298492,-0.12233421206474304,-0.12217061221599579,-0.12208279967308044,-0.1218089759349823,-0.11999177187681198,-0.1197853833436966,-0.11962955445051193,-0.11958702653646469,-0.11941784620285034,-0.11935384571552277,-0.11919812858104706,-0.11914721131324768,-0.11903439462184906,-0.1189572885632515,-0.11871179938316345,-0.11817525327205658,-0.117585688829422,-0.117521733045578,-0.11743713170289993,-0.11740445345640182,-0.11721067130565643,-0.11702290177345276,-0.11673754453659058,-0.11667982488870621,-0.11638715863227844,-0.11623871326446533,-0.11548957973718643,-0.1150643453001976,-0.11494909226894379,-0.11485493183135986,-0.11474751681089401,-0.1145506203174591,-0.11454707384109497,-0.11390566825866699,-0.11387762427330017,-0.11384759843349457,-0.11307135969400406,-0.11296381801366806,-0.11272025108337402,-0.11238504946231842,-0.11205396056175232,-0.11196936666965485,-0.11191493272781372,-0.11189057677984238,-0.11188863217830658,-0.11166545003652573,-0.11131013929843903,-0.11130467057228088,-0.11041200160980225,-0.11024691164493561,-0.10928162932395935,-0.10895292460918427,-0.10885486006736755,-0.10845039039850235,-0.10833702981472015,-0.10826244950294495,-0.10804299265146255,-0.10698054730892181,-0.10686670243740082,-0.10646992921829224,-0.10626372694969177,-0.10611464083194733,-0.10609658062458038,-0.10505252331495285,-0.1042771190404892,-0.1036238819360733,-0.10322047024965286,-0.10293729603290558,-0.10241398215293884,-0.10191680490970612,-0.10135378688573837,-0.10044093430042267,-0.10036454349756241,-0.1002168208360672,-0.09940455853939056,-0.09922637045383453,-0.09803183376789093,-0.09790034592151642,-0.09769482165575027,-0.0976211279630661,-0.0976051613688469,-0.09733109176158905,-0.097303107380867,-0.09729020297527313,-0.09714969247579575,-0.09703069925308228,-0.09676531702280045,-0.09663048386573792,-0.0965542271733284,-0.09638278186321259,-0.09616478532552719,-0.09607083350419998,-0.09591034054756165,-0.09583596885204315,-0.09548231959342957,-0.09481470286846161,-0.0944761335849762,-0.0941854864358902,-0.09369580447673798,-0.09352903068065643,-0.09314370900392532,-0.09308390319347382,-0.09279361367225647,-0.09274706244468689,-0.09265067428350449,-0.09248341619968414,-0.092140793800354,-0.09192364662885666,-0.09190700948238373,-0.09184282273054123,-0.09146241843700409,-0.09021319448947906,-0.09019605070352554,-0.08974594622850418,-0.08957082033157349,-0.08925387263298035,-0.08885950595140457,-0.0885954350233078,-0.08859239518642426,-0.0885598212480545,-0.08815079927444458,-0.08768792450428009,-0.08744389563798904,-0.08700856566429138,-0.08671067655086517,-0.08634335547685623,-0.08608090132474899,-0.0851254016160965,-0.08511805534362793,-0.08427192270755768,-0.0841720774769783,-0.08415865153074265,-0.08405879139900208,-0.0839993879199028,-0.08305800706148148,-0.08302763849496841,-0.08262141793966293,-0.0818590596318245,-0.08169131726026535,-0.08168157190084457,-0.08161669224500656,-0.08141662180423737,-0.08136042207479477,-0.08132926374673843,-0.08130274713039398,-0.08096352219581604,-0.08073940873146057,-0.080436110496521,-0.07992619276046753,-0.07926257699728012,-0.07925699651241302,-0.0788460522890091,-0.0788249671459198,-0.07829688489437103,-0.07733887434005737,-0.07728362828493118,-0.07670266926288605,-0.07600321620702744,-0.07596807926893234,-0.07556597888469696,-0.07548748701810837,-0.07537256181240082,-0.0751056969165802,-0.07483641058206558,-0.07461845874786377,-0.07446543127298355,-0.07422386109828949,-0.07397166639566422,-0.07384008169174194,-0.0736788660287857,-0.07365573197603226,-0.07352723926305771,-0.07311482727527618,-0.07294103503227234,-0.07260396331548691,-0.07214562594890594,-0.07205989956855774,-0.0716267079114914,-0.07148534059524536,-0.07139955461025238,-0.07139436900615692,-0.0711161345243454,-0.07035595923662186,-0.07009471952915192,-0.06998436152935028,-0.06980276107788086,-0.06931822746992111,-0.06920085102319717,-0.06895352900028229,-0.06858465075492859,-0.06845360994338989,-0.06774327903985977,-0.06757144629955292,-0.06705895066261292,-0.06644289195537567,-0.06597881764173508,-0.06597621738910675,-0.06569632887840271,-0.06511193513870239,-0.06502976268529892,-0.0650043934583664,-0.06494414061307907,-0.06487969309091568,-0.06476560235023499,-0.06432882696390152,-0.06410938501358032,-0.06357298791408539,-0.06345175206661224,-0.06344518810510635,-0.06328767538070679,-0.06300374865531921,-0.06197510287165642,-0.06177119165658951,-0.061642955988645554,-0.061460863798856735,-0.061069097369909286,-0.06103844195604324,-0.060997214168310165,-0.059956274926662445,-0.05951511114835739,-0.05841652303934097,-0.057642772793769836,-0.05749008432030678,-0.05604368448257446,-0.055712830275297165,-0.05569581687450409,-0.05544918030500412,-0.055347710847854614,-0.05527333915233612,-0.054759182035923004,-0.05463678762316704,-0.05441737920045853,-0.05410592257976532,-0.05390765517950058,-0.053799375891685486,-0.05363333225250244,-0.052974261343479156,-0.052633754909038544,-0.05254127457737923,-0.05229365825653076,-0.05216643214225769,-0.0521588996052742,-0.051976870745420456,-0.051760584115982056,-0.05170232430100441,-0.05144735053181648,-0.051153652369976044,-0.05100294202566147,-0.05079367756843567,-0.050719402730464935,-0.050676893442869186,-0.05030515417456627,-0.05014137923717499,-0.05009682476520538,-0.05004667490720749,-0.050039470195770264,-0.04912712052464485,-0.04844933748245239,-0.048269905149936676,-0.0482342429459095,-0.04792932793498039,-0.04783945530653,-0.04767116904258728,-0.047588154673576355,-0.0475660040974617,-0.047361068427562714,-0.04720943421125412,-0.04708857834339142,-0.046895768493413925,-0.046582505106925964,-0.04658158868551254,-0.046495430171489716,-0.04643728584051132,-0.0462726429104805,-0.04626857489347458,-0.04604073613882065,-0.04602646082639694,-0.04591917619109154,-0.04569111764431,-0.04533856362104416,-0.04500425606966019,-0.04483736306428909,-0.04466266557574272,-0.04454880952835083,-0.044363319873809814,-0.04414862394332886,-0.04387336224317551,-0.043291207402944565,-0.042726218700408936,-0.04234291613101959,-0.04218270629644394,-0.041857942938804626,-0.04181407392024994,-0.041586317121982574,-0.041421033442020416,-0.04137961566448212,-0.04112916439771652,-0.0409306064248085,-0.04081829637289047,-0.04071802273392677,-0.04064582288265228,-0.04031560197472572,-0.04004262387752533,-0.03987504541873932,-0.03967464342713356,-0.03966266289353371,-0.03959595412015915,-0.039087578654289246,-0.03889399766921997,-0.03884298726916313,-0.03854021430015564,-0.038252126425504684,-0.03819072246551514,-0.03781614452600479,-0.03713565319776535,-0.03681124746799469,-0.03676418215036392,-0.03666281700134277,-0.0365983285009861,-0.03623327612876892,-0.03622011840343475,-0.03596179559826851,-0.03576047345995903,-0.03551365062594414,-0.03504914790391922,-0.034939445555210114,-0.03488403558731079,-0.03458040952682495,-0.03439910709857941,-0.03425174951553345,-0.03421660512685776,-0.034163735806941986,-0.03415084630250931,-0.03412797302007675,-0.03404888138175011,-0.03396444022655487,-0.03361726924777031,-0.033154115080833435,-0.03301592171192169,-0.03280361741781235,-0.03280346840620041,-0.03258445858955383,-0.03176568076014519,-0.03161739557981491,-0.03148036450147629,-0.031242474913597107,-0.031025171279907227,-0.030582193285226822,-0.030451476573944092,-0.030162125825881958,-0.030059590935707092,-0.02989400178194046,-0.029702670872211456,-0.0296475887298584,-0.02955242618918419,-0.029447093605995178,-0.028919054195284843,-0.028705552220344543,-0.02867145836353302,-0.028587304055690765,-0.028265006840229034,-0.02787575125694275,-0.027808595448732376,-0.027698524296283722,-0.027625445276498795,-0.02760317176580429,-0.0276006031781435,-0.027084752917289734,-0.026931636035442352,-0.02649122290313244,-0.025961339473724365,-0.025523312389850616,-0.02524128183722496,-0.025157593190670013,-0.024614334106445312,-0.02426660992205143,-0.023954905569553375,-0.023931875824928284,-0.023909926414489746,-0.023616552352905273,-0.023521915078163147,-0.023339781910181046,-0.0230579674243927,-0.023015104234218597,-0.02292206883430481,-0.022659361362457275,-0.022187702357769012,-0.022127259522676468,-0.02190997451543808,-0.021815117448568344,-0.02168944478034973,-0.02152809500694275,-0.021286439150571823,-0.021019216626882553,-0.020299255847930908,-0.020244941115379333,-0.02014164626598358,-0.01994490996003151,-0.019698239862918854,-0.01854398101568222,-0.018108021467924118,-0.017855767160654068,-0.017692839726805687,-0.017585478723049164,-0.01711181178689003,-0.017068229615688324,-0.016374170780181885,-0.015982478857040405,-0.015708137303590775,-0.015540782362222672,-0.014906762167811394,-0.014440633356571198,-0.014024302363395691,-0.013909727334976196,-0.013846220448613167,-0.013502143323421478,-0.013400867581367493,-0.013349425047636032,-0.01325446367263794,-0.013194695115089417,-0.01220592949539423,-0.01219731941819191,-0.011778697371482849,-0.011758815497159958,-0.011606425046920776,-0.01131525170058012,-0.010978348553180695,-0.010729439556598663,-0.010652482509613037,-0.010089285671710968,-0.009981660172343254,-0.009646113961935043,-0.009600544348359108,-0.009375978261232376,-0.008647866547107697,-0.008494164794683456,-0.007590677589178085,-0.007544323801994324,-0.0074037909507751465,-0.007228679955005646,-0.007174111902713776,-0.007023576647043228,-0.006899092346429825,-0.0068932510912418365,-0.0066937655210494995,-0.006654568016529083,-0.0066235922276973724,-0.006428178399801254,-0.006316378712654114,-0.005824193358421326,-0.005238689482212067,-0.004879573360085487,-0.004866205155849457,-0.004734627902507782,-0.004381820559501648,-0.004097647964954376,-0.0036894462537020445,-0.0036844685673713684,-0.0033591091632843018,-0.0031170230358839035,-0.0027523264288902283,-0.0025530457496643066,-0.0025396645069122314,-0.0020676888525485992,-0.0018094033002853394,-0.001794165000319481,-0.00178556889295578,-0.001628255471587181,-0.001475166529417038,-0.0008095353841781616,-0.0006021875888109207,-0.0005012676119804382,-0.00032666511833667755,-8.309632539749146e-05,0.00022055581212043762,0.0003392919898033142,0.0008981600403785706,0.0009881258010864258,0.0010398700833320618,0.0011029988527297974,0.0016991477459669113,0.00196988508105278,0.002308681607246399,0.0024138689041137695,0.0026856232434511185,0.0029969140887260437,0.003605131059885025,0.004256941378116608,0.004424691200256348,0.004429172724485397,0.004557359963655472,0.0048121679574251175,0.004970155656337738,0.005025433376431465,0.0051264092326164246,0.005531804636120796,0.00573006272315979,0.005950449034571648,0.006211027503013611,0.006214506924152374,0.006675213575363159,0.0067306384444236755,0.006792962551116943,0.006802277639508247,0.0069135576486587524,0.007122814655303955,0.007515337318181992,0.0075816623866558075,0.007587119936943054,0.008262738585472107,0.008647195994853973,0.008996032178401947,0.009179748594760895,0.009271204471588135,0.009556859731674194,0.009783092886209488,0.00997646152973175,0.010517314076423645,0.010571811348199844,0.01064566895365715,0.010793283581733704,0.01080717146396637,0.010934855788946152,0.011637590825557709,0.011798370629549026,0.011900652199983597,0.01216142624616623,0.012205272912979126,0.012224063277244568,0.012382090091705322,0.01263703778386116,0.012790877372026443,0.013235140591859818,0.013268455862998962,0.013827458024024963,0.013855263590812683,0.014055972918868065,0.014100972563028336,0.014586389064788818,0.014887992292642593,0.014895834028720856,0.015181224793195724,0.015213698148727417,0.015318641439080238,0.015824319794774055,0.01600232720375061,0.016015799716114998,0.016401130706071854,0.01665995642542839,0.016767319291830063,0.01690134033560753,0.016961712390184402,0.01730748638510704,0.017425943166017532,0.017536867409944534,0.017866311594843864,0.017893418669700623,0.017951786518096924,0.01814929023385048,0.018186040222644806,0.018340840935707092,0.018534159287810326,0.01860826089978218,0.018644653260707855,0.01865461841225624,0.01890593394637108,0.01916312426328659,0.0192977637052536,0.019551485776901245,0.019683077931404114,0.020146938040852547,0.020439326763153076,0.0206865593791008,0.020808681845664978,0.021128103137016296,0.021276868879795074,0.021653860807418823,0.021816067397594452,0.02183520793914795,0.02188269980251789,0.021989338099956512,0.02255675196647644,0.022650688886642456,0.02265658974647522,0.022831503301858902,0.023068875074386597,0.023360151797533035,0.02386055700480938,0.02391732856631279,0.024072498083114624,0.024076826870441437,0.024221263825893402,0.02435302734375,0.02440837398171425,0.024590998888015747,0.02498719096183777,0.02511054277420044,0.02538587525486946,0.025679349899291992,0.026015546172857285,0.026110805571079254,0.026156406849622726,0.026225149631500244,0.026335008442401886,0.027156256139278412,0.02716304361820221,0.02766711264848709,0.027774952352046967,0.0279521644115448,0.028126195073127747,0.02823925018310547,0.028312362730503082,0.02853202074766159,0.028916556388139725,0.029005378484725952,0.029127005487680435,0.02915060520172119,0.029518181458115578,0.029691997915506363,0.029739581048488617,0.030102338641881943,0.030398137867450714,0.03043818473815918,0.03046111762523651,0.030523452907800674,0.03056427836418152,0.030650168657302856,0.03069303184747696,0.031326841562986374,0.031404729932546616,0.03142981231212616,0.0314621776342392,0.031539127230644226,0.03161431849002838,0.03193795680999756,0.03304401785135269,0.03315013647079468,0.0334034189581871,0.03345653414726257,0.03372365981340408,0.03439787030220032,0.034477055072784424,0.03449449688196182,0.03457534313201904,0.03461785241961479,0.03467559814453125,0.0347866490483284,0.035059668123722076,0.03519819676876068,0.035303302109241486,0.03566167131066322,0.03574717789888382,0.036111053079366684,0.036361612379550934,0.03671824559569359,0.036760907620191574,0.0369216650724411,0.03696779906749725,0.03719475865364075,0.03732915595173836,0.03733089938759804,0.03771830350160599,0.03787565976381302,0.038135282695293427,0.03815583884716034,0.038397982716560364,0.03845739737153053,0.03864498436450958,0.03865823894739151,0.03901996463537216,0.03910177946090698,0.03922301530838013,0.03930068388581276,0.0396563820540905,0.03985753655433655,0.03988603502511978,0.03993067890405655,0.03997337818145752,0.04024355113506317,0.04028181731700897,0.04043545573949814,0.04047996923327446,0.0407337062060833,0.04102347046136856,0.041337478905916214,0.04166211932897568,0.041959650814533234,0.042138271033763885,0.04253387823700905,0.0428885892033577,0.04295773431658745,0.04320219159126282,0.04362399876117706,0.04365504905581474,0.04391990602016449,0.04403987526893616,0.04410494118928909,0.044363122433423996,0.04506045579910278,0.04518996924161911,0.04530278220772743,0.04616640880703926,0.04670212045311928,0.0469793975353241,0.04719119146466255,0.047986678779125214,0.048399265855550766,0.04858260229229927,0.04859710484743118,0.049826961010694504,0.05084754154086113,0.05122444033622742,0.05171498656272888,0.05215170979499817,0.052497055381536484,0.05278179794549942,0.0528145469725132,0.05283848196268082,0.05299105495214462,0.0531492605805397,0.05331982672214508,0.053740493953228,0.054307036101818085,0.05430980771780014,0.054466888308525085,0.05484636127948761,0.05490915849804878,0.054952472448349,0.055106259882450104,0.05513481795787811,0.055207155644893646,0.05567997321486473,0.05569981783628464,0.05574394017457962,0.05608799681067467,0.056394062936306,0.05648580193519592,0.05658897012472153,0.056608378887176514,0.056650519371032715,0.05666539445519447,0.057075969874858856,0.05711289495229721,0.057348527014255524,0.057543497532606125,0.057547032833099365,0.05755807086825371,0.05783756449818611,0.058022066950798035,0.058703817427158356,0.058807969093322754,0.05903419852256775,0.05971057713031769,0.059743158519268036,0.05983427166938782,0.06006070226430893,0.06146470457315445,0.06178480386734009,0.06190519034862518,0.06195671111345291,0.061964839696884155,0.061972521245479584,0.06234290450811386,0.062377139925956726,0.06284025311470032,0.06284599006175995,0.0628819614648819,0.06305375695228577,0.0630742758512497,0.06352953612804413,0.06385494768619537,0.06396392732858658,0.06403495371341705,0.0640891045331955,0.0641482025384903,0.06424053013324738,0.0645168349146843,0.06483893096446991,0.06505274027585983,0.06505808979272842,0.06569726765155792,0.0663769543170929,0.0666310116648674,0.06679525971412659,0.06688141822814941,0.0672650933265686,0.06738056242465973,0.0674171894788742,0.06752438843250275,0.06838151812553406,0.06838399171829224,0.0684218779206276,0.06896084547042847,0.06922446936368942,0.06974124163389206,0.06981856375932693,0.07041260600090027,0.0706639364361763,0.07078948616981506,0.07082443684339523,0.07095052301883698,0.07102224230766296,0.07129111140966415,0.07140876352787018,0.07142335176467896,0.0715828537940979,0.07178322970867157,0.07204820215702057,0.07248936593532562,0.07259704172611237,0.07264696061611176,0.07397440820932388,0.0741058886051178,0.07431502640247345,0.07463695853948593,0.07497267425060272,0.07562688738107681,0.07629513740539551,0.07664117217063904,0.07664553821086884,0.07676289975643158,0.07732284814119339,0.07802323997020721,0.0780314952135086,0.07803623378276825,0.07853715121746063,0.07860861718654633,0.0789240300655365,0.07961812615394592,0.08017335087060928,0.08071552962064743,0.08073785156011581,0.0807807445526123,0.08103294670581818,0.08116777241230011,0.081250861287117,0.08126066625118256,0.08127214759588242,0.08143755048513412,0.08182372152805328,0.0823313370347023,0.08251740783452988,0.08325883746147156,0.08363575488328934,0.08401660621166229,0.08441638946533203,0.08482252061367035,0.08512755483388901,0.08543282002210617,0.08545716851949692,0.08554534614086151,0.08576567471027374,0.08624902367591858,0.08673350512981415,0.08692459762096405,0.08727296441793442,0.0873308777809143,0.08775961399078369,0.08855710923671722,0.08863223344087601,0.08867496252059937,0.08873399347066879,0.08897179365158081,0.08973564207553864,0.08980494737625122,0.08986146748065948,0.08994290232658386,0.09004724770784378,0.09041112661361694,0.09051945060491562,0.09059040248394012,0.09061402082443237,0.09100961685180664,0.09111933410167694,0.09160448610782623,0.09204831719398499,0.09206114709377289,0.09261666238307953,0.09289640188217163,0.09309554100036621,0.09322009980678558,0.09340646862983704,0.09350615739822388,0.09378662705421448,0.09385109692811966,0.09402640163898468,0.09429492801427841,0.09471535682678223,0.0952562764286995,0.0956147313117981,0.09595675766468048,0.0962754487991333,0.09655111283063889,0.09666086733341217,0.09677335619926453,0.09682891517877579,0.09727107733488083,0.09729655832052231,0.09739628434181213,0.0975266844034195,0.0983319878578186,0.09860240668058395,0.09909859299659729,0.09954680502414703,0.09975090622901917,0.0997605174779892,0.09984198212623596,0.09985682368278503,0.09987647086381912,0.09990245848894119,0.10123315453529358,0.10191501677036285,0.10208049416542053,0.10233785957098007,0.1027296930551529,0.1027885228395462,0.10284311324357986,0.1034221276640892,0.10419198870658875,0.10422471910715103,0.1042269915342331,0.10425233095884323,0.10445847362279892,0.10481150448322296,0.10488854348659515,0.10517530143260956,0.10532466322183609,0.10563285648822784,0.10563609004020691,0.10564647614955902,0.1060703694820404,0.10650505125522614,0.10653091967105865,0.1065433919429779,0.10692648589611053,0.10768720507621765,0.10770978033542633,0.1077406033873558,0.10779467970132828,0.10803049057722092,0.10804249346256256,0.10807466506958008,0.10814530402421951,0.10836795717477798,0.1083751991391182,0.10857752710580826,0.10866639018058777,0.10898706316947937,0.10926076769828796,0.10960275679826736,0.10960326343774796,0.10970062017440796,0.10975833237171173,0.10980092734098434,0.10987742990255356,0.11003182083368301,0.1108916848897934,0.11092567443847656,0.11107558012008667,0.11118149012327194,0.11150950938463211,0.11171180009841919,0.1121329590678215,0.11287392675876617,0.11343398690223694,0.11408228427171707,0.11450423300266266,0.11473455280065536,0.11474832147359848,0.11494498699903488,0.11496113240718842,0.1151941791176796,0.11630018800497055,0.11657509207725525,0.11681581288576126,0.11693838238716125,0.11713317036628723,0.11730623245239258,0.11758439242839813,0.11808336526155472,0.11823330819606781,0.11847467720508575,0.1191624104976654,0.11975829303264618,0.12021438777446747,0.12101519852876663,0.12118960916996002,0.12134809792041779,0.12143447995185852,0.12158551067113876,0.12163648009300232,0.12167523801326752,0.12183552980422974,0.12183791399002075,0.12197264283895493,0.12199722230434418,0.12215687334537506,0.12234468013048172,0.12250714004039764,0.12250752002000809,0.12295033037662506,0.12314668297767639,0.1232089251279831,0.1236167848110199,0.1236238181591034,0.12366844713687897,0.12389741092920303,0.1239178478717804,0.12392131984233856,0.1241280734539032,0.1246749758720398,0.12508776783943176,0.1251998096704483,0.12540799379348755,0.12546542286872864,0.12555816769599915,0.1257845163345337,0.1278320848941803,0.12797164916992188,0.12804782390594482,0.12811776995658875,0.1286415457725525,0.1286802887916565,0.12884080410003662,0.1289718747138977,0.1295197308063507,0.12983927130699158,0.13059207797050476,0.13060259819030762,0.1307069957256317,0.13127875328063965,0.13150884211063385,0.13157835602760315,0.13165883719921112,0.1319185495376587,0.13215121626853943,0.13251878321170807,0.13369864225387573,0.1337987184524536,0.13505467772483826,0.135660320520401,0.13605034351348877,0.13605749607086182,0.13618388772010803,0.13625392317771912,0.1365775316953659,0.1367727667093277,0.13754457235336304,0.13796329498291016,0.1380249559879303,0.1385187953710556,0.13901306688785553,0.1392008513212204,0.1393749713897705,0.13970263302326202,0.1402176171541214,0.1402202993631363,0.1403096467256546,0.14136728644371033,0.14148135483264923,0.1414881944656372,0.14171414077281952,0.14174750447273254,0.14193016290664673,0.1421862691640854,0.14224426448345184,0.14272382855415344,0.14272405207157135,0.1427375078201294,0.14275714755058289,0.14275875687599182,0.14298906922340393,0.1432887613773346,0.14342018961906433,0.1447264552116394,0.14494916796684265,0.14544722437858582,0.1456206738948822,0.14626242220401764,0.1464029997587204,0.14640505611896515,0.14705747365951538,0.14738957583904266,0.14753015339374542,0.1476060152053833,0.14763246476650238,0.14769238233566284,0.1479332149028778,0.14831975102424622,0.14850690960884094,0.14863687753677368,0.1487383246421814,0.14893750846385956,0.14926591515541077,0.14986461400985718,0.14999285340309143,0.1500864326953888,0.15042485296726227,0.15052714943885803,0.15083007514476776,0.1509615182876587,0.15115059912204742,0.1513618528842926,0.15164881944656372,0.15200552344322205,0.15224333107471466,0.15355250239372253,0.15365998446941376,0.15400056540966034,0.15502163767814636,0.15522345900535583,0.15569664537906647,0.15603819489479065,0.15610180795192719,0.15629681944847107,0.15684136748313904,0.15689857304096222,0.15720777213573456,0.15735457837581635,0.15740114450454712,0.15759724378585815,0.15813866257667542,0.15818162262439728,0.1589650958776474,0.1593179702758789,0.1594090461730957,0.16004282236099243,0.16015298664569855,0.1602773517370224,0.1605985164642334,0.16093507409095764,0.16120994091033936,0.16151092946529388,0.16168999671936035,0.1618443876504898,0.16211844980716705,0.1631992757320404,0.16343370079994202,0.1639542132616043,0.1642121821641922,0.1642252504825592,0.1643516719341278,0.1645573526620865,0.1646171510219574,0.16464975476264954,0.16465416550636292,0.16484171152114868,0.1649736762046814,0.1652263104915619,0.1652544140815735,0.165409117937088,0.1654459834098816,0.16571074724197388,0.16590392589569092,0.16592998802661896,0.16594409942626953,0.16610322892665863,0.16613110899925232,0.16615721583366394,0.1674366593360901,0.1677984595298767,0.16789510846138,0.16816741228103638,0.16845332086086273,0.16857485473155975,0.16891518235206604,0.1692141890525818,0.170608252286911,0.17160922288894653,0.17161746323108673,0.1728554666042328,0.17306670546531677,0.17316710948944092,0.17339622974395752,0.1735159456729889,0.17424798011779785,0.1745157539844513,0.17472833395004272,0.1747504472732544,0.17507514357566833,0.17520058155059814,0.17545345425605774,0.17602553963661194,0.1767783761024475,0.17679598927497864,0.17689915001392365,0.17715363204479218,0.17717333137989044,0.1773555725812912,0.1773919314146042,0.17741577327251434,0.1776334047317505,0.17809952795505524,0.17810705304145813,0.17815950512886047,0.17868439853191376,0.1787056028842926,0.17925095558166504,0.17942026257514954,0.1798989176750183,0.1801350861787796,0.1805625855922699,0.18143171072006226,0.1819840669631958,0.1826052963733673,0.18284162878990173,0.183221697807312,0.18327730894088745,0.18382544815540314,0.1847233772277832,0.1847594678401947,0.18477563560009003,0.18500694632530212,0.1854119747877121,0.1855679750442505,0.18579141795635223,0.18615129590034485,0.18620705604553223,0.18677611649036407,0.18731607496738434,0.18738140165805817,0.18802744150161743,0.18819472193717957,0.18876780569553375,0.18878938257694244,0.18883219361305237,0.18932271003723145,0.18981900811195374,0.1902221292257309,0.1904297024011612,0.1910402923822403,0.19112008810043335,0.19117093086242676,0.19260650873184204,0.19265073537826538,0.19313223659992218,0.19354543089866638,0.19399207830429077,0.19440339505672455,0.19464680552482605,0.1952100247144699,0.1952613890171051,0.19635215401649475,0.19658228754997253,0.19668594002723694,0.19690805673599243,0.1974802315235138,0.19794653356075287,0.19808891415596008,0.19832545518875122,0.19869203865528107,0.19964216649532318,0.19977271556854248,0.19984941184520721,0.20064353942871094,0.20077386498451233,0.20081877708435059,0.20085465908050537,0.2011781632900238,0.20159994065761566,0.2025371938943863,0.2027936726808548,0.20299743115901947,0.2032427191734314,0.20386327803134918,0.20387330651283264,0.20410498976707458,0.2043498158454895,0.20521530508995056,0.20604152977466583,0.2062496691942215,0.2062801569700241,0.20721817016601562,0.20813162624835968,0.20826753973960876,0.20835891366004944,0.20841477811336517,0.209328293800354,0.20974141359329224,0.2104036509990692,0.21077445149421692,0.2107773870229721,0.21153686940670013,0.21185927093029022,0.21197667717933655,0.21220897138118744,0.21303462982177734,0.21314403414726257,0.21459922194480896,0.21488696336746216,0.21490582823753357,0.21647080779075623,0.216853529214859,0.21741965413093567,0.21845997869968414,0.21851228177547455,0.2190689742565155,0.2194083034992218,0.21974420547485352,0.2205740213394165,0.22115473449230194,0.22120961546897888,0.221273735165596,0.22145360708236694,0.22164154052734375,0.22167591750621796,0.22179076075553894,0.22190918028354645,0.22221365571022034,0.22247792780399323,0.22356197237968445,0.22473067045211792,0.22492830455303192,0.22493189573287964,0.22504037618637085,0.22522848844528198,0.22566531598567963,0.22636008262634277,0.2266441434621811,0.22744911909103394,0.22750812768936157,0.2276664674282074,0.2289353311061859,0.22950972616672516,0.23006783425807953,0.23047280311584473,0.23092666268348694,0.2316877394914627,0.23249495029449463,0.23287394642829895,0.2331078201532364,0.23361092805862427,0.2345111072063446,0.23506930470466614,0.2351807951927185,0.23523984849452972,0.23557251691818237,0.2358279675245285,0.236711323261261,0.23684921860694885,0.2380121946334839,0.23867294192314148,0.23914891481399536,0.24056759476661682,0.24086958169937134,0.24161013960838318,0.2417798638343811,0.24233494699001312,0.24250173568725586,0.2428436577320099,0.24296747148036957,0.24517306685447693,0.24536016583442688,0.245498925447464,0.24584299325942993,0.24603979289531708,0.24613696336746216,0.2463463544845581,0.24784940481185913,0.24917107820510864,0.24923647940158844,0.249455064535141,0.2495693862438202,0.2500011920928955,0.25015413761138916,0.25078606605529785,0.2509346604347229,0.2512505054473877,0.2528504729270935,0.2533496916294098,0.253726601600647,0.25395435094833374,0.25439342856407166,0.25495219230651855,0.2550591230392456,0.2554484009742737,0.2559589445590973,0.2573176920413971,0.257791668176651,0.2596319615840912,0.2603590190410614,0.2613692879676819,0.2619733214378357,0.26250147819519043,0.2629869282245636,0.26380619406700134,0.26399150490760803,0.2641010284423828,0.2643062472343445,0.2649729251861572,0.26550185680389404,0.26570725440979004,0.26573237776756287,0.26674842834472656,0.26747822761535645,0.2676236033439636,0.26800817251205444,0.26827025413513184,0.2688325345516205,0.2697206735610962,0.27109378576278687,0.2712813913822174,0.27166569232940674,0.2717607915401459,0.271933376789093,0.2721232771873474,0.2722722589969635,0.27294450998306274,0.27323758602142334,0.27346694469451904,0.27452483773231506,0.27473607659339905,0.2759549617767334,0.2764086127281189,0.27759718894958496,0.27774640917778015,0.27775856852531433,0.27790769934654236,0.27840524911880493,0.27883538603782654,0.2789432406425476,0.27913492918014526,0.279418021440506,0.27993789315223694,0.2819926142692566,0.283115029335022,0.2831352949142456,0.28374090790748596,0.2848427891731262,0.28493717312812805,0.285216361284256,0.2854456901550293,0.285515159368515,0.28690922260284424,0.28720638155937195,0.2889143228530884,0.2893584072589874,0.2914760708808899,0.29172128438949585,0.29176315665245056,0.2932558059692383,0.2941657304763794,0.2942688763141632,0.29450541734695435,0.29463648796081543,0.2951832413673401,0.29573309421539307,0.2961916923522949,0.29674655199050903,0.2969428300857544,0.29708331823349,0.29833686351776123,0.2986718416213989,0.29933199286460876,0.29956912994384766,0.3000544309616089,0.3027605414390564,0.3036375641822815,0.30411821603775024,0.30461108684539795,0.3058544993400574,0.3061261773109436,0.30620384216308594,0.3065544664859772,0.3073898255825043,0.30740731954574585,0.30867719650268555,0.3087170422077179,0.30878281593322754,0.30930766463279724,0.3099557161331177,0.31079697608947754,0.3113082945346832,0.312272846698761,0.31375300884246826,0.31578338146209717,0.3160553574562073,0.3166874051094055,0.3168720602989197,0.31819140911102295,0.3187706768512726,0.3192371428012848,0.31958654522895813,0.3200255036354065,0.3209696412086487,0.32127314805984497,0.32332897186279297,0.32352331280708313,0.32502248883247375,0.32515639066696167,0.3260651230812073,0.3261169195175171,0.3271741271018982,0.32877305150032043,0.3304249048233032,0.3304411768913269,0.33054959774017334,0.3311692476272583,0.3318707346916199,0.33305180072784424,0.3348824083805084,0.33494508266448975,0.3349847197532654,0.3352029323577881,0.33562546968460083,0.3358309864997864,0.33615514636039734,0.33728328347206116,0.3375259041786194,0.3420347273349762,0.34305649995803833,0.3437345325946808,0.3455268442630768,0.34925636649131775,0.3499022126197815,0.3515167832374573,0.3522719740867615,0.3535952568054199,0.3557480573654175,0.3557564914226532,0.3557805120944977,0.36038196086883545,0.36082959175109863,0.3627750873565674,0.36447882652282715,0.364937424659729,0.3653159737586975,0.36709195375442505,0.37004420161247253,0.37128257751464844,0.3754814863204956,0.37617385387420654,0.3769248127937317,0.3787806034088135,0.38452017307281494,0.3878105878829956,0.3880842924118042,0.38835620880126953,0.38889995217323303,0.39016926288604736,0.39255332946777344,0.3957067131996155,0.39758819341659546,0.40023353695869446,0.4006197452545166,0.40504544973373413,0.4058617055416107,0.40719854831695557,0.4078054130077362,0.41025134921073914,0.4132252335548401,0.4142649173736572,0.4178260862827301,0.41921067237854004,0.41992437839508057,0.4202953577041626,0.42050331830978394,0.4211675524711609,0.4221840798854828,0.4229235351085663,0.4249638020992279,0.4286890923976898,0.4339965283870697,0.43474483489990234,0.44505149126052856,0.4522128701210022,0.4528507590293884,0.45534548163414,0.46174436807632446,0.46238386631011963,0.46331650018692017,0.46355241537094116,0.46360689401626587,0.4649401903152466,0.4723982512950897,0.47787442803382874,0.4812849164009094,0.48292380571365356,0.48345255851745605,0.4855753183364868,0.4856012761592865,0.4951299726963043,0.49663567543029785,0.49670302867889404,0.5057171583175659,0.5074869990348816,0.5092968940734863,0.5259848237037659,0.5280298590660095,0.5308126211166382,0.5317872762680054,0.5416597127914429,0.5465433597564697,0.5588400959968567,0.5593377351760864,0.5720466375350952,0.5731290578842163,0.5856268405914307,0.5978965759277344,0.6173349618911743,0.6613454222679138,0.6705150008201599,0.6711070537567139,0.6725019216537476,0.791365385055542,0.9298486709594727],\"yaxis\":\"y\",\"type\":\"scattergl\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"index\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"value\"}},\"legend\":{\"title\":{\"text\":\"variable\"},\"tracegroupgap\":0},\"margin\":{\"t\":60}},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('f86a8daa-770d-4b0a-97e4-79ebc6df7da2');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "px.line(np.sort(res[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1: 22 [32, 163, 472, 493, 544, 726, 733, 959, 1019, 1089, 1190, 1203, 1227, 1280, 1287, 1448, 1502, 1782, 1794, 1899, 1980, 1985]\n",
      "Layer 2: 0 []\n",
      "Layer 3: 3 [96, 272, 2014]\n",
      "Layer 4: 1 [265]\n",
      "Layer 5: 4 [166, 188, 881, 1885]\n"
     ]
    }
   ],
   "source": [
    "threshold = 0.5\n",
    "for layer in range(1, 6):\n",
    "    layer_res = res[layer]\n",
    "    print(f\"Layer {layer}: {np.sum(layer_res > threshold)}\", np.argwhere(layer_res > threshold).flatten().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.2433, device='cuda:0') tensor(4.3510, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "loss, _ = model.run_with_cache(prompts, return_type=\"loss\", loss_per_token=True)\n",
    "loss = loss[:, -1].mean()\n",
    "\n",
    "ablate_head_hook = [get_ablate_attention_hook(layer=0, head=7, mean_activation=mean_attention_activations, pos=-2)]\n",
    "with model.hooks(fwd_hooks=ablate_head_hook):\n",
    "    ablated_loss, _ = model.run_with_cache(prompts, return_type=\"loss\", loss_per_token=True)\n",
    "    ablated_loss = ablated_loss[:, -1].mean()\n",
    "\n",
    "print(loss, ablated_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total effect MLP1: 4.28 (+3.03)\n",
      "Total effect MLP2: 2.14 (+0.90)\n",
      "Total effect MLP3: 0.44 (+-0.80)\n",
      "Total effect MLP4: 1.07 (+-0.18)\n",
      "Total effect MLP5: 3.14 (+1.89)\n"
     ]
    }
   ],
   "source": [
    "# Get ablated cache \n",
    "# This isn't actually clean path patching\n",
    "# E.g. when activating MLP5, the layer sees all activated activations of earlier layers, not just the attention head\n",
    "\n",
    "\n",
    "with model.hooks(fwd_hooks=ablate_head_hook):\n",
    "    _, ablated_cache = model.run_with_cache(prompts, return_type=\"loss\", loss_per_token=True)\n",
    "\n",
    "\n",
    "\n",
    "original_loss, original_cache = model.run_with_cache(prompts, return_type=\"loss\", loss_per_token=True)\n",
    "\n",
    "losses = [original_loss[:, -1].cpu().tolist()]\n",
    "def ablate_component_hook(value, hook):\n",
    "    value[:, -2] = ablated_cache[hook.name][:, -2]\n",
    "    return value\n",
    "\n",
    "def activate_component_hook(value, hook):\n",
    "    value[:, -2] = original_cache[hook.name][:, -2]\n",
    "    return value\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for layer in range(1, 6):\n",
    "    component = f\"blocks.{layer}.mlp.hook_post\"\n",
    "    ablate_mlp_hook = [(component, ablate_component_hook)]\n",
    "    with model.hooks(fwd_hooks=ablate_mlp_hook):\n",
    "        ablated_loss, _ = model.run_with_cache(prompts, return_type=\"loss\", loss_per_token=True)\n",
    "        ablated_loss = ablated_loss[:, -1].mean()\n",
    "        print(f\"Total effect MLP{layer}: {ablated_loss:.2f} (+{ablated_loss-loss:.2f})\")\n",
    "    \n",
    "    activate_layers = [i for i in range(1, 6) if i != layer]\n",
    "    activate_mlp_hooks = [(f\"blocks.{i}.mlp.hook_post\", activate_component_hook) for i in activate_layers]\n",
    "    activate_attn_hooks = [(f\"blocks.{i}.hook_attn_out\", activate_component_hook) for i in range(1, 6)]\n",
    "    with model.hooks(fwd_hooks=ablate_mlp_hook+activate_mlp_hooks+activate_attn_hooks):\n",
    "        ablated_loss, _ = model.run_with_cache(prompts, return_type=\"loss\", loss_per_token=True)\n",
    "        ablated_loss = ablated_loss[:, -1]\n",
    "        #print(f\"Direct effect MLP{layer}: {ablated_loss:.2f} (+{ablated_loss-loss:.2f})\")\n",
    "        losses.append(ablated_loss.cpu().tolist())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>                            <div id=\"fb17bbed-fd60-441c-b248-0915a387cdcf\" class=\"plotly-graph-div\" style=\"height:525px; width:1000px;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"fb17bbed-fd60-441c-b248-0915a387cdcf\")) {                    Plotly.newPlot(                        \"fb17bbed-fd60-441c-b248-0915a387cdcf\",                        [{\"error_y\":{\"array\":[1.22082798044939],\"type\":\"data\",\"visible\":true},\"name\":\"Original\",\"x\":[\"Original\"],\"y\":[1.243345800289535],\"type\":\"bar\"},{\"error_y\":{\"array\":[1.69871392014552],\"type\":\"data\",\"visible\":true},\"name\":\"MLP1\",\"x\":[\"MLP1\"],\"y\":[2.7386141328280793],\"type\":\"bar\"},{\"error_y\":{\"array\":[1.7789931988146106],\"type\":\"data\",\"visible\":true},\"name\":\"MLP2\",\"x\":[\"MLP2\"],\"y\":[2.981179159246385],\"type\":\"bar\"},{\"error_y\":{\"array\":[0.46508542031046884],\"type\":\"data\",\"visible\":true},\"name\":\"MLP3\",\"x\":[\"MLP3\"],\"y\":[0.2965080899680379],\"type\":\"bar\"},{\"error_y\":{\"array\":[1.3414367851657796],\"type\":\"data\",\"visible\":true},\"name\":\"MLP4\",\"x\":[\"MLP4\"],\"y\":[1.4730863459530519],\"type\":\"bar\"},{\"error_y\":{\"array\":[1.3226916405132172],\"type\":\"data\",\"visible\":true},\"name\":\"MLP5\",\"x\":[\"MLP5\"],\"y\":[3.136679318472743],\"type\":\"bar\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"title\":{\"text\":\"Loss when patching ablated L0H7 information to later MLP layers\"},\"xaxis\":{\"title\":{\"text\":\"\"}},\"yaxis\":{\"title\":{\"text\":\"\"}},\"barmode\":\"group\",\"width\":1000,\"showlegend\":false},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('fb17bbed-fd60-441c-b248-0915a387cdcf');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "haystack_utils.plot_barplot(losses, [\"Original\"] + [f\"MLP{layer}\" for layer in range(1, 6)], legend=False, title=f\"Loss when patching ablated L0H7 information to later MLP layers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ablated loss 4.35098123550415\n",
      "tensor(5.1810, device='cuda:0')\n",
      "Patched loss: 3.21 (+1.97)\n"
     ]
    }
   ],
   "source": [
    "# CURSED PATCHING FOR DIRECT EFFECTS\n",
    "\n",
    "# mean ablate everything except previous token and mlp1+2+5\n",
    "# Be sure they are independ trigram tables\n",
    "model.set_use_attn_result(True)\n",
    "original_loss, original_cache = model.run_with_cache(prompts, return_type=\"loss\", loss_per_token=True)\n",
    "_, random_cache = model.run_with_cache(random_prompts)\n",
    "def mean_ablate_component_hook(value, hook):\n",
    "    value[:, :-3] = random_cache[hook.name].mean((0, 1))\n",
    "    return value\n",
    "\n",
    "# Get ablated cache '\n",
    "with model.hooks(fwd_hooks=ablate_head_hook):\n",
    "    ablated_loss, ablated_cache = model.run_with_cache(prompts, return_type=\"loss\", loss_per_token=True)\n",
    "    print(\"Ablated loss\", ablated_loss[:, -1].mean().item())\n",
    "\n",
    "mean_ablate_everything_hooks = [(f\"blocks.{layer}.mlp.hook_post\", mean_ablate_component_hook) for layer in range(0, 6)] +  [(f\"blocks.{layer}.attn.hook_z\", mean_ablate_component_hook) for layer in range(0, 6)]\n",
    "\n",
    "# Activate L0H7\n",
    "def activate_head_7_hook(value, hook):\n",
    "    value[:, -2, 7, :] = original_cache[hook.name][:, -2, 7, :] \n",
    "    return value\n",
    "head_7_hook = [(f\"blocks.0.attn.hook_result\", activate_head_7_hook)]\n",
    "\n",
    "def ablate_component_hook(value, hook):\n",
    "    value[:, -2] = ablated_cache[hook.name][:, -2]\n",
    "    return value\n",
    "\n",
    "# Semi clean original cache\n",
    "# Leave L0H7 active, ablate all later components\n",
    "ablate_all_hook = [(f\"blocks.{i}.attn.hook_z\", ablate_component_hook) for i in range(0, 6)] + [(f\"blocks.{i}.mlp.hook_post\", ablate_component_hook) for i in range(0, 6)]\n",
    "with model.hooks(fwd_hooks=ablate_all_hook+head_7_hook):\n",
    "    half_ablated_loss, half_ablated_cache = model.run_with_cache(prompts, return_type=\"loss\", loss_per_token=True)\n",
    "    print(half_ablated_loss[:, -1].mean())\n",
    "\n",
    "def half_activate_component_hook(value, hook):\n",
    "    value[:, -2] = half_ablated_cache[hook.name][:, -2]\n",
    "    return value\n",
    "\n",
    "\n",
    "loss = original_loss[:, -1].mean()\n",
    "\n",
    "patch_important = [(f\"blocks.{layer}.mlp.hook_pre\", half_activate_component_hook) for layer in [1, 2, 5]] #Grab value before post hooks\n",
    "ablate_mlp_hook = [(f\"blocks.{layer}.mlp.hook_post\", ablate_component_hook) for layer in [0, 3, 4]]\n",
    "ablate_attn_hooks = [(f\"blocks.{layer}.attn.hook_z\", ablate_component_hook) for layer in range(0, 6)]\n",
    "with model.hooks(fwd_hooks=ablate_mlp_hook+ablate_attn_hooks+mean_ablate_everything_hooks+patch_important+head_7_hook):\n",
    "    ablated_loss, _ = model.run_with_cache(prompts, return_type=\"loss\", loss_per_token=True)\n",
    "    ablated_loss = ablated_loss[:, -1].mean()\n",
    "    print(f\"Patched loss: {ablated_loss:.2f} (+{ablated_loss-loss:.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ActivationCache with keys ['hook_embed', 'blocks.0.hook_resid_pre', 'blocks.0.ln1.hook_scale', 'blocks.0.ln1.hook_normalized', 'blocks.0.attn.hook_q', 'blocks.0.attn.hook_k', 'blocks.0.attn.hook_v', 'blocks.0.attn.hook_rot_q', 'blocks.0.attn.hook_rot_k', 'blocks.0.attn.hook_attn_scores', 'blocks.0.attn.hook_pattern', 'blocks.0.attn.hook_z', 'blocks.0.hook_attn_out', 'blocks.0.ln2.hook_scale', 'blocks.0.ln2.hook_normalized', 'blocks.0.mlp.hook_pre', 'blocks.0.mlp.hook_post', 'blocks.0.hook_mlp_out', 'blocks.0.hook_resid_post', 'blocks.1.hook_resid_pre', 'blocks.1.ln1.hook_scale', 'blocks.1.ln1.hook_normalized', 'blocks.1.attn.hook_q', 'blocks.1.attn.hook_k', 'blocks.1.attn.hook_v', 'blocks.1.attn.hook_rot_q', 'blocks.1.attn.hook_rot_k', 'blocks.1.attn.hook_attn_scores', 'blocks.1.attn.hook_pattern', 'blocks.1.attn.hook_z', 'blocks.1.hook_attn_out', 'blocks.1.ln2.hook_scale', 'blocks.1.ln2.hook_normalized', 'blocks.1.mlp.hook_pre', 'blocks.1.mlp.hook_post', 'blocks.1.hook_mlp_out', 'blocks.1.hook_resid_post', 'blocks.2.hook_resid_pre', 'blocks.2.ln1.hook_scale', 'blocks.2.ln1.hook_normalized', 'blocks.2.attn.hook_q', 'blocks.2.attn.hook_k', 'blocks.2.attn.hook_v', 'blocks.2.attn.hook_rot_q', 'blocks.2.attn.hook_rot_k', 'blocks.2.attn.hook_attn_scores', 'blocks.2.attn.hook_pattern', 'blocks.2.attn.hook_z', 'blocks.2.hook_attn_out', 'blocks.2.ln2.hook_scale', 'blocks.2.ln2.hook_normalized', 'blocks.2.mlp.hook_pre', 'blocks.2.mlp.hook_post', 'blocks.2.hook_mlp_out', 'blocks.2.hook_resid_post', 'blocks.3.hook_resid_pre', 'blocks.3.ln1.hook_scale', 'blocks.3.ln1.hook_normalized', 'blocks.3.attn.hook_q', 'blocks.3.attn.hook_k', 'blocks.3.attn.hook_v', 'blocks.3.attn.hook_rot_q', 'blocks.3.attn.hook_rot_k', 'blocks.3.attn.hook_attn_scores', 'blocks.3.attn.hook_pattern', 'blocks.3.attn.hook_z', 'blocks.3.hook_attn_out', 'blocks.3.ln2.hook_scale', 'blocks.3.ln2.hook_normalized', 'blocks.3.mlp.hook_pre', 'blocks.3.mlp.hook_post', 'blocks.3.hook_mlp_out', 'blocks.3.hook_resid_post', 'blocks.4.hook_resid_pre', 'blocks.4.ln1.hook_scale', 'blocks.4.ln1.hook_normalized', 'blocks.4.attn.hook_q', 'blocks.4.attn.hook_k', 'blocks.4.attn.hook_v', 'blocks.4.attn.hook_rot_q', 'blocks.4.attn.hook_rot_k', 'blocks.4.attn.hook_attn_scores', 'blocks.4.attn.hook_pattern', 'blocks.4.attn.hook_z', 'blocks.4.hook_attn_out', 'blocks.4.ln2.hook_scale', 'blocks.4.ln2.hook_normalized', 'blocks.4.mlp.hook_pre', 'blocks.4.mlp.hook_post', 'blocks.4.hook_mlp_out', 'blocks.4.hook_resid_post', 'blocks.5.hook_resid_pre', 'blocks.5.ln1.hook_scale', 'blocks.5.ln1.hook_normalized', 'blocks.5.attn.hook_q', 'blocks.5.attn.hook_k', 'blocks.5.attn.hook_v', 'blocks.5.attn.hook_rot_q', 'blocks.5.attn.hook_rot_k', 'blocks.5.attn.hook_attn_scores', 'blocks.5.attn.hook_pattern', 'blocks.5.attn.hook_z', 'blocks.5.hook_attn_out', 'blocks.5.ln2.hook_scale', 'blocks.5.ln2.hook_normalized', 'blocks.5.mlp.hook_pre', 'blocks.5.mlp.hook_post', 'blocks.5.hook_mlp_out', 'blocks.5.hook_resid_post', 'ln_final.hook_scale', 'ln_final.hook_normalized']"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 5.18 (+3.94)\n"
     ]
    }
   ],
   "source": [
    "original_loss, _ = model.run_with_cache(prompts, return_type=\"loss\", loss_per_token=True)\n",
    "loss = original_loss[:, -1].mean()\n",
    "\n",
    "ablate_mlp_hook = [(f\"blocks.{layer}.mlp.hook_post\", ablate_component_hook) for layer in [0, 1, 2, 3, 4, 5]]\n",
    "ablate_attn_hooks = [(f\"blocks.{layer}.hook_attn_out\", ablate_component_hook) for layer in range(1, 6)]\n",
    "with model.hooks(fwd_hooks=ablate_mlp_hook+ablate_attn_hooks+mean_ablate_everything_hooks):\n",
    "    ablated_loss, _ = model.run_with_cache(prompts, return_type=\"loss\", loss_per_token=True)\n",
    "    ablated_loss = ablated_loss[:, -1].mean()\n",
    "    print(f\"Loss: {ablated_loss:.2f} (+{ablated_loss-loss:.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 4.56 (+3.32)\n"
     ]
    }
   ],
   "source": [
    "original_loss, _ = model.run_with_cache(prompts, return_type=\"loss\", loss_per_token=True)\n",
    "loss = original_loss[:, -1].mean()\n",
    "\n",
    "ablate_mlp_hook = [(f\"blocks.{layer}.mlp.hook_post\", ablate_component_hook) for layer in [0, 1, 3, 4, 5]]\n",
    "ablate_attn_hooks = [(f\"blocks.{layer}.hook_attn_out\", ablate_component_hook) for layer in range(1, 6)]\n",
    "with model.hooks(fwd_hooks=ablate_mlp_hook+ablate_attn_hooks+mean_ablate_everything_hooks):\n",
    "    ablated_loss, _ = model.run_with_cache(prompts, return_type=\"loss\", loss_per_token=True)\n",
    "    ablated_loss = ablated_loss[:, -1].mean()\n",
    "    print(f\"Loss: {ablated_loss:.2f} (+{ablated_loss-loss:.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 3.35 (+2.10)\n"
     ]
    }
   ],
   "source": [
    "original_loss, _ = model.run_with_cache(prompts, return_type=\"loss\", loss_per_token=True)\n",
    "loss = original_loss[:, -1].mean()\n",
    "\n",
    "ablate_mlp_hook = [(f\"blocks.{layer}.mlp.hook_post\", ablate_component_hook) for layer in [0, 2, 3, 4, 5]]\n",
    "ablate_attn_hooks = [(f\"blocks.{layer}.hook_attn_out\", ablate_component_hook) for layer in range(1, 6)]\n",
    "with model.hooks(fwd_hooks=ablate_mlp_hook+ablate_attn_hooks+mean_ablate_everything_hooks):\n",
    "    ablated_loss, _ = model.run_with_cache(prompts, return_type=\"loss\", loss_per_token=True)\n",
    "    ablated_loss = ablated_loss[:, -1].mean()\n",
    "    print(f\"Loss: {ablated_loss:.2f} (+{ablated_loss-loss:.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "_, random_cache = model.run_with_cache(random_prompts)\n",
    "def mean_ablate_component_hook(value, hook):\n",
    "    value[:, :-3] = random_cache[hook.name].mean((0, 1))\n",
    "    return value\n",
    "\n",
    "mean_ablate_everything_hooks = [(f\"blocks.{layer}.mlp.hook_post\", mean_ablate_component_hook) for layer in range(0, 6)] +  [(f\"blocks.{layer}.hook_attn_out\", mean_ablate_component_hook) for layer in range(0, 6)]\n",
    "\n",
    "\n",
    "test_prompts = haystack_utils.generate_random_prompts(\"orschlägen\", model, common_tokens, 200, length=20)\n",
    "\n",
    "original_loss, _ = model.run_with_cache(test_prompts, return_type=\"loss\", loss_per_token=True)\n",
    "loss = original_loss[:, -1].mean()\n",
    "\n",
    "ablate_mlp_hook = [(f\"blocks.{layer}.mlp.hook_post\", ablate_component_hook) for layer in [0, 1, 2, 3, 4]]\n",
    "ablate_attn_hooks = [(f\"blocks.{layer}.hook_attn_out\", ablate_component_hook) for layer in range(1, 6)]\n",
    "with model.hooks(fwd_hooks=ablate_mlp_hook+ablate_attn_hooks+mean_ablate_everything_hooks):\n",
    "    ablated_loss, _ = model.run_with_cache(test_prompts, return_type=\"loss\", loss_per_token=True)\n",
    "    ablated_loss = ablated_loss[:, -1].mean()\n",
    "    print(f\"Loss: {ablated_loss:.2f} (+{ablated_loss-loss:.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total effect Attn1: 1.00 (+-0.24)\n",
      "Direct effect Attn1: 1.00 (+-0.24)\n",
      "Total effect Attn2: 1.03 (+-0.22)\n",
      "Direct effect Attn2: 1.28 (+0.03)\n",
      "Total effect Attn3: 1.10 (+-0.15)\n",
      "Direct effect Attn3: 1.28 (+0.04)\n",
      "Total effect Attn4: 1.24 (+-0.00)\n",
      "Direct effect Attn4: 1.24 (+-0.00)\n",
      "Total effect Attn5: 1.26 (+0.02)\n",
      "Direct effect Attn5: 1.26 (+0.02)\n"
     ]
    }
   ],
   "source": [
    "for layer in range(1, 6):\n",
    "    component = f\"blocks.{layer}.hook_attn_out\"\n",
    "    ablate_attn_hook = [(component, ablate_component_hook)]\n",
    "    with model.hooks(fwd_hooks=ablate_attn_hook):\n",
    "        ablated_loss, _ = model.run_with_cache(prompts, return_type=\"loss\", loss_per_token=True)\n",
    "        ablated_loss = ablated_loss[:, -1].mean()\n",
    "        print(f\"Total effect Attn{layer}: {ablated_loss:.2f} (+{ablated_loss-loss:.2f})\")\n",
    "    \n",
    "    activate_layers = [i for i in range(1, 6) if i != layer]\n",
    "    activate_mlp_hooks = [(f\"blocks.{i}.mlp.hook_post\", activate_component_hook) for i in range(1, 6)]\n",
    "    activate_attn_hooks = [(f\"blocks.{i}.hook_attn_out\", activate_component_hook) for i in activate_layers]\n",
    "    with model.hooks(fwd_hooks=ablate_attn_hook+activate_mlp_hooks+activate_attn_hooks):\n",
    "        ablated_loss, _ = model.run_with_cache(prompts, return_type=\"loss\", loss_per_token=True)\n",
    "        ablated_loss = ablated_loss[:, -1].mean()\n",
    "        print(f\"Direct effect Attn{layer}: {ablated_loss:.2f} (+{ablated_loss-loss:.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.2433, device='cuda:0') tensor(0.6377, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "def get_ablate_neurons_hook(neurons):\n",
    "    def ablate_neurons_hook(value, hook):\n",
    "        value[:, -2, neurons] = ablated_cache[hook.name][:, -2, neurons]\n",
    "        return value\n",
    "    return ablate_neurons_hook\n",
    "loss, _ = model.run_with_cache(prompts, return_type=\"loss\", loss_per_token=True)\n",
    "loss = loss[:, -1].mean()\n",
    "\n",
    "\n",
    "component = f\"blocks.1.mlp.hook_post\"\n",
    "neurons = torch.LongTensor([32, 163, 472, 493, 544, 726, 733, 959, 1019, 1089, 1190, 1203, 1227, 1280, 1287, 1448, 1502, 1782, 1794, 1899, 1980, 1985])\n",
    "ablate_neurons_hook = [(component, get_ablate_neurons_hook(neurons))]\n",
    "\n",
    "ablate_head_hook = [get_ablate_attention_hook(layer=0, head=7, mean_activation=mean_attention_activations, pos=-2)]\n",
    "with model.hooks(fwd_hooks=ablate_neurons_hook):\n",
    "    ablated_loss, _ = model.run_with_cache(prompts, return_type=\"loss\", loss_per_token=True)\n",
    "    ablated_loss = ablated_loss[:, -1].mean()\n",
    "\n",
    "print(loss, ablated_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLP1 MLP2 direct effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1\n",
      "[3.295524835586548, 2.1306657791137695]\n",
      "['gen', 'ge']\n",
      "Layer 2\n",
      "[2.3385634422302246, 2.1701858043670654, 0.8951563835144043]\n",
      "['ges', 'gen', 'ge']\n",
      "Layer 3\n",
      "[2.1852922439575195, 1.3046529293060303, 1.0308555364608765]\n",
      "['ges', 'ge', 'gs']\n",
      "Layer 4\n",
      "[5.201571941375732, 2.4203109741210938, 2.1195180416107178, 1.4416520595550537]\n",
      "['gar', 'g', 'ges', 'gs']\n",
      "Layer 5\n",
      "[2.6164331436157227, 2.348926067352295, 0.8594949245452881, 0.13929252326488495]\n",
      "['ges', 'gs', 'ge', 'gen']\n"
     ]
    }
   ],
   "source": [
    "# Get ablated cache \n",
    "with model.hooks(fwd_hooks=ablate_head_hook+deactivate_neurons_fwd_hooks):\n",
    "    _, ablated_cache = model.run_with_cache(prompts, return_type=\"loss\", loss_per_token=True)\n",
    "\n",
    "with model.hooks(fwd_hooks=deactivate_neurons_fwd_hooks):\n",
    "    original_logits, original_cache = model.run_with_cache(prompts, return_type=\"logits\", loss_per_token=True)\n",
    "\n",
    "def to_logprobs(logits, pos=-2):\n",
    "    return logits.log_softmax(dim=-1)[:, pos]\n",
    "\n",
    "original_logprobs = to_logprobs(original_logits)\n",
    "\n",
    "def ablate_component_hook(value, hook):\n",
    "    value[:, -2] = ablated_cache[hook.name][:, -2]\n",
    "    return value\n",
    "\n",
    "def activate_component_hook(value, hook):\n",
    "    value[:, -2] = original_cache[hook.name][:, -2]\n",
    "    return value\n",
    "\n",
    "for layer in range(1, 6):\n",
    "    component = f\"blocks.{layer}.mlp.hook_post\"\n",
    "    ablate_mlp_hook = [(component, ablate_component_hook)]\n",
    "    activate_layers = [i for i in range(1, 6) if i != layer]\n",
    "    activate_mlp_hooks = [(f\"blocks.{i}.mlp.hook_post\", activate_component_hook) for i in activate_layers]\n",
    "    activate_attn_hooks = [(f\"blocks.{i}.hook_attn_out\", activate_component_hook) for i in range(1, 6)]\n",
    "    with model.hooks(fwd_hooks=ablate_mlp_hook+activate_mlp_hooks+activate_attn_hooks+deactivate_neurons_fwd_hooks):\n",
    "        ablated_logits, _ = model.run_with_cache(prompts, return_type=\"logits\", loss_per_token=True)\n",
    "    ablated_logprobs = to_logprobs(ablated_logits)\n",
    "    diffs = (original_logprobs - ablated_logprobs).mean(0)\n",
    "    diffs[all_ignore] = 0\n",
    "    diffs[original_logprobs.mean(0) < -7] = 0\n",
    "\n",
    "    sorted_diffs, sorted_indices = diffs.sort(descending=True)\n",
    "    print(\"Layer\", layer)\n",
    "    print(sorted_diffs[:10][sorted_diffs[:10]>0].tolist())\n",
    "    print(model.to_str_tokens(sorted_indices[:10][sorted_diffs[:10]>0]))\n",
    "    #print(sorted_diffs[-10:], model.to_str_tokens(sorted_indices[-10:]))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
