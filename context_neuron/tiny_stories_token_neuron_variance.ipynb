{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from transformer_lens import HookedTransformer\n",
    "from jaxtyping import Float, Int, Bool\n",
    "from torch import Tensor\n",
    "from tqdm.auto import tqdm\n",
    "import plotly.io as pio\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px \n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from IPython.display import display, HTML\n",
    "from datasets import load_dataset\n",
    "from collections import Counter\n",
    "import pickle\n",
    "import os\n",
    "import plotly.graph_objects as go\n",
    "from scipy.stats import norm, variation, skew, kurtosis\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import json\n",
    "from tqdm import trange\n",
    "\n",
    "import haystack_utils\n",
    "from haystack_utils import get_mlp_activations\n",
    "from hook_utils import get_ablate_neuron_hook, save_activation\n",
    "from pythia_160m_utils import get_neuron_accuracy, ablation_effect\n",
    "import plotting_utils\n",
    "from plotting_utils import plot_neuron_acts, color_binned_histogram\n",
    "\n",
    "pio.renderers.default = \"notebook_connected+notebook\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.autograd.set_grad_enabled(False)\n",
    "torch.set_grad_enabled(False)\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model roneneldan/TinyStories-1M into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "model = HookedTransformer.from_pretrained(\"roneneldan/TinyStories-1M\",\n",
    "    center_unembed=True,\n",
    "    center_writing_weights=True,\n",
    "    fold_ln=True,\n",
    "    device=\"cuda\")\n",
    "\n",
    "with open('data/TinyStories-train.txt', 'r') as f:\n",
    "    full_data = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14815490\n"
     ]
    }
   ],
   "source": [
    "print(len(full_data))\n",
    "prompts = [data[:400] for data in full_data[:200]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7357\n"
     ]
    }
   ],
   "source": [
    "tokens_set = set()\n",
    "for prompt in full_data[:20000]:\n",
    "    tokens = model.to_tokens(prompt).flatten().tolist()\n",
    "    tokens_set.update(tokens)\n",
    "\n",
    "\n",
    "print(len(tokens_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "\n",
    "N = 10_000\n",
    "\n",
    "# Resize the tokenizer vocabulary to the top 10,000 tokens\n",
    "# restricted_vocab = sorted(model.tokenizer.get_vocab().items(), key=operator.itemgetter(1))[:N]\n",
    "# model.tokenizer = {k: v for k, v in restricted_vocab}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_tokens(model, data):\n",
    "    tokens = torch.empty(0, dtype=int).cuda()\n",
    "    for item in data:\n",
    "        tokens = torch.cat([tokens, model.to_tokens(item)[0]], dim=0)\n",
    "    tokens = tokens.flatten()\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_neuron_token_variance(\n",
    "        model: HookedTransformer, data: list[str],\n",
    "        tokens: torch.Tensor | None=None,\n",
    "        disable_tqdm=True, hook_pre=False\n",
    ") -> pd.DataFrame:\n",
    "    '''\n",
    "    Get the variance of the activations of a neuron for each token in the dataset.\n",
    "    This looks like a tensor of tokens and a tensor of activations, and building:\n",
    "      a tensor of token x activation_sum (10000 x d_mlp)\n",
    "      a tensor of token_count\n",
    "      repeat layer by layer (or do it all at once if we have memory)\n",
    "    '''\n",
    "    token_neuron_acts = torch.zeros(model.cfg.n_layers, model.cfg.d_vocab, model.cfg.d_mlp).cuda()\n",
    "    token_neuron_acts_squared = torch.zeros(model.cfg.n_layers, model.cfg.d_vocab, model.cfg.d_mlp).cuda()\n",
    "\n",
    "    if tokens is None:\n",
    "        tokens = get_all_tokens(model, data)\n",
    "    # Repeat the token counts for each neuron so we can do per-neuron operations later\n",
    "    token_counts = torch.bincount(tokens, minlength=model.cfg.d_vocab).unsqueeze(1).repeat(1, model.cfg.d_mlp)\n",
    "\n",
    "    for layer in trange(model.cfg.n_layers):\n",
    "        acts = get_mlp_activations(data, layer, model, mean=False, disable_tqdm=disable_tqdm, hook_pre=hook_pre, \n",
    "                                   context_crop_start=0, context_crop_end=400)\n",
    "        std = torch.std(acts, dim=0)\n",
    "        acts = acts / std\n",
    "        for i in range(tokens.shape[0]):\n",
    "            token_neuron_acts[layer, tokens[i]] += acts[i]\n",
    "            token_neuron_acts_squared[layer, tokens[i]] += acts[i] ** 2\n",
    "\n",
    "        mean_acts = token_neuron_acts[layer] / (token_counts + 1e-8)\n",
    "        mean_acts_squared = token_neuron_acts_squared[layer] / (token_counts + 1e-8)\n",
    "        var_acts = mean_acts_squared - mean_acts ** 2\n",
    "        token_neuron_acts[layer] = var_acts\n",
    "\n",
    "    return token_neuron_acts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [40:37<00:00, 304.72s/it]\n"
     ]
    }
   ],
   "source": [
    "tokens = get_all_tokens(model, full_data[:20000])\n",
    "token_neuron_acts = get_neuron_token_variance(model, full_data[:20000], tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7357\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([3.3559e-03, 4.0625e-03, 1.0918e-03, 1.4053e-03, 7.7228e-03, 6.1107e-03,\n",
       "        5.4795e-03, 6.3961e-03, 5.3644e-06, 1.1842e-03, 1.6332e-03, 2.5883e-03,\n",
       "        5.3342e-03, 6.0445e-02, 7.4473e-03, 5.0735e-03, 5.1242e-03, 2.7890e-03,\n",
       "        2.0046e-03, 2.2921e-03, 2.4658e-02, 2.8625e-03, 3.0652e-04, 3.4750e-04,\n",
       "        3.2605e-03, 4.5653e-03, 1.2042e-03, 3.4957e-03, 3.4612e-03, 3.4347e-02,\n",
       "        1.7056e-02, 5.2084e-04, 2.5506e-03, 2.8339e-03, 9.7350e-03, 1.2945e-03,\n",
       "        1.5241e-03, 2.6124e-03, 1.9694e-02, 4.6659e-03, 1.9001e-03, 2.6565e-03,\n",
       "        3.1988e-03, 6.9871e-03, 1.5029e-02, 5.8228e-04, 8.1278e-03, 2.1648e-04,\n",
       "        1.8340e-04, 2.8374e-03, 8.9211e-03, 1.1770e-02, 5.1433e-04, 4.0309e-02,\n",
       "        2.7327e-03, 1.0933e-02, 2.0526e-02, 1.0729e-03, 1.1268e-03, 8.2975e-03,\n",
       "        8.3030e-04, 2.4622e-03, 9.9301e-05, 5.3285e-03, 8.5162e-03, 1.9978e-03,\n",
       "        1.3846e-03, 9.9334e-04, 8.5476e-04, 6.8040e-03, 2.3564e-03, 4.1972e-03,\n",
       "        2.6683e-03, 3.0035e-02, 2.1154e-02, 1.3825e-02, 1.4499e-03, 2.4363e-02,\n",
       "        2.1446e-04, 1.8734e-04, 2.9038e-03, 4.3035e-05, 2.5976e-04, 5.6851e-04,\n",
       "        1.4305e-05, 1.5610e-02, 7.1020e-04, 1.6999e-04, 6.4930e-04, 5.8069e-03,\n",
       "        4.5405e-03, 3.3398e-03, 1.1384e-05, 7.5521e-03, 8.1864e-04, 8.0113e-03,\n",
       "        2.3817e-03, 2.0387e-02, 1.5182e-03, 7.0643e-04, 1.4674e-03, 4.9446e-03,\n",
       "        5.3704e-03, 1.3281e-02, 2.3489e-03, 1.6595e-03, 1.0167e-02, 6.9779e-04,\n",
       "        1.1492e-02, 1.8059e-02, 1.6297e-03, 3.2854e-04, 1.3359e-03, 4.6247e-03,\n",
       "        1.9221e-03, 1.0428e-03, 7.5220e-03, 5.2236e-02, 5.9145e-03, 8.0183e-04,\n",
       "        6.5001e-03, 4.5036e-03, 1.0260e-02, 5.6717e-03, 5.1232e-03, 3.5362e-02,\n",
       "        9.8071e-03, 1.1819e-03, 8.0104e-03, 1.9118e-03, 1.7492e-02, 2.0809e-03,\n",
       "        4.7230e-03, 1.0249e-03, 1.4432e-03, 1.2177e-04, 4.1182e-03, 1.6475e-04,\n",
       "        2.0852e-03, 3.7390e-03, 1.1683e-05, 1.5737e-03, 1.8317e-03, 2.1999e-03,\n",
       "        5.1008e-03, 1.5967e-03, 4.9798e-03, 3.1171e-03, 2.8763e-02, 9.2278e-03,\n",
       "        5.3773e-03, 3.4274e-03, 1.1284e-02, 1.3984e-03, 5.2619e-04, 1.2294e-02,\n",
       "        2.4522e-03, 1.4085e-03, 6.1467e-04, 1.2759e-03, 1.3183e-03, 3.4766e-03,\n",
       "        2.7915e-03, 2.1228e-03, 1.4651e-03, 2.3166e-03, 5.4581e-03, 1.2185e-03,\n",
       "        7.1517e-04, 1.4136e-03, 1.1407e-02, 9.1400e-03, 5.6908e-03, 1.3284e-03,\n",
       "        1.6934e-04, 8.1811e-03, 1.5396e-03, 7.5898e-03, 1.1218e-02, 6.8967e-03,\n",
       "        1.4187e-02, 1.0411e-02, 2.4921e-03, 3.4128e-03, 2.0328e-03, 9.4433e-03,\n",
       "        9.3127e-03, 5.9696e-03, 2.9379e-03, 1.0721e-03, 9.6076e-03, 3.9880e-03,\n",
       "        1.9467e-03, 6.0886e-03, 9.2395e-03, 6.1989e-06, 9.0659e-04, 4.5180e-03,\n",
       "        3.8765e-02, 2.1108e-03, 1.6207e-03, 5.6705e-03, 3.9428e-03, 4.2462e-03,\n",
       "        5.6779e-04, 2.9016e-02, 2.9600e-03, 2.5925e-03, 8.3615e-03, 2.7487e-03,\n",
       "        8.3787e-03, 2.0733e-03, 1.9416e-03, 3.1879e-03, 1.6779e-02, 4.4820e-03,\n",
       "        4.0728e-03, 2.3068e-03, 1.2058e-03, 7.5944e-03, 8.6824e-03, 3.2967e-03,\n",
       "        6.4540e-04, 4.6509e-03, 2.0538e-03, 3.2781e-03, 1.2405e-02, 3.8868e-04,\n",
       "        1.2369e-03, 9.5773e-03, 8.1345e-04, 1.6624e-02, 4.1898e-03, 3.4668e-03,\n",
       "        6.2943e-05, 1.4121e-03, 4.5166e-03, 2.6000e-04, 2.0913e-03, 1.2195e-03,\n",
       "        2.3139e-04, 2.6172e-03, 3.3593e-03, 2.5323e-02, 2.9616e-03, 1.6094e-02,\n",
       "        1.1182e-03, 6.0808e-03, 4.0812e-03, 1.9557e-03, 1.3894e-03, 1.6336e-03,\n",
       "        1.0904e-02, 4.2903e-04, 1.6724e-03, 1.0841e-02], device='cuda:0')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_token_indices = list(tokens_set)\n",
    "print(len(valid_token_indices))\n",
    "\n",
    "valid_neuron_acts = {}\n",
    "for layer in range(model.cfg.n_layers):\n",
    "    valid_neuron_acts[layer] = token_neuron_acts[layer][valid_token_indices]\n",
    "    # Remove any weird numbers caused by tokens that never appear in the neuron act dataset\n",
    "    valid_neuron_acts[layer][valid_neuron_acts[layer] > 1e10] = 0\n",
    "    valid_neuron_acts[layer][valid_neuron_acts[layer] < -1e10] = 0\n",
    "\n",
    "    with open(f'data/neuron_token_vars_{layer}', 'wb') as f:\n",
    "        pickle.dump(token_neuron_acts[layer], f)    \n",
    "\n",
    "valid_neuron_acts[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'min'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 27\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[39mwith\u001b[39;00m gzip\u001b[39m.\u001b[39mopen(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mdata/tiny_stories/neuron_vars_\u001b[39m\u001b[39m{\u001b[39;00mlayer\u001b[39m}\u001b[39;00m\u001b[39m.pkl.gz\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m     26\u001b[0m     layer_neuron_acts \u001b[39m=\u001b[39m pickle\u001b[39m.\u001b[39mload(f)\n\u001b[0;32m---> 27\u001b[0m     activations, offset, scale \u001b[39m=\u001b[39m quantize_8bit(layer_neuron_acts)\n\u001b[1;32m     28\u001b[0m     pickle\u001b[39m.\u001b[39mdump(f, activations)\n",
      "Cell \u001b[0;32mIn[40], line 7\u001b[0m, in \u001b[0;36mquantize_8bit\u001b[0;34m(input)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mquantize_8bit\u001b[39m(\u001b[39minput\u001b[39m):\n\u001b[0;32m----> 7\u001b[0m     offset \u001b[39m=\u001b[39m \u001b[39minput\u001b[39;49m\u001b[39m.\u001b[39;49mmin(axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mvalues\n\u001b[1;32m      8\u001b[0m     scale \u001b[39m=\u001b[39m (\u001b[39minput\u001b[39m\u001b[39m.\u001b[39mmax(axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mvalues \u001b[39m-\u001b[39m offset) \u001b[39m/\u001b[39m \u001b[39m255\u001b[39m\n\u001b[1;32m      9\u001b[0m     quant \u001b[39m=\u001b[39m ((\u001b[39minput\u001b[39m \u001b[39m-\u001b[39m offset) \u001b[39m/\u001b[39m scale)\u001b[39m.\u001b[39mfloat()\u001b[39m.\u001b[39mround()\u001b[39m.\u001b[39mclamp(\u001b[39m0\u001b[39m, \u001b[39m255\u001b[39m)\u001b[39m.\u001b[39mto(torch\u001b[39m.\u001b[39muint8)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'min'"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "import pickle\n",
    "import os\n",
    "import torch.quantization\n",
    "\n",
    "def quantize_8bit(input):\n",
    "    offset = input.min(axis=0).values\n",
    "    scale = (input.max(axis=0).values - offset) / 255\n",
    "    quant = ((input - offset) / scale).float().round().clamp(0, 255).to(torch.uint8)\n",
    "    return quant, offset, scale\n",
    "\n",
    "def unquantize_8bit(input, offset, scale):\n",
    "    \"\"\"Unquantize a tensor to a given precision.\n",
    "\n",
    "    Args:\n",
    "        input (torch.Tensor): The tensor to quantize.\n",
    "        precision (int): The number of bits to quantize to.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The quantized tensor.\n",
    "    \"\"\"\n",
    "    return input.to(torch.float16) * scale + offset\n",
    "\n",
    "# for layer in range(model.cfg.n_layers):\n",
    "#     with gzip.open(f'data/tiny_stories/neuron_vars_{layer}.pkl.gz', 'rb') as f:\n",
    "#         layer_neuron_acts = pickle.load(f)\n",
    "#         activations, offset, scale = quantize_8bit(layer_neuron_acts)\n",
    "#         pickle.dump(f, activations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensors = [torch.tensor([1, 2]), torch.tensor([3, 4, 5]), torch.tensor([6])]\n",
    "lengths = [len(tensor) for tensor in tensors]\n",
    "result = torch.empty(sum(lengths), dtype=torch.int64)\n",
    "\n",
    "start = 0\n",
    "for tensor in tensors:\n",
    "    end = start + len(tensor)\n",
    "    result[start:end] = tensor\n",
    "    start = end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# large_acts_df = get_neuron_moments(model, prompts, [])\n",
    "# layer_neuron_tuple = large_acts_df.sort_values('skew', ascending=False).iloc[:1][['layer', 'neuron']].values.tolist()[0]\n",
    "# layer, neuron = layer_neuron_tuple\n",
    "\n",
    "# hook_name = f'blocks.{layer}.mlp.hook_post'\n",
    "# with model.hooks([(hook_name, save_activation)]):\n",
    "#     model(prompts[-1])\n",
    "# acts = model.hook_dict[hook_name].ctx['activation']\n",
    "# haystack_utils.clean_print_strings_as_html(model.to_str_tokens(prompts[-1]), acts[0, :, neuron], max_value=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
