{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model EleutherAI/pythia-160m into HookedTransformer\n",
      "data/german_europarl.json: Loaded 2000 examples with 152 to 2000 characters each.\n",
      "data/english_europarl.json: Loaded 2000 examples with 165 to 2000 characters each.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from transformer_lens import HookedTransformer\n",
    "from tqdm.auto import tqdm\n",
    "import plotly.io as pio\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px \n",
    "import pickle\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import haystack_utils\n",
    "import hook_utils\n",
    "import plotting_utils\n",
    "import probing_utils\n",
    "from probing_utils import get_and_score_new_word_probe\n",
    "from sklearn import preprocessing\n",
    "\n",
    "pio.renderers.default = \"notebook_connected+notebook\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.autograd.set_grad_enabled(False)\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "model = HookedTransformer.from_pretrained(\"EleutherAI/pythia-160m\",\n",
    "    center_unembed=True,\n",
    "    center_writing_weights=True,\n",
    "    fold_ln=True,\n",
    "    device=device)\n",
    "\n",
    "german_data = haystack_utils.load_json_data(\"data/german_europarl.json\")[:200]\n",
    "english_data = haystack_utils.load_json_data(\"data/english_europarl.json\")[:200]\n",
    "\n",
    "LAYER, NEURON = 8, 2994"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            f1       mcc\n",
      "1426  0.859275  0.742336\n",
      "149   0.795104  0.641232\n",
      "2994  0.817568  0.637815\n",
      "1815  0.803144  0.625381\n",
      "1080  0.821160  0.624540\n",
      "626   0.760399  0.622951\n",
      "2288  0.789410  0.621133\n",
      "2636  0.811525  0.614484\n",
      "1404  0.816850  0.613347\n",
      "2830  0.775984  0.608591\n",
      "N1426_N2994 0.8784987612671973 0.7724234688752558\n"
     ]
    }
   ],
   "source": [
    "with open(f'data/pythia_160m/layer_8/single_neurons_df.pkl', 'rb') as f:\n",
    "    one_sparse_probe_scores_df = pickle.load(f)\n",
    "\n",
    "with open(f'data/pythia_160m/layer_8/neuron_and_2994_df.pkl', 'rb') as f:\n",
    "    two_sparse_probe_scores_df = pickle.load(f)\n",
    "\n",
    "top_one_scores = one_sparse_probe_scores_df.sort_values(by='mcc', ascending=False).head(10)\n",
    "# top_one_scores_indices = top_one_scores.index\n",
    "print(top_one_scores.index)\n",
    "\n",
    "# for i in range(model.cfg.d_mlp):\n",
    "#     one_scores = one_sparse_probe_scores_df.loc[i]\n",
    "#     if one_scores['mcc'] > 0.62:\n",
    "#         print(f'N{i}', one_scores['f1'], one_scores['mcc'])\n",
    "for i in range(model.cfg.d_mlp):\n",
    "    two_scores = two_sparse_probe_scores_df.loc[i]\n",
    "    if two_scores['mcc'] > 0.75:\n",
    "        print(f'N{i}_N2994', two_scores['f1'], two_scores['mcc'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
