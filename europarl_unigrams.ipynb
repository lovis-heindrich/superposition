{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch import einsum\n",
    "from tqdm.auto import tqdm\n",
    "import seaborn as sns\n",
    "from transformer_lens import HookedTransformer, ActivationCache, utils\n",
    "from datasets import load_dataset\n",
    "from einops import einsum\n",
    "import pandas as pd\n",
    "from transformer_lens import utils\n",
    "from rich.table import Table, Column\n",
    "from rich import print as rprint\n",
    "from jaxtyping import Float, Int, Bool\n",
    "from torch import Tensor\n",
    "import einops\n",
    "import functools\n",
    "from transformer_lens.hook_points import HookPoint\n",
    "# import circuitsvis\n",
    "from IPython.display import HTML\n",
    "from plotly.express import line\n",
    "import plotly.express as px\n",
    "from tqdm.auto import tqdm\n",
    "import json\n",
    "import gc\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from plotly.subplots import make_subplots\n",
    "# Plotly needs a different renderer for VSCode/Notebooks vs Colab argh\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = \"notebook_connected\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.autograd.set_grad_enabled(False)\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "from haystack_utils import load_txt_data, get_mlp_activations, line, two_histogram\n",
    "import haystack_utils\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pile(stream=False):\n",
    "    dataset = load_dataset(\"EleutherAI/pile\", name=\"europarl\", split=\"train\")\n",
    "    dataset = dataset.shuffle(seed=42)\n",
    "    return dataset\n",
    "\n",
    "def get_random_samples(dataset, language=\"fr\", n=100, min_length=100, max_length=1000):\n",
    "    pbar = tqdm(total=n)\n",
    "    language_data = []\n",
    "    for i, example in enumerate(dataset):\n",
    "        sentence_language = example[\"meta\"][-4:-2]\n",
    "        sentence = example[\"text\"]\n",
    "        if (len(sentence) >= min_length):\n",
    "            if (len(sentence) > max_length):\n",
    "                sentence = sentence[:max_length]\n",
    "            if (len(language_data) < n) and (sentence_language==language):\n",
    "                language_data.append(sentence)\n",
    "                pbar.update(1)\n",
    "        if (len(language_data) >= n):\n",
    "            pbar.close()\n",
    "            return language_data\n",
    "    print(f\"Warning: not enough data found, returning {len(language_data)} samples\")\n",
    "    pbar.close()\n",
    "    return language_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d7065d6e1c7462b94a965857aa9b280",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a65797a290a4a3da24b32b7b7556350",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset pile/europarl to /root/.cache/huggingface/datasets/EleutherAI___pile/europarl/0.0.0/ebea56d358e91cf4d37b0fde361d563bed1472fbd8221a21b38fc8bb4ba554fb...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36bcffe7a93b492c8a22470ae5e1ce22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97b98c40e59547408a69227d2d81e3fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/1.48G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1bce8df938c4f6392628c787d5e2dc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset pile downloaded and prepared to /root/.cache/huggingface/datasets/EleutherAI___pile/europarl/0.0.0/ebea56d358e91cf4d37b0fde361d563bed1472fbd8221a21b38fc8bb4ba554fb. Subsequent calls will reuse this data.\n"
     ]
    }
   ],
   "source": [
    "dataset = load_pile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9447e6b2be2a4ce69f7798858787e9de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "german_europarl = get_random_samples(dataset, language=\"de\", n=2000, min_length=100, max_length=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fa099ca5e2c4d1fa15f0d7be475f830",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "english_europarl = get_random_samples(dataset, language=\"en\", n=2000, min_length=100, max_length=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model EleutherAI/pythia-70m into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "model = HookedTransformer.from_pretrained(\"EleutherAI/pythia-70m\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_token_occurrences(prompts: list[str]):\n",
    "    token_counts = torch.zeros(model.cfg.d_vocab).to(device)\n",
    "    for prompt in tqdm(prompts):\n",
    "        # Remove BOS\n",
    "        tokens = model.to_tokens(prompt).flatten()[1:]\n",
    "        token_counts[tokens] += 1\n",
    "    return token_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42f7c2f4542b4deca2b4358b1db3647e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', '.', ' der', ' die', 'äsident', ' Pr', 'en', ' und', ',', 'ung', 'ä', 'st', 'ch', ' den', ' in', 're', 't', ' des', ' zu', ' für', 'z', ' von', 'ischen', 'n', 'ü', 'ge', 'gen', ' auf', ')', ' ist', 'icht', ' über', 'men', 'te', ' er', 'in', 'ig', 'g', ' im', 'es', 'f', '-', ' das', 'Der', ' eine', 'le', ' w', 'ten', ' an', 'ß', ' (', ' dass', ' ein', 'ren', 'hen', 'e', ' dem', 'w', 's', ' mit', ' dies', ' ', ' Europ', ' wir', ' z', ' W', 'ungen', ' ich', 'it', ' Z', ' Herr', 'h', ' nicht', '!', ' zur', ' B', 'ich', ' ver', ' -', ' es', 'lich', 'chte', ' Ver', 'i', ' An', 'igen', 'hr', ' mö', 'l', 'et', ' um', 'em', 'heit', ' V', ' werden', 'u', ' g', ' d', ' be', 'b']\n"
     ]
    }
   ],
   "source": [
    "german_unigram_counts = count_token_occurrences(german_europarl)\n",
    "german_unigram_highest_counts, german_unigram_tokens = torch.topk(german_unigram_counts, 100)\n",
    "german_unigram_labels = model.to_str_tokens(german_unigram_tokens)\n",
    "\n",
    "print(german_unigram_labels[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38ad637565b34f5f876b97db4caf60e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' the', '\\n', '.', ',', ' of', ' to', ' and', ' in', ' on', ' a', ' is', ' that', ' for', ' President', ' this', ' I', ')', 'President', '-', ' have', ' by', ' be', ' (', ' European', ' with', ' we', 'The', ' are', ' it', ' as', ' not', ' Mr', ' which', ' -', ' has', ' will', ' ', ' Parliament', ' an', ' next', ' would', ' item', ' at', ' The', ' all', ' been', \"'s\", ' Commission', ' was', ':', ' from', ' like', ' our', ' you', ' also', ' but', ' should', ' report', ' behalf', '(', ' very', ' Council', 'I', ' Committee', ' Union', ' its', ' there', ' my', 'ate', ' We', ' their', 'Mr', ' vote', ' am', ' It', ' can', 'deb', ' debate', ' one', ' who', ' so', ' This', ' or', ' time', '/', ' do', ' Member', ' gentlemen', ' more', ' us', ' these', ' other', ' about', ' now', \"'\", ' were', ' important', ' States', ' first', ' must']\n"
     ]
    }
   ],
   "source": [
    "english_unigram_counts = count_token_occurrences(english_europarl)\n",
    "english_unigram_highest_counts, english_unigram_tokens = torch.topk(english_unigram_counts, 100)\n",
    "english_unigram_labels = model.to_str_tokens(english_unigram_tokens)\n",
    "\n",
    "print(english_unigram_labels[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Todo \n",
    "# Take top X unigrams\n",
    "# Remove unigrams present in both English and German top\n",
    "# Store unigrams, compare ablation scores"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
