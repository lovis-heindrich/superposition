{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_lens import HookedTransformer\n",
    "import torch\n",
    "import einops\n",
    "from transformer_lens import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model attn-only-2l into HookedTransformer\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "torch.set_grad_enabled(False)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = HookedTransformer.from_pretrained(\"attn-only-2l\", device=device, fold_ln=True)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bigram_logits(tokens, model: HookedTransformer, add_bias=False):\n",
    "    embed = model.W_E[tokens, :]\n",
    "    unembed = einops.einsum(embed, model.W_U, \"batch pos d_model, d_model d_vocab_out -> batch pos d_vocab_out\")\n",
    "    if add_bias:\n",
    "        unembed += model.b_U\n",
    "    return unembed\n",
    "\n",
    "def get_topk_words(logits, model, pos=-1, k=10):\n",
    "    # Input with batch dim = 1\n",
    "    topk, topk_indices = torch.topk(logits[0, pos], k=k)\n",
    "    tokens = model.tokenizer.convert_ids_to_tokens(topk_indices)\n",
    "    return tokens\n",
    "\n",
    "def get_log_probs(logits, tokens):\n",
    "    log_probs = logits.log_softmax(dim=-1)\n",
    "    log_probs_for_tokens = log_probs[:, :-1].gather(dim=-1, index=tokens[:, 1:].unsqueeze(-1)).squeeze(-1)\n",
    "    return log_probs_for_tokens"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigram and embedding analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 15, 48262])\n"
     ]
    }
   ],
   "source": [
    "text = \"Social security is a government program that produces exuberant daisies\"\n",
    "tokens = model.to_tokens(text)\n",
    "logits, cache = model.run_with_cache(tokens)\n",
    "print(logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". Media is a great- that is a-ance andemonies.\n"
     ]
    }
   ],
   "source": [
    "predicted_tokens = logits[0].argmax(dim=-1)\n",
    "predicted_text = model.to_string(predicted_tokens)\n",
    "print(predicted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10967, device='cuda:0')  Media\n",
      "tensor(3860, device='cuda:0')  security\n"
     ]
    }
   ],
   "source": [
    "media_token = predicted_tokens[1]\n",
    "security_token = tokens[0, 2]\n",
    "\n",
    "print(media_token, model.to_string(media_token))\n",
    "print(security_token, model.to_string(security_token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 1, 15, 512])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "head_results = cache.stack_head_results(apply_ln=True)\n",
    "head_results.shape # head batch pos res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram most likely tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Ġand', ',', 'Ġthe', 'Ċ', '.', 'Ġa', 'Ġin', ')', 'Ġto', 'Ġnot']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Unigram most likely tokens\")\n",
    "get_topk_words(model.b_U.view(1, 1, -1), model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model loss: 5.388212\n",
      "Ablated loss: 7.431926\n",
      "Bigram loss 9.814844131469727\n"
     ]
    }
   ],
   "source": [
    "def ablate_mlp_hook(value, hook):\n",
    "    return torch.zeros_like(value)\n",
    "\n",
    "layer_to_ablate = 0\n",
    "original_losses = []\n",
    "ablation_losses = []\n",
    "\n",
    "original_loss = model(tokens, return_type=\"loss\")\n",
    "\n",
    "ablated_loss = model.run_with_hooks(\n",
    "        tokens, \n",
    "        return_type=\"loss\", \n",
    "        fwd_hooks=[(\n",
    "            f\"blocks.0.hook_attn_out\", \n",
    "            ablate_mlp_hook\n",
    "            )]\n",
    "        )\n",
    "\n",
    "bigram_pred = get_bigram_logits(tokens, model)\n",
    "bigram_loss = - get_log_probs(bigram_pred, tokens).mean()\n",
    "\n",
    "print(f\"Original model loss: {original_loss:.6f}\")\n",
    "print(f\"Ablated loss: {ablated_loss:.6f}\")\n",
    "print(f\"Bigram loss {bigram_loss}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Head attribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Result of individual heads, shape=(head, batch, pos, d_model)\n",
    "head_results = cache.stack_head_results(apply_ln=True)\n",
    "\n",
    "# Directions of output logits in the residual stream, shape=(batch, pos, d_model)\n",
    "model_tokens = logits.argmax(dim=-1)\n",
    "directions = model.tokens_to_residual_directions(model_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ -2.2576,  -4.5832,  -3.0620,   2.2735,   2.3002,   9.1988,  -2.8497,\n",
      "         -1.3467,  -1.7043,   1.4677,  19.0915,   5.8268,  13.0674,   4.6253,\n",
      "          8.9530, -35.5903], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Dot product between the head activations and the output directions \n",
    "head_attribution = einops.einsum(head_results, directions, \"head batch pos d_model, batch pos d_model -> head\")\n",
    "print(head_attribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0223, -0.0876, -0.0189, -0.0100,  0.0426,  0.0146, -0.0363,  0.0008,\n",
      "        -0.0137,  0.1088, -0.0167,  0.1116,  0.0736, -0.0306,  0.0404, -0.0601],\n",
      "       device='cuda:0')\n",
      "tensor(0.2165, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "cos = torch.nn.CosineSimilarity(dim=-1)\n",
    "# Cosine similarities between head outputs and directions, shape=(d_head, pos)\n",
    "similarities = cos(directions[0], head_results[:, 0])\n",
    "# Head similarity for \"security\" token\n",
    "print(similarities[:, 2])\n",
    "print(torch.max(similarities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0244,  0.1912,  0.0667,  0.0734,  0.1852,  0.0781,  0.0873, -0.0318,\n",
      "         0.1332,  0.1174,  0.1448,  0.0312,  0.2665,  0.0455,  0.0561],\n",
      "       device='cuda:0')\n",
      "tensor(0.2665, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Cosine similarity for the embedding\n",
    "embed = model.W_E[tokens, :]\n",
    "similarities = cos(directions[0], embed[0])\n",
    "print(similarities)\n",
    "print(torch.max(similarities))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Induction analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text = \"Harry Potter is great. Harry\"\n",
    "text = \"Leonard Potter is great. Leonard\"\n",
    "tokens = model.to_tokens(text)\n",
    "logits, cache = model.run_with_cache(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Potter direction shape torch.Size([512])\n",
      "Tried to stack head results when they weren't cached. Computing head results now\n",
      "Head result shape torch.Size([16, 1, 8, 512])\n",
      "Cosine similarity shape torch.Size([16, 8])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0334, -0.0452,  0.0025, -0.0315, -0.0109, -0.0741,  0.0194, -0.0182,\n",
       "         0.0218,  0.0739,  0.0436, -0.0171,  0.0950, -0.0613,  0.1445,  0.1164],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute cosine similarity of prediction \" Potter\" to head outputs\n",
    "potter_token = model.to_tokens(\" Potter\", prepend_bos=False)\n",
    "potter_direction = model.tokens_to_residual_directions(potter_token)\n",
    "print(\"Potter direction shape\", potter_direction.shape)\n",
    "\n",
    "head_results = cache.stack_head_results(apply_ln=True)\n",
    "print(\"Head result shape\", head_results.shape) # head batch pos res\n",
    "\n",
    "cos = torch.nn.CosineSimilarity(dim=-1)\n",
    "similarities = cos(potter_direction, head_results[:, 0])\n",
    "print(\"Cosine similarity shape\", similarities.shape) # head position\n",
    "# Similarity of Potter following the last Harry\n",
    "similarities[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ġis',\n",
       " 'ĠCohen',\n",
       " 'ĠBernstein',\n",
       " 'Ġhas',\n",
       " 'ĠPotter',\n",
       " 'ĠNim',\n",
       " 'Ġwas',\n",
       " 'Ġand',\n",
       " ',',\n",
       " 'âĢĻ']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Most likely next words for \"Leonard Potter is great. Leonard\"\n",
    "get_topk_words(logits, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['town', 'sson', 'ounce', 'stown', 'esque', 'nier', 'ĠNim', 'ĠCohen', 'ounced', 'ette']\n"
     ]
    }
   ],
   "source": [
    "bigram_logits = get_bigram_logits(tokens, model)\n",
    "print(get_topk_words(bigram_logits, model, pos=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<|BOS|>', 'Leon', 'ard', ' Potter', ' is', ' great', '.', ' Leonard']\n",
      "Tokenized answer: [' Potter']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"font-weight: bold\">        Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">13.33</span><span style=\"font-weight: bold\"> Prob:  </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4.40</span><span style=\"font-weight: bold\">% Token: | Potter|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m4\u001b[0m\u001b[1m        Logit: \u001b[0m\u001b[1;36m13.33\u001b[0m\u001b[1m Prob:  \u001b[0m\u001b[1;36m4.40\u001b[0m\u001b[1m% Token: | Potter|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 14.11 Prob:  9.62% Token: | is|\n",
      "Top 1th token. Logit: 13.53 Prob:  5.38% Token: | Cohen|\n",
      "Top 2th token. Logit: 13.43 Prob:  4.86% Token: | Bernstein|\n",
      "Top 3th token. Logit: 13.34 Prob:  4.45% Token: | has|\n",
      "Top 4th token. Logit: 13.33 Prob:  4.40% Token: | Potter|\n",
      "Top 5th token. Logit: 12.83 Prob:  2.65% Token: | Nim|\n",
      "Top 6th token. Logit: 12.69 Prob:  2.31% Token: | was|\n",
      "Top 7th token. Logit: 12.64 Prob:  2.21% Token: | and|\n",
      "Top 8th token. Logit: 12.61 Prob:  2.14% Token: |,|\n",
      "Top 9th token. Logit: 12.58 Prob:  2.07% Token: |’|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' Potter'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' Potter'\u001b[0m, \u001b[1;36m4\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "utils.test_prompt(\"Leonard Potter is great. Leonard\", \" Potter\", model, prepend_bos=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
