{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_lens import HookedTransformer\n",
    "import torch\n",
    "import einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "torch.set_grad_enabled(False)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model attn-only-2l into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "model = HookedTransformer.from_pretrained(\"attn-only-2l\", device=device, fold_ln=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 15, 48262])\n"
     ]
    }
   ],
   "source": [
    "text = \"Social security is a government program that produces exuberant daisies\"\n",
    "tokens = model.to_tokens(text)\n",
    "logits, cache = model.run_with_cache(tokens)\n",
    "print(logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". Media is a great- that is a-ance andemonies.\n"
     ]
    }
   ],
   "source": [
    "predicted_tokens = logits[0].argmax(dim=-1)\n",
    "predicted_text = model.to_string(predicted_tokens)\n",
    "print(predicted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10967, device='cuda:0')  Media\n",
      "tensor(3860, device='cuda:0')  security\n"
     ]
    }
   ],
   "source": [
    "media_token = predicted_tokens[1]\n",
    "security_token = tokens[0, 2]\n",
    "\n",
    "print(media_token, model.to_string(media_token))\n",
    "print(security_token, model.to_string(security_token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 1, 15, 512])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "head_results = cache.stack_head_results(apply_ln=True)\n",
    "head_results.shape # head batch pos res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([48262, 48262])\n"
     ]
    }
   ],
   "source": [
    "W_EU = einops.einsum(model.W_E, model.W_U, \"d_vocab_in d_model, d_model d_vocab_out -> d_vocab_in d_vocab_out\")\n",
    "print(W_EU.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.5"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "50000*50000 / 1e9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 15, 48262])\n"
     ]
    }
   ],
   "source": [
    "bigram_predictions = W_EU[tokens, :]\n",
    "print(bigram_predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['oupe',\n",
       " 'aders',\n",
       " 'fully',\n",
       " 'igu',\n",
       " 'ange',\n",
       " 'ools',\n",
       " 'aded',\n",
       " 'Ġdeposit',\n",
       " 'Ġforces',\n",
       " 'iels']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_topk, bigram_topk_indices = torch.topk(bigram_predictions, k=10, dim=-1)\n",
    "model.tokenizer.convert_ids_to_tokens(bigram_topk_indices[0, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[42933, 42706,  6396, 29264,  2335, 18724, 12907, 22803, 15039, 12393],\n",
       "        [  266,   778, 15732,  7887,  1322,   899, 10975, 38135, 24299,   382],\n",
       "        [36407, 15760,  2830, 35365,   904, 22944,  7708, 18292,  5457, 38301],\n",
       "        [ 4546, 37559, 35463,  7595, 47593, 35606,  7283,  5093, 22697, 26466],\n",
       "        [34311, 23979, 46119, 37725, 22440, 13762, 20373,  3221, 26422, 43532],\n",
       "        [25990,  4223, 13037, 16736, 35665, 19246, 10609,  6166,  8081,  1174],\n",
       "        [32625, 32540, 35056,  5214, 15999,  6766,   273, 44044, 17458, 47409],\n",
       "        [ 7492,  5161, 13210,   349,  2082,  2875,   891,  2697,  4325,  3138],\n",
       "        [18367, 31026,  2631, 18750, 43757, 10890, 38699,   739,  2151, 45001],\n",
       "        [28147,  7592, 19694,   738, 43026, 20174, 47058, 35792, 37221, 42023],\n",
       "        [10818,  1048,   527,   592,  2083, 19249,  1915,  5785,   582,  9265],\n",
       "        [ 9767, 33845,  4527, 46829,  2329, 17228, 46828,   627,  2959, 13314],\n",
       "        [14706, 17464, 15770, 23979,  2022, 21955,   929, 24167, 41454, 33784],\n",
       "        [17206, 10598, 33440,  2620, 35970, 38106,  2902, 14325, 41582, 17353],\n",
       "        [ 6294, 25051, 31026, 16707,  2390,  1645,  5951,  4595,  3946, 46245]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_topk_indices[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ġand', ',', 'Ġthe', 'Ċ', '.', 'Ġa', 'Ġin', ')', 'Ġto', 'Ġnot']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topk, topk_indices = torch.topk(model.b_U, k=10)\n",
    "model.tokenizer.convert_ids_to_tokens(topk_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ġand',\n",
       " 'Ġand',\n",
       " 'Ġand',\n",
       " 'Ġthe',\n",
       " 'Ċ',\n",
       " 'Ġand',\n",
       " 'Ġand',\n",
       " 'Ġthe',\n",
       " 'Ġthe',\n",
       " ',',\n",
       " 'Ġand',\n",
       " 'Ġand',\n",
       " 'Ġand',\n",
       " 'Ġand',\n",
       " 'Ġand']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.tokenizer.convert_ids_to_tokens(bigram_predictions[0].argmax(dim=-1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 15, 48262])\n"
     ]
    }
   ],
   "source": [
    "text = \"Social security is a government program that produces exuberant daisies\"\n",
    "tokens = model.to_tokens(text)\n",
    "logits, cache = model.run_with_cache(tokens)\n",
    "print(logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_log_probs(logits, tokens):\n",
    "    log_probs = logits.log_softmax(dim=-1)\n",
    "    log_probs_for_tokens = log_probs[:, :-1].gather(dim=-1, index=tokens[:, 1:].unsqueeze(-1)).squeeze(-1)\n",
    "    return log_probs_for_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model loss: 5.388212\n",
      "Ablated loss: 7.431926\n",
      "Bigram loss 9.814844131469727\n"
     ]
    }
   ],
   "source": [
    "def ablate_mlp_hook(value, hook):\n",
    "    return torch.zeros_like(value)\n",
    "\n",
    "layer_to_ablate = 0\n",
    "original_losses = []\n",
    "ablation_losses = []\n",
    "\n",
    "original_loss = model(tokens, return_type=\"loss\")\n",
    "\n",
    "ablated_loss = model.run_with_hooks(\n",
    "        tokens, \n",
    "        return_type=\"loss\", \n",
    "        fwd_hooks=[(\n",
    "            f\"blocks.0.hook_attn_out\", \n",
    "            ablate_mlp_hook\n",
    "            )]\n",
    "        )\n",
    "\n",
    "bigram_pred = W_EU[tokens, :]\n",
    "bigram_loss = - get_log_probs(bigram_pred, tokens).mean()\n",
    "\n",
    "print(f\"Original model loss: {original_loss:.6f}\")\n",
    "print(f\"Ablated loss: {ablated_loss:.6f}\")\n",
    "print(f\"Bigram loss {bigram_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 15, 48262])\n",
      "Tried to stack head results when they weren't cached. Computing head results now\n",
      "torch.Size([16, 1, 15, 512])\n"
     ]
    }
   ],
   "source": [
    "# Compare head contribution to final logits\n",
    "\n",
    "text = \"Social security is a government program that produces exuberant daisies\"\n",
    "tokens = model.to_tokens(text)\n",
    "logits, cache = model.run_with_cache(tokens)\n",
    "print(logits.shape)\n",
    "\n",
    "head_results = cache.stack_head_results(apply_ln=True)\n",
    "print(head_results.shape) # head batch pos res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ -2.2576,  -4.5832,  -3.0620,   2.2735,   2.3002,   9.1988,  -2.8497,\n",
      "         -1.3467,  -1.7043,   1.4677,  19.0915,   5.8268,  13.0674,   4.6253,\n",
      "          8.9530, -35.5903], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "model_tokens = logits.argmax(dim=-1)\n",
    "directions = model.tokens_to_residual_directions(model_tokens)\n",
    "\n",
    "head_attribution = einops.einsum(head_results, directions, \"head batch pos d_model, batch pos d_model -> head\")\n",
    "print(head_attribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 15, 512])\n",
      "torch.Size([16, 1, 15, 512])\n"
     ]
    }
   ],
   "source": [
    "print(directions.shape)\n",
    "print(head_results.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 15])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([-0.0223, -0.0876, -0.0189, -0.0100,  0.0426,  0.0146, -0.0363,  0.0008,\n",
       "        -0.0137,  0.1088, -0.0167,  0.1116,  0.0736, -0.0306,  0.0404, -0.0601],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos = torch.nn.CosineSimilarity(dim=-1)\n",
    "similarities = cos(directions[0], head_results[:, 0])\n",
    "print(similarities.shape) # head position\n",
    "similarities[:, 2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2165, device='cuda:0')"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([15])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.2665, device='cuda:0')"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed = model.W_E[tokens, :]\n",
    "similarities = cos(directions[0], embed[0])\n",
    "print(similarities.shape)\n",
    "torch.max(similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text = \"Harry Potter is great. Harry\"\n",
    "text = \"Leonard Potter is great. Leonard\"\n",
    "tokens = model.to_tokens(text)\n",
    "logits, cache = model.run_with_cache(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[27754]], device='cuda:0')\n",
      "Potter direction shape torch.Size([512])\n",
      "Tried to stack head results when they weren't cached. Computing head results now\n",
      "Head result shape torch.Size([16, 1, 8, 512])\n",
      "Cosine similarity shape torch.Size([16, 8])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0334, -0.0452,  0.0025, -0.0315, -0.0109, -0.0741,  0.0194, -0.0182,\n",
       "         0.0218,  0.0739,  0.0436, -0.0171,  0.0950, -0.0613,  0.1445,  0.1164],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "potter_token = model.to_tokens(\" Potter\", prepend_bos=False)\n",
    "print(potter_token)\n",
    "potter_direction = model.tokens_to_residual_directions(potter_token)\n",
    "print(\"Potter direction shape\", potter_direction.shape)\n",
    "\n",
    "\n",
    "head_results = cache.stack_head_results(apply_ln=True)\n",
    "print(\"Head result shape\", head_results.shape) # head batch pos res\n",
    "\n",
    "cos = torch.nn.CosineSimilarity(dim=-1)\n",
    "similarities = cos(potter_direction, head_results[:, 0])\n",
    "print(\"Cosine similarity shape\", similarities.shape) # head position\n",
    "similarities[:, -1] # Similarity of Potter following the last Harry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bigram_logits(tokens, model: HookedTransformer, add_bias=False):\n",
    "    embed = model.W_E[tokens, :]\n",
    "    unembed = einops.einsum(embed, model.W_U, \"batch pos d_model, d_model d_vocab_out -> batch pos d_vocab_out\")\n",
    "    if add_bias:\n",
    "        unembed += model.b_U\n",
    "    return unembed\n",
    "\n",
    "def get_topk_words(logits, pos=-1, k=10):\n",
    "    # Input with batch dim = 1\n",
    "    topk, topk_indices = torch.topk(logits[0, pos], k=k)\n",
    "    tokens = model.tokenizer.convert_ids_to_tokens(topk_indices)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ġis',\n",
       " 'ĠCohen',\n",
       " 'ĠBernstein',\n",
       " 'Ġhas',\n",
       " 'ĠPotter',\n",
       " 'ĠNim',\n",
       " 'Ġwas',\n",
       " 'Ġand',\n",
       " ',',\n",
       " 'âĢĻ']"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_topk_words(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['town', 'sson', 'ounce', 'stown', 'esque', 'nier', 'ĠNim', 'ĠCohen', 'ounced', 'ette']\n"
     ]
    }
   ],
   "source": [
    "bigram_logits = get_bigram_logits(tokens, model)\n",
    "print(get_topk_words(bigram_logits, pos=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<|BOS|>', 'Leon', 'ard', ' Potter', ' is', ' great', '.', ' Leonard']\n",
      "Tokenized answer: [' Potter']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"font-weight: bold\">        Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">13.33</span><span style=\"font-weight: bold\"> Prob:  </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4.40</span><span style=\"font-weight: bold\">% Token: | Potter|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m4\u001b[0m\u001b[1m        Logit: \u001b[0m\u001b[1;36m13.33\u001b[0m\u001b[1m Prob:  \u001b[0m\u001b[1;36m4.40\u001b[0m\u001b[1m% Token: | Potter|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 14.11 Prob:  9.62% Token: | is|\n",
      "Top 1th token. Logit: 13.53 Prob:  5.38% Token: | Cohen|\n",
      "Top 2th token. Logit: 13.43 Prob:  4.86% Token: | Bernstein|\n",
      "Top 3th token. Logit: 13.34 Prob:  4.45% Token: | has|\n",
      "Top 4th token. Logit: 13.33 Prob:  4.40% Token: | Potter|\n",
      "Top 5th token. Logit: 12.83 Prob:  2.65% Token: | Nim|\n",
      "Top 6th token. Logit: 12.69 Prob:  2.31% Token: | was|\n",
      "Top 7th token. Logit: 12.64 Prob:  2.21% Token: | and|\n",
      "Top 8th token. Logit: 12.61 Prob:  2.14% Token: |,|\n",
      "Top 9th token. Logit: 12.58 Prob:  2.07% Token: |’|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' Potter'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' Potter'\u001b[0m, \u001b[1;36m4\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformer_lens import utils\n",
    "utils.test_prompt(\"Leonard Potter is great. Leonard\", \" Potter\", model, prepend_bos=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
