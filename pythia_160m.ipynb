{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from transformer_lens import HookedTransformer, ActivationCache, utils, patching\n",
    "from jaxtyping import Float, Int, Bool\n",
    "from torch import Tensor\n",
    "from tqdm.auto import tqdm\n",
    "import plotly.io as pio\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "from torchmetrics.regression import KendallRankCorrCoef, SpearmanCorrCoef\n",
    "from collections import defaultdict\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "pio.renderers.default = \"notebook_connected+notebook\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.autograd.set_grad_enabled(False)\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "from haystack_utils import get_mlp_activations\n",
    "import haystack_utils\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model EleutherAI/pythia-160m into HookedTransformer\n",
      "data/german_europarl.json: Loaded 2000 examples with 152 to 2000 characters each.\n",
      "data/english_europarl.json: Loaded 2000 examples with 165 to 2000 characters each.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54056ac0188e4096954a62db4b98e2b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8779d0cae8ac45638544c68c3cfbd3f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2c219e4c1bb42ba9463681e5752bab1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12ad9d947dc34644adacf474dc34301e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5b0a1a696b6461ebf1c3bc30ebeaba0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cb5884b3c4c49eea06e4ed4b80f1d2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8de990ad82344a2e8429cb756aac7021",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9c1e7822c334cb58cb79d6a57d4b844",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d0fcf90aad8416d8acd0e246b2123fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b78fd1c9ce02451eb761489a31c7c9b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "881d6875672d4179809ddb787a8f642b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7791c0d9d2714dc8b125033d6c05b629",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = HookedTransformer.from_pretrained(\"EleutherAI/pythia-160m\",\n",
    "    center_unembed=True,\n",
    "    center_writing_weights=True,\n",
    "    fold_ln=True,\n",
    "    device=device)\n",
    "\n",
    "german_data = haystack_utils.load_json_data(\"data/german_europarl.json\")[:200]\n",
    "english_data = haystack_utils.load_json_data(\"data/english_europarl.json\")[:200]\n",
    "\n",
    "german_neurons_with_f1 = [\n",
    "    [5, 2649, 1.0],\n",
    "    [8,\t2994, 1.0],\n",
    "    [11, 2911, 0.99],\n",
    "    [10, 1129, 0.97],\n",
    "    [6, 1838, 0.65],\n",
    "    [7, 1594, 0.65],\n",
    "    [11, 1819, 0.61],\n",
    "    [11, 2014, 0.56],\n",
    "    [10, 753, 0.54],\n",
    "    [11, 205, 0.48],\n",
    "]\n",
    "\n",
    "important_german_neurons = defaultdict(list)\n",
    "for layer, neuron, f1 in german_neurons_with_f1:\n",
    "    if f1 > 0.9:\n",
    "        important_german_neurons[layer].append(neuron)\n",
    "\n",
    "english_activations = {}\n",
    "german_activations = {}\n",
    "for layer in [layer for layer, _, _ in german_neurons_with_f1]:\n",
    "    english_activations[layer] = get_mlp_activations(english_data, layer, model, mean=False)\n",
    "    german_activations[layer] = get_mlp_activations(german_data, layer, model, mean=False)\n",
    "\n",
    "mean_context_neuron_acts_active = {}\n",
    "mean_context_neuron_acts_inactive = {}\n",
    "for layer, neurons in important_german_neurons.items():\n",
    "    mean_context_neuron_acts_active[layer] = german_activations[layer][:, neurons].mean()\n",
    "    mean_context_neuron_acts_inactive[layer] = english_activations[layer][:, neurons].mean()\n",
    "\n",
    "def get_deactivate_neurons_hook(layer):\n",
    "    def deactivate_neurons_hook(value, hook):\n",
    "        value[:, :, important_german_neurons[layer]] = mean_context_neuron_acts_inactive[layer]\n",
    "        return value\n",
    "    return deactivate_neurons_hook\n",
    "deactivate_neurons_fwd_hooks=[(f'blocks.{layer}.mlp.hook_post', get_deactivate_neurons_hook(layer)) for layer in important_german_neurons.keys()]\n",
    "\n",
    "def get_activate_neurons_hook(layer):\n",
    "    def activate_neurons_hook(value, hook):\n",
    "        value[:, :, important_german_neurons[layer]] = mean_context_neuron_acts_active[layer]\n",
    "        return value\n",
    "    return activate_neurons_hook\n",
    "activate_neurons_fwd_hooks=[(f'blocks.{layer}.mlp.hook_post', get_activate_neurons_hook(layer)) for layer in important_german_neurons.keys()]\n",
    "\n",
    "all_ignore, not_ignore = haystack_utils.get_weird_tokens(model, plot_norms=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check classification accuracy of German neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_single_neuron_lr(layer, neuron, num_samples=5000):\n",
    "    # Check accuracy of logistic regression\n",
    "    A = torch.concat([german_activations[layer][:num_samples, neuron], english_activations[layer][:num_samples, neuron]]).view(-1, 1).cpu().numpy()\n",
    "    y = torch.concat([torch.ones(num_samples), torch.zeros(num_samples)]).cpu().numpy()\n",
    "    A_train, A_test, y_train, y_test = train_test_split(A, y, test_size=0.2)\n",
    "    lr_model = LogisticRegression()\n",
    "    lr_model.fit(A_train, y_train)\n",
    "    test_acc = lr_model.score(A_test, y_test)\n",
    "    train_acc = lr_model.score(A_train, y_train)\n",
    "    f1 = sklearn.metrics.f1_score(y_test, lr_model.predict(A_test))\n",
    "    return train_acc, test_acc, f1\n",
    "    \n",
    "def get_neuron_accuracy(layer, neuron, plot=False):\n",
    "    mean_english_activation = english_activations[layer][:,neuron].mean()\n",
    "    mean_german_activation = german_activations[layer][:,neuron].mean()\n",
    "    \n",
    "    if plot:\n",
    "        haystack_utils.two_histogram(english_activations[layer][:,neuron], german_activations[layer][:,neuron], \"English\", \"German\", \"Activation\", \"Frequency\", f\"L{layer}N{neuron} activations on English vs German text\")\n",
    "    train_acc, test_acc, f1 = run_single_neuron_lr(layer, neuron)\n",
    "    print(f\"\\nL{layer}N{neuron}: F1={f1:.2f}, Train acc={train_acc:.2f}, and test acc={test_acc:.2f}\")\n",
    "    print(f\"Mean activation English={mean_english_activation:.2f}, German={mean_german_activation:.2f}\")\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "L5N2649: F1=0.97, Train acc=0.97, and test acc=0.97\n",
      "Mean activation English=-0.07, German=2.39\n",
      "\n",
      "L8N2994: F1=0.98, Train acc=0.98, and test acc=0.98\n",
      "Mean activation English=-0.06, German=4.11\n",
      "\n",
      "L11N2911: F1=0.80, Train acc=0.76, and test acc=0.76\n",
      "Mean activation English=1.04, German=0.02\n",
      "\n",
      "L10N1129: F1=0.67, Train acc=0.64, and test acc=0.64\n",
      "Mean activation English=2.56, German=1.50\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "6",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m f1s \u001b[39m=\u001b[39m []\n\u001b[1;32m      2\u001b[0m \u001b[39mfor\u001b[39;00m layer, neuron, reported_f1 \u001b[39min\u001b[39;00m german_neurons_with_f1:\n\u001b[0;32m----> 3\u001b[0m     f1s\u001b[39m.\u001b[39mappend(get_neuron_accuracy(layer, neuron))\n\u001b[1;32m      5\u001b[0m german_neuron_names \u001b[39m=\u001b[39m [\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mL\u001b[39m\u001b[39m{\u001b[39;00mlayer\u001b[39m}\u001b[39;00m\u001b[39mN\u001b[39m\u001b[39m{\u001b[39;00mneuron\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39mfor\u001b[39;00m layer, neuron, _ \u001b[39min\u001b[39;00m german_neurons_with_f1]\n\u001b[1;32m      6\u001b[0m haystack_utils\u001b[39m.\u001b[39mline(f1s, xlabel\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m, ylabel\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mF1 score of sparse probe\u001b[39m\u001b[39m\"\u001b[39m, title\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mSparse probe performance on individual German neurons\u001b[39m\u001b[39m\"\u001b[39m, xticks\u001b[39m=\u001b[39mgerman_neuron_names, show_legend\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[10], line 14\u001b[0m, in \u001b[0;36mget_neuron_accuracy\u001b[0;34m(layer, neuron, plot)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_neuron_accuracy\u001b[39m(layer, neuron, plot\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m---> 14\u001b[0m     mean_english_activation \u001b[39m=\u001b[39m english_activations[layer][:,neuron]\u001b[39m.\u001b[39mmean()\n\u001b[1;32m     15\u001b[0m     mean_german_activation \u001b[39m=\u001b[39m german_activations[layer][:,neuron]\u001b[39m.\u001b[39mmean()\n\u001b[1;32m     17\u001b[0m     \u001b[39mif\u001b[39;00m plot:\n",
      "\u001b[0;31mKeyError\u001b[0m: 6"
     ]
    }
   ],
   "source": [
    "f1s = []\n",
    "for layer, neuron, reported_f1 in german_neurons_with_f1:\n",
    "    f1s.append(get_neuron_accuracy(layer, neuron))\n",
    "\n",
    "german_neuron_names = [f\"L{layer}N{neuron}\" for layer, neuron, _ in german_neurons_with_f1]\n",
    "haystack_utils.line(f1s, xlabel=\"\", ylabel=\"F1 score of sparse probe\", title=\"Sparse probe performance on individual German neurons\", xticks=german_neuron_names, show_legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mlp_loss_difference(prompts: list[str], model: HookedTransformer, layer=5, neuron=1336, shift=0, weight_by_logprob=False, log_prob_weight=0):\n",
    "\n",
    "    def deactivate_mlp_neuron(value, hook):\n",
    "        value[:, :, neuron] += shift\n",
    "        return value\n",
    "\n",
    "    loss_differences = []\n",
    "    for prompt in tqdm(prompts): \n",
    "        # with model.hooks(fwd_hooks=deactivate_neurons_fwd_hooks):\n",
    "        #     deactivated_loss, deactivated_cache = model.run_with_cache(prompt)\n",
    "        with model.hooks(fwd_hooks=activate_neurons_fwd_hooks + [(f'blocks.{layer}.mlp.hook_pre', deactivate_mlp_neuron)]):\n",
    "            deactivated_logit, deactivated_loss = model(prompt, return_type=\"both\", loss_per_token=True)\n",
    "        with model.hooks(fwd_hooks=activate_neurons_fwd_hooks):\n",
    "            activated_logit, activated_loss = model(prompt, return_type=\"both\", loss_per_token=True)\n",
    "        \n",
    "        if not weight_by_logprob:\n",
    "            # Shape pos \n",
    "            loss_difference, index = (deactivated_loss - activated_loss).flatten().max(0)\n",
    "            loss_differences.append(loss_difference.item())\n",
    "        else:\n",
    "            # Shape pos\n",
    "            answer_tokens = model.to_tokens(prompt)[0, 1:]\n",
    "            # Batch pos d_vocab -> pos d_vocab\n",
    "            deactivated_answer_logits = deactivated_logit.log_softmax(-1)[0, :-1, :]\n",
    "            range_tensor = torch.arange(answer_tokens.shape[0])\n",
    "            deactivated_answer_logits = deactivated_answer_logits[range_tensor, answer_tokens]\n",
    "            activated_answer_logits = activated_logit.log_softmax(-1)[0, :-1, :]\n",
    "            activated_answer_logits = activated_answer_logits[range_tensor, answer_tokens]\n",
    "            max_answer_logit, _ = torch.stack((deactivated_answer_logits, activated_answer_logits)).max(0)\n",
    "\n",
    "            diff_by_pos = (deactivated_loss - activated_loss).flatten()\n",
    "            scaled_difference, _ = (diff_by_pos + (max_answer_logit*log_prob_weight)).max(0)\n",
    "            loss_differences.append(scaled_difference.item())\n",
    "    \n",
    "    return loss_differences"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
