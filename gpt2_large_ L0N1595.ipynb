{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm.auto import tqdm, trange\n",
    "from transformer_lens import HookedTransformer\n",
    "from jaxtyping import Float, Int, Bool\n",
    "from torch import Tensor\n",
    "from tqdm.auto import tqdm\n",
    "import plotly.io as pio\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px \n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from IPython.display import display, HTML\n",
    "from datasets import load_dataset\n",
    "from collections import Counter\n",
    "import pickle\n",
    "import os\n",
    "import haystack_utils\n",
    "from transformer_lens import utils\n",
    "from fancy_einsum import einsum\n",
    "import einops\n",
    "import json\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from datasets import load_dataset\n",
    "import random\n",
    "import math\n",
    "import random\n",
    "import neel.utils as nutils\n",
    "from neel_plotly import *\n",
    "import circuitsvis as cv\n",
    "\n",
    "import hook_utils\n",
    "import haystack_utils\n",
    "\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "pio.renderers.default = \"notebook_connected+notebook\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.autograd.set_grad_enabled(False)\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-large into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7083"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model: HookedTransformer = HookedTransformer.from_pretrained(\"gpt2-large\")\n",
    "\n",
    "data = load_dataset(\"stas/openwebtext-10k\", split=\"train\")\n",
    "strings = [i for i in data[\"text\"] if len(i)>2000]\n",
    "len(strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "batched_dot_product = torch.vmap(torch.dot, (0, None))\n",
    "batched_projection = torch.vmap(haystack_utils.get_collinear_component, (0, None))\n",
    "\n",
    "def neuron_to_context_neuron_DLA(\n",
    "        model: HookedTransformer, \n",
    "        prompt: str | list[str], \n",
    "        pos=np.s_[-1:], \n",
    "        context_neuron=tuple[int, int]\n",
    ") -> tuple[Float[Tensor, \"component\"], list[str]]:\n",
    "    '''Gets full resid decomposition including all neurons. Unbatched.'''\n",
    "    _, cache = model.run_with_cache(prompt)\n",
    "    layer, neuron = context_neuron\n",
    "    neuron_attrs, neuron_labels = cache.stack_neuron_results(layer, apply_ln=True, return_labels=True, pos_slice=pos)\n",
    "    neuron_attrs = neuron_attrs.squeeze(1)\n",
    "    answer_residual_direction = model.W_in[layer, :, neuron]\n",
    "\n",
    "    results = []\n",
    "    for i in range(neuron_attrs.shape[1]):\n",
    "        results.append(batched_projection(neuron_attrs[:, i], answer_residual_direction).norm(dim=-1))\n",
    "    return torch.stack(results), neuron_labels\n",
    "\n",
    "def components_to_context_neuron_DLA(\n",
    "        model: HookedTransformer, \n",
    "        prompt: str | list[str], \n",
    "        pos=np.s_[-1:], \n",
    "        context_neuron=tuple[int, int]\n",
    ") -> tuple[Float[Tensor, \"component\"], list[str]]:\n",
    "    '''Gets full resid decomposition including all neurons. Unbatched.'''\n",
    "    _, cache = model.run_with_cache(prompt)\n",
    "    layer, neuron = context_neuron\n",
    "    attrs, labels = cache.get_full_resid_decomposition(layer, apply_ln=True, return_labels=True, pos_slice=pos, expand_neurons=False)\n",
    "    attrs = attrs.squeeze(1)\n",
    "    answer_residual_direction = model.W_in[layer, :, neuron]\n",
    "\n",
    "    results = []\n",
    "    for i in range(attrs.shape[1]):\n",
    "        results.append(batched_projection(attrs[:, i], answer_residual_direction).norm(dim=-1))\n",
    "    return torch.stack(results), labels\n",
    "\n",
    "def resid_to_context_neuron_DLA(\n",
    "        model: HookedTransformer, \n",
    "        prompt: str | list[str], \n",
    "        pos=np.s_[-1:], \n",
    "        context_neuron:tuple[int, int]=(0,0)\n",
    ") -> tuple[Float[Tensor, \"component\"], list[str]]:\n",
    "    '''Gets full resid decomposition including all neurons. Unbatched.'''\n",
    "    _, cache = model.run_with_cache(prompt)\n",
    "    layer, neuron = context_neuron\n",
    "    all_attrs, labels = cache.get_full_resid_decomposition(layer+1, apply_ln=True, return_labels=True, pos_slice=pos)\n",
    "    all_attrs = all_attrs.squeeze(1)\n",
    "    \n",
    "    answer_residual_direction = model.W_in[layer, :, neuron]\n",
    "\n",
    "    results = []\n",
    "    for i in range(all_attrs.shape[1]):\n",
    "        results.append(batched_projection(all_attrs[:, i], answer_residual_direction).norm(dim=-1))\n",
    "    return torch.stack(results), labels\n",
    "\n",
    "def get_neuron_mean_acts(model: HookedTransformer, data: list[str], layer_neuron_dict: dict[int, list[int]]) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    sorted_layer_neuron_tuples = []\n",
    "    sorted_acts = []\n",
    "\n",
    "    for layer, neurons in layer_neuron_dict.items():\n",
    "        mean_acts = haystack_utils.get_mlp_activations(data, layer, model, context_crop_start=0, hook_pre=False, neurons=neurons, disable_tqdm=True)\n",
    "        sorted_layer_neuron_tuples.extend([(layer, neuron) for neuron in neurons])\n",
    "        sorted_acts.extend(mean_acts)\n",
    "        assert len(sorted_layer_neuron_tuples) == len(sorted_acts)\n",
    "\n",
    "    return sorted_layer_neuron_tuples, sorted_acts\n",
    "\n",
    "def get_unspecified_neurons(model: HookedTransformer, layer_neuron_dict: dict[int, list[int]]):\n",
    "    unspecified = []\n",
    "    for layer in range(model.cfg.n_layers):\n",
    "        for neuron in range(model.cfg.d_mlp):\n",
    "            if not neuron in layer_neuron_dict[layer]:\n",
    "                unspecified.append((layer, neuron))\n",
    "    return unspecified\n",
    "\n",
    "def get_neuron_loss_increases(model: HookedTransformer, data: list[str], prompt: str, positionwise: bool=False) -> torch.Tensor:\n",
    "    n_tokens = model.to_tokens(prompt).shape[1] - 1\n",
    "    original_loss = model([prompt], return_type='loss', loss_per_token=positionwise)\n",
    "    \n",
    "    losses = []\n",
    "    for layer in trange(model.cfg.n_layers):\n",
    "        mean_acts = haystack_utils.get_mlp_activations(data[:200], layer, model, disable_tqdm=True, context_crop_start=0)\n",
    "        for neuron in range(model.cfg.d_mlp):\n",
    "            hook = hook_utils.get_ablate_neuron_hook(layer, neuron, mean_acts[neuron])\n",
    "            with model.hooks([hook]):\n",
    "                ablated_loss = model([prompt], return_type='loss', loss_per_token=positionwise)\n",
    "                losses.append((ablated_loss - original_loss)[0])\n",
    "    return torch.stack(losses).reshape(n_tokens, model.cfg.n_layers * model.cfg.d_mlp)\n",
    "\n",
    "def compare_dla_and_ablation(model: HookedTransformer, dla_attrs_by_neuron: torch.Tensor, ablation_losses_by_neuron: torch.Tensor, num_neurons=20):\n",
    "    print(\"DLA:\")\n",
    "    values, indices = torch.topk(dla_attrs_by_neuron, num_neurons, dim=-1)\n",
    "    layer_indices, neuron_indices = np.unravel_index(indices.cpu().numpy(), (model.cfg.n_layers, model.cfg.d_mlp))\n",
    "    print(list(zip(layer_indices.tolist(), neuron_indices.tolist())))\n",
    "    print(dla_attrs_by_neuron[indices.tolist()])\n",
    "\n",
    "    print(\"Ablation:\")\n",
    "    loss_increases_by_neuron = ablation_losses_by_neuron\n",
    "    values, indices = torch.topk(loss_increases_by_neuron, num_neurons)\n",
    "    layer_indices, neuron_indices = np.unravel_index(indices.cpu().numpy()[:num_neurons], (model.cfg.n_layers, model.cfg.d_mlp))\n",
    "    print(list(zip(layer_indices.tolist(), neuron_indices.tolist())))\n",
    "    print(dla_attrs_by_neuron[indices.tolist()])\n",
    "\n",
    "def get_hook_inputs_for_token_index(model: HookedTransformer, data: list[str], loss_increases_by_neuron: torch.Tensor, k=40):\n",
    "    values, indices = torch.topk(loss_increases_by_neuron, k)\n",
    "\n",
    "    layer_indices, neuron_indices = np.unravel_index(indices.cpu().numpy(), (model.cfg.n_layers, model.cfg.d_mlp))\n",
    "    layer_neuron_dict = defaultdict(list)\n",
    "    for layer, neuron in zip(layer_indices, neuron_indices):\n",
    "        layer_neuron_dict[layer].append(neuron)\n",
    "\n",
    "    sorted_dla_layer_neuron_tuples = []\n",
    "    sorted_acts = []\n",
    "    for layer, neurons in layer_neuron_dict.items():\n",
    "        mean_acts = haystack_utils.get_mlp_activations(data, layer, model, context_crop_start=0, neurons=neurons, disable_tqdm=True)\n",
    "        sorted_dla_layer_neuron_tuples.extend([(layer, neuron) for neuron in neurons])\n",
    "        sorted_acts.extend(mean_acts)\n",
    "        assert len(sorted_dla_layer_neuron_tuples) == len(sorted_acts)\n",
    "\n",
    "    return sorted_dla_layer_neuron_tuples, sorted_acts\n",
    "\n",
    "def unravel_top_k(neuron_attrs: torch.Tensor, k: int=10):\n",
    "    values, indices = torch.topk(neuron_attrs, k)\n",
    "    layer_indices, neuron_indices = np.unravel_index(indices.cpu().numpy(), (model.cfg.n_layers, model.cfg.d_mlp))\n",
    "    return list(zip(layer_indices.tolist(), neuron_indices.tolist()))\n",
    "\n",
    "def resid_to_head_DLA(\n",
    "        model: HookedTransformer, \n",
    "        prompt: str | list[str], \n",
    "        head: tuple[int, int],\n",
    "        pos=np.s_[-1:], \n",
    "        \n",
    ") -> tuple[Float[Tensor, \"component\"], list[str]]:\n",
    "    '''Gets full resid decomposition and return the composition of each element of the given K matrix. Unbatched.'''\n",
    "    tokens = model.to_tokens(prompt)\n",
    "    _, cache = model.run_with_cache(prompt)\n",
    "    layer, head_index = head\n",
    "    all_attrs, labels = cache.get_full_resid_decomposition(layer, apply_ln=True, return_labels=True, pos_slice=pos)\n",
    "    all_attrs = all_attrs.squeeze(1)\n",
    "    answer_residual_direction = model.W_K[layer, head_index, :]\n",
    "    results = torch.zeros(all_attrs.shape[1], all_attrs.shape[0], answer_residual_direction.shape[1])\n",
    "    for i in range(all_attrs.shape[1]): # for each token\n",
    "        for j in range(answer_residual_direction.shape[1]): # for each direction in head input\n",
    "            token_attrs = all_attrs[:, i]\n",
    "            answer = answer_residual_direction[:, j]\n",
    "            results[i, :, j] = batched_projection(token_attrs, answer).norm(dim=-1)\n",
    "    return results, labels\n",
    "\n",
    "\n",
    "def mask_scores(attn_scores: Float[Tensor, \"query_nctx key_nctx\"]):\n",
    "    '''Mask the attention scores so that tokens don't attend to previous tokens.'''\n",
    "    # assert attn_scores.shape == (model.cfg.n_ctx, model.cfg.n_ctx)\n",
    "    mask = torch.tril(torch.ones_like(attn_scores)).bool()\n",
    "    neg_inf = torch.tensor(-1.0e6).to(attn_scores.device)\n",
    "    masked_attn_scores = torch.where(mask, attn_scores, neg_inf)\n",
    "    return masked_attn_scores\n",
    "    \n",
    "def resid_to_head_DLA_custom(\n",
    "        model: HookedTransformer, \n",
    "        prompt: str | list[str], \n",
    "        head: tuple[int, int]\n",
    "        \n",
    ") -> tuple[Float[Tensor, \"component\"], list[str]]:\n",
    "    '''For last two tokens, figure out which components contribute the most to them paying attention to each other.'''\n",
    "    _, cache = model.run_with_cache(prompt)\n",
    "    layer, head_index = head\n",
    "\n",
    "    all_attrs, labels = cache.get_full_resid_decomposition(layer, apply_ln=True, return_labels=True, pos_slice=np.s_[-2:], expand_neurons=False)\n",
    "    all_attrs = all_attrs.squeeze(1).permute(1, 0, 2)\n",
    "\n",
    "    W_QK = model.W_Q[layer, head_index] @ model.W_K[layer, head_index].T\n",
    "\n",
    "    pos_by_pos_scores = all_attrs[0] @ W_QK @ all_attrs[1].T\n",
    "    # masked_scaled = mask_scores(pos_by_pos_scores / model.cfg.d_head ** 0.5)\n",
    "    # pos_by_pos_pattern = torch.softmax(masked_scaled, dim=-1)\n",
    "    return pos_by_pos_scores, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    }
   ],
   "source": [
    "# # Redo the upstream neurons method to include full MLP components in addition to the individual neuron break down\n",
    "# # L0N1595\n",
    "\n",
    "# attrs, labels = components_to_context_neuron_DLA(model, \" An eye for an\", pos=np.s_[-1:], context_neuron=(0, 1595))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"I climbed up the pear tree and picked a pear. I climbed up the apple tree and picked an\",\n",
    "    \"I climbed up the pear tree and picked a pear. I climbed up the orange tree and picked an\",\n",
    "    \"I climbed up the apple tree and picked an apple. I climbed up the pear tree and picked a\",\n",
    "    \"I climbed up the apple tree and picked an apple. I climbed up the banana tree and picked a\",\n",
    "    \"I climbed up the apple tree and picked an apple. I climbed up the cherry tree and picked a\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_objects = [\n",
    "    \"pen\", \"hat\", \"cup\", \"bag\", \"box\", \"car\", \"dog\", \"cat\", \"key\", \"bed\",\n",
    "    \"pot\", \"pan\", \"jar\", \"jug\", \"rug\", \"bat\", \"ball\", \"shoe\", \"ship\", \"bike\",\n",
    "    \"desk\", \"door\", \"bell\", \"book\", \"bowl\", \"coin\", \"comb\", \"cord\", \"disk\", \"doll\",\n",
    "    \"drum\", \"flag\", \"fork\", \"lamp\", \"lock\", \"mug\", \"nail\", \"pipe\", \"ring\", \"rope\",\n",
    "    \"seed\", \"skirt\", \"spoon\", \"stamp\", \"star\", \"stick\", \"tent\", \"tie\", \"tooth\", \"toy\",\n",
    "    \"tree\", \"watch\", \"whip\", \"bird\", \"boat\", \"boot\", \"cane\", \"card\", \"chain\", \"chair\",\n",
    "    \"chalk\", \"clock\", \"cloth\", \"cloud\", \"coat\", \"crab\", \"disk\", \"dress\", \"drop\", \"drum\",\n",
    "    \"duck\", \"dust\", \"fence\", \"flag\", \"floor\", \"flower\", \"fly\", \"fog\", \"fork\", \"fruit\",\n",
    "    \"glass\", \"glove\", \"grass\", \"hair\", \"hand\", \"harp\", \"hat\", \"hill\", \"horn\", \"horse\",\n",
    "    \"house\", \"island\", \"jewel\", \"jug\", \"kettle\", \"key\", \"kite\", \"knife\", \"leaf\", \"leg\",\n",
    "    \"library\", \"light\", \"line\", \"loaf\", \"lock\", \"machine\", \"man\", \"map\", \"moon\", \"net\",\n",
    "    \"nose\", \"nut\", \"office\", \"orange\", \"oven\", \"parcel\", \"pen\", \"pencil\", \"picture\", \"pig\",\n",
    "    \"pin\", \"pipe\", \"plane\", \"plate\", \"plough\", \"pocket\", \"pot\", \"potato\", \"prison\", \"pump\",\n",
    "    \"rail\", \"rat\", \"receipt\", \"ring\", \"rod\", \"roof\", \"root\", \"sail\", \"school\", \"scissors\",\n",
    "    \"screw\", \"seed\", \"sheep\", \"shelf\", \"ship\", \"shirt\", \"shoe\", \"skin\", \"skirt\", \"snake\",\n",
    "    \"sock\", \"spade\", \"sponge\", \"spoon\", \"spring\", \"square\", \"stamp\", \"star\", \"station\", \"stem\",\n",
    "    \"stick\", \"stocking\", \"stomach\", \"store\", \"street\", \"sun\", \"table\", \"tail\", \"thread\", \"throat\",\n",
    "    \"thumb\", \"ticket\", \"toe\", \"tongue\", \"tooth\", \"town\", \"train\", \"tray\", \"tree\", \"trousers\",\n",
    "    \"umbrella\", \"wall\", \"watch\", \"wheel\", \"whistle\", \"window\", \"wire\", \"wing\", \"worm\", \"yarn\"\n",
    "]\n",
    "vowel_objects = [\n",
    "    \"apple\", \"apron\", \"arm\", \"ankle\", \"arrow\", \"atom\", \"ant\", \"anchor\", \"album\", \"axe\",\n",
    "    \"ear\", \"egg\", \"elbow\", \"engine\", \"eagle\", \"earring\", \"envelope\", \"eye\", \"eel\", \"earth\",\n",
    "    \"ice\", \"iron\", \"ink\", \"island\", \"ivy\", \"igloo\", \"insect\", \"instrument\", \"image\", \"indicator\",\n",
    "    \"oak\", \"oar\", \"ocean\", \"octopus\", \"onion\", \"orange\", \"organ\", \"oven\", \"owl\", \"ox\",\n",
    "    \"umbrella\", \"urn\", \"utensil\", \"uniform\", \"ukelele\", \"unit\", \"unicorn\", \"upstairs\", \"underwear\", \"urchin\",\n",
    "    \"emerald\", \"end\", \"elephant\", \"elm\", \"easel\", \"eraser\", \"eskimo\", \"entrance\", \"estate\", \"echo\",\n",
    "    \"ash\", \"art\", \"armchair\", \"air\", \"arch\", \"anvil\", \"alloy\", \"alley\", \"atom\", \"amulet\",\n",
    "    \"olive\", \"opera\", \"opal\", \"ottoman\", \"orchid\", \"orbit\", \"ostrich\", \"oxen\", \"oil\", \"ounce\",\n",
    "    \"iceberg\", \"iris\", \"idea\", \"iguanodon\", \"inlet\", \"icon\", \"input\", \"isle\", \"itch\", \"issue\",\n",
    "    \"udder\", \"uplift\", \"update\", \"upgrade\", \"undo\", \"uptake\", \"upbeat\", \"upturn\", \"upload\", \"upstream\",\n",
    "    \"antenna\", \"almond\", \"arena\", \"aorta\", \"ape\", \"asteroid\", \"aster\", \"auction\", \"audio\", \"avocado\",\n",
    "    \"edge\", \"eel\", \"eel\", \"equipment\", \"escalator\", \"essence\", \"emblem\", \"echo\", \"engineer\", \"equator\",\n",
    "    \"opal\", \"orchard\", \"oboe\", \"oval\", \"oven\", \"overcoat\", \"oyster\", \"ounce\", \"outlet\", \"outline\",\n",
    "    \"aerial\", \"airplane\", \"awning\", \"award\", \"agent\", \"agate\", \"arc\", \"arena\", \"armadillo\", \"apricot\"\n",
    "]\n",
    "word_list = common_objects[:50] + vowel_objects[:50]\n",
    "token_lengths = [len(model.to_tokens(\" \"+word, prepend_bos=False).squeeze(0)) for word in word_list]\n",
    "word_list = [word_list[i] for i in range(len(word_list)) if token_lengths[i]==1]\n",
    "len(word_list)\n",
    "# word_list\n",
    "# %%\n",
    "prompt_template = \"I climbed up the pear tree and picked a pear. I climbed up the {} tree and picked\"\n",
    "prompt_list = [prompt_template.format(word) for word in word_list]\n",
    "tree_tokens = model.to_tokens(prompt_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prompt_template = \"I climbed up the pear tree and picked a pear. I climbed up the {} tree and picked an\"\n",
    "vowel_prompt_list = [prompt_template.format(word) for word in vowel_objects]\n",
    "\n",
    "prompt_template = \"I climbed up the pear tree and picked a pear. I climbed up the {} tree and picked a\"\n",
    "common_objects_prompt_list = [prompt_template.format(word) for word in common_objects]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96 186\n",
      "44 4\n"
     ]
    }
   ],
   "source": [
    "common_objects_prompts = []\n",
    "vowel_prompts = []\n",
    "\n",
    "multi_token_vowel_prompts = []\n",
    "multi_token_common_objects_prompts = []\n",
    "\n",
    "for item in common_objects_prompt_list:\n",
    "    if model.to_tokens(item).shape == (1, 21):\n",
    "        common_objects_prompts.append(item)\n",
    "    else:\n",
    "        multi_token_common_objects_prompts.append(item)\n",
    "\n",
    "for item in vowel_prompt_list:\n",
    "    if model.to_tokens(item).shape == (1, 21):\n",
    "        vowel_prompts.append(item)\n",
    "    else:\n",
    "        multi_token_vowel_prompts.append(item)\n",
    "\n",
    "print(len(vowel_prompts), len(common_objects_prompts))\n",
    "print(len(multi_token_vowel_prompts), len(multi_token_common_objects_prompts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12205264687538148 0.0005269411206245422 0.04601924934170463\n",
      "1.435411212119189 1.4191379714012147\n"
     ]
    }
   ],
   "source": [
    "# Neuron seems to work consistently across fruits\n",
    "\n",
    "\n",
    "LAYER, NEURON = 0, 1595\n",
    "\n",
    "def hook(value, hook):\n",
    "    value[0, -5, NEURON] = 0\n",
    "hook_name = f'blocks.{LAYER}.mlp.hook_post'\n",
    "hooks = [(hook_name, hook)]\n",
    "\n",
    "vowel_loss_diffs = []\n",
    "vowel_original_losses = []\n",
    "for prompt in vowel_prompts[:50]:\n",
    "    loss, cache = model.run_with_cache(prompt, return_type='loss', loss_per_token=True, names_filter=[hook_name])\n",
    "    with model.hooks(hooks):\n",
    "        ablated_loss, ablated_cache = model.run_with_cache(prompt, return_type='loss', loss_per_token=True)\n",
    "    vowel_loss_diffs.append((ablated_loss[0, -1] - loss[0, -1]).item())\n",
    "    vowel_original_losses.append(loss[0, -1].item())\n",
    "\n",
    "def multi_token_hook(value, hook):\n",
    "    value[0, -6:-4, NEURON] = 0\n",
    "hook_name = f'blocks.{LAYER}.mlp.hook_post'\n",
    "multi_token_hooks = [(hook_name, multi_token_hook)]\n",
    "\n",
    "multi_token_vowel_loss_diffs = []\n",
    "multi_token_original_losses = []\n",
    "for prompt in multi_token_vowel_prompts[:50]:\n",
    "    loss, cache = model.run_with_cache(prompt, return_type='loss', loss_per_token=True, names_filter=[hook_name])\n",
    "    with model.hooks(multi_token_hooks):\n",
    "        ablated_loss, ablated_cache = model.run_with_cache(prompt, return_type='loss', loss_per_token=True)\n",
    "    multi_token_vowel_loss_diffs.append((ablated_loss[0, -1] - loss[0, -1]).item())\n",
    "    multi_token_original_losses.append(loss[0, -1].item())\n",
    "\n",
    "\n",
    "consonant_losses = []\n",
    "for prompt in common_objects_prompts[:50]:\n",
    "    loss, cache = model.run_with_cache(prompt, return_type='loss', loss_per_token=True, names_filter=[hook_name])\n",
    "    with model.hooks(hooks):\n",
    "        ablated_loss, ablated_cache = model.run_with_cache(prompt, return_type='loss', loss_per_token=True)\n",
    "    consonant_losses.append((ablated_loss[0, -1] - loss[0, -1]).item())\n",
    "\n",
    "    # haystack_utils.line(cache[hook_name][0, :, NEURON].cpu().tolist())\n",
    "\n",
    "print(np.mean(vowel_loss_diffs), np.mean(consonant_losses), np.mean(multi_token_vowel_loss_diffs))\n",
    "print(np.mean(multi_token_original_losses), np.mean(vowel_original_losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import einsum\n",
    "\n",
    "values = []\n",
    "pos = np.s_[-5:-4]\n",
    "results = []\n",
    "\n",
    "for prompt in vowel_prompts[:50]:\n",
    "    _, cache = model.run_with_cache(prompt)\n",
    "    attrs, labels = cache.get_full_resid_decomposition(LAYER  + 1, apply_ln=True, return_labels=True, pos_slice=pos, expand_neurons=False)\n",
    "    attrs = attrs.squeeze(1)\n",
    "    answer_residual_direction = model.W_in[LAYER, :, NEURON]\n",
    "\n",
    "    results.append(einsum(\"c d, d -> c\", attrs.squeeze(1), answer_residual_direction))\n",
    "\n",
    "results = torch.stack(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>                            <div id=\"839b34ff-65ed-413a-bdf1-c285f6d43ea0\" class=\"plotly-graph-div\" style=\"height:800px; width:800px;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"839b34ff-65ed-413a-bdf1-c285f6d43ea0\")) {                    Plotly.newPlot(                        \"839b34ff-65ed-413a-bdf1-c285f6d43ea0\",                        [{\"hovertemplate\":\"variable=0\\u003cbr\\u003eindex=%{x}\\u003cbr\\u003evalue=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"0\",\"line\":{\"color\":\"#636efa\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"0\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22],\"xaxis\":\"x\",\"y\":[0.14524678885936737,-0.005004983860999346,0.07049392908811569,0.2655397057533264,0.035599786788225174,0.002865297021344304,0.012656157836318016,0.013102383352816105,0.057764533907175064,0.008675203658640385,0.0910017341375351,-0.026114236563444138,-0.013397185131907463,-0.0012944273184984922,0.1630738377571106,-0.005034209229052067,0.00516130356118083,-0.0014565312303602695,-0.021006755530834198,-0.0009425382013432682,0.2671651542186737,-0.0005315595190040767,-0.10056909918785095],\"yaxis\":\"y\",\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"\"},\"tickmode\":\"array\",\"tickvals\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22],\"ticktext\":[\"L0H0\",\"L0H1\",\"L0H2\",\"L0H3\",\"L0H4\",\"L0H5\",\"L0H6\",\"L0H7\",\"L0H8\",\"L0H9\",\"L0H10\",\"L0H11\",\"L0H12\",\"L0H13\",\"L0H14\",\"L0H15\",\"L0H16\",\"L0H17\",\"L0H18\",\"L0H19\",\"embed\",\"pos_embed\",\"bias\"],\"range\":[-0.2,22.2]},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"\"}},\"legend\":{\"title\":{\"text\":\"variable\"},\"tracegroupgap\":0},\"title\":{\"text\":\"DLA for 'pear', [' alley']\"},\"width\":800,\"showlegend\":true,\"height\":800},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('839b34ff-65ed-413a-bdf1-c285f6d43ea0');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "values = results.mean(dim=0).cpu().tolist()\n",
    "values.pop(-4)\n",
    "labels.pop(-4)\n",
    "\n",
    "\n",
    "haystack_utils.line(values, xticks=labels, title=f\"DLA for 'pear', {model.to_str_tokens(model.to_tokens(prompt)[0, pos])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
