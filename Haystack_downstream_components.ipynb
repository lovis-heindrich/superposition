{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch import einsum\n",
    "from tqdm.auto import tqdm\n",
    "import seaborn as sns\n",
    "from transformer_lens import HookedTransformer, ActivationCache, utils\n",
    "from datasets import load_dataset\n",
    "from einops import einsum\n",
    "import pandas as pd\n",
    "from transformer_lens import utils\n",
    "from rich.table import Table, Column\n",
    "from rich import print as rprint\n",
    "from jaxtyping import Float, Int, Bool\n",
    "from torch import Tensor\n",
    "import einops\n",
    "import functools\n",
    "from transformer_lens.hook_points import HookPoint\n",
    "# import circuitsvis\n",
    "from IPython.display import HTML\n",
    "from plotly.express import line\n",
    "import plotly.express as px\n",
    "from tqdm.auto import tqdm\n",
    "import json\n",
    "import gc\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from plotly.subplots import make_subplots\n",
    "# Plotly needs a different renderer for VSCode/Notebooks vs Colab argh\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = \"notebook_connected\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.autograd.set_grad_enabled(False)\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "from haystack_utils import load_txt_data, get_mlp_activations, line\n",
    "import haystack_utils\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m-v0 into HookedTransformer\n",
      "kde4_french.txt: Loaded 1007 examples with 505 to 5345 characters each.\n",
      "kde4_english.txt: Loaded 1007 examples with 501 to 5295 characters each.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d92fafa6aad3420a88510901469b8292",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 222.00 MiB (GPU 0; 23.65 GiB total capacity; 1.27 GiB already allocated; 42.00 MiB free; 1.35 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m kde_french \u001b[39m=\u001b[39m load_txt_data(\u001b[39m\"\u001b[39m\u001b[39mkde4_french.txt\u001b[39m\u001b[39m\"\u001b[39m)[:\u001b[39m300\u001b[39m]\n\u001b[1;32m      5\u001b[0m kde_english \u001b[39m=\u001b[39m load_txt_data(\u001b[39m\"\u001b[39m\u001b[39mkde4_english.txt\u001b[39m\u001b[39m\"\u001b[39m)[:\u001b[39m300\u001b[39m]\n\u001b[0;32m----> 7\u001b[0m french_activations \u001b[39m=\u001b[39m get_mlp_activations(kde_french, \u001b[39m3\u001b[39;49m, model, num_prompts\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m, mean\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m      8\u001b[0m english_activations \u001b[39m=\u001b[39m get_mlp_activations(kde_english, \u001b[39m3\u001b[39m, model, num_prompts\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m, mean\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/Documents/haystack_utils.py:60\u001b[0m, in \u001b[0;36mget_mlp_activations\u001b[0;34m(prompts, layer, model, num_prompts, context_crop_start, context_crop_end, mean)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(num_prompts)):\n\u001b[1;32m     59\u001b[0m     tokens \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mto_tokens(prompts[i])\n\u001b[0;32m---> 60\u001b[0m     _, cache \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mrun_with_cache(tokens)\n\u001b[1;32m     61\u001b[0m     act \u001b[39m=\u001b[39m cache[act_label][:, context_crop_start:context_crop_end, :]\n\u001b[1;32m     62\u001b[0m     act \u001b[39m=\u001b[39m einops\u001b[39m.\u001b[39mrearrange(act, \u001b[39m\"\u001b[39m\u001b[39mbatch pos d_mlp -> (batch pos) d_mlp\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformer_lens/HookedTransformer.py:429\u001b[0m, in \u001b[0;36mHookedTransformer.run_with_cache\u001b[0;34m(self, return_cache_object, remove_batch_dim, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    413\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun_with_cache\u001b[39m(\n\u001b[1;32m    414\u001b[0m     \u001b[39mself\u001b[39m, \u001b[39m*\u001b[39mmodel_args, return_cache_object\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, remove_batch_dim\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[1;32m    415\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    422\u001b[0m     Union[ActivationCache, Dict[\u001b[39mstr\u001b[39m, torch\u001b[39m.\u001b[39mTensor]],\n\u001b[1;32m    423\u001b[0m ]:\n\u001b[1;32m    424\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    425\u001b[0m \u001b[39m    Wrapper around run_with_cache in HookedRootModule. If return_cache_object is True, this will return an\u001b[39;00m\n\u001b[1;32m    426\u001b[0m \u001b[39m    ActivationCache object, with a bunch of useful HookedTransformer specific methods, otherwise it will return a\u001b[39;00m\n\u001b[1;32m    427\u001b[0m \u001b[39m    dictionary of activations as in HookedRootModule.\u001b[39;00m\n\u001b[1;32m    428\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 429\u001b[0m     out, cache_dict \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mrun_with_cache(\n\u001b[1;32m    430\u001b[0m         \u001b[39m*\u001b[39;49mmodel_args, remove_batch_dim\u001b[39m=\u001b[39;49mremove_batch_dim, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    431\u001b[0m     )\n\u001b[1;32m    432\u001b[0m     \u001b[39mif\u001b[39;00m return_cache_object:\n\u001b[1;32m    433\u001b[0m         cache \u001b[39m=\u001b[39m ActivationCache(\n\u001b[1;32m    434\u001b[0m             cache_dict, \u001b[39mself\u001b[39m, has_batch_dim\u001b[39m=\u001b[39m\u001b[39mnot\u001b[39;00m remove_batch_dim\n\u001b[1;32m    435\u001b[0m         )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformer_lens/hook_points.py:457\u001b[0m, in \u001b[0;36mHookedRootModule.run_with_cache\u001b[0;34m(self, names_filter, device, remove_batch_dim, incl_bwd, reset_hooks_end, clear_contexts, *model_args, **model_kwargs)\u001b[0m\n\u001b[1;32m    447\u001b[0m cache_dict, fwd, bwd \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_caching_hooks(\n\u001b[1;32m    448\u001b[0m     names_filter, incl_bwd, device, remove_batch_dim\u001b[39m=\u001b[39mremove_batch_dim\n\u001b[1;32m    449\u001b[0m )\n\u001b[1;32m    451\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhooks(\n\u001b[1;32m    452\u001b[0m     fwd_hooks\u001b[39m=\u001b[39mfwd,\n\u001b[1;32m    453\u001b[0m     bwd_hooks\u001b[39m=\u001b[39mbwd,\n\u001b[1;32m    454\u001b[0m     reset_hooks_end\u001b[39m=\u001b[39mreset_hooks_end,\n\u001b[1;32m    455\u001b[0m     clear_contexts\u001b[39m=\u001b[39mclear_contexts,\n\u001b[1;32m    456\u001b[0m ):\n\u001b[0;32m--> 457\u001b[0m     model_out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(\u001b[39m*\u001b[39;49mmodel_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs)\n\u001b[1;32m    458\u001b[0m     \u001b[39mif\u001b[39;00m incl_bwd:\n\u001b[1;32m    459\u001b[0m         model_out\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformer_lens/HookedTransformer.py:375\u001b[0m, in \u001b[0;36mHookedTransformer.forward\u001b[0;34m(self, input, return_type, loss_per_token, prepend_bos, stop_at_layer, past_kv_cache)\u001b[0m\n\u001b[1;32m    373\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    374\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 375\u001b[0m     logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49munembed(residual)  \u001b[39m# [batch, pos, d_vocab]\u001b[39;00m\n\u001b[1;32m    376\u001b[0m     \u001b[39mif\u001b[39;00m return_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mlogits\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    377\u001b[0m         \u001b[39mreturn\u001b[39;00m logits\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformer_lens/components.py:58\u001b[0m, in \u001b[0;36mUnembed.forward\u001b[0;34m(self, residual)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m     55\u001b[0m     \u001b[39mself\u001b[39m, residual: Float[torch\u001b[39m.\u001b[39mTensor, \u001b[39m\"\u001b[39m\u001b[39mbatch pos d_model\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m     56\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Float[torch\u001b[39m.\u001b[39mTensor, \u001b[39m\"\u001b[39m\u001b[39mbatch pos d_vocab_out\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[1;32m     57\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m---> 58\u001b[0m         einsum(\n\u001b[1;32m     59\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mbatch pos d_model, d_model vocab -> batch pos vocab\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     60\u001b[0m             residual,\n\u001b[1;32m     61\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mW_U,\n\u001b[1;32m     62\u001b[0m         )\n\u001b[1;32m     63\u001b[0m         \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mb_U\n\u001b[1;32m     64\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/fancy_einsum/__init__.py:136\u001b[0m, in \u001b[0;36meinsum\u001b[0;34m(equation, *operands)\u001b[0m\n\u001b[1;32m    134\u001b[0m backend \u001b[39m=\u001b[39m get_backend(operands[\u001b[39m0\u001b[39m])\n\u001b[1;32m    135\u001b[0m new_equation \u001b[39m=\u001b[39m convert_equation(equation)\n\u001b[0;32m--> 136\u001b[0m \u001b[39mreturn\u001b[39;00m backend\u001b[39m.\u001b[39;49meinsum(new_equation, \u001b[39m*\u001b[39;49moperands)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/fancy_einsum/__init__.py:54\u001b[0m, in \u001b[0;36mTorchBackend.einsum\u001b[0;34m(self, equation, *operands)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39meinsum\u001b[39m(\u001b[39mself\u001b[39m, equation, \u001b[39m*\u001b[39moperands):\n\u001b[0;32m---> 54\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtorch\u001b[39m.\u001b[39;49meinsum(equation, \u001b[39m*\u001b[39;49moperands)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/functional.py:378\u001b[0m, in \u001b[0;36meinsum\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    373\u001b[0m     \u001b[39mreturn\u001b[39;00m einsum(equation, \u001b[39m*\u001b[39m_operands)\n\u001b[1;32m    375\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(operands) \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m2\u001b[39m \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m opt_einsum\u001b[39m.\u001b[39menabled:\n\u001b[1;32m    376\u001b[0m     \u001b[39m# the path for contracting 0 or 1 time(s) is already optimized\u001b[39;00m\n\u001b[1;32m    377\u001b[0m     \u001b[39m# or the user has disabled using opt_einsum\u001b[39;00m\n\u001b[0;32m--> 378\u001b[0m     \u001b[39mreturn\u001b[39;00m _VF\u001b[39m.\u001b[39;49meinsum(equation, operands)  \u001b[39m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    380\u001b[0m path \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    381\u001b[0m \u001b[39mif\u001b[39;00m opt_einsum\u001b[39m.\u001b[39mis_available():\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 222.00 MiB (GPU 0; 23.65 GiB total capacity; 1.27 GiB already allocated; 42.00 MiB free; 1.35 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "model = HookedTransformer.from_pretrained(\"pythia-70m-v0\", fold_ln=True, device=device)\n",
    "\n",
    "kde_french = load_txt_data(\"kde4_french.txt\")\n",
    "kde_english = load_txt_data(\"kde4_english.txt\")\n",
    "\n",
    "french_activations = get_mlp_activations(kde_french, 3, model, num_prompts=100, mean=True)\n",
    "english_activations = get_mlp_activations(kde_english, 3, model, num_prompts=100, mean=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Absolute activation difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context lengths: [368, 326, 195, 177, 173, 529, 339, 198, 213, 168, 230, 198, 184, 134, 215, 188, 793, 180, 190, 197]\n",
      "Mean loss (batch size = 4): 5.2905\n",
      "Mean loss (batch size = 1): 3.5187\n",
      "Mean loss (batch size = 4, crop = (10:150)): 3.7173\n",
      "Mean loss (batch size = 1, crop = (10:150)): 3.6928\n",
      "torch.Size([1, 368])\n",
      "torch.Size([1, 367])\n",
      "torch.Size([1, 326])\n",
      "torch.Size([1, 325])\n",
      "torch.Size([1, 195])\n",
      "torch.Size([1, 194])\n",
      "torch.Size([1, 177])\n",
      "torch.Size([1, 176])\n",
      "torch.Size([1, 173])\n",
      "torch.Size([1, 172])\n",
      "torch.Size([1, 529])\n",
      "torch.Size([1, 528])\n",
      "torch.Size([1, 339])\n",
      "torch.Size([1, 338])\n",
      "torch.Size([1, 198])\n",
      "torch.Size([1, 197])\n",
      "torch.Size([1, 213])\n",
      "torch.Size([1, 212])\n",
      "torch.Size([1, 168])\n",
      "torch.Size([1, 167])\n",
      "torch.Size([1, 230])\n",
      "torch.Size([1, 229])\n",
      "torch.Size([1, 198])\n",
      "torch.Size([1, 197])\n",
      "torch.Size([1, 184])\n",
      "torch.Size([1, 183])\n",
      "torch.Size([1, 134])\n",
      "torch.Size([1, 133])\n",
      "torch.Size([1, 215])\n",
      "torch.Size([1, 214])\n",
      "torch.Size([1, 188])\n",
      "torch.Size([1, 187])\n",
      "torch.Size([1, 793])\n",
      "torch.Size([1, 792])\n",
      "torch.Size([1, 180])\n",
      "torch.Size([1, 179])\n",
      "torch.Size([1, 190])\n",
      "torch.Size([1, 189])\n",
      "torch.Size([1, 197])\n",
      "torch.Size([1, 196])\n",
      "tensor([133.8795, 147.8726, 133.6614, 117.9387, 113.0749, 119.7327, 100.6653,\n",
      "         95.4874, 107.4255,  96.9834], device='cuda:0')\n",
      "tensor([20., 20., 20., 20., 20., 20., 20., 20., 20., 20.], device='cuda:0')\n",
      "tensor(3.4020, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(prompts: list[str], model: HookedTransformer, batch_size, crop_context=None):\n",
    "    losses = []\n",
    "    for batch_index in range(0, len(prompts), batch_size):\n",
    "        if batch_index+batch_size >= len(prompts):\n",
    "            batch = prompts[batch_index:]\n",
    "        else:\n",
    "            batch = prompts[batch_index:batch_index+batch_size]\n",
    "        tokens = model.to_tokens(batch)\n",
    "        if crop_context is not None:\n",
    "            tokens = tokens[:, :crop_context]\n",
    "        loss = model(tokens, return_type=\"loss\")\n",
    "        losses.append(loss.item())\n",
    "    return losses\n",
    "\n",
    "examples = kde_french[:20]\n",
    "print(\"Context lengths:\", [model.to_tokens(example).shape[1] for example in examples])\n",
    "\n",
    "losses_batched = evaluate_model(examples, model, batch_size=4)\n",
    "losses_single = evaluate_model(examples, model, batch_size=1)\n",
    "print(f\"Mean loss (batch size = 4): {np.mean(losses_batched):.4f}\")\n",
    "print(f\"Mean loss (batch size = 1): {np.mean(losses_single):.4f}\")\n",
    "\n",
    "losses_batched = evaluate_model(examples, model, batch_size=4, crop_context=150)\n",
    "losses_single = evaluate_model(examples, model, batch_size=1, crop_context=150)\n",
    "print(f\"Mean loss (batch size = 4, crop = (10:150)): {np.mean(losses_batched):.4f}\")\n",
    "print(f\"Mean loss (batch size = 1, crop = (10:150)): {np.mean(losses_single):.4f}\")\n",
    "\n",
    "print(haystack_utils.get_average_loss(examples, model, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ablated_mlp_difference(prompts: list[str], model: HookedTransformer, neurons: list[int], layer_to_ablate: int, layer_to_cache: int, mean_neuron_activations: Float[Tensor, \"d_mlp\"]):\n",
    "    original_losses = []\n",
    "    ablated_losses = []\n",
    "    mean_differences = []\n",
    "    for prompt in tqdm(prompts):\n",
    "        neurons = torch.LongTensor(neurons)\n",
    "\n",
    "        def ablate_neuron_hook(value, hook):\n",
    "            value[:, :, neurons] = mean_neuron_activations[neurons]\n",
    "            return value\n",
    "        \n",
    "        tokens = model.to_tokens(prompt)\n",
    "        original_loss, original_cache = model.run_with_cache(tokens, return_type=\"loss\")\n",
    "\n",
    "        with model.hooks(fwd_hooks=[(f'blocks.{layer_to_ablate}.mlp.hook_post', ablate_neuron_hook)]):\n",
    "            ablated_loss, ablated_cache = model.run_with_cache(tokens, return_type=\"loss\")\n",
    "\n",
    "        original_losses.append(original_loss.item())\n",
    "        ablated_losses.append(ablated_loss.item())\n",
    "\n",
    "        block_name = f'blocks.{layer_to_cache}.mlp.hook_post'\n",
    "        original_activations = original_cache[block_name][:, 1:]\n",
    "        ablated_activations = ablated_cache[block_name][:, 1:]\n",
    "        mean_difference = original_activations.mean((0, 1)) - ablated_activations.mean((0, 1))\n",
    "        mean_differences.append(mean_difference)\n",
    "        \n",
    "        \n",
    "    print(f\"Original loss: {np.mean(original_losses):.2f}, ablated loss: {np.mean(ablated_losses):.2f} (+{((np.mean(ablated_losses) - np.mean(original_losses)) / np.mean(original_losses))*100:.2f}%)\")\n",
    "    return torch.stack(mean_differences).mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b44dc87bde64ce7b5a188d5dd5d2212",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1007 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original loss: 3.58, ablated loss: 3.81 (+6.23%)\n"
     ]
    }
   ],
   "source": [
    "layer_to_cache = 5\n",
    "difference = get_ablated_mlp_difference(kde_french, model, neurons=[609], layer_to_ablate=3, layer_to_cache=layer_to_cache, mean_neuron_activations=english_activations)\n",
    "sorted_differences, sorted_neurons = torch.topk(difference.abs(), len(difference), largest=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-2.24.1.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>                            <div id=\"30618491-1f09-4b98-b7b2-e7df87c64d2a\" class=\"plotly-graph-div\" style=\"height:525px; width:800px;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"30618491-1f09-4b98-b7b2-e7df87c64d2a\")) {                    Plotly.newPlot(                        \"30618491-1f09-4b98-b7b2-e7df87c64d2a\",                        [{\"alignmentgroup\":\"True\",\"bingroup\":\"x\",\"hovertemplate\":\"variable=0\\u003cbr\\u003evalue=%{x}\\u003cbr\\u003ecount=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"0\",\"marker\":{\"color\":\"#636efa\",\"pattern\":{\"shape\":\"\"}},\"name\":\"0\",\"offsetgroup\":\"0\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[-0.004645323380827904,-0.0006135707953944802,0.006211026106029749,-0.0023223008029162884,0.2327786684036255,0.09119820594787598,-0.02565385028719902,-0.005280673503875732,0.4755672812461853,-0.034190110862255096,-0.054232873022556305,0.004980749916285276,-0.019466059282422066,0.03750162571668625,-0.004625342786312103,-0.05223977938294411,-0.00662841834127903,-0.0011434017214924097,0.003507128683850169,-0.0006512617692351341,-0.031298473477363586,0.002459162613376975,0.027653368189930916,0.0016784269828349352,-0.006818631198257208,-0.0013570624869316816,0.12331562489271164,0.006186551880091429,-0.004329038318246603,-0.0005643892800435424,0.13134510815143585,0.13440468907356262,0.04212970659136772,-8.097838144749403e-05,0.019621465355157852,0.13554994761943817,-0.01879565417766571,-0.012909244745969772,-0.006440310273319483,0.010840327478945255,-0.002423811238259077,0.024102535098791122,0.002338350284844637,-0.02377164550125599,0.12799502909183502,-0.010644332505762577,-0.019269417971372604,-0.07665657997131348,-0.030872246250510216,0.026922939345240593,0.07366587221622467,0.0031236561480909586,-0.04073585569858551,-0.006546936929225922,0.013087025843560696,-0.0006540080648846924,0.08187743276357651,-0.004644008353352547,0.00680073956027627,-0.031160127371549606,0.0017055294010788202,-0.02658403292298317,0.0006125970976427197,0.0019007707014679909,-0.003380240872502327,-0.09584049880504608,-0.02766546793282032,0.05313342064619064,0.20201188325881958,-0.00021276251936797053,-0.005416876636445522,-0.004874255042523146,-0.001717028091661632,-0.013062408193945885,-0.00013391401444096118,0.05430471897125244,0.02404961735010147,0.014560569077730179,0.2670234143733978,-0.018329406157135963,0.026148423552513123,-0.03559798747301102,-0.02308821491897106,-0.02931184135377407,0.03448230400681496,-0.03662554919719696,-0.021771015599370003,0.012280729599297047,0.0069723972119390965,-0.0019983688835054636,0.007716473191976547,0.07439253479242325,-0.0023675377015024424,-0.012216311879456043,0.04376031458377838,0.01786501705646515,0.032304342836141586,-4.884891677647829e-05,-0.015596076846122742,0.0022217207588255405,0.01940593123435974,-0.009608650580048561,-0.008382496424019337,-0.0030829431489109993,-0.027551500126719475,-0.004960605408996344,0.004959185142070055,-0.013233313336968422,0.07829070836305618,0.005535759497433901,-0.009854357689619064,0.019035005941987038,0.07812854647636414,0.01704447716474533,-0.0026784678921103477,-0.007833966054022312,0.0008230480016209185,-0.004492195323109627,-0.029198158532381058,-0.01203115377575159,-0.038675371557474136,-0.004065187647938728,0.007983937859535217,0.013701398856937885,-0.006909351330250502,0.006315912585705519,0.03573121130466461,-0.04256712645292282,-0.03846442326903343,0.0550394207239151,-0.007610934786498547,0.04226302728056908,-0.0035426546819508076,0.07739276438951492,-0.03821787238121033,0.19483861327171326,0.0840592086315155,-0.026929188519716263,0.13730394840240479,-0.0028363363817334175,-0.0024230959825217724,-0.06524324417114258,0.09647700190544128,-0.004153861664235592,0.03496704250574112,-0.0014769397675991058,0.000716153415851295,-0.015922365710139275,-0.008596647530794144,0.18682272732257843,0.017738336697220802,0.00499457772821188,-0.005662546958774328,-0.004876901861280203,-0.005144450813531876,0.1257522851228714,-0.036264095455408096,-0.001237646210938692,0.08352188766002655,-0.0346485860645771,0.1616830825805664,-0.006481830030679703,0.015121004544198513,-0.011759946122765541,0.46629250049591064,-0.00012570680701173842,0.055384695529937744,0.0033954777754843235,-0.0187497828155756,0.01344127394258976,-0.0011843998217955232,-0.04351057857275009,-0.011929989792406559,-0.020333198830485344,0.002144609345123172,0.010741379112005234,0.035986848175525665,0.01863459125161171,-0.01343497820198536,0.004832802806049585,-0.006559884175658226,0.06058422476053238,0.004469039849936962,-0.04413089528679848,0.002259019995108247,0.029072852805256844,-0.027805430814623833,-0.009250285103917122,-0.09631294757127762,0.00239430065266788,0.028439249843358994,0.001321184798143804,-0.00038142435369081795,-0.005320667754858732,-0.0008561763679608703,-0.0021416135132312775,0.008763382211327553,-0.00367051106877625,-0.04767953231930733,0.01133044809103012,-0.008641785942018032,-0.0077082132920622826,-0.0720132440328598,-0.02421153150498867,0.07249529659748077,-0.00946598220616579,-0.00855285581201315,2.0396086256369017e-05,-0.023946130648255348,0.0618835911154747,-0.013288420625030994,-0.007933610118925571,-0.006715268827974796,-0.0034554041922092438,0.014431426301598549,0.016366638243198395,-0.045327167958021164,0.008712362498044968,-0.009479145519435406,-0.001403353875502944,0.13572488725185394,-0.002399495802819729,0.07260766625404358,0.04540465399622917,0.02109360694885254,0.00023905295529402792,0.0024470933713018894,0.008172227069735527,0.0027550053782761097,0.022070016711950302,0.018289444968104362,0.013978859409689903,0.00029938362422399223,-0.023739337921142578,0.018198223784565926,-0.006178872659802437,-0.004085725639015436,0.0006666285335086286,-0.008144227787852287,-0.0054969326592981815,0.08197493851184845,0.01602175459265709,0.01348323468118906,-0.026553507894277573,-0.038671765476465225,0.36461880803108215,-0.02884887158870697,0.00888101663440466,0.007274200674146414,-0.15119893848896027,0.005923308897763491,-0.016512800008058548,0.05529538914561272,0.006336717866361141,0.08251772075891495,-0.016786793246865273,-0.011971934698522091,-0.0009484487818554044,-0.008206126280128956,0.0037469654344022274,0.005253982730209827,-0.003672755789011717,-0.019498152658343315,-0.011908313259482384,0.024486467242240906,-0.03408610820770264,0.0017425530822947621,-0.008616133593022823,-0.002347673522308469,-0.007160406094044447,0.059091322124004364,-0.0020027263090014458,-0.0028564040549099445,0.7151039838790894,-0.0006852666847407818,-0.0024386667646467686,0.083573117852211,0.08594661951065063,-0.012701072730123997,-0.0068289851769804955,0.003838600590825081,0.01020309329032898,-0.041130904108285904,-0.0040390463545918465,-0.007627944927662611,0.00585701409727335,-0.029864469543099403,0.08259961009025574,-0.024698644876480103,-0.000967020052485168,0.00329541671089828,-0.07398667186498642,0.14083249866962433,0.03286178410053253,-0.021708423271775246,0.04532960057258606,0.02075503207743168,0.18267777562141418,0.043853409588336945,0.004376462660729885,-0.0027334270998835564,0.007169906981289387,0.02234051562845707,0.004969336558133364,-0.005891704000532627,-0.06657475978136063,9.28310037124902e-05,0.005743356887251139,-0.04793088883161545,-0.023715943098068237,0.017679058015346527,0.0030600386671721935,0.03275010734796524,0.05318927392363548,-0.10728469491004944,0.2242056131362915,-0.076106958091259,0.00092313316417858,0.011235120706260204,0.01210846845060587,-0.0010622264817357063,-0.05312846228480339,0.0037233016919344664,0.05737196281552315,-0.034578196704387665,0.012936769984662533,-0.005463656038045883,-0.009469049051404,0.00888145249336958,0.019744567573070526,0.01870867609977722,-0.0022325634490698576,0.12119568139314651,0.040669478476047516,0.025166822597384453,0.0017070136964321136,0.028708485886454582,0.006022738758474588,0.26141679286956787,0.0014207932399585843,-0.06447597593069077,0.016764488071203232,0.11732637882232666,0.004105713684111834,-0.004814429208636284,0.012246796861290932,0.10192477703094482,0.17693564295768738,-0.0007355821435339749,-0.03347811475396156,-0.014609555713832378,-0.006126635707914829,0.0027084294706583023,-0.005466727074235678,0.03005545400083065,-0.03432851657271385,0.007611542474478483,0.005371881648898125,-0.022434458136558533,0.0047711837105453014,0.16913701593875885,0.021934451535344124,0.03904346004128456,0.07397399097681046,-0.005108790006488562,-0.005905330181121826,-0.02482876367866993,0.004662281833589077,-0.0027700013015419245,-0.017945075407624245,0.043566856533288956,-0.004872388206422329,0.05656660348176956,0.024963503703475,-0.058777038007974625,0.01805882342159748,-0.037189047783613205,0.11925248056650162,0.016575461253523827,-0.021097378805279732,0.25930845737457275,0.00939209945499897,0.015815258026123047,0.04103551805019379,0.019390473142266273,0.053330954164266586,0.029388079419732094,0.15640121698379517,-0.005792062729597092,0.43345460295677185,-0.02450730837881565,-0.001303442637436092,-0.003044743090867996,0.10689829289913177,0.3325088918209076,0.6447742581367493,-0.005973880644887686,0.007946888916194439,-0.004644936416298151,-0.04483106732368469,-0.0028580818325281143,0.05176461488008499,-0.0006939719896763563,0.02337520569562912,0.12098231911659241,0.16007085144519806,-0.00888090394437313,0.08727092295885086,0.011360259726643562,-0.0034168707206845284,-0.005248309578746557,0.2624962627887726,0.004252549260854721,-0.0026407004334032536,0.16106028854846954,0.04277381673455238,-0.00389108550734818,-0.07597167789936066,-0.011579112149775028,0.15368907153606415,0.00022675551008433104,0.10313454270362854,-0.009468460455536842,0.1962377279996872,-0.0016221326077356935,0.00029552963678725064,-0.02555929310619831,0.0021601717453449965,-0.01031428948044777,0.002413794631138444,0.025079447776079178,-0.025905484333634377,-0.0478019118309021,-0.05192553251981735,0.005343184340745211,-0.015992341563105583,0.07725362479686737,-0.005795230157673359,-0.007238119374960661,0.01094044093042612,0.002869050484150648,-0.008423339575529099,0.010094499215483665,0.005537369288504124,0.007910804823040962,0.004955550190061331,-0.006594975478947163,0.19218173623085022,0.007896769791841507,0.024150965735316277,-0.008427185006439686,-0.09604674577713013,0.0780588760972023,0.25326448678970337,0.047422248870134354,0.1743670254945755,-0.0344657264649868,-0.015459696762263775,-0.018376650288701057,-0.026004843413829803,0.0028442994225770235,-0.02230938710272312,0.004753670189529657,0.010539766401052475,-0.00148924277164042,-0.016027241945266724,0.02781638130545616,-0.007628034334629774,-0.00835263542830944,0.1096518412232399,-0.010711949318647385,-0.039188411086797714,0.10321196168661118,-0.0247778482735157,-0.014221103861927986,-0.0002377372729824856,-0.0005625562043860555,-0.00378999300301075,0.002650839975103736,0.005614641588181257,0.0021122063044458628,-0.0014538391260430217,-0.017902418971061707,-0.008921094238758087,-0.010604092851281166,-0.014301209710538387,0.06111978739500046,-0.012830739840865135,-0.04581701382994652,-0.034180719405412674,-0.0035378679167479277,-0.003915918059647083,-0.022359535098075867,0.18568958342075348,-0.0028292073402553797,-0.016676057130098343,-0.06898065656423569,0.04153300076723099,-0.030075110495090485,0.20476014912128448,-0.012250819243490696,0.025306658819317818,0.07820349186658859,-0.0027492251247167587,-0.019432995468378067,0.21428819000720978,0.01615178771317005,0.026307538151741028,0.01670979894697666,-0.004726966843008995,-0.0039007500745356083,-0.007685080636292696,0.018467726185917854,-0.0063986810855567455,0.03138565644621849,-0.051885586231946945,-0.03129023686051369,-0.00015455683751497418,-0.006551394239068031,0.04772450774908066,-0.005057564005255699,-0.015551462769508362,0.018300887197256088,-0.0018398023676127195,-0.002238574670627713,-0.014439648017287254,0.02157958596944809,0.013950186781585217,0.00876048393547535,-0.010417478159070015,-0.012676275335252285,-0.033112749457359314,-0.044060077518224716,0.0002095323579851538,-0.006116357631981373,-0.03470306098461151,-0.021999502554535866,0.040735404938459396,0.02515917830169201,-0.02910681441426277,-0.05824805423617363,-0.03459558263421059,-0.04319583997130394,-0.00424543721601367,0.0022184893023222685,-0.03242030739784241,0.017039675265550613,-0.0484451986849308,0.18472568690776825,0.05882516875863075,-0.06909169256687164,-0.010895411483943462,0.06825373321771622,0.010713516734540462,0.15993937849998474,-0.00032455293694511056,0.18790464103221893,-0.015733711421489716,-0.003856375813484192,-0.07407155632972717,-0.011340947821736336,0.014287256635725498,-0.016506236046552658,-0.0024463669396936893,0.004075180739164352,0.0557367280125618,-0.005277235526591539,-0.02991952933371067,0.062955342233181,0.029610877856612206,-0.024953069165349007,0.0002470625622663647,-0.012903396971523762,0.006526927929371595,0.004548841621726751,0.022486066445708275,0.0007031903369352221,-0.011098256334662437,0.0005035708309151232,0.0032104996498674154,0.06272915750741959,-0.0684456154704094,-0.010339964181184769,0.025569817051291466,0.33163654804229736,0.023786699399352074,0.16603894531726837,0.10121912509202957,-0.001756035489961505,0.07259692996740341,-0.016242336481809616,-0.04860978201031685,-8.345860987901688e-05,-0.003397058229893446,-0.017157841473817825,-0.004535212647169828,0.03417337313294411,-0.03125790134072304,-0.003416928695514798,0.11330369114875793,0.015504159033298492,0.0007054204470477998,0.002143912948668003,0.005326759070158005,0.0042032115161418915,0.010523386299610138,0.10482992976903915,-0.0021694309543818235,0.26951536536216736,-0.017837433144450188,-0.00331664877012372,0.005073220934718847,0.004082296974956989,0.09451381117105484,0.03464705869555473,0.03391594439744949,-0.0052289292216300964,-0.004800704307854176,0.018097663298249245,-0.014803260564804077,-0.004989847540855408,-0.001730074523948133,0.0040077706798911095,0.018346887081861496,0.005145813804119825,-0.0025808000937104225,-0.01082519255578518,0.012089118361473083,-0.005893669091165066,0.007837154902517796,0.016619132831692696,-0.11683531105518341,-0.030878420919179916,0.03874893859028816,0.01964626833796501,-0.0023278628941625357,0.1399078667163849,0.007210419978946447,-0.004307441413402557,0.269916832447052,-0.0013868447858840227,-0.1695656031370163,-0.014497349038720131,0.14366765320301056,-0.0014249662635847926,-0.007958335801959038,-0.02990604378283024,-0.015408259816467762,0.16690245270729065,0.004093199502676725,0.0113722775131464,0.2574058473110199,-0.03723914921283722,-0.005313652101904154,0.1175064742565155,0.007029347587376833,0.018345093354582787,-0.04573103040456772,0.012454308569431305,0.07771650701761246,-0.009636890143156052,-0.012712844647467136,0.020751796662807465,-0.022131318226456642,0.010591204278171062,0.16774116456508636,-0.002430237829685211,-0.0009518674924038351,-0.0028939773328602314,-0.008413219824433327,0.7021034359931946,-0.004593895748257637,-0.05697479471564293,0.031899407505989075,-0.00564712705090642,-0.012817032635211945,-0.011005566455423832,0.03029026836156845,-0.007993020117282867,-0.019384397193789482,-0.006633098237216473,-0.03597939759492874,0.019736649468541145,0.0033234665170311928,-0.05308502912521362,0.0004870092961937189,-0.006691153161227703,0.004072875715792179,0.0001741456799209118,0.011532062664628029,0.12515853345394135,-0.0024855416268110275,0.0016229311004281044,0.027072101831436157,-0.015455175191164017,-0.01008776668459177,-0.0009218432824127376,0.06798726320266724,-0.00783933512866497,-3.0617207812611014e-05,-0.009295592084527016,-0.07932408154010773,-0.00687395641580224,0.1431867629289627,-0.00045731739373877645,-0.03998025506734848,-0.0031201064120978117,0.11930657923221588,0.004745644982904196,-0.09319789707660675,-0.0013601406244561076,0.01293825265020132,-0.016451817005872726,-0.03374045714735985,0.0006025142502039671,-0.054400086402893066,-0.0020821986254304647,-0.015911521390080452,-0.013667616061866283,0.009617472998797894,0.002743715886026621,0.00190294801723212,-0.013422864489257336,0.005748935975134373,-0.037115808576345444,0.0012604562798514962,-0.016096405684947968,-0.15017960965633392,-0.07030035555362701,-0.04598486050963402,-0.0540970154106617,0.13845375180244446,-0.0347503237426281,-0.021337419748306274,0.006293866783380508,-0.0015634908340871334,0.0017558600520715117,0.002428863663226366,-0.02030053921043873,-0.0073841772973537445,-0.0044910963624715805,-0.016195237636566162,-0.0016314175445586443,-0.005505475681275129,-0.0011613357346504927,-0.004724542610347271,0.005980738904327154,0.03129478543996811,-0.017269013449549675,-0.029777269810438156,0.018768662586808205,0.4496127665042877,0.00811882596462965,-0.0062787216156721115,-0.028741732239723206,0.14184775948524475,-0.01827649027109146,-0.004730533342808485,0.022858362644910812,-0.039055924862623215,-0.0037741390988230705,0.02216062881052494,-0.006822084076702595,-0.022219838574528694,-0.0031061519403010607,0.015547524206340313,0.0012053503887727857,0.24697650969028473,0.01726008579134941,0.03361523896455765,-0.020579129457473755,0.0005952127976343036,0.05917824059724808,-0.011360236443579197,-0.0010688354959711432,-0.007384570315480232,0.020344341173768044,0.018285203725099564,0.015983980149030685,0.0032236382830888033,-0.023425282910466194,0.03253170847892761,-0.019351141527295113,0.15788492560386658,-0.0015373859787359834,0.043802183121442795,0.038994546979665756,0.005289250984787941,0.0440250001847744,-0.01019661221653223,0.045799873769283295,-0.02068101242184639,-0.02181634120643139,-0.013764999806880951,0.018948905169963837,0.02005467377603054,-0.04877547174692154,-0.010378369130194187,-0.08346307277679443,-0.019546769559383392,-0.0061738621443510056,-0.005458676256239414,-0.0019041401101276278,-0.00388139090500772,0.028182240203022957,-0.022431911900639534,0.0016829529777169228,0.08426406234502792,-0.02080768533051014,0.0016358850989490747,0.03728378936648369,-0.02933930978178978,0.007680997252464294,0.11926108598709106,-0.0010842502815648913,-0.006671981420367956,-0.0038993319030851126,0.12526939809322357,-0.009594859555363655,0.007146642077714205,0.29807400703430176,0.0034718073438853025,0.01965172588825226,-0.004924194421619177,-0.04352927207946777,-0.006110663991421461,-0.003484979970380664,-0.007376831956207752,-0.003320864401757717,-0.011675694026052952,0.0003005851758643985,0.2064177393913269,0.08183728158473969,0.028174210339784622,-0.0066987271420657635,0.001033266307786107,-0.0022913780994713306,-0.0033756920602172613,-0.007533141411840916,-0.0018931328086182475,0.019904425367712975,-0.013701756484806538,-0.01549797598272562,0.01763528771698475,0.06595145910978317,0.11633290350437164,-0.013472571969032288,0.24593673646450043,0.011428305879235268,0.0724676102399826,-0.013047810643911362,0.018081646412611008,-0.014121285639703274,0.00750051299110055,-0.013511184602975845,-0.015663716942071915,0.010975291021168232,-0.013771725818514824,0.015799034386873245,0.00951027125120163,-0.12452271580696106,-0.11089813709259033,0.01142094749957323,-0.007148229517042637,-0.015153313055634499,0.010475631803274155,-0.00967076513916254,-0.024661241099238396,-0.0019146709237247705,-0.07029953598976135,0.006173660513013601,0.00130002002697438,-0.06499484926462173,0.0058143362402915955,-0.024107709527015686,0.0672040730714798,-0.007166843395680189,0.027964241802692413,-0.004705498926341534,-0.011204089969396591,-0.01149528007954359,0.036704424768686295,-0.04268772155046463,0.021465161815285683,0.02688482403755188,-0.005603816360235214,0.005904897581785917,0.014251536689698696,-0.0048247952945530415,-0.00841717142611742,-0.030222831293940544,0.05570515990257263,0.15783332288265228,-0.0029077879153192043,-0.013279736042022705,0.006261611357331276,-0.0034303239081054926,-0.008986243978142738,-0.004338517785072327,-0.002749864012002945,0.004587458912283182,-0.03217484429478645,0.019904693588614464,0.05330304801464081,0.11668098717927933,0.07740892469882965,-0.016966281458735466,0.00114299519918859,-0.003060737159103155,-0.03000989370048046,0.06922119855880737,-0.002891987096518278,-0.005907096434384584,0.03855481371283531,-0.002681676298379898,0.0007276261458173394,-0.012625539675354958,0.14193429052829742,-0.011389006860554218,0.0024175855796784163,0.0024989501107484102,-0.06170210987329483,-0.004142274148762226,0.0053007029928267,-0.011431713588535786,0.053042422980070114,0.03914254531264305,0.16052965819835663,-0.02241349406540394,0.3769306540489197,0.1330377459526062,0.003628513775765896,-0.0023240672890096903,-0.003510682610794902,-0.0008418576908297837,0.052458345890045166,0.06342268735170364,-0.05779702588915825,0.0009665636462159455,0.011660641990602016,-0.0016172986943274736,0.011291385628283024,-0.007652284577488899,-0.007639882620424032,0.003984332550317049,-0.00881350226700306,-0.014142331667244434,-0.0066144950687885284,0.0019868467934429646,0.0011205708142369986,-0.008894686587154865,0.013787327334284782,0.0067970328964293,0.004437526222318411,0.11156465113162994,0.07290370762348175,0.011270391754806042,-0.00605591107159853,-0.006146933883428574,-0.007420091889798641,0.10880278795957565,-8.844283001963049e-05,0.16606493294239044,0.03330807387828827,-0.005711493082344532,0.015811175107955933,-0.07801679521799088,-0.006777422968298197,0.09255651384592056,-0.017146069556474686,0.009059440344572067,0.01906786859035492,-0.0076634990982711315,-0.04309551790356636,0.033984776586294174,0.029127536341547966,-0.008027986623346806,0.007354626432061195,0.1828276515007019,0.0194369088858366,0.0407400019466877,0.0019024164648726583,0.004665226675570011,-0.025351058691740036,-0.07044241577386856,0.018658621236681938,0.009803936816751957,-0.009771366603672504,0.07509534806013107,0.15327595174312592,0.018968436866998672,-0.007611309643834829,-0.010631338693201542,-0.0017966796876862645,-0.03768572956323624,0.23759528994560242,0.02470596507191658,-0.06615696847438812,-0.006485378369688988,-0.0017880152445286512,0.001523154671303928,-0.024319952353835106,-0.02332352101802826,-0.005580590572208166,0.023989219218492508,0.0034704909194260836,-0.00033376546343788505,0.03359565511345863,-0.000708562380168587,-0.0430237241089344,-0.0070528932847082615,-0.0011050524190068245,-0.0015286998823285103,0.002506842603906989,-0.03358328342437744,-0.003511614864692092,0.0642290785908699,0.040008559823036194,-0.00779785867780447,-0.002303608227521181,0.29567956924438477,-0.0025706375017762184,-0.0029398768674582243,0.0076227448880672455,-0.006286370102316141,0.06972219049930573,0.009650678373873234,-0.0951254591345787,-0.08068142831325531,-0.005132866092026234,0.0361783429980278,-0.0039693680591881275,-0.002510352060198784,-0.009762120433151722,-0.008541019633412361,-0.008257095701992512,0.030181797221302986,-0.003766467794775963,0.1061728373169899,0.05162457004189491,0.003402468515560031,-0.0025937685277312994,0.3230321705341339,-0.06642474979162216,0.06410231441259384,-0.017422324046492577,0.043324559926986694,-0.0021142996847629547,-0.01041413750499487,0.0196029394865036,0.0181856919080019,-0.05249043181538582,-0.03578395023941994,-0.003609477775171399,-0.0006784294382669032,-0.007609120570123196,0.02598446235060692,0.0370706245303154,-0.018633205443620682,0.1923745721578598,-0.0037924800999462605,0.01789742335677147,0.013406919315457344,0.08204102516174316,-0.0022507256362587214,-0.019943196326494217,-0.009909498505294323,-0.0017817311454564333,-0.024083692580461502,-0.011157270520925522,-0.0023527236189693213,0.010085945948958397,-0.0017198718851432204,-0.03498110920190811,-0.014404958114027977,-0.004392370115965605,0.04913190379738808,0.011584466323256493,-0.014011186547577381,-0.02441040240228176,-0.011385753750801086,-0.03652049973607063,0.03036545217037201,-0.005891898181289434,-0.00157821923494339,-0.015826549381017685,-0.0039986963383853436,-0.04266893118619919,-0.008434160612523556,0.03635667264461517,0.006451014894992113,-0.018399642780423164,-0.00980960763990879,0.018338965252041817,-0.006078473292291164,0.22527751326560974,-0.011685337871313095,0.005056838504970074,-0.002161466283723712,0.0965087041258812,0.0011393133318051696,-0.005253040697425604,0.06451068073511124,-0.03725248575210571,0.16766385734081268,-0.09150706231594086,0.008580617606639862,-0.001435588113963604,-0.011402921751141548,-0.003917391877621412,0.007557549048215151,-0.0023886444978415966,-0.08813900500535965,-0.010764949023723602,0.0327359139919281,0.009191558696329594,-0.008000902831554413,-0.018008533865213394,-0.11261468380689621,0.03791925683617592,0.0009747942094691098,-0.04047652333974838,0.005821712780743837,-0.0004936784971505404,0.2557392716407776,-0.007076673209667206,-0.007721791975200176,-0.008724471554160118,0.0043579754419624805,0.006793629378080368,0.10502229630947113,0.0016076330794021487,-0.0061688171699643135,-0.00931484904140234,0.04424995929002762,0.0034328680485486984,0.0011769401608034968,-0.0032385119702667,0.36818036437034607,0.0057825068943202496,-0.025934070348739624,0.008220373652875423,0.012777482159435749,0.004134287126362324,-0.00012134964345023036,0.024717988446354866,-0.010410107672214508,-0.0023795966990292072,-0.0011412777239456773,-0.008064279332756996,0.0036303820088505745,0.004866228438913822,-0.009563934989273548,-0.021202856674790382,0.0003320100950077176,-0.0002730013511609286,0.0065003931522369385,-0.005530638620257378,-0.011651456356048584,0.008563837967813015,0.00023506574507337064,-0.006021418608725071,-0.00835716724395752,-0.004468047991394997,0.006193817127496004,0.03319437429308891,-0.0030667490791529417,0.002814396284520626,0.035355038940906525,-0.028481241315603256,0.05490713194012642,0.010853673331439495,0.11014629900455475,-0.02913879044353962,-0.014762099832296371,0.00031280904659070075,0.000875157187692821,0.13134296238422394,0.0033765232656151056,-0.0004918284248560667,-0.002955836709588766,-0.02269008196890354,0.009330661967396736,-0.005479465238749981,0.009236116893589497,0.007872579619288445,-0.024570589885115623,0.001487296656705439,0.0049378471449017525,0.03555244579911232,-0.021412445232272148,-0.007951798848807812,0.03462604433298111,0.010282321833074093,-0.03979175165295601,-0.005732799414545298,-0.017113735899329185,0.001282449928112328,-0.007210575044155121,0.031060829758644104,-0.001900586299598217,-0.03938841074705124,0.014068374410271645,-0.0033107097260653973,-0.005565598141402006,0.015286793000996113,0.016343284398317337,0.10815134644508362,-0.0016033734427765012,0.46433597803115845,0.017660273239016533,-0.0056843520142138,-0.00874087493866682,-0.017062470316886902,-0.004077388904988766,0.0004820502072107047,-0.006619837135076523,0.0007684517186135054,-0.060957491397857666,-0.010780052281916142,5.508913454832509e-05,-0.0038190658669918776,-0.01061638817191124,0.027053840458393097,-0.04979003593325615,0.14397838711738586,0.0011796506587415934,0.02072049304842949,-0.011525571346282959,0.0031215399503707886,0.0025810080114752054,-0.017662039026618004,0.19520127773284912,0.0024459215346723795,-0.012508242391049862,-0.01442319992929697,-0.008623898960649967,0.007272772025316954,0.02260277047753334,0.009376311674714088,-0.03582313284277916,-0.020936591550707817,-0.0023045360576361418,-0.01696351356804371,-0.004953877069056034,-0.007205745205283165,-0.03199430927634239,-0.0026386796962469816,0.03561324253678322,0.007090986240655184,0.10718517750501633,-0.03511999920010567,-0.010877243243157864,-0.023249084129929543,-0.07879941910505295,-0.0011116330279037356,-0.04503946751356125,0.1708606779575348,-0.002129443222656846,0.08004669845104218,0.004753422923386097,0.14789164066314697,0.12220485508441925,-0.020711520686745644,-0.013182460330426693,-0.0009794860379770398,-0.02741640992462635,0.023379355669021606,0.0010439935140311718,-0.0028736393433064222,0.0008302833302877843,0.004096653778105974,-0.008826665580272675,-0.01931418664753437,-0.04237866401672363,-0.012535720132291317,0.0018907159101217985,0.11748744547367096,0.035596102476119995,-0.0022851808462291956,0.00418534129858017,0.022144149988889694,-0.001404042006470263,-0.0018977478612214327,0.049445122480392456,0.01776270382106304,0.011356511153280735,-0.04441717267036438,-0.009513629600405693,0.027786292135715485,-0.02541246823966503,0.022508660331368446,-0.015743274241685867,0.27291667461395264,-0.009756638668477535,-0.025712283328175545,0.020345909520983696,-0.00871054083108902,-0.004963746760040522,-0.009299900382757187,0.02016942948102951,-0.010220242664217949,0.037694890052080154,-0.0075469911098480225,0.052184876054525375,0.000887289468664676,0.08541423082351685,0.024568550288677216,0.08538856357336044,-0.059511005878448486,0.012458611279726028,-0.015515626408159733,0.010159764438867569,0.06897284835577011,-0.03447938710451126,0.16399632394313812,-0.04998223856091499,0.00020231009693816304,-0.0041260141879320145,-0.06825830042362213,0.06558836996555328,0.01380221452564001,0.0008309902041219175,0.017802491784095764,0.09520471841096878,-0.0487152636051178,-0.021025989204645157,0.006112170871347189,0.12221290171146393,0.002938147634267807,0.04998905956745148,-0.0036118063144385815,-0.06316586583852768,-0.0058440291322767735,-0.03773864731192589,0.0006562479538843036,0.10272473841905594,-0.010360483080148697,0.08872395008802414,0.06351196765899658,-0.001708085648715496,0.04725276306271553,0.02245994657278061,-0.0024894285015761852,0.004841802641749382,0.006264668423682451,0.0008174914983101189,0.013216445222496986,0.00600079633295536,-0.005014002788811922,-0.022229228168725967,-0.015498019754886627,-0.04271545261144638,0.368029922246933,-0.07112966477870941,-0.04165295511484146,-0.00030508710187859833,-0.015243109315633774,-0.0029428601264953613,-0.05135967582464218,-0.030448783189058304,0.022483084350824356,0.0911000669002533,0.012549461796879768,-0.011926796287298203,-0.0021933047100901604,0.13494281470775604,-0.00127229664940387,0.008968133479356766,0.005450271535664797,-0.0047204578295350075,-0.02085125632584095,-0.01964852400124073,-0.005046352278441191,0.009805957786738873,-0.0394960381090641,-0.008213965222239494,-0.008124257437884808,-0.002741971518844366,0.02412932552397251,0.014900324866175652,-0.038762517273426056,0.1382521688938141,0.008772000670433044,-0.0027131461538374424,0.012623153626918793,0.0023084827698767185,0.01243567280471325,-0.018935086205601692,0.19905489683151245,-0.004780257120728493,-0.004849452059715986,0.05828788876533508,0.008615103550255299,0.02895723097026348,-0.008389431051909924,-0.0002348009147681296,0.03394893929362297,-0.00039796048076823354,0.00999914389103651,0.0005597217241302133,-0.0016907774843275547,0.003194719785824418,-0.003161480650305748,-0.005979879759252071,-0.006496640853583813,0.0012029822682961822,-0.005960612092167139,-0.005062459502369165,-0.0016604530392214656,0.008943748660385609,0.006120567210018635,-0.0135439932346344,0.0040636309422552586,0.06569337844848633,0.10855767875909805,0.0027664799708873034,0.015515856444835663,0.09193465858697891,-0.0033126031048595905,-0.003478655591607094,0.0011752006830647588,0.003606056096032262,-0.00381202413700521,0.03139845281839371,-0.004691296257078648,0.008881072513759136,0.0235002264380455,0.00313839060254395,0.02686176635324955,0.004147181287407875,0.000217597174923867,-0.008492023684084415,-0.0010400564642623067,0.00117802107706666,-0.07982265204191208,0.010167456232011318,5.553199662244879e-05,0.021433696150779724,0.004075667355209589,0.008399575017392635,0.0508265495300293,-0.05633857101202011,-0.033397648483514786,0.28793516755104065,-0.028594478964805603,0.12135059386491776,0.0013808369403705,-0.008517998270690441,0.02545752376317978,-0.008553244173526764,-0.0043160999193787575,-0.0020054392516613007,0.008603914640843868,0.06479897350072861,0.004998365417122841,-0.02895604819059372,-0.007717465050518513,0.024201126769185066,0.003905882593244314,-0.013592272996902466,-0.03428139165043831,-0.014637231826782227,0.1490512490272522,-0.00034012843389064074,-0.006364384666085243,-0.017286131158471107,0.01810504123568535,-0.009558781050145626,-0.011136685498058796,-0.010058942250907421,-0.0014265036443248391,-0.013765980489552021,-0.01710091345012188,0.049054425209760666,-0.10371655225753784,-0.007323550991714001,-0.08865638822317123,-0.007064612116664648,0.3285045027732849,0.006808513775467873,-0.0037162145599722862,-0.004848431330174208,-0.008494154550135136,-0.032225124537944794,0.019756073132157326,-0.006351488176733255,0.18079043924808502,0.0026048619765788317,0.03151674196124077,-0.008392182178795338,0.014482584781944752,-0.030743399634957314,-0.002995154820382595,-0.01270469930022955,-0.009532894007861614,-0.0061738500371575356,-0.053123489022254944,0.003180046798661351,0.014878197573125362,0.046956341713666916,-0.0005900323740206659,-0.0008826784323900938,0.012729604728519917,0.056269463151693344,0.01005245465785265,0.02024129591882229,-0.00042419706005603075,0.00648133922368288,0.02463185414671898,-0.0735914558172226,0.005615568719804287,-0.005991941783577204,-0.003632650710642338,-0.004744823090732098,-0.006432231981307268,-0.03376390412449837,0.0007069408893585205,-0.04842748865485191,-0.020728599280118942,-0.004445075057446957,0.0147767448797822,0.05915725603699684,0.08184788376092911,0.045627836138010025,0.031243911013007164,-0.024494560435414314,-0.029142163693904877,0.06907400488853455,0.004327754024416208,0.15357573330402374,-0.00029525364516302943,-0.00040494572021998465,0.0366804301738739,0.017053499817848206,-0.003721823450177908,0.2545783817768097,-0.00221962109208107,-0.013193899765610695,-0.009748036041855812,0.11705754697322845,-0.0021531153470277786,0.0037345578894019127,0.018371790647506714,-0.014920076355338097,0.11165942251682281,0.0878935158252716,0.08338324725627899,-0.010616227053105831,-0.0007391522522084415,0.12551648914813995,-0.0026229247450828552,-0.010789523832499981,-0.005751214921474457,-0.0024054760579019785,0.016988350078463554,-0.03573700785636902,0.003556898795068264,0.007158081512898207,0.05468452349305153,-0.000603493070229888,-0.000842444715090096,0.010537060908973217,-0.012297667562961578,0.00917762890458107,-0.0053700110875070095,0.04751155525445938,0.0005898523377254605,0.004800593946129084,-0.0025314483791589737,0.009670219384133816,0.030652357265353203,-0.06139807030558586,-0.002149915089830756,-0.0037781463470309973,-0.0014558159746229649,0.013892155140638351,0.002187439939007163,0.05376477539539337,0.059534553438425064,0.017411621287465096,0.009198996238410473,0.001760056591592729,-0.010746831074357033,0.0466480478644371,-0.0013649120228365064,-0.11272266507148743,0.0007056379108689725,-0.02396744303405285,-0.003919505979865789,0.022042442113161087,0.003003419144079089,-0.06318529695272446,0.0027488262858241796,0.15614208579063416,0.024798456579446793,0.05312059447169304,-0.018069367855787277,0.019212760031223297,-0.01528739184141159,0.009897335432469845,-0.004436385817825794,-0.10346077382564545,0.0006315588834695518,0.0042404793202877045,0.04455256462097168,-0.0028996518813073635,0.0030962631572037935,0.007907131686806679,2.7400268663768657e-05,0.060295358300209045,0.004758120980113745,0.005688143894076347,0.00676322914659977,-0.01838665083050728,0.01481980923563242,0.006691809743642807,-0.006849246099591255,0.0005701743066310883,-0.029365647584199905,-0.0010683619184419513,-0.005470510572195053,-0.10448227822780609,-0.07623245567083359,0.012111179530620575,-0.2225882112979889,0.1298702210187912,-0.005190649535506964,0.05476167052984238,0.014025951735675335,0.00857254583388567,-0.06545867770910263,0.006106734275817871,-0.018225250765681267,-0.014307031407952309,0.0003084052004851401,-0.014754836447536945,-0.006976374424993992,-0.023238735273480415,-0.017937198281288147,0.03877367451786995,-0.005412362981587648,0.019320078194141388,-0.0034332498908042908,0.13801901042461395,0.0014400392537936568,-0.009429506957530975,-0.0033151591196656227,-0.02926521934568882,-0.0010916549945250154,-0.033753182739019394,0.05348818376660347,0.010852190665900707,-0.045622337609529495,-0.018893059343099594,-0.006555935833603144,-0.006272980011999607,0.011761952191591263,0.008184493519365788,-0.025232350453734398,0.13410350680351257,0.40544581413269043,0.0015167058445513248,-0.04204099625349045,-0.013966966420412064,-0.015834452584385872,0.1067967489361763,0.04291495308279991,0.011779457330703735,-0.001429949072189629,-0.04952361807227135,0.12945152819156647,0.1335470974445343,0.01982252486050129,-0.00035363357164897025,-0.007534808479249477,-0.00411676149815321,0.09163408726453781,0.0774405226111412,0.004003316164016724,-0.020266778767108917,0.03342480957508087,-0.0435444712638855,-0.0015623284270986915,-0.02186496928334236,-0.030464228242635727,-0.03658100590109825,-0.010070175863802433,0.002299197716638446,-0.0012453848030418158,-0.005830487236380577,0.005820601247251034,0.0257840845733881,-0.004663384519517422,0.13734126091003418,0.0014365866081789136,0.0433829128742218,-0.09178528934717178,-0.022411681711673737,-0.007879648357629776,0.06809079647064209,-0.0040582106448709965,0.11829433590173721,-0.0037157570477575064,-0.006518784910440445,0.009547491557896137,0.28181490302085876,-0.035756196826696396,0.14461606740951538,-0.002281556837260723,0.002891390584409237,0.015687663108110428,0.20241858065128326,-0.003139608073979616,-0.0008519576513208449,0.003254883922636509,-0.07853251695632935,-0.018154149875044823,0.015280574560165405,0.17916575074195862,-0.0037144548259675503,-0.0041615343652665615,-0.03089146874845028,0.005945422686636448,0.007137307897210121,-0.0005477950908243656,0.003536880249157548,-0.008505893871188164,-0.006446379702538252,0.00027735542971640825,-0.04193270578980446,0.3338053822517395,0.031810350716114044,0.10544309765100479,0.23975159227848053,0.0005587625782936811,-0.023013848811388016,-0.01071850210428238,0.007370698731392622,0.09768355637788773,0.01971047930419445,0.0016883047064766288,-0.00917544960975647,-0.0036962388549000025,0.046506769955158234,0.011317381635308266,-0.002333947690203786,0.013348739594221115,0.05155550688505173,-0.030291244387626648,-0.00899321399629116,-0.021145114675164223,0.0855623185634613,-0.057683877646923065,5.252078335615806e-05,0.0019291870994493365,0.009595438838005066,-0.02401663362979889,0.004500989336520433,0.02150658704340458,0.02422977052628994,-0.021921928972005844,-0.005051084794104099,0.08100710064172745,-0.002993336645886302,0.03699222579598427,0.0022450059186667204,0.10020679980516434,0.01063156221061945,0.020464206114411354,-0.006552248261868954,-0.018838148564100266,-0.038058001548051834,0.10634129494428635,-0.0074209170415997505,-0.004866218194365501,0.044745124876499176,-0.04774334281682968,0.06545179337263107,0.008351823315024376,0.017093539237976074,-0.0060453698970377445,-0.004145095590502024,0.0014852146850898862,0.005009259097278118,0.001929452526383102,-0.21230286359786987,-0.004228735808283091,0.18628549575805664,-0.03997153788805008,-0.003742232220247388,0.010015047155320644,-0.0016807265346869826,0.021686136722564697,-0.0031222105026245117,0.0012818065006285906,0.011147798970341682,-0.00714830681681633,-0.00883499439805746,0.0018975541461259127,0.008169332519173622,0.0019716338720172644,-0.00515509070828557,-0.0003622272051870823,-0.037656668573617935,-0.00658918684348464,0.0208412054926157,-0.009780173189938068,0.00878075510263443,0.025212695822119713,0.016925333067774773,-0.0030745710246264935,-0.014459687285125256,0.06636550277471542,0.014687002636492252,-0.027812965214252472,-0.005659019574522972,0.018415335565805435,-0.009795209392905235,0.002720228396356106,-0.003281672252342105,-0.027501314878463745,0.07384859025478363,0.25367021560668945,-0.01263529621064663,0.14308375120162964,-0.004755285102874041,-0.04626384377479553,-0.0005542453145608306,0.0006256737979128957,-0.003167901188135147,0.0018583748023957014,0.007266860920935869,0.0007195816142484546,0.005628225859254599,0.002653253497555852,-0.004811942111700773,-0.015579337254166603,-0.0024410313926637173,-0.10080337524414062,0.004706834442913532,-0.031285643577575684,-0.0027609372045844793,0.005819553509354591,-0.06505478918552399,-0.012379378080368042,-0.018037598580121994,-0.0028167259879410267,-0.04295041039586067,0.013854687102138996,-0.047677621245384216,0.26138344407081604,-0.008875313214957714,0.01098692137748003,-0.004627347458153963,0.00456903874874115,-0.017383797094225883,-0.04610520973801613,-0.004826548509299755,0.193433478474617,0.000742267700843513,0.010442436672747135,-0.0028510289266705513,0.0165012888610363,-0.0010042298818007112,-0.0018556040013208985,-0.009705777280032635,0.031213829293847084,0.0019134018803015351,-0.002130404580384493,0.00832712184637785,-0.003673820523545146,0.007053881883621216,-0.011295661330223083,0.30656275153160095,0.0010879304027184844,0.08579948544502258,0.04332021623849869,0.007784802466630936,0.03600413724780083,0.24051912128925323,0.0690382868051529,0.054072629660367966,-0.0035297200083732605,0.09624233096837997,0.006554580759257078,-0.0655292272567749,-0.004928871523588896,-0.08121484518051147,-0.01266222633421421,-0.028307966887950897,0.029365980997681618,-0.03655896335840225,0.047396350651979446,0.016367986798286438,-0.009927487000823021,0.02729138918220997,-0.03237565606832504,-0.06018773838877678,0.003198847873136401,-0.007139565423130989,0.010953920893371105,0.00010566187847871333,-0.00013481680070981383,0.026561906561255455,0.02055240422487259,0.02320883423089981,-0.01769384555518627,0.007691116537898779,0.04747616872191429,-0.0071908035315573215,0.006552349776029587,-0.06908347457647324,0.014700685627758503,0.03726712241768837,0.16564375162124634,-0.0225816797465086,0.04482758790254593,-0.020686747506260872,-0.011316627264022827,-0.006370814051479101,-0.016762185841798782,-0.019496845081448555,0.015307708643376827,0.02765669859945774,0.003505965229123831,0.06109580397605896,-0.017066506668925285,-0.019980134442448616,0.012795533053576946,-0.012051139958202839,0.013357100076973438,0.008846103213727474,-0.0337233692407608,0.21133801341056824,-0.013723395764827728,0.17360681295394897,-0.0150143476203084,0.012557808309793472,-0.02442353405058384,0.003009416628628969,-0.004966390319168568,-0.009103739634156227,-0.006267064716666937,-0.0014554071240127087,0.014199575409293175,-0.03332756459712982,-0.003501723287627101,0.0670798197388649,-0.05746007338166237,-0.05579916387796402,0.0082521578297019,0.005237843841314316,-0.04761631786823273,0.003509849077090621,-0.04927787929773331,0.027307048439979553,-0.004781262017786503,-0.09593001753091812,-0.0027698492631316185,0.008240529336035252,0.031100409105420113,0.005466221831738949,-0.01759127341210842,-0.052050162106752396,0.06866944581270218,-0.006722321268171072,-0.0020299293100833893,0.17670908570289612,-7.762580025882926e-06,-0.0005740740452893078,0.00020726295770145953,0.23863306641578674,0.009297841228544712,-0.006725121755152941,0.025133689865469933,-0.04371579736471176,0.004475854337215424,0.09412465244531631,-0.020594444125890732,-0.07767914980649948,-0.0009654976893216372,0.010739658959209919,0.005194088909775019,0.00643455795943737,-0.024905897676944733,0.07631761580705643,-0.027573103085160255,0.002267657546326518,0.01171517837792635,-0.09409350901842117,0.08068454265594482,-0.0014431275194510818,0.02602120116353035,0.0013530601281672716,0.03953414037823677,0.013417122885584831,-0.0008673043921589851,-0.006039174273610115,-0.008484253659844398,-0.020787326619029045,-0.008483167737722397,-0.004864317364990711,0.021094322204589844,-0.005285999737679958,0.0012628112453967333,-0.011076346971094608,0.007044570986181498,-0.049877382814884186,0.003070265054702759,-0.005625518970191479,-0.026657452806830406,0.0003971998521592468,0.10205654799938202,-0.007883220911026001,-0.01204186026006937,-0.008006732910871506,-0.02623228169977665,0.02230960689485073,0.0006800586124882102,-0.04813557118177414,-0.028131570667028427,-0.005847499240189791,0.3122236728668213,-0.001921644783578813,-0.01942548155784607,0.03049268200993538,-0.015554457902908325,0.032692909240722656,-0.03236415609717369,0.011522967368364334,0.0011694699060171843,-0.0068742018193006516,0.014808828942477703,-0.008516282774508,-0.034259214997291565,0.010230842977762222,0.003968236967921257,-0.016327692195773125,0.10799972712993622,0.0073707145638763905,-0.004085784777998924,0.024297188967466354,-0.005330495070666075,-0.00450188247486949,-0.010111082345247269,-0.05762214958667755,-0.013808288611471653],\"xaxis\":\"x\",\"yaxis\":\"y\",\"type\":\"histogram\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"value\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"count\"}},\"legend\":{\"title\":{\"text\":\"variable\"},\"tracegroupgap\":0},\"title\":{\"text\":\"Difference in layer 5 neuron activations between original and ablated context neuron\"},\"barmode\":\"relative\",\"width\":800},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('30618491-1f09-4b98-b7b2-e7df87c64d2a');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "px.histogram(difference.cpu().numpy(), title=f\"Difference in layer {layer_to_cache} neuron activations between original and ablated context neuron\", width=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>                            <div id=\"ec61e322-624e-46d6-a636-266dc14a9d27\" class=\"plotly-graph-div\" style=\"height:525px; width:800px;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"ec61e322-624e-46d6-a636-266dc14a9d27\")) {                    Plotly.newPlot(                        \"ec61e322-624e-46d6-a636-266dc14a9d27\",                        [{\"coloraxis\":\"coloraxis\",\"name\":\"0\",\"z\":[[-0.004645323380827904,-0.0006135707953944802,0.006211026106029749,-0.0023223008029162884,0.2327786684036255,0.09119820594787598,-0.02565385028719902,-0.005280673503875732,0.4755672812461853,-0.034190110862255096,-0.054232873022556305,0.004980749916285276,-0.019466059282422066,0.03750162571668625,-0.004625342786312103,-0.05223977938294411,-0.00662841834127903,-0.0011434017214924097,0.003507128683850169,-0.0006512617692351341,-0.031298473477363586,0.002459162613376975,0.027653368189930916,0.0016784269828349352,-0.006818631198257208,-0.0013570624869316816,0.12331562489271164,0.006186551880091429,-0.004329038318246603,-0.0005643892800435424,0.13134510815143585,0.13440468907356262,0.04212970659136772,-8.097838144749403e-05,0.019621465355157852,0.13554994761943817,-0.01879565417766571,-0.012909244745969772,-0.006440310273319483,0.010840327478945255,-0.002423811238259077,0.024102535098791122,0.002338350284844637,-0.02377164550125599,0.12799502909183502,-0.010644332505762577,-0.019269417971372604,-0.07665657997131348,-0.030872246250510216,0.026922939345240593,0.07366587221622467,0.0031236561480909586,-0.04073585569858551,-0.006546936929225922,0.013087025843560696,-0.0006540080648846924,0.08187743276357651,-0.004644008353352547,0.00680073956027627,-0.031160127371549606,0.0017055294010788202,-0.02658403292298317,0.0006125970976427197,0.0019007707014679909],[-0.003380240872502327,-0.09584049880504608,-0.02766546793282032,0.05313342064619064,0.20201188325881958,-0.00021276251936797053,-0.005416876636445522,-0.004874255042523146,-0.001717028091661632,-0.013062408193945885,-0.00013391401444096118,0.05430471897125244,0.02404961735010147,0.014560569077730179,0.2670234143733978,-0.018329406157135963,0.026148423552513123,-0.03559798747301102,-0.02308821491897106,-0.02931184135377407,0.03448230400681496,-0.03662554919719696,-0.021771015599370003,0.012280729599297047,0.0069723972119390965,-0.0019983688835054636,0.007716473191976547,0.07439253479242325,-0.0023675377015024424,-0.012216311879456043,0.04376031458377838,0.01786501705646515,0.032304342836141586,-4.884891677647829e-05,-0.015596076846122742,0.0022217207588255405,0.01940593123435974,-0.009608650580048561,-0.008382496424019337,-0.0030829431489109993,-0.027551500126719475,-0.004960605408996344,0.004959185142070055,-0.013233313336968422,0.07829070836305618,0.005535759497433901,-0.009854357689619064,0.019035005941987038,0.07812854647636414,0.01704447716474533,-0.0026784678921103477,-0.007833966054022312,0.0008230480016209185,-0.004492195323109627,-0.029198158532381058,-0.01203115377575159,-0.038675371557474136,-0.004065187647938728,0.007983937859535217,0.013701398856937885,-0.006909351330250502,0.006315912585705519,0.03573121130466461,-0.04256712645292282],[-0.03846442326903343,0.0550394207239151,-0.007610934786498547,0.04226302728056908,-0.0035426546819508076,0.07739276438951492,-0.03821787238121033,0.19483861327171326,0.0840592086315155,-0.026929188519716263,0.13730394840240479,-0.0028363363817334175,-0.0024230959825217724,-0.06524324417114258,0.09647700190544128,-0.004153861664235592,0.03496704250574112,-0.0014769397675991058,0.000716153415851295,-0.015922365710139275,-0.008596647530794144,0.18682272732257843,0.017738336697220802,0.00499457772821188,-0.005662546958774328,-0.004876901861280203,-0.005144450813531876,0.1257522851228714,-0.036264095455408096,-0.001237646210938692,0.08352188766002655,-0.0346485860645771,0.1616830825805664,-0.006481830030679703,0.015121004544198513,-0.011759946122765541,0.46629250049591064,-0.00012570680701173842,0.055384695529937744,0.0033954777754843235,-0.0187497828155756,0.01344127394258976,-0.0011843998217955232,-0.04351057857275009,-0.011929989792406559,-0.020333198830485344,0.002144609345123172,0.010741379112005234,0.035986848175525665,0.01863459125161171,-0.01343497820198536,0.004832802806049585,-0.006559884175658226,0.06058422476053238,0.004469039849936962,-0.04413089528679848,0.002259019995108247,0.029072852805256844,-0.027805430814623833,-0.009250285103917122,-0.09631294757127762,0.00239430065266788,0.028439249843358994,0.001321184798143804],[-0.00038142435369081795,-0.005320667754858732,-0.0008561763679608703,-0.0021416135132312775,0.008763382211327553,-0.00367051106877625,-0.04767953231930733,0.01133044809103012,-0.008641785942018032,-0.0077082132920622826,-0.0720132440328598,-0.02421153150498867,0.07249529659748077,-0.00946598220616579,-0.00855285581201315,2.0396086256369017e-05,-0.023946130648255348,0.0618835911154747,-0.013288420625030994,-0.007933610118925571,-0.006715268827974796,-0.0034554041922092438,0.014431426301598549,0.016366638243198395,-0.045327167958021164,0.008712362498044968,-0.009479145519435406,-0.001403353875502944,0.13572488725185394,-0.002399495802819729,0.07260766625404358,0.04540465399622917,0.02109360694885254,0.00023905295529402792,0.0024470933713018894,0.008172227069735527,0.0027550053782761097,0.022070016711950302,0.018289444968104362,0.013978859409689903,0.00029938362422399223,-0.023739337921142578,0.018198223784565926,-0.006178872659802437,-0.004085725639015436,0.0006666285335086286,-0.008144227787852287,-0.0054969326592981815,0.08197493851184845,0.01602175459265709,0.01348323468118906,-0.026553507894277573,-0.038671765476465225,0.36461880803108215,-0.02884887158870697,0.00888101663440466,0.007274200674146414,-0.15119893848896027,0.005923308897763491,-0.016512800008058548,0.05529538914561272,0.006336717866361141,0.08251772075891495,-0.016786793246865273],[-0.011971934698522091,-0.0009484487818554044,-0.008206126280128956,0.0037469654344022274,0.005253982730209827,-0.003672755789011717,-0.019498152658343315,-0.011908313259482384,0.024486467242240906,-0.03408610820770264,0.0017425530822947621,-0.008616133593022823,-0.002347673522308469,-0.007160406094044447,0.059091322124004364,-0.0020027263090014458,-0.0028564040549099445,0.7151039838790894,-0.0006852666847407818,-0.0024386667646467686,0.083573117852211,0.08594661951065063,-0.012701072730123997,-0.0068289851769804955,0.003838600590825081,0.01020309329032898,-0.041130904108285904,-0.0040390463545918465,-0.007627944927662611,0.00585701409727335,-0.029864469543099403,0.08259961009025574,-0.024698644876480103,-0.000967020052485168,0.00329541671089828,-0.07398667186498642,0.14083249866962433,0.03286178410053253,-0.021708423271775246,0.04532960057258606,0.02075503207743168,0.18267777562141418,0.043853409588336945,0.004376462660729885,-0.0027334270998835564,0.007169906981289387,0.02234051562845707,0.004969336558133364,-0.005891704000532627,-0.06657475978136063,9.28310037124902e-05,0.005743356887251139,-0.04793088883161545,-0.023715943098068237,0.017679058015346527,0.0030600386671721935,0.03275010734796524,0.05318927392363548,-0.10728469491004944,0.2242056131362915,-0.076106958091259,0.00092313316417858,0.011235120706260204,0.01210846845060587],[-0.0010622264817357063,-0.05312846228480339,0.0037233016919344664,0.05737196281552315,-0.034578196704387665,0.012936769984662533,-0.005463656038045883,-0.009469049051404,0.00888145249336958,0.019744567573070526,0.01870867609977722,-0.0022325634490698576,0.12119568139314651,0.040669478476047516,0.025166822597384453,0.0017070136964321136,0.028708485886454582,0.006022738758474588,0.26141679286956787,0.0014207932399585843,-0.06447597593069077,0.016764488071203232,0.11732637882232666,0.004105713684111834,-0.004814429208636284,0.012246796861290932,0.10192477703094482,0.17693564295768738,-0.0007355821435339749,-0.03347811475396156,-0.014609555713832378,-0.006126635707914829,0.0027084294706583023,-0.005466727074235678,0.03005545400083065,-0.03432851657271385,0.007611542474478483,0.005371881648898125,-0.022434458136558533,0.0047711837105453014,0.16913701593875885,0.021934451535344124,0.03904346004128456,0.07397399097681046,-0.005108790006488562,-0.005905330181121826,-0.02482876367866993,0.004662281833589077,-0.0027700013015419245,-0.017945075407624245,0.043566856533288956,-0.004872388206422329,0.05656660348176956,0.024963503703475,-0.058777038007974625,0.01805882342159748,-0.037189047783613205,0.11925248056650162,0.016575461253523827,-0.021097378805279732,0.25930845737457275,0.00939209945499897,0.015815258026123047,0.04103551805019379],[0.019390473142266273,0.053330954164266586,0.029388079419732094,0.15640121698379517,-0.005792062729597092,0.43345460295677185,-0.02450730837881565,-0.001303442637436092,-0.003044743090867996,0.10689829289913177,0.3325088918209076,0.6447742581367493,-0.005973880644887686,0.007946888916194439,-0.004644936416298151,-0.04483106732368469,-0.0028580818325281143,0.05176461488008499,-0.0006939719896763563,0.02337520569562912,0.12098231911659241,0.16007085144519806,-0.00888090394437313,0.08727092295885086,0.011360259726643562,-0.0034168707206845284,-0.005248309578746557,0.2624962627887726,0.004252549260854721,-0.0026407004334032536,0.16106028854846954,0.04277381673455238,-0.00389108550734818,-0.07597167789936066,-0.011579112149775028,0.15368907153606415,0.00022675551008433104,0.10313454270362854,-0.009468460455536842,0.1962377279996872,-0.0016221326077356935,0.00029552963678725064,-0.02555929310619831,0.0021601717453449965,-0.01031428948044777,0.002413794631138444,0.025079447776079178,-0.025905484333634377,-0.0478019118309021,-0.05192553251981735,0.005343184340745211,-0.015992341563105583,0.07725362479686737,-0.005795230157673359,-0.007238119374960661,0.01094044093042612,0.002869050484150648,-0.008423339575529099,0.010094499215483665,0.005537369288504124,0.007910804823040962,0.004955550190061331,-0.006594975478947163,0.19218173623085022],[0.007896769791841507,0.024150965735316277,-0.008427185006439686,-0.09604674577713013,0.0780588760972023,0.25326448678970337,0.047422248870134354,0.1743670254945755,-0.0344657264649868,-0.015459696762263775,-0.018376650288701057,-0.026004843413829803,0.0028442994225770235,-0.02230938710272312,0.004753670189529657,0.010539766401052475,-0.00148924277164042,-0.016027241945266724,0.02781638130545616,-0.007628034334629774,-0.00835263542830944,0.1096518412232399,-0.010711949318647385,-0.039188411086797714,0.10321196168661118,-0.0247778482735157,-0.014221103861927986,-0.0002377372729824856,-0.0005625562043860555,-0.00378999300301075,0.002650839975103736,0.005614641588181257,0.0021122063044458628,-0.0014538391260430217,-0.017902418971061707,-0.008921094238758087,-0.010604092851281166,-0.014301209710538387,0.06111978739500046,-0.012830739840865135,-0.04581701382994652,-0.034180719405412674,-0.0035378679167479277,-0.003915918059647083,-0.022359535098075867,0.18568958342075348,-0.0028292073402553797,-0.016676057130098343,-0.06898065656423569,0.04153300076723099,-0.030075110495090485,0.20476014912128448,-0.012250819243490696,0.025306658819317818,0.07820349186658859,-0.0027492251247167587,-0.019432995468378067,0.21428819000720978,0.01615178771317005,0.026307538151741028,0.01670979894697666,-0.004726966843008995,-0.0039007500745356083,-0.007685080636292696],[0.018467726185917854,-0.0063986810855567455,0.03138565644621849,-0.051885586231946945,-0.03129023686051369,-0.00015455683751497418,-0.006551394239068031,0.04772450774908066,-0.005057564005255699,-0.015551462769508362,0.018300887197256088,-0.0018398023676127195,-0.002238574670627713,-0.014439648017287254,0.02157958596944809,0.013950186781585217,0.00876048393547535,-0.010417478159070015,-0.012676275335252285,-0.033112749457359314,-0.044060077518224716,0.0002095323579851538,-0.006116357631981373,-0.03470306098461151,-0.021999502554535866,0.040735404938459396,0.02515917830169201,-0.02910681441426277,-0.05824805423617363,-0.03459558263421059,-0.04319583997130394,-0.00424543721601367,0.0022184893023222685,-0.03242030739784241,0.017039675265550613,-0.0484451986849308,0.18472568690776825,0.05882516875863075,-0.06909169256687164,-0.010895411483943462,0.06825373321771622,0.010713516734540462,0.15993937849998474,-0.00032455293694511056,0.18790464103221893,-0.015733711421489716,-0.003856375813484192,-0.07407155632972717,-0.011340947821736336,0.014287256635725498,-0.016506236046552658,-0.0024463669396936893,0.004075180739164352,0.0557367280125618,-0.005277235526591539,-0.02991952933371067,0.062955342233181,0.029610877856612206,-0.024953069165349007,0.0002470625622663647,-0.012903396971523762,0.006526927929371595,0.004548841621726751,0.022486066445708275],[0.0007031903369352221,-0.011098256334662437,0.0005035708309151232,0.0032104996498674154,0.06272915750741959,-0.0684456154704094,-0.010339964181184769,0.025569817051291466,0.33163654804229736,0.023786699399352074,0.16603894531726837,0.10121912509202957,-0.001756035489961505,0.07259692996740341,-0.016242336481809616,-0.04860978201031685,-8.345860987901688e-05,-0.003397058229893446,-0.017157841473817825,-0.004535212647169828,0.03417337313294411,-0.03125790134072304,-0.003416928695514798,0.11330369114875793,0.015504159033298492,0.0007054204470477998,0.002143912948668003,0.005326759070158005,0.0042032115161418915,0.010523386299610138,0.10482992976903915,-0.0021694309543818235,0.26951536536216736,-0.017837433144450188,-0.00331664877012372,0.005073220934718847,0.004082296974956989,0.09451381117105484,0.03464705869555473,0.03391594439744949,-0.0052289292216300964,-0.004800704307854176,0.018097663298249245,-0.014803260564804077,-0.004989847540855408,-0.001730074523948133,0.0040077706798911095,0.018346887081861496,0.005145813804119825,-0.0025808000937104225,-0.01082519255578518,0.012089118361473083,-0.005893669091165066,0.007837154902517796,0.016619132831692696,-0.11683531105518341,-0.030878420919179916,0.03874893859028816,0.01964626833796501,-0.0023278628941625357,0.1399078667163849,0.007210419978946447,-0.004307441413402557,0.269916832447052],[-0.0013868447858840227,-0.1695656031370163,-0.014497349038720131,0.14366765320301056,-0.0014249662635847926,-0.007958335801959038,-0.02990604378283024,-0.015408259816467762,0.16690245270729065,0.004093199502676725,0.0113722775131464,0.2574058473110199,-0.03723914921283722,-0.005313652101904154,0.1175064742565155,0.007029347587376833,0.018345093354582787,-0.04573103040456772,0.012454308569431305,0.07771650701761246,-0.009636890143156052,-0.012712844647467136,0.020751796662807465,-0.022131318226456642,0.010591204278171062,0.16774116456508636,-0.002430237829685211,-0.0009518674924038351,-0.0028939773328602314,-0.008413219824433327,0.7021034359931946,-0.004593895748257637,-0.05697479471564293,0.031899407505989075,-0.00564712705090642,-0.012817032635211945,-0.011005566455423832,0.03029026836156845,-0.007993020117282867,-0.019384397193789482,-0.006633098237216473,-0.03597939759492874,0.019736649468541145,0.0033234665170311928,-0.05308502912521362,0.0004870092961937189,-0.006691153161227703,0.004072875715792179,0.0001741456799209118,0.011532062664628029,0.12515853345394135,-0.0024855416268110275,0.0016229311004281044,0.027072101831436157,-0.015455175191164017,-0.01008776668459177,-0.0009218432824127376,0.06798726320266724,-0.00783933512866497,-3.0617207812611014e-05,-0.009295592084527016,-0.07932408154010773,-0.00687395641580224,0.1431867629289627],[-0.00045731739373877645,-0.03998025506734848,-0.0031201064120978117,0.11930657923221588,0.004745644982904196,-0.09319789707660675,-0.0013601406244561076,0.01293825265020132,-0.016451817005872726,-0.03374045714735985,0.0006025142502039671,-0.054400086402893066,-0.0020821986254304647,-0.015911521390080452,-0.013667616061866283,0.009617472998797894,0.002743715886026621,0.00190294801723212,-0.013422864489257336,0.005748935975134373,-0.037115808576345444,0.0012604562798514962,-0.016096405684947968,-0.15017960965633392,-0.07030035555362701,-0.04598486050963402,-0.0540970154106617,0.13845375180244446,-0.0347503237426281,-0.021337419748306274,0.006293866783380508,-0.0015634908340871334,0.0017558600520715117,0.002428863663226366,-0.02030053921043873,-0.0073841772973537445,-0.0044910963624715805,-0.016195237636566162,-0.0016314175445586443,-0.005505475681275129,-0.0011613357346504927,-0.004724542610347271,0.005980738904327154,0.03129478543996811,-0.017269013449549675,-0.029777269810438156,0.018768662586808205,0.4496127665042877,0.00811882596462965,-0.0062787216156721115,-0.028741732239723206,0.14184775948524475,-0.01827649027109146,-0.004730533342808485,0.022858362644910812,-0.039055924862623215,-0.0037741390988230705,0.02216062881052494,-0.006822084076702595,-0.022219838574528694,-0.0031061519403010607,0.015547524206340313,0.0012053503887727857,0.24697650969028473],[0.01726008579134941,0.03361523896455765,-0.020579129457473755,0.0005952127976343036,0.05917824059724808,-0.011360236443579197,-0.0010688354959711432,-0.007384570315480232,0.020344341173768044,0.018285203725099564,0.015983980149030685,0.0032236382830888033,-0.023425282910466194,0.03253170847892761,-0.019351141527295113,0.15788492560386658,-0.0015373859787359834,0.043802183121442795,0.038994546979665756,0.005289250984787941,0.0440250001847744,-0.01019661221653223,0.045799873769283295,-0.02068101242184639,-0.02181634120643139,-0.013764999806880951,0.018948905169963837,0.02005467377603054,-0.04877547174692154,-0.010378369130194187,-0.08346307277679443,-0.019546769559383392,-0.0061738621443510056,-0.005458676256239414,-0.0019041401101276278,-0.00388139090500772,0.028182240203022957,-0.022431911900639534,0.0016829529777169228,0.08426406234502792,-0.02080768533051014,0.0016358850989490747,0.03728378936648369,-0.02933930978178978,0.007680997252464294,0.11926108598709106,-0.0010842502815648913,-0.006671981420367956,-0.0038993319030851126,0.12526939809322357,-0.009594859555363655,0.007146642077714205,0.29807400703430176,0.0034718073438853025,0.01965172588825226,-0.004924194421619177,-0.04352927207946777,-0.006110663991421461,-0.003484979970380664,-0.007376831956207752,-0.003320864401757717,-0.011675694026052952,0.0003005851758643985,0.2064177393913269],[0.08183728158473969,0.028174210339784622,-0.0066987271420657635,0.001033266307786107,-0.0022913780994713306,-0.0033756920602172613,-0.007533141411840916,-0.0018931328086182475,0.019904425367712975,-0.013701756484806538,-0.01549797598272562,0.01763528771698475,0.06595145910978317,0.11633290350437164,-0.013472571969032288,0.24593673646450043,0.011428305879235268,0.0724676102399826,-0.013047810643911362,0.018081646412611008,-0.014121285639703274,0.00750051299110055,-0.013511184602975845,-0.015663716942071915,0.010975291021168232,-0.013771725818514824,0.015799034386873245,0.00951027125120163,-0.12452271580696106,-0.11089813709259033,0.01142094749957323,-0.007148229517042637,-0.015153313055634499,0.010475631803274155,-0.00967076513916254,-0.024661241099238396,-0.0019146709237247705,-0.07029953598976135,0.006173660513013601,0.00130002002697438,-0.06499484926462173,0.0058143362402915955,-0.024107709527015686,0.0672040730714798,-0.007166843395680189,0.027964241802692413,-0.004705498926341534,-0.011204089969396591,-0.01149528007954359,0.036704424768686295,-0.04268772155046463,0.021465161815285683,0.02688482403755188,-0.005603816360235214,0.005904897581785917,0.014251536689698696,-0.0048247952945530415,-0.00841717142611742,-0.030222831293940544,0.05570515990257263,0.15783332288265228,-0.0029077879153192043,-0.013279736042022705,0.006261611357331276],[-0.0034303239081054926,-0.008986243978142738,-0.004338517785072327,-0.002749864012002945,0.004587458912283182,-0.03217484429478645,0.019904693588614464,0.05330304801464081,0.11668098717927933,0.07740892469882965,-0.016966281458735466,0.00114299519918859,-0.003060737159103155,-0.03000989370048046,0.06922119855880737,-0.002891987096518278,-0.005907096434384584,0.03855481371283531,-0.002681676298379898,0.0007276261458173394,-0.012625539675354958,0.14193429052829742,-0.011389006860554218,0.0024175855796784163,0.0024989501107484102,-0.06170210987329483,-0.004142274148762226,0.0053007029928267,-0.011431713588535786,0.053042422980070114,0.03914254531264305,0.16052965819835663,-0.02241349406540394,0.3769306540489197,0.1330377459526062,0.003628513775765896,-0.0023240672890096903,-0.003510682610794902,-0.0008418576908297837,0.052458345890045166,0.06342268735170364,-0.05779702588915825,0.0009665636462159455,0.011660641990602016,-0.0016172986943274736,0.011291385628283024,-0.007652284577488899,-0.007639882620424032,0.003984332550317049,-0.00881350226700306,-0.014142331667244434,-0.0066144950687885284,0.0019868467934429646,0.0011205708142369986,-0.008894686587154865,0.013787327334284782,0.0067970328964293,0.004437526222318411,0.11156465113162994,0.07290370762348175,0.011270391754806042,-0.00605591107159853,-0.006146933883428574,-0.007420091889798641],[0.10880278795957565,-8.844283001963049e-05,0.16606493294239044,0.03330807387828827,-0.005711493082344532,0.015811175107955933,-0.07801679521799088,-0.006777422968298197,0.09255651384592056,-0.017146069556474686,0.009059440344572067,0.01906786859035492,-0.0076634990982711315,-0.04309551790356636,0.033984776586294174,0.029127536341547966,-0.008027986623346806,0.007354626432061195,0.1828276515007019,0.0194369088858366,0.0407400019466877,0.0019024164648726583,0.004665226675570011,-0.025351058691740036,-0.07044241577386856,0.018658621236681938,0.009803936816751957,-0.009771366603672504,0.07509534806013107,0.15327595174312592,0.018968436866998672,-0.007611309643834829,-0.010631338693201542,-0.0017966796876862645,-0.03768572956323624,0.23759528994560242,0.02470596507191658,-0.06615696847438812,-0.006485378369688988,-0.0017880152445286512,0.001523154671303928,-0.024319952353835106,-0.02332352101802826,-0.005580590572208166,0.023989219218492508,0.0034704909194260836,-0.00033376546343788505,0.03359565511345863,-0.000708562380168587,-0.0430237241089344,-0.0070528932847082615,-0.0011050524190068245,-0.0015286998823285103,0.002506842603906989,-0.03358328342437744,-0.003511614864692092,0.0642290785908699,0.040008559823036194,-0.00779785867780447,-0.002303608227521181,0.29567956924438477,-0.0025706375017762184,-0.0029398768674582243,0.0076227448880672455],[-0.006286370102316141,0.06972219049930573,0.009650678373873234,-0.0951254591345787,-0.08068142831325531,-0.005132866092026234,0.0361783429980278,-0.0039693680591881275,-0.002510352060198784,-0.009762120433151722,-0.008541019633412361,-0.008257095701992512,0.030181797221302986,-0.003766467794775963,0.1061728373169899,0.05162457004189491,0.003402468515560031,-0.0025937685277312994,0.3230321705341339,-0.06642474979162216,0.06410231441259384,-0.017422324046492577,0.043324559926986694,-0.0021142996847629547,-0.01041413750499487,0.0196029394865036,0.0181856919080019,-0.05249043181538582,-0.03578395023941994,-0.003609477775171399,-0.0006784294382669032,-0.007609120570123196,0.02598446235060692,0.0370706245303154,-0.018633205443620682,0.1923745721578598,-0.0037924800999462605,0.01789742335677147,0.013406919315457344,0.08204102516174316,-0.0022507256362587214,-0.019943196326494217,-0.009909498505294323,-0.0017817311454564333,-0.024083692580461502,-0.011157270520925522,-0.0023527236189693213,0.010085945948958397,-0.0017198718851432204,-0.03498110920190811,-0.014404958114027977,-0.004392370115965605,0.04913190379738808,0.011584466323256493,-0.014011186547577381,-0.02441040240228176,-0.011385753750801086,-0.03652049973607063,0.03036545217037201,-0.005891898181289434,-0.00157821923494339,-0.015826549381017685,-0.0039986963383853436,-0.04266893118619919],[-0.008434160612523556,0.03635667264461517,0.006451014894992113,-0.018399642780423164,-0.00980960763990879,0.018338965252041817,-0.006078473292291164,0.22527751326560974,-0.011685337871313095,0.005056838504970074,-0.002161466283723712,0.0965087041258812,0.0011393133318051696,-0.005253040697425604,0.06451068073511124,-0.03725248575210571,0.16766385734081268,-0.09150706231594086,0.008580617606639862,-0.001435588113963604,-0.011402921751141548,-0.003917391877621412,0.007557549048215151,-0.0023886444978415966,-0.08813900500535965,-0.010764949023723602,0.0327359139919281,0.009191558696329594,-0.008000902831554413,-0.018008533865213394,-0.11261468380689621,0.03791925683617592,0.0009747942094691098,-0.04047652333974838,0.005821712780743837,-0.0004936784971505404,0.2557392716407776,-0.007076673209667206,-0.007721791975200176,-0.008724471554160118,0.0043579754419624805,0.006793629378080368,0.10502229630947113,0.0016076330794021487,-0.0061688171699643135,-0.00931484904140234,0.04424995929002762,0.0034328680485486984,0.0011769401608034968,-0.0032385119702667,0.36818036437034607,0.0057825068943202496,-0.025934070348739624,0.008220373652875423,0.012777482159435749,0.004134287126362324,-0.00012134964345023036,0.024717988446354866,-0.010410107672214508,-0.0023795966990292072,-0.0011412777239456773,-0.008064279332756996,0.0036303820088505745,0.004866228438913822],[-0.009563934989273548,-0.021202856674790382,0.0003320100950077176,-0.0002730013511609286,0.0065003931522369385,-0.005530638620257378,-0.011651456356048584,0.008563837967813015,0.00023506574507337064,-0.006021418608725071,-0.00835716724395752,-0.004468047991394997,0.006193817127496004,0.03319437429308891,-0.0030667490791529417,0.002814396284520626,0.035355038940906525,-0.028481241315603256,0.05490713194012642,0.010853673331439495,0.11014629900455475,-0.02913879044353962,-0.014762099832296371,0.00031280904659070075,0.000875157187692821,0.13134296238422394,0.0033765232656151056,-0.0004918284248560667,-0.002955836709588766,-0.02269008196890354,0.009330661967396736,-0.005479465238749981,0.009236116893589497,0.007872579619288445,-0.024570589885115623,0.001487296656705439,0.0049378471449017525,0.03555244579911232,-0.021412445232272148,-0.007951798848807812,0.03462604433298111,0.010282321833074093,-0.03979175165295601,-0.005732799414545298,-0.017113735899329185,0.001282449928112328,-0.007210575044155121,0.031060829758644104,-0.001900586299598217,-0.03938841074705124,0.014068374410271645,-0.0033107097260653973,-0.005565598141402006,0.015286793000996113,0.016343284398317337,0.10815134644508362,-0.0016033734427765012,0.46433597803115845,0.017660273239016533,-0.0056843520142138,-0.00874087493866682,-0.017062470316886902,-0.004077388904988766,0.0004820502072107047],[-0.006619837135076523,0.0007684517186135054,-0.060957491397857666,-0.010780052281916142,5.508913454832509e-05,-0.0038190658669918776,-0.01061638817191124,0.027053840458393097,-0.04979003593325615,0.14397838711738586,0.0011796506587415934,0.02072049304842949,-0.011525571346282959,0.0031215399503707886,0.0025810080114752054,-0.017662039026618004,0.19520127773284912,0.0024459215346723795,-0.012508242391049862,-0.01442319992929697,-0.008623898960649967,0.007272772025316954,0.02260277047753334,0.009376311674714088,-0.03582313284277916,-0.020936591550707817,-0.0023045360576361418,-0.01696351356804371,-0.004953877069056034,-0.007205745205283165,-0.03199430927634239,-0.0026386796962469816,0.03561324253678322,0.007090986240655184,0.10718517750501633,-0.03511999920010567,-0.010877243243157864,-0.023249084129929543,-0.07879941910505295,-0.0011116330279037356,-0.04503946751356125,0.1708606779575348,-0.002129443222656846,0.08004669845104218,0.004753422923386097,0.14789164066314697,0.12220485508441925,-0.020711520686745644,-0.013182460330426693,-0.0009794860379770398,-0.02741640992462635,0.023379355669021606,0.0010439935140311718,-0.0028736393433064222,0.0008302833302877843,0.004096653778105974,-0.008826665580272675,-0.01931418664753437,-0.04237866401672363,-0.012535720132291317,0.0018907159101217985,0.11748744547367096,0.035596102476119995,-0.0022851808462291956],[0.00418534129858017,0.022144149988889694,-0.001404042006470263,-0.0018977478612214327,0.049445122480392456,0.01776270382106304,0.011356511153280735,-0.04441717267036438,-0.009513629600405693,0.027786292135715485,-0.02541246823966503,0.022508660331368446,-0.015743274241685867,0.27291667461395264,-0.009756638668477535,-0.025712283328175545,0.020345909520983696,-0.00871054083108902,-0.004963746760040522,-0.009299900382757187,0.02016942948102951,-0.010220242664217949,0.037694890052080154,-0.0075469911098480225,0.052184876054525375,0.000887289468664676,0.08541423082351685,0.024568550288677216,0.08538856357336044,-0.059511005878448486,0.012458611279726028,-0.015515626408159733,0.010159764438867569,0.06897284835577011,-0.03447938710451126,0.16399632394313812,-0.04998223856091499,0.00020231009693816304,-0.0041260141879320145,-0.06825830042362213,0.06558836996555328,0.01380221452564001,0.0008309902041219175,0.017802491784095764,0.09520471841096878,-0.0487152636051178,-0.021025989204645157,0.006112170871347189,0.12221290171146393,0.002938147634267807,0.04998905956745148,-0.0036118063144385815,-0.06316586583852768,-0.0058440291322767735,-0.03773864731192589,0.0006562479538843036,0.10272473841905594,-0.010360483080148697,0.08872395008802414,0.06351196765899658,-0.001708085648715496,0.04725276306271553,0.02245994657278061,-0.0024894285015761852],[0.004841802641749382,0.006264668423682451,0.0008174914983101189,0.013216445222496986,0.00600079633295536,-0.005014002788811922,-0.022229228168725967,-0.015498019754886627,-0.04271545261144638,0.368029922246933,-0.07112966477870941,-0.04165295511484146,-0.00030508710187859833,-0.015243109315633774,-0.0029428601264953613,-0.05135967582464218,-0.030448783189058304,0.022483084350824356,0.0911000669002533,0.012549461796879768,-0.011926796287298203,-0.0021933047100901604,0.13494281470775604,-0.00127229664940387,0.008968133479356766,0.005450271535664797,-0.0047204578295350075,-0.02085125632584095,-0.01964852400124073,-0.005046352278441191,0.009805957786738873,-0.0394960381090641,-0.008213965222239494,-0.008124257437884808,-0.002741971518844366,0.02412932552397251,0.014900324866175652,-0.038762517273426056,0.1382521688938141,0.008772000670433044,-0.0027131461538374424,0.012623153626918793,0.0023084827698767185,0.01243567280471325,-0.018935086205601692,0.19905489683151245,-0.004780257120728493,-0.004849452059715986,0.05828788876533508,0.008615103550255299,0.02895723097026348,-0.008389431051909924,-0.0002348009147681296,0.03394893929362297,-0.00039796048076823354,0.00999914389103651,0.0005597217241302133,-0.0016907774843275547,0.003194719785824418,-0.003161480650305748,-0.005979879759252071,-0.006496640853583813,0.0012029822682961822,-0.005960612092167139],[-0.005062459502369165,-0.0016604530392214656,0.008943748660385609,0.006120567210018635,-0.0135439932346344,0.0040636309422552586,0.06569337844848633,0.10855767875909805,0.0027664799708873034,0.015515856444835663,0.09193465858697891,-0.0033126031048595905,-0.003478655591607094,0.0011752006830647588,0.003606056096032262,-0.00381202413700521,0.03139845281839371,-0.004691296257078648,0.008881072513759136,0.0235002264380455,0.00313839060254395,0.02686176635324955,0.004147181287407875,0.000217597174923867,-0.008492023684084415,-0.0010400564642623067,0.00117802107706666,-0.07982265204191208,0.010167456232011318,5.553199662244879e-05,0.021433696150779724,0.004075667355209589,0.008399575017392635,0.0508265495300293,-0.05633857101202011,-0.033397648483514786,0.28793516755104065,-0.028594478964805603,0.12135059386491776,0.0013808369403705,-0.008517998270690441,0.02545752376317978,-0.008553244173526764,-0.0043160999193787575,-0.0020054392516613007,0.008603914640843868,0.06479897350072861,0.004998365417122841,-0.02895604819059372,-0.007717465050518513,0.024201126769185066,0.003905882593244314,-0.013592272996902466,-0.03428139165043831,-0.014637231826782227,0.1490512490272522,-0.00034012843389064074,-0.006364384666085243,-0.017286131158471107,0.01810504123568535,-0.009558781050145626,-0.011136685498058796,-0.010058942250907421,-0.0014265036443248391],[-0.013765980489552021,-0.01710091345012188,0.049054425209760666,-0.10371655225753784,-0.007323550991714001,-0.08865638822317123,-0.007064612116664648,0.3285045027732849,0.006808513775467873,-0.0037162145599722862,-0.004848431330174208,-0.008494154550135136,-0.032225124537944794,0.019756073132157326,-0.006351488176733255,0.18079043924808502,0.0026048619765788317,0.03151674196124077,-0.008392182178795338,0.014482584781944752,-0.030743399634957314,-0.002995154820382595,-0.01270469930022955,-0.009532894007861614,-0.0061738500371575356,-0.053123489022254944,0.003180046798661351,0.014878197573125362,0.046956341713666916,-0.0005900323740206659,-0.0008826784323900938,0.012729604728519917,0.056269463151693344,0.01005245465785265,0.02024129591882229,-0.00042419706005603075,0.00648133922368288,0.02463185414671898,-0.0735914558172226,0.005615568719804287,-0.005991941783577204,-0.003632650710642338,-0.004744823090732098,-0.006432231981307268,-0.03376390412449837,0.0007069408893585205,-0.04842748865485191,-0.020728599280118942,-0.004445075057446957,0.0147767448797822,0.05915725603699684,0.08184788376092911,0.045627836138010025,0.031243911013007164,-0.024494560435414314,-0.029142163693904877,0.06907400488853455,0.004327754024416208,0.15357573330402374,-0.00029525364516302943,-0.00040494572021998465,0.0366804301738739,0.017053499817848206,-0.003721823450177908],[0.2545783817768097,-0.00221962109208107,-0.013193899765610695,-0.009748036041855812,0.11705754697322845,-0.0021531153470277786,0.0037345578894019127,0.018371790647506714,-0.014920076355338097,0.11165942251682281,0.0878935158252716,0.08338324725627899,-0.010616227053105831,-0.0007391522522084415,0.12551648914813995,-0.0026229247450828552,-0.010789523832499981,-0.005751214921474457,-0.0024054760579019785,0.016988350078463554,-0.03573700785636902,0.003556898795068264,0.007158081512898207,0.05468452349305153,-0.000603493070229888,-0.000842444715090096,0.010537060908973217,-0.012297667562961578,0.00917762890458107,-0.0053700110875070095,0.04751155525445938,0.0005898523377254605,0.004800593946129084,-0.0025314483791589737,0.009670219384133816,0.030652357265353203,-0.06139807030558586,-0.002149915089830756,-0.0037781463470309973,-0.0014558159746229649,0.013892155140638351,0.002187439939007163,0.05376477539539337,0.059534553438425064,0.017411621287465096,0.009198996238410473,0.001760056591592729,-0.010746831074357033,0.0466480478644371,-0.0013649120228365064,-0.11272266507148743,0.0007056379108689725,-0.02396744303405285,-0.003919505979865789,0.022042442113161087,0.003003419144079089,-0.06318529695272446,0.0027488262858241796,0.15614208579063416,0.024798456579446793,0.05312059447169304,-0.018069367855787277,0.019212760031223297,-0.01528739184141159],[0.009897335432469845,-0.004436385817825794,-0.10346077382564545,0.0006315588834695518,0.0042404793202877045,0.04455256462097168,-0.0028996518813073635,0.0030962631572037935,0.007907131686806679,2.7400268663768657e-05,0.060295358300209045,0.004758120980113745,0.005688143894076347,0.00676322914659977,-0.01838665083050728,0.01481980923563242,0.006691809743642807,-0.006849246099591255,0.0005701743066310883,-0.029365647584199905,-0.0010683619184419513,-0.005470510572195053,-0.10448227822780609,-0.07623245567083359,0.012111179530620575,-0.2225882112979889,0.1298702210187912,-0.005190649535506964,0.05476167052984238,0.014025951735675335,0.00857254583388567,-0.06545867770910263,0.006106734275817871,-0.018225250765681267,-0.014307031407952309,0.0003084052004851401,-0.014754836447536945,-0.006976374424993992,-0.023238735273480415,-0.017937198281288147,0.03877367451786995,-0.005412362981587648,0.019320078194141388,-0.0034332498908042908,0.13801901042461395,0.0014400392537936568,-0.009429506957530975,-0.0033151591196656227,-0.02926521934568882,-0.0010916549945250154,-0.033753182739019394,0.05348818376660347,0.010852190665900707,-0.045622337609529495,-0.018893059343099594,-0.006555935833603144,-0.006272980011999607,0.011761952191591263,0.008184493519365788,-0.025232350453734398,0.13410350680351257,0.40544581413269043,0.0015167058445513248,-0.04204099625349045],[-0.013966966420412064,-0.015834452584385872,0.1067967489361763,0.04291495308279991,0.011779457330703735,-0.001429949072189629,-0.04952361807227135,0.12945152819156647,0.1335470974445343,0.01982252486050129,-0.00035363357164897025,-0.007534808479249477,-0.00411676149815321,0.09163408726453781,0.0774405226111412,0.004003316164016724,-0.020266778767108917,0.03342480957508087,-0.0435444712638855,-0.0015623284270986915,-0.02186496928334236,-0.030464228242635727,-0.03658100590109825,-0.010070175863802433,0.002299197716638446,-0.0012453848030418158,-0.005830487236380577,0.005820601247251034,0.0257840845733881,-0.004663384519517422,0.13734126091003418,0.0014365866081789136,0.0433829128742218,-0.09178528934717178,-0.022411681711673737,-0.007879648357629776,0.06809079647064209,-0.0040582106448709965,0.11829433590173721,-0.0037157570477575064,-0.006518784910440445,0.009547491557896137,0.28181490302085876,-0.035756196826696396,0.14461606740951538,-0.002281556837260723,0.002891390584409237,0.015687663108110428,0.20241858065128326,-0.003139608073979616,-0.0008519576513208449,0.003254883922636509,-0.07853251695632935,-0.018154149875044823,0.015280574560165405,0.17916575074195862,-0.0037144548259675503,-0.0041615343652665615,-0.03089146874845028,0.005945422686636448,0.007137307897210121,-0.0005477950908243656,0.003536880249157548,-0.008505893871188164],[-0.006446379702538252,0.00027735542971640825,-0.04193270578980446,0.3338053822517395,0.031810350716114044,0.10544309765100479,0.23975159227848053,0.0005587625782936811,-0.023013848811388016,-0.01071850210428238,0.007370698731392622,0.09768355637788773,0.01971047930419445,0.0016883047064766288,-0.00917544960975647,-0.0036962388549000025,0.046506769955158234,0.011317381635308266,-0.002333947690203786,0.013348739594221115,0.05155550688505173,-0.030291244387626648,-0.00899321399629116,-0.021145114675164223,0.0855623185634613,-0.057683877646923065,5.252078335615806e-05,0.0019291870994493365,0.009595438838005066,-0.02401663362979889,0.004500989336520433,0.02150658704340458,0.02422977052628994,-0.021921928972005844,-0.005051084794104099,0.08100710064172745,-0.002993336645886302,0.03699222579598427,0.0022450059186667204,0.10020679980516434,0.01063156221061945,0.020464206114411354,-0.006552248261868954,-0.018838148564100266,-0.038058001548051834,0.10634129494428635,-0.0074209170415997505,-0.004866218194365501,0.044745124876499176,-0.04774334281682968,0.06545179337263107,0.008351823315024376,0.017093539237976074,-0.0060453698970377445,-0.004145095590502024,0.0014852146850898862,0.005009259097278118,0.001929452526383102,-0.21230286359786987,-0.004228735808283091,0.18628549575805664,-0.03997153788805008,-0.003742232220247388,0.010015047155320644],[-0.0016807265346869826,0.021686136722564697,-0.0031222105026245117,0.0012818065006285906,0.011147798970341682,-0.00714830681681633,-0.00883499439805746,0.0018975541461259127,0.008169332519173622,0.0019716338720172644,-0.00515509070828557,-0.0003622272051870823,-0.037656668573617935,-0.00658918684348464,0.0208412054926157,-0.009780173189938068,0.00878075510263443,0.025212695822119713,0.016925333067774773,-0.0030745710246264935,-0.014459687285125256,0.06636550277471542,0.014687002636492252,-0.027812965214252472,-0.005659019574522972,0.018415335565805435,-0.009795209392905235,0.002720228396356106,-0.003281672252342105,-0.027501314878463745,0.07384859025478363,0.25367021560668945,-0.01263529621064663,0.14308375120162964,-0.004755285102874041,-0.04626384377479553,-0.0005542453145608306,0.0006256737979128957,-0.003167901188135147,0.0018583748023957014,0.007266860920935869,0.0007195816142484546,0.005628225859254599,0.002653253497555852,-0.004811942111700773,-0.015579337254166603,-0.0024410313926637173,-0.10080337524414062,0.004706834442913532,-0.031285643577575684,-0.0027609372045844793,0.005819553509354591,-0.06505478918552399,-0.012379378080368042,-0.018037598580121994,-0.0028167259879410267,-0.04295041039586067,0.013854687102138996,-0.047677621245384216,0.26138344407081604,-0.008875313214957714,0.01098692137748003,-0.004627347458153963,0.00456903874874115],[-0.017383797094225883,-0.04610520973801613,-0.004826548509299755,0.193433478474617,0.000742267700843513,0.010442436672747135,-0.0028510289266705513,0.0165012888610363,-0.0010042298818007112,-0.0018556040013208985,-0.009705777280032635,0.031213829293847084,0.0019134018803015351,-0.002130404580384493,0.00832712184637785,-0.003673820523545146,0.007053881883621216,-0.011295661330223083,0.30656275153160095,0.0010879304027184844,0.08579948544502258,0.04332021623849869,0.007784802466630936,0.03600413724780083,0.24051912128925323,0.0690382868051529,0.054072629660367966,-0.0035297200083732605,0.09624233096837997,0.006554580759257078,-0.0655292272567749,-0.004928871523588896,-0.08121484518051147,-0.01266222633421421,-0.028307966887950897,0.029365980997681618,-0.03655896335840225,0.047396350651979446,0.016367986798286438,-0.009927487000823021,0.02729138918220997,-0.03237565606832504,-0.06018773838877678,0.003198847873136401,-0.007139565423130989,0.010953920893371105,0.00010566187847871333,-0.00013481680070981383,0.026561906561255455,0.02055240422487259,0.02320883423089981,-0.01769384555518627,0.007691116537898779,0.04747616872191429,-0.0071908035315573215,0.006552349776029587,-0.06908347457647324,0.014700685627758503,0.03726712241768837,0.16564375162124634,-0.0225816797465086,0.04482758790254593,-0.020686747506260872,-0.011316627264022827],[-0.006370814051479101,-0.016762185841798782,-0.019496845081448555,0.015307708643376827,0.02765669859945774,0.003505965229123831,0.06109580397605896,-0.017066506668925285,-0.019980134442448616,0.012795533053576946,-0.012051139958202839,0.013357100076973438,0.008846103213727474,-0.0337233692407608,0.21133801341056824,-0.013723395764827728,0.17360681295394897,-0.0150143476203084,0.012557808309793472,-0.02442353405058384,0.003009416628628969,-0.004966390319168568,-0.009103739634156227,-0.006267064716666937,-0.0014554071240127087,0.014199575409293175,-0.03332756459712982,-0.003501723287627101,0.0670798197388649,-0.05746007338166237,-0.05579916387796402,0.0082521578297019,0.005237843841314316,-0.04761631786823273,0.003509849077090621,-0.04927787929773331,0.027307048439979553,-0.004781262017786503,-0.09593001753091812,-0.0027698492631316185,0.008240529336035252,0.031100409105420113,0.005466221831738949,-0.01759127341210842,-0.052050162106752396,0.06866944581270218,-0.006722321268171072,-0.0020299293100833893,0.17670908570289612,-7.762580025882926e-06,-0.0005740740452893078,0.00020726295770145953,0.23863306641578674,0.009297841228544712,-0.006725121755152941,0.025133689865469933,-0.04371579736471176,0.004475854337215424,0.09412465244531631,-0.020594444125890732,-0.07767914980649948,-0.0009654976893216372,0.010739658959209919,0.005194088909775019],[0.00643455795943737,-0.024905897676944733,0.07631761580705643,-0.027573103085160255,0.002267657546326518,0.01171517837792635,-0.09409350901842117,0.08068454265594482,-0.0014431275194510818,0.02602120116353035,0.0013530601281672716,0.03953414037823677,0.013417122885584831,-0.0008673043921589851,-0.006039174273610115,-0.008484253659844398,-0.020787326619029045,-0.008483167737722397,-0.004864317364990711,0.021094322204589844,-0.005285999737679958,0.0012628112453967333,-0.011076346971094608,0.007044570986181498,-0.049877382814884186,0.003070265054702759,-0.005625518970191479,-0.026657452806830406,0.0003971998521592468,0.10205654799938202,-0.007883220911026001,-0.01204186026006937,-0.008006732910871506,-0.02623228169977665,0.02230960689485073,0.0006800586124882102,-0.04813557118177414,-0.028131570667028427,-0.005847499240189791,0.3122236728668213,-0.001921644783578813,-0.01942548155784607,0.03049268200993538,-0.015554457902908325,0.032692909240722656,-0.03236415609717369,0.011522967368364334,0.0011694699060171843,-0.0068742018193006516,0.014808828942477703,-0.008516282774508,-0.034259214997291565,0.010230842977762222,0.003968236967921257,-0.016327692195773125,0.10799972712993622,0.0073707145638763905,-0.004085784777998924,0.024297188967466354,-0.005330495070666075,-0.00450188247486949,-0.010111082345247269,-0.05762214958667755,-0.013808288611471653]],\"type\":\"heatmap\",\"xaxis\":\"x\",\"yaxis\":\"y\",\"hovertemplate\":\"Difference: %{z:.4f}\\u003cbr\\u003eNeuron: %{customdata}\",\"customdata\":[[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63],[64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127],[128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191],[192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255],[256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319],[320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383],[384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447],[448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510,511],[512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542,543,544,545,546,547,548,549,550,551,552,553,554,555,556,557,558,559,560,561,562,563,564,565,566,567,568,569,570,571,572,573,574,575],[576,577,578,579,580,581,582,583,584,585,586,587,588,589,590,591,592,593,594,595,596,597,598,599,600,601,602,603,604,605,606,607,608,609,610,611,612,613,614,615,616,617,618,619,620,621,622,623,624,625,626,627,628,629,630,631,632,633,634,635,636,637,638,639],[640,641,642,643,644,645,646,647,648,649,650,651,652,653,654,655,656,657,658,659,660,661,662,663,664,665,666,667,668,669,670,671,672,673,674,675,676,677,678,679,680,681,682,683,684,685,686,687,688,689,690,691,692,693,694,695,696,697,698,699,700,701,702,703],[704,705,706,707,708,709,710,711,712,713,714,715,716,717,718,719,720,721,722,723,724,725,726,727,728,729,730,731,732,733,734,735,736,737,738,739,740,741,742,743,744,745,746,747,748,749,750,751,752,753,754,755,756,757,758,759,760,761,762,763,764,765,766,767],[768,769,770,771,772,773,774,775,776,777,778,779,780,781,782,783,784,785,786,787,788,789,790,791,792,793,794,795,796,797,798,799,800,801,802,803,804,805,806,807,808,809,810,811,812,813,814,815,816,817,818,819,820,821,822,823,824,825,826,827,828,829,830,831],[832,833,834,835,836,837,838,839,840,841,842,843,844,845,846,847,848,849,850,851,852,853,854,855,856,857,858,859,860,861,862,863,864,865,866,867,868,869,870,871,872,873,874,875,876,877,878,879,880,881,882,883,884,885,886,887,888,889,890,891,892,893,894,895],[896,897,898,899,900,901,902,903,904,905,906,907,908,909,910,911,912,913,914,915,916,917,918,919,920,921,922,923,924,925,926,927,928,929,930,931,932,933,934,935,936,937,938,939,940,941,942,943,944,945,946,947,948,949,950,951,952,953,954,955,956,957,958,959],[960,961,962,963,964,965,966,967,968,969,970,971,972,973,974,975,976,977,978,979,980,981,982,983,984,985,986,987,988,989,990,991,992,993,994,995,996,997,998,999,1000,1001,1002,1003,1004,1005,1006,1007,1008,1009,1010,1011,1012,1013,1014,1015,1016,1017,1018,1019,1020,1021,1022,1023],[1024,1025,1026,1027,1028,1029,1030,1031,1032,1033,1034,1035,1036,1037,1038,1039,1040,1041,1042,1043,1044,1045,1046,1047,1048,1049,1050,1051,1052,1053,1054,1055,1056,1057,1058,1059,1060,1061,1062,1063,1064,1065,1066,1067,1068,1069,1070,1071,1072,1073,1074,1075,1076,1077,1078,1079,1080,1081,1082,1083,1084,1085,1086,1087],[1088,1089,1090,1091,1092,1093,1094,1095,1096,1097,1098,1099,1100,1101,1102,1103,1104,1105,1106,1107,1108,1109,1110,1111,1112,1113,1114,1115,1116,1117,1118,1119,1120,1121,1122,1123,1124,1125,1126,1127,1128,1129,1130,1131,1132,1133,1134,1135,1136,1137,1138,1139,1140,1141,1142,1143,1144,1145,1146,1147,1148,1149,1150,1151],[1152,1153,1154,1155,1156,1157,1158,1159,1160,1161,1162,1163,1164,1165,1166,1167,1168,1169,1170,1171,1172,1173,1174,1175,1176,1177,1178,1179,1180,1181,1182,1183,1184,1185,1186,1187,1188,1189,1190,1191,1192,1193,1194,1195,1196,1197,1198,1199,1200,1201,1202,1203,1204,1205,1206,1207,1208,1209,1210,1211,1212,1213,1214,1215],[1216,1217,1218,1219,1220,1221,1222,1223,1224,1225,1226,1227,1228,1229,1230,1231,1232,1233,1234,1235,1236,1237,1238,1239,1240,1241,1242,1243,1244,1245,1246,1247,1248,1249,1250,1251,1252,1253,1254,1255,1256,1257,1258,1259,1260,1261,1262,1263,1264,1265,1266,1267,1268,1269,1270,1271,1272,1273,1274,1275,1276,1277,1278,1279],[1280,1281,1282,1283,1284,1285,1286,1287,1288,1289,1290,1291,1292,1293,1294,1295,1296,1297,1298,1299,1300,1301,1302,1303,1304,1305,1306,1307,1308,1309,1310,1311,1312,1313,1314,1315,1316,1317,1318,1319,1320,1321,1322,1323,1324,1325,1326,1327,1328,1329,1330,1331,1332,1333,1334,1335,1336,1337,1338,1339,1340,1341,1342,1343],[1344,1345,1346,1347,1348,1349,1350,1351,1352,1353,1354,1355,1356,1357,1358,1359,1360,1361,1362,1363,1364,1365,1366,1367,1368,1369,1370,1371,1372,1373,1374,1375,1376,1377,1378,1379,1380,1381,1382,1383,1384,1385,1386,1387,1388,1389,1390,1391,1392,1393,1394,1395,1396,1397,1398,1399,1400,1401,1402,1403,1404,1405,1406,1407],[1408,1409,1410,1411,1412,1413,1414,1415,1416,1417,1418,1419,1420,1421,1422,1423,1424,1425,1426,1427,1428,1429,1430,1431,1432,1433,1434,1435,1436,1437,1438,1439,1440,1441,1442,1443,1444,1445,1446,1447,1448,1449,1450,1451,1452,1453,1454,1455,1456,1457,1458,1459,1460,1461,1462,1463,1464,1465,1466,1467,1468,1469,1470,1471],[1472,1473,1474,1475,1476,1477,1478,1479,1480,1481,1482,1483,1484,1485,1486,1487,1488,1489,1490,1491,1492,1493,1494,1495,1496,1497,1498,1499,1500,1501,1502,1503,1504,1505,1506,1507,1508,1509,1510,1511,1512,1513,1514,1515,1516,1517,1518,1519,1520,1521,1522,1523,1524,1525,1526,1527,1528,1529,1530,1531,1532,1533,1534,1535],[1536,1537,1538,1539,1540,1541,1542,1543,1544,1545,1546,1547,1548,1549,1550,1551,1552,1553,1554,1555,1556,1557,1558,1559,1560,1561,1562,1563,1564,1565,1566,1567,1568,1569,1570,1571,1572,1573,1574,1575,1576,1577,1578,1579,1580,1581,1582,1583,1584,1585,1586,1587,1588,1589,1590,1591,1592,1593,1594,1595,1596,1597,1598,1599],[1600,1601,1602,1603,1604,1605,1606,1607,1608,1609,1610,1611,1612,1613,1614,1615,1616,1617,1618,1619,1620,1621,1622,1623,1624,1625,1626,1627,1628,1629,1630,1631,1632,1633,1634,1635,1636,1637,1638,1639,1640,1641,1642,1643,1644,1645,1646,1647,1648,1649,1650,1651,1652,1653,1654,1655,1656,1657,1658,1659,1660,1661,1662,1663],[1664,1665,1666,1667,1668,1669,1670,1671,1672,1673,1674,1675,1676,1677,1678,1679,1680,1681,1682,1683,1684,1685,1686,1687,1688,1689,1690,1691,1692,1693,1694,1695,1696,1697,1698,1699,1700,1701,1702,1703,1704,1705,1706,1707,1708,1709,1710,1711,1712,1713,1714,1715,1716,1717,1718,1719,1720,1721,1722,1723,1724,1725,1726,1727],[1728,1729,1730,1731,1732,1733,1734,1735,1736,1737,1738,1739,1740,1741,1742,1743,1744,1745,1746,1747,1748,1749,1750,1751,1752,1753,1754,1755,1756,1757,1758,1759,1760,1761,1762,1763,1764,1765,1766,1767,1768,1769,1770,1771,1772,1773,1774,1775,1776,1777,1778,1779,1780,1781,1782,1783,1784,1785,1786,1787,1788,1789,1790,1791],[1792,1793,1794,1795,1796,1797,1798,1799,1800,1801,1802,1803,1804,1805,1806,1807,1808,1809,1810,1811,1812,1813,1814,1815,1816,1817,1818,1819,1820,1821,1822,1823,1824,1825,1826,1827,1828,1829,1830,1831,1832,1833,1834,1835,1836,1837,1838,1839,1840,1841,1842,1843,1844,1845,1846,1847,1848,1849,1850,1851,1852,1853,1854,1855],[1856,1857,1858,1859,1860,1861,1862,1863,1864,1865,1866,1867,1868,1869,1870,1871,1872,1873,1874,1875,1876,1877,1878,1879,1880,1881,1882,1883,1884,1885,1886,1887,1888,1889,1890,1891,1892,1893,1894,1895,1896,1897,1898,1899,1900,1901,1902,1903,1904,1905,1906,1907,1908,1909,1910,1911,1912,1913,1914,1915,1916,1917,1918,1919],[1920,1921,1922,1923,1924,1925,1926,1927,1928,1929,1930,1931,1932,1933,1934,1935,1936,1937,1938,1939,1940,1941,1942,1943,1944,1945,1946,1947,1948,1949,1950,1951,1952,1953,1954,1955,1956,1957,1958,1959,1960,1961,1962,1963,1964,1965,1966,1967,1968,1969,1970,1971,1972,1973,1974,1975,1976,1977,1978,1979,1980,1981,1982,1983],[1984,1985,1986,1987,1988,1989,1990,1991,1992,1993,1994,1995,1996,1997,1998,1999,2000,2001,2002,2003,2004,2005,2006,2007,2008,2009,2010,2011,2012,2013,2014,2015,2016,2017,2018,2019,2020,2021,2022,2023,2024,2025,2026,2027,2028,2029,2030,2031,2032,2033,2034,2035,2036,2037,2038,2039,2040,2041,2042,2043,2044,2045,2046,2047]]}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"scaleanchor\":\"y\",\"constrain\":\"domain\",\"visible\":false},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"autorange\":\"reversed\",\"constrain\":\"domain\",\"visible\":false},\"coloraxis\":{\"colorscale\":[[0.0,\"rgb(103,0,31)\"],[0.1,\"rgb(178,24,43)\"],[0.2,\"rgb(214,96,77)\"],[0.3,\"rgb(244,165,130)\"],[0.4,\"rgb(253,219,199)\"],[0.5,\"rgb(247,247,247)\"],[0.6,\"rgb(209,229,240)\"],[0.7,\"rgb(146,197,222)\"],[0.8,\"rgb(67,147,195)\"],[0.9,\"rgb(33,102,172)\"],[1.0,\"rgb(5,48,97)\"]],\"cmid\":0.0},\"title\":{\"text\":\"Difference in activations between original and ablated model at MLP layer 5 \\u003cbr\\u003e rearranged from a 1D vector into a grid\"},\"width\":800},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('ec61e322-624e-46d6-a636-266dc14a9d27');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def imshow(tensor, renderer=None, label_neurons=False, **kwargs):\n",
    "    preset_kwargs = {\n",
    "        \"color_continuous_midpoint\": 0.0,\n",
    "        \"color_continuous_scale\": \"RdBu\"\n",
    "    }\n",
    "\n",
    "    fig = px.imshow(utils.to_numpy(tensor), **{**preset_kwargs, **kwargs})\n",
    "    fig.update_xaxes(visible=False)\n",
    "    fig.update_yaxes(visible=False)\n",
    "    if label_neurons:\n",
    "        fig.update(data=[{'customdata': np.arange(len(tensor.flatten())).reshape(tensor.shape[0], -1), 'hovertemplate': \"Difference: %{z:.4f}<br>Neuron: %{customdata}\"}])\n",
    "    fig.show(renderer=renderer)\n",
    "\n",
    "imshow(difference.view(32, -1), label_neurons=True, title=\"\"\"Difference in activations between original and ablated model at MLP layer 5 <br> rearranged from a 1D vector into a grid\"\"\", width=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>                            <div id=\"25427d0e-87c8-4728-a0f8-13dd2a5974e0\" class=\"plotly-graph-div\" style=\"height:525px; width:1400px;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"25427d0e-87c8-4728-a0f8-13dd2a5974e0\")) {                    Plotly.newPlot(                        \"25427d0e-87c8-4728-a0f8-13dd2a5974e0\",                        [{\"hovertemplate\":\"variable=0\\u003cbr\\u003eindex=%{x}\\u003cbr\\u003evalue=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"0\",\"line\":{\"color\":\"#636efa\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"0\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99],\"xaxis\":\"x\",\"y\":[0.7151039838790894,0.7021034359931946,0.6447742581367493,0.4755672812461853,0.46629250049591064,0.46433597803115845,0.4496127665042877,0.43345460295677185,0.40544581413269043,0.3769306540489197,0.36818036437034607,0.368029922246933,0.36461880803108215,0.3338053822517395,0.3325088918209076,0.33163654804229736,0.3285045027732849,0.3230321705341339,0.3122236728668213,0.30656275153160095,0.29807400703430176,0.29567956924438477,0.28793516755104065,0.28181490302085876,0.27291667461395264,0.269916832447052,0.26951536536216736,0.2670234143733978,0.2624962627887726,0.26141679286956787,0.26138344407081604,0.25930845737457275,0.2574058473110199,0.2557392716407776,0.2545783817768097,0.25367021560668945,0.25326448678970337,0.24697650969028473,0.24593673646450043,0.24051912128925323,0.23975159227848053,0.23863306641578674,0.23759528994560242,0.2327786684036255,0.22527751326560974,0.2242056131362915,0.2225882112979889,0.21428819000720978,0.21230286359786987,0.21133801341056824,0.2064177393913269,0.20476014912128448,0.20241858065128326,0.20201188325881958,0.19905489683151245,0.1962377279996872,0.19520127773284912,0.19483861327171326,0.193433478474617,0.1923745721578598,0.19218173623085022,0.18790464103221893,0.18682272732257843,0.18628549575805664,0.18568958342075348,0.18472568690776825,0.1828276515007019,0.18267777562141418,0.18079043924808502,0.17916575074195862,0.17693564295768738,0.17670908570289612,0.1743670254945755,0.17360681295394897,0.1708606779575348,0.1695656031370163,0.16913701593875885,0.16774116456508636,0.16766385734081268,0.16690245270729065,0.16606493294239044,0.16603894531726837,0.16564375162124634,0.16399632394313812,0.1616830825805664,0.16106028854846954,0.16052965819835663,0.16007085144519806,0.15993937849998474,0.15788492560386658,0.15783332288265228,0.15640121698379517,0.15614208579063416,0.15368907153606415,0.15357573330402374,0.15327595174312592,0.15119893848896027,0.15017960965633392,0.1490512490272522,0.14789164066314697],\"yaxis\":\"y\",\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Neuron\"},\"tickmode\":\"array\",\"tickvals\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99],\"ticktext\":[273,670,395,8,164,1209,751,389,1661,929,1138,1353,245,1731,394,584,1479,1042,2023,1874,820,1020,1444,1706,1293,639,608,78,411,338,1851,380,651,1124,1536,1823,453,767,847,1880,1734,1972,995,4,1095,315,1625,505,1786,1934,831,499,1712,68,1389,423,1232,135,1859,1059,447,556,149,1788,493,548,978,297,1487,1719,347,1968,455,1936,1257,641,360,665,1104,648,962,586,1915,1315,160,414,927,405,554,783,892,387,1594,419,1530,989,249,727,1463,1261]},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Absolute difference\"}},\"legend\":{\"title\":{\"text\":\"variable\"},\"tracegroupgap\":0},\"title\":{\"text\":\"Top absolute neuron differences in layer 5\"},\"width\":1400},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('25427d0e-87c8-4728-a0f8-13dd2a5974e0');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def line(x, xlabel=\"\", ylabel=\"\", title=\"\", xticks=None, width=800, hover_data=None):\n",
    "    fig = px.line(x, title=title)\n",
    "    fig.update_layout(xaxis_title=xlabel, yaxis_title=ylabel, width=width)\n",
    "    if xticks != None:\n",
    "        fig.update_layout(\n",
    "            xaxis = dict(\n",
    "            tickmode = 'array',\n",
    "            tickvals = [i for i in range(len(xticks))],\n",
    "            ticktext = xticks\n",
    "            )\n",
    "        )\n",
    "    if hover_data != None:\n",
    "        fig.update(data=[{'customdata': hover_data, 'hovertemplate': \"Loss: %{y:.4f} (+%{customdata:.2f}%)\"}])\n",
    "    fig.show()\n",
    "\n",
    "# Plotting all differences seems to break Jupyter\n",
    "line(sorted_differences.cpu().numpy()[:100], xlabel=\"Neuron\", ylabel=\"Absolute difference\", xticks=sorted_neurons.cpu().tolist()[:100], title=f\"Top absolute neuron differences in layer {layer_to_cache}\", width=1400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss_patched_mlp_neurons(prompts: list[str], model: HookedTransformer, mean_neuron_activations, patch_neurons, patch_layer=5, ablate_neurons=(609), ablate_layer=3, crop_context: None | tuple[int, int]=None):\n",
    "    original_losses = []\n",
    "    patched_losses = []\n",
    "    for prompt in tqdm(prompts):\n",
    "        \n",
    "        original_loss, _, _, ablated_cache = haystack_utils.get_caches_single_prompt(prompt, model, mean_neuron_activations, ablate_neurons, ablate_layer, crop_context=crop_context)\n",
    "        \n",
    "        if crop_context is not None:\n",
    "            tokens = model.to_tokens(prompt)[:, crop_context[0]:crop_context[1]]\n",
    "        else:\n",
    "            tokens = model.to_tokens(prompt)\n",
    "        def patch_hook(value, hook):\n",
    "            # Batch, pos, d_mlp\n",
    "            value[:, :, patch_neurons] = ablated_cache[f'blocks.{patch_layer}.mlp.hook_post'][:, :, patch_neurons]\n",
    "        \n",
    "        with model.hooks(fwd_hooks=[(f'blocks.{patch_layer}.mlp.hook_post', patch_hook)]):\n",
    "            patched_loss = model(tokens, return_type=\"loss\")\n",
    "        \n",
    "        original_losses.append(original_loss)\n",
    "        patched_losses.append(patched_loss.item())\n",
    "\n",
    "    print(f\"Original loss: {np.mean(original_losses):.2f}, patched loss: {np.mean(patched_losses):.2f} (+{((np.mean(patched_losses) - np.mean(original_losses)) / np.mean(original_losses))*100:.2f}%)\")\n",
    "    return np.mean(original_losses), np.mean(patched_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patching neurons: [273, 670, 395, 8, 164]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81ed78567452447e830e6da8cbd5b8de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1007 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original loss: 3.59, patched loss: 3.60 (+0.35%)\n"
     ]
    }
   ],
   "source": [
    "patch_neurons = sorted_neurons.cpu().tolist()[:5]\n",
    "print(\"Patching neurons:\", patch_neurons)\n",
    "_, _ = get_loss_patched_mlp_neurons(kde_french, model, english_activations, patch_neurons=patch_neurons, patch_layer=5, ablate_neurons=[609], ablate_layer=3, crop_context=(0, 500))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Patching neurons with high activation difference has a small effect on the loss\n",
    "- Activation doesn't necessarily correspond to changed log probs of the correct token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patching neurons: [273, 670, 395, 8, 164, 1209, 751, 389, 1661, 929, 1138, 1353, 245, 1731, 394, 584, 1479, 1042, 2023, 1874]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f244880ef41145fdb5ac30d7f00484d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1007 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original loss: 3.59, patched loss: 3.62 (+0.99%)\n"
     ]
    }
   ],
   "source": [
    "patch_neurons = sorted_neurons.cpu().tolist()[:20]\n",
    "print(\"Patching neurons:\", patch_neurons)\n",
    "_, _ = get_loss_patched_mlp_neurons(kde_french, model, english_activations, patch_neurons=patch_neurons, patch_layer=5, ablate_neurons=[609], ablate_layer=3, crop_context=(0, 500))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logit attribution for all layers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Run model with and without ablating the French neuron, save both clean and ablated activations\n",
    "- Run model again without ablation\n",
    "- Simulate the effect of individual ablated components\n",
    "- To simulate ablating a component:\n",
    "    - Before the final layernorm, subtract the cached activation the component from the unablated run\n",
    "    - Then add the activation of the ablated run\n",
    "- This allows to compute the effect of running a component with corrupted activations without letting its output affecting later components\n",
    "- However, the cached ablated activations of later components will still be influenced by earlier components "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DLA(prompts: list[str], model: HookedTransformer, mean_neuron_activations, neurons = [609], layer_to_ablate=3, patched_component=8, crop_context: None | tuple[int, int]=None):\n",
    "    # TODO think about layer normalization\n",
    "    original_losses = []\n",
    "    patched_losses = []\n",
    "    for prompt in tqdm(prompts):\n",
    "\n",
    "        neurons = torch.LongTensor(neurons)\n",
    "        def ablate_neuron_hook(value, hook):\n",
    "            value[:, :, neurons] = mean_neuron_activations[neurons]\n",
    "            return value\n",
    "        \n",
    "        if crop_context is not None:\n",
    "            tokens = model.to_tokens(prompt)[:, crop_context[0]:crop_context[1]]\n",
    "        else:\n",
    "            tokens = model.to_tokens(prompt)\n",
    "        original_loss, original_cache = model.run_with_cache(tokens, return_type=\"loss\")\n",
    "\n",
    "        with model.hooks(fwd_hooks=[(f'blocks.{layer_to_ablate}.mlp.hook_post', ablate_neuron_hook)]):\n",
    "            ablated_loss, ablated_cache = model.run_with_cache(tokens, return_type=\"loss\")\n",
    "\n",
    "        # component, batch, pos, residual\n",
    "        # TODO figure out if we need layer norm here\n",
    "        original_per_layer_residual, original_labels = original_cache.decompose_resid(layer=-1, return_labels=True, apply_ln=False)\n",
    "        ablated_per_layer_residual, ablated_labels = ablated_cache.decompose_resid(layer=-1, return_labels=True, apply_ln=False)\n",
    "\n",
    "        # ['embed', '0_attn_out', '0_mlp_out', '1_attn_out', '1_mlp_out', '2_attn_out', '2_mlp_out', '3_attn_out', '3_mlp_out', '4_attn_out', '4_mlp_out', '5_attn_out', '5_mlp_out']\n",
    "        def swap_cache_hook(value, hook):\n",
    "            # Batch, pos, residual\n",
    "            value -= original_per_layer_residual[patched_component]\n",
    "            value += ablated_per_layer_residual[patched_component]\n",
    "        \n",
    "        with model.hooks(fwd_hooks=[(f'blocks.5.hook_resid_post', swap_cache_hook)]):\n",
    "            patched_loss = model(tokens, return_type=\"loss\")\n",
    "\n",
    "        original_losses.append(original_loss.item())\n",
    "        patched_losses.append(patched_loss.item())\n",
    "\n",
    "\n",
    "    print(f\"Original loss: {np.mean(original_losses):.2f}, patched loss: {np.mean(patched_losses):.2f} (+{((np.mean(patched_losses) - np.mean(original_losses)) / np.mean(original_losses))*100:.2f}%)\")\n",
    "    return np.mean(original_losses), np.mean(patched_losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Component: 3_mlp_out\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6877a60c07034a3c9457455fb1ff9cfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1007 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original loss: 3.59, patched loss: 3.65 (+1.72%)\n",
      "Component: 4_attn_out\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f287a07f7375442ba9ed74c0866d6a5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1007 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original loss: 3.59, patched loss: 3.60 (+0.41%)\n",
      "Component: 4_mlp_out\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1b706b7f6ae44cd9ca84e32d755cd43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1007 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original loss: 3.59, patched loss: 3.64 (+1.30%)\n",
      "Component: 5_attn_out\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da0547d2e5f94de3a04186eb2b8ad050",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1007 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original loss: 3.59, patched loss: 3.62 (+0.76%)\n",
      "Component: 5_mlp_out\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db923ddc837543c0ace8112c3819c2a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1007 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original loss: 3.59, patched loss: 3.73 (+3.99%)\n"
     ]
    }
   ],
   "source": [
    "# Layer 3 MLP logit attribution = direct effect of ablating the context neuron\n",
    "# Logit attribution of later components when ablating the context neuron\n",
    "# Not sure how clean this is - e.g. layer 5 MLP will get the accumulated effects of all previous layers from ablating the context neuron\n",
    "component_names = ['embed', '0_attn_out', '0_mlp_out', '1_attn_out', '1_mlp_out', '2_attn_out', '2_mlp_out', '3_attn_out', '3_mlp_out', '4_attn_out', '4_mlp_out', '5_attn_out', '5_mlp_out']\n",
    "\n",
    "components = []\n",
    "losses = []\n",
    "for later_component in range(8, 13):\n",
    "    print(f\"Component: {component_names[later_component]}\")\n",
    "    original_loss, patched_loss = DLA(kde_french, model, english_activations, patched_component=later_component, crop_context=(0, 500))\n",
    "    if len(losses) == 0:\n",
    "        components.append(\"Original loss\")\n",
    "        losses.append(original_loss)\n",
    "    components.append(component_names[later_component])\n",
    "    losses.append(patched_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'losses' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m percent_increase \u001b[39m=\u001b[39m ((np\u001b[39m.\u001b[39marray(losses) \u001b[39m-\u001b[39m losses[\u001b[39m0\u001b[39m]) \u001b[39m/\u001b[39m losses[\u001b[39m0\u001b[39m]) \u001b[39m*\u001b[39m \u001b[39m100\u001b[39m\n\u001b[1;32m      2\u001b[0m line(losses, xlabel\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mComponent\u001b[39m\u001b[39m\"\u001b[39m, ylabel\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mLoss\u001b[39m\u001b[39m\"\u001b[39m, title\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mLoss of individual patching individual components when ablating L3N609\u001b[39m\u001b[39m\"\u001b[39m, xticks\u001b[39m=\u001b[39mcomponents, width\u001b[39m=\u001b[39m\u001b[39m800\u001b[39m, hover_data\u001b[39m=\u001b[39mpercent_increase\u001b[39m.\u001b[39mtolist())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'losses' is not defined"
     ]
    }
   ],
   "source": [
    "percent_increase = ((np.array(losses) - losses[0]) / losses[0]) * 100\n",
    "line(losses, xlabel=\"Component\", ylabel=\"Loss\", title=\"Loss of individual patching individual components when ablating L3N609\", xticks=components, width=800, hover_data=percent_increase.tolist())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Total effect of ablating L3N609: 5.97% increase in loss\n",
    "- Direct effect of ablating L3N609: 1.5% increase in loss\n",
    "- Added direct effects of all later components and ~8%\n",
    "- The French neuron must directly boost relevant words"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Does it make sense that the direct loss attribution of individual components sums to a higher total loss than the total loss of ablating the neuron\n",
    "- Yes?: \n",
    "    - Components make similar mistakes so that ablating all of them leads to fewer loss\n",
    "    - Later components receive the residual stream input of accumulated mistakes (not clean path patching)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check individual contributions of L5 neurons"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Motivation: The output of MLP layer 5 caused the biggest increase in loss in the previous analysis. We want to find which neurons specifically are responsible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check, compare output directions from the residual stream\n",
    "\n",
    "def get_answer_token_logit_difference(prompts: list[str], model: HookedTransformer, mean_neuron_activations, neurons = [609], layer_to_ablate=3, layer_to_compare=5, crop_context: None | tuple[int, int]=None):\n",
    "    # Computes output logit difference of the correct token between the outputs of a MLP layer with and without ablated neurons\n",
    "    # TODO think about layer normalization\n",
    "    differences = []\n",
    "    for prompt in tqdm(prompts):\n",
    "        model.reset_hooks()\n",
    "        if crop_context is not None:\n",
    "            tokens = model.to_tokens(prompt)[:, crop_context[0]:crop_context[1]]\n",
    "        else:\n",
    "            tokens = model.to_tokens(prompt)\n",
    "        original_loss, original_cache = model.run_with_cache(tokens, return_type=\"loss\")\n",
    "        answer_tokens = tokens[:, 1:]\n",
    "\n",
    "        # Shape batch pos residual\n",
    "        mlp_post = original_cache[f'blocks.{layer_to_compare}.hook_mlp_out']\n",
    "        # Shape batch pos-1 residual\n",
    "        normalized_mlp_post = original_cache.apply_ln_to_stack(mlp_post)[:,:-1]\n",
    "        correct_token_directions = model.W_U[:, answer_tokens].squeeze(1) # embed pos\n",
    "        # Shape position\n",
    "        original_unembedded = einops.einsum(normalized_mlp_post, correct_token_directions, 'batch pos residual, residual pos -> batch pos').squeeze(0)\n",
    "\n",
    "        def ablate_neuron_hook(value, hook):\n",
    "            value[:, :, neurons] = mean_neuron_activations[neurons]\n",
    "            return value\n",
    "    \n",
    "        with model.hooks(fwd_hooks=[(f'blocks.{layer_to_ablate}.mlp.hook_post', ablate_neuron_hook)]):\n",
    "            ablated_loss, ablated_cache = model.run_with_cache(tokens, return_type=\"loss\")\n",
    "        \n",
    "        mlp_post = ablated_cache[f'blocks.{layer_to_compare}.hook_mlp_out']\n",
    "        normalized_mlp_post = ablated_cache.apply_ln_to_stack(mlp_post)[:,:-1]\n",
    "        ablated_unembedded = einops.einsum(normalized_mlp_post, correct_token_directions, 'batch pos residual, residual pos -> batch pos').squeeze(0)\n",
    "\n",
    "        # Shape: pos\n",
    "        difference = (original_unembedded - ablated_unembedded).detach().cpu().mean().item()\n",
    "        differences.append(difference)\n",
    "    print(\"Mean difference:\", np.mean(differences))\n",
    "\n",
    "#get_answer_token_logit_difference(kde_french, model, english_activations, crop_context=(10, 500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_neuron_logit_contribution(cache: ActivationCache, model: HookedTransformer, answer_tokens: Int[Tensor, \"batch pos\"], layer: int) -> Float[Tensor, \"neuron pos\"]:\n",
    "    # Expexts cache from a single example, won't work on batched examples\n",
    "    # Get per neuron output of MLP layer\n",
    "    neuron_directions = cache.get_neuron_results(layer, neuron_slice=utils.Slice(input_slice=None), pos_slice=utils.Slice(input_slice=None))\n",
    "    neuron_directions = einops.rearrange(neuron_directions, 'batch pos neuron residual -> neuron batch pos residual')\n",
    "    # Apply LN? LN is usually never applied to individual MLP output directions, it is applied to the sum of MLP output directions\n",
    "    # LN leads to finding top tokens with slightly higher loss attribution\n",
    "    #scaled_neuron_directions = neuron_directions[:, 0, :-1, :]\n",
    "    scaled_neuron_directions = cache.apply_ln_to_stack(neuron_directions)[:, 0, :-1, :] # neuron pos embed\n",
    "    # Unembed of correct answer tokens\n",
    "    correct_token_directions = model.W_U[:, answer_tokens].squeeze(1) # embed pos # d_model answer_tokens\n",
    "    # Neuron attribution to correct answer token by position\n",
    "    unembedded = einops.einsum(scaled_neuron_directions, correct_token_directions, 'neuron pos residual, residual pos -> neuron pos') # neuron pos\n",
    "    return unembedded\n",
    "\n",
    "# This doesn't work at all!\n",
    "def get_neuron_logit_contribution_wrong(cache: ActivationCache, model: HookedTransformer, answer_tokens: Int[Tensor, \"batch pos\"], layer: int) -> Float[Tensor, \"neuron pos\"]:\n",
    "    # Get per neuron output of MLP layer\n",
    "    neuron_directions = cache.get_neuron_results(layer, neuron_slice=utils.Slice(input_slice=None), pos_slice=utils.Slice(input_slice=None))\n",
    "    neuron_directions = einops.rearrange(neuron_directions, 'batch pos neuron residual -> neuron batch pos residual')\n",
    "    assert neuron_directions.shape[1] == 1, \"Expexts cache from a single example, won't work on batched examples\"\n",
    "    # Apply ln? LN is never applied to individual MLP output directions, not sure if it matters\n",
    "    #scaled_neuron_directions = neuron_directions[:, 0, :-1, :]\n",
    "    scaled_neuron_directions = cache.apply_ln_to_stack(neuron_directions)[:, 0, :-1, :]\n",
    "    unembedded = einops.einsum(scaled_neuron_directions, model.W_U, 'neuron pos residual, residual output_dim -> neuron output_dim')\n",
    "    unembedded = unembedded.softmax(-1)[:, answer_tokens[0]]\n",
    "    return unembedded # neuron answer_prob"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Check if neurons are the same with and without LN\n",
    "- Repeat loss increase analysis for new top and bottom neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d42425134ce24119b12ce8660230d60e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1007 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total activation difference on correct token: 0.37233126163482666\n"
     ]
    }
   ],
   "source": [
    "def MLP_attribution(prompts: list[str], model: HookedTransformer, mean_neuron_activations, neurons = [609], layer_to_ablate=3, layer_to_compare=5, crop_context: None | tuple[int, int]=None):\n",
    "    # TODO think about layer normalization\n",
    "    differences = torch.zeros(model.cfg.d_mlp)\n",
    "    for prompt in tqdm(prompts):\n",
    "        model.reset_hooks()\n",
    "        if crop_context is not None:\n",
    "            tokens = model.to_tokens(prompt)[:, crop_context[0]:crop_context[1]]\n",
    "        else:\n",
    "            tokens = model.to_tokens(prompt)\n",
    "        original_loss, original_cache = model.run_with_cache(tokens, return_type=\"loss\")\n",
    "        answer_tokens = tokens[:, 1:]\n",
    "        \n",
    "        # Shape neuron pos\n",
    "        original_unembedded = get_neuron_logit_contribution(original_cache, model, answer_tokens, layer=layer_to_compare)\n",
    "        \n",
    "        def ablate_neuron_hook(value, hook):\n",
    "            value[:, :, neurons] = mean_neuron_activations[neurons]\n",
    "            return value\n",
    "    \n",
    "        with model.hooks(fwd_hooks=[(f'blocks.{layer_to_ablate}.mlp.hook_post', ablate_neuron_hook)]):\n",
    "            ablated_loss, ablated_cache = model.run_with_cache(tokens, return_type=\"loss\")\n",
    "\n",
    "        # Shape: neuron pos\n",
    "        ablated_unembedded = get_neuron_logit_contribution(ablated_cache, model, answer_tokens, layer=layer_to_compare)\n",
    "        # Positive diff -> ablated has lower activation on correct token\n",
    "        # Shape: neuron\n",
    "        difference = (original_unembedded - ablated_unembedded).mean(1).detach().cpu()\n",
    "        differences += difference\n",
    "    \n",
    "    mean_difference = differences / len(prompts)\n",
    "    print(\"Total activation difference on correct token:\", mean_difference.sum().item())\n",
    "    sorted_differences, sorted_neurons = torch.topk(mean_difference, len(mean_difference), largest=True)\n",
    "    return sorted_differences, sorted_neurons\n",
    "\n",
    "haystack_utils.clean_cache()\n",
    "layer_to_compare=5\n",
    "sorted_differences, sorted_neurons = MLP_attribution(kde_french, model, english_activations, layer_to_compare=layer_to_compare, crop_context=(0, 500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>                            <div id=\"a58eadb8-faaa-4e60-812a-7d94b602593a\" class=\"plotly-graph-div\" style=\"height:525px; width:800px;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"a58eadb8-faaa-4e60-812a-7d94b602593a\")) {                    Plotly.newPlot(                        \"a58eadb8-faaa-4e60-812a-7d94b602593a\",                        [{\"hovertemplate\":\"variable=0\\u003cbr\\u003eindex=%{x}\\u003cbr\\u003evalue=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"0\",\"line\":{\"color\":\"#636efa\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"0\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29],\"xaxis\":\"x\",\"y\":[0.08461214601993561,0.04759485274553299,0.03019873984158039,0.024128390476107597,0.01896042563021183,0.013806466944515705,0.013721026480197906,0.013473210856318474,0.012234045192599297,0.01184495072811842,0.011024907231330872,0.010849898681044579,0.010374963283538818,0.009693855419754982,0.009565649554133415,0.00949173141270876,0.00936784129589796,0.009315874427556992,0.008360042236745358,0.007807486690580845,0.0076604862697422504,0.007384961936622858,0.007326149381697178,0.007324960082769394,0.007073892746120691,0.00699077732861042,0.006845083553344011,0.006686041597276926,0.006685232277959585,0.0064358278177678585],\"yaxis\":\"y\",\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Neuron\"},\"tickmode\":\"array\",\"tickvals\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29],\"ticktext\":[395,670,584,1622,1138,1487,1658,493,273,767,1283,1825,4,1020,1661,1479,1672,347,315,1844,1644,453,1874,586,164,759,472,1104,1884,1702]},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Logit difference on correct token\"}},\"legend\":{\"title\":{\"text\":\"variable\"},\"tracegroupgap\":0},\"title\":{\"text\":\"Top positive neuron logit differences on correct tokens on layer 5\"},\"width\":800},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('a58eadb8-faaa-4e60-812a-7d94b602593a');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>                            <div id=\"77f18c67-9e40-4f9f-824a-802a270f903e\" class=\"plotly-graph-div\" style=\"height:525px; width:800px;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"77f18c67-9e40-4f9f-824a-802a270f903e\")) {                    Plotly.newPlot(                        \"77f18c67-9e40-4f9f-824a-802a270f903e\",                        [{\"hovertemplate\":\"variable=0\\u003cbr\\u003eindex=%{x}\\u003cbr\\u003evalue=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"0\",\"line\":{\"color\":\"#636efa\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"0\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29],\"xaxis\":\"x\",\"y\":[-0.00509016215801239,-0.005194589961320162,-0.005227178800851107,-0.0052933464758098125,-0.0053576016798615456,-0.005443326663225889,-0.005465829744935036,-0.005701709073036909,-0.005766805727034807,-0.005866533610969782,-0.006229724735021591,-0.006250201724469662,-0.006486672442406416,-0.0067767915315926075,-0.0068545774556696415,-0.006912577897310257,-0.007049124222248793,-0.0073684207163751125,-0.007421499118208885,-0.007794040255248547,-0.007920109666883945,-0.008427614346146584,-0.008761937730014324,-0.010151831433176994,-0.011045901104807854,-0.012480740435421467,-0.012979354709386826,-0.014713485725224018,-0.044681720435619354,-0.0901414230465889],\"yaxis\":\"y\",\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Neuron\"},\"tickmode\":\"array\",\"tickvals\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29],\"ticktext\":[469,100,1027,636,869,1336,1934,929,265,291,1475,1716,1839,414,641,455,1232,1456,394,1708,1786,1972,8,1209,1353,1444,2023,651,1257,389]},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Logit difference on correct token\"}},\"legend\":{\"title\":{\"text\":\"variable\"},\"tracegroupgap\":0},\"title\":{\"text\":\"Top negative neuron logit differences on correct tokens on layer 5\"},\"width\":800},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('77f18c67-9e40-4f9f-824a-802a270f903e');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "line(sorted_differences.cpu().numpy()[:30], xlabel=\"Neuron\", ylabel=\"Logit difference on correct token\", xticks=sorted_neurons.cpu().tolist()[:30], title=f\"Top positive neuron logit differences on correct tokens on layer {layer_to_compare}\", width=800)\n",
    "line(sorted_differences.cpu().numpy()[-30:], xlabel=\"Neuron\", ylabel=\"Logit difference on correct token\", xticks=sorted_neurons.cpu().tolist()[-30:], title=f\"Top negative neuron logit differences on correct tokens on layer {layer_to_compare}\", width=800)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test effect of patching top logit difference neurons on loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check: ablate whole layer - should lead to loss increase of 4%\n",
    "# _, _ = get_loss_patched_mlp_neurons(kde_french, model, english_activations, patch_neurons=[i for i in range(model.cfg.d_mlp)], patch_layer=5, ablate_neurons=[609], ablate_layer=3, crop_context=(0, 500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patched: [395, 670, 584, 1622, 1138]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36008791991141aa97dda85c1c98da17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1007 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original loss: 3.59, patched loss: 3.60 (+0.17%)\n"
     ]
    }
   ],
   "source": [
    "# Ablate top neurons\n",
    "top_neurons = sorted_neurons.cpu().tolist()[:5]\n",
    "print(\"Patched:\", top_neurons)\n",
    "_, _ = get_loss_patched_mlp_neurons(kde_french, model, english_activations, patch_neurons=top_neurons, patch_layer=5, ablate_neurons=[609], ablate_layer=3, crop_context=(0, 500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patched: [1444, 2023, 651, 1257, 389]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7f1675a7a1848e184632634b6ad987b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1007 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original loss: 3.59, patched loss: 3.60 (+0.37%)\n"
     ]
    }
   ],
   "source": [
    "# Sanity check: ablate bottom neurons - should lead to a much lower increase in loss than top neurons\n",
    "worst_neurons = sorted_neurons.cpu().tolist()[-5:]\n",
    "print(\"Patched:\", worst_neurons)\n",
    "_, _ = get_loss_patched_mlp_neurons(kde_french, model, english_activations, patch_neurons=worst_neurons, patch_layer=5, ablate_neurons=[609], ablate_layer=3, crop_context=(0, 500))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Patching all neurons leads to the expected increase in loss of 4% (in line with our layer ablation results)\n",
    "- Patching the top 5 neurons from our analysis should result in a high-ish loss increase but we get 0.18%\n",
    "- Patching the bottom 5 neurons should result in a very low increase in loss (technically it should be negative looking at our attribution curve) but we get 0.37%"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
