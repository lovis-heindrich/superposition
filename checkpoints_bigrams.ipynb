{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from transformer_lens import HookedTransformer\n",
    "from jaxtyping import Float, Int, Bool\n",
    "from torch import Tensor\n",
    "from tqdm.auto import tqdm\n",
    "import plotly.io as pio\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px \n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from IPython.display import display, HTML\n",
    "from datasets import load_dataset\n",
    "from collections import Counter\n",
    "import pickle\n",
    "import os\n",
    "import haystack_utils\n",
    "from transformer_lens import utils\n",
    "from fancy_einsum import einsum\n",
    "import einops\n",
    "import json\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from datasets import load_dataset\n",
    "import random\n",
    "import math\n",
    "import random\n",
    "import neel.utils as nutils\n",
    "from neel_plotly import *\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.utils import shuffle\n",
    "import probing_utils\n",
    "import pickle\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "import gzip\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotting_utils\n",
    "import re\n",
    "\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "pio.renderers.default = \"notebook_connected+notebook\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "#torch.autograd.set_grad_enabled(False)\n",
    "#torch.set_grad_enabled(False)\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n",
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model EleutherAI/pythia-70m into HookedTransformer\n",
      "data/german_europarl.json: Loaded 2000 examples with 152 to 2000 characters each.\n",
      "data/english_europarl.json: Loaded 2000 examples with 165 to 2000 characters each.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f954f966da1747139eabda4e1df69870",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_model(checkpoint: int) -> HookedTransformer:\n",
    "    model = HookedTransformer.from_pretrained(\"EleutherAI/pythia-70m\",\n",
    "        checkpoint_index=checkpoint,\n",
    "        center_unembed=True,\n",
    "        center_writing_weights=True,\n",
    "        fold_ln=True,\n",
    "        device=device)\n",
    "    return model\n",
    "\n",
    "NUM_CHECKPOINTS = 143\n",
    "LAYER, NEURON = 3, 669\n",
    "model = get_model(142)\n",
    "german_data = haystack_utils.load_json_data(\"data/german_europarl.json\")\n",
    "english_data = haystack_utils.load_json_data(\"data/english_europarl.json\")\n",
    "all_ignore, _ = haystack_utils.get_weird_tokens(model, plot_norms=False)\n",
    "common_tokens = haystack_utils.get_common_tokens(german_data, model, all_ignore, k=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find ambiguous bigrams and trigrams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7ed14a2eed64c06bbc3afb53d803cd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "003e7eb488ca4597b1167530cf4c5aa2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_bigram_token_counts(data, model: HookedTransformer):\n",
    "    counts = torch.zeros((model.cfg.d_vocab, model.cfg.d_vocab))\n",
    "    for sentence in tqdm(data):\n",
    "        tokens = model.to_tokens(sentence).flatten().cpu()\n",
    "        next_tokens = tokens[1:]\n",
    "        for i in range(len(tokens) - 2):\n",
    "            counts[tokens[i], next_tokens[i]] += 1\n",
    "    return counts\n",
    "\n",
    "english_bigram_counts = get_bigram_token_counts(english_data, model)\n",
    "german_bigram_counts = get_bigram_token_counts(german_data, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_punctuation_tokens(model):\n",
    "    punctuation = [\"\\n\", \".\", \",\", \"!\", \"?\", \";\", \":\", \"-\", \"(\", \")\", \"[\", \"]\", \"{\", \"}\", \"<\", \">\", \"/\", \"\\\\\", \"\\\"\", \"'\"]\n",
    "    leading_space_punctuation = [\" \" + char for char in punctuation]\n",
    "    punctuation_tokens = model.to_tokens(punctuation + leading_space_punctuation + [' â€“', \" \", '  ', \"<|endoftext|>\"])[:, 1].flatten()\n",
    "    return punctuation_tokens\n",
    "\n",
    "def get_number_tokens(model:HookedTransformer):\n",
    "    all_tokens = [i for i in range(model.cfg.d_vocab)]\n",
    "    number_tokens = []\n",
    "    for token in all_tokens:\n",
    "        str_token = model.to_single_str_token(token)\n",
    "        try:\n",
    "            float(str_token)\n",
    "            number_tokens.append(token)\n",
    "        except ValueError:\n",
    "            pass\n",
    "    return torch.LongTensor(number_tokens)\n",
    "\n",
    "def get_all_non_letter_tokens(model: HookedTransformer):\n",
    "    all_tokens = [i for i in range(model.cfg.d_vocab)]\n",
    "    letter_tokens = []\n",
    "    for token in all_tokens:\n",
    "        str_token = model.to_single_str_token(token)\n",
    "        if not bool(re.search(r'[a-zA-Z]', str_token)):\n",
    "            letter_tokens.append(token)\n",
    "    return torch.LongTensor(letter_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation_tokens = get_punctuation_tokens(model)\n",
    "number_tokens = get_number_tokens(model)\n",
    "non_letter_tokens = get_all_non_letter_tokens(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_common_tokens(token_counts, threshold=100):\n",
    "    top_tokens = torch.argwhere(token_counts > threshold).flatten()\n",
    "    return top_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "german_bigram_counts[non_letter_tokens] = 0\n",
    "german_bigram_counts[:, non_letter_tokens] = 0\n",
    "english_bigram_counts[non_letter_tokens] = 0\n",
    "english_bigram_counts[:, non_letter_tokens] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50304, 50304])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "german_bigram_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93 [' all', ' EU', ' inform', 'ations', 'idents', 'ia', ' find', 'a', 'b', 'i', 'k', 'l', ' will', 'n', 'o', ' so', 's', 'y', 'z', ' international', ' K', ' Mon', ' her', ' hand', 'ann', ' V', ' best', ' St', ' also', 'PE', ' am', ' get', ' t', ' a', 'in', 're', ' bring', 'er', ' Union', 'en', 'is', 'it', 'ed', 'es', 'an', ' an', 'ing', 'ar', ' in', 'ou', 'as', ' and', 'ro', ' national', 'el', ' T', ' I', 'ol', 'am', 'ation', ' be', ' S', ' for', ' C', ' he', ' M', ' we', ' set', ' P', ' de', 'ise', 'os', ' B', ' H', 'ers', ' D', ' F', ' W', ' R', ' not', ' L', 'ort', ' un', ' G', ' E', 'ies', ' Bar', ' O', ' end', ' me', ' J', 'ast', 'ans']\n"
     ]
    }
   ],
   "source": [
    "top_german_tokens = get_common_tokens(german_bigram_counts.sum(0))\n",
    "top_english_tokens = get_common_tokens(english_bigram_counts.sum(0))\n",
    "common_both = torch.tensor(list(set(top_english_tokens.tolist()).intersection(set(top_german_tokens.tolist()))))\n",
    "print(len(common_both), model.to_str_tokens(torch.LongTensor(common_both)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' all', 'en', ' the'] 270.0 326.0\n",
      "[' EU', ' und', \"'s\"] 51.0 76.0\n",
      "[' inform', 'ieren', ' you'] 58.0 59.0\n",
      "['b', 'ens', 'oda'] 292.0 87.0\n",
      "['l', 'uss', 'ause'] 302.0 214.0\n",
      "[' will', 'kom', ' be'] 102.0 640.0\n",
      "[' so', 'z', ' that'] 196.0 170.0\n",
      "[' Mon', 'aten', 'etary'] 70.0 106.0\n",
      "[' St', 'ell', 'ras'] 116.0 147.0\n",
      "[' am', ' Don', 'ending'] 109.0 130.0\n",
      "[' t', 'ats', 'abled'] 63.0 83.0\n",
      "[' Union', ' und', ' and'] 111.0 149.0\n",
      "[' an', ' die', ' important'] 424.0 129.0\n",
      "[' in', ' der', ' the'] 1261.0 3212.0\n",
      "[' and', 'eren', ' the'] 339.0 1773.0\n",
      "['ro', 'ffen', 'so'] 108.0 77.0\n",
      "[' I', 'hn', ' would'] 528.0 1020.0\n",
      "['ol', 'ge', 'ences'] 89.0 53.0\n",
      "['am', 'ten', ' President'] 110.0 629.0\n",
      "[' be', 'gr', ' a'] 257.0 161.0\n",
      "[' S', 'itz', 'wo'] 901.0 78.0\n",
      "[' for', 'dern', ' the'] 59.0 1641.0\n",
      "[' we', 'il', ' are'] 291.0 645.0\n",
      "[' P', 'unk', 'PE'] 973.0 79.0\n",
      "[' not', 'wend', ' only'] 125.0 150.0\n",
      "['ort', 'en', 'eur'] 72.0 380.0\n",
      "[' end', 'g', ' of'] 94.0 129.0\n",
      "[' me', 'ine', ' to'] 533.0 111.0\n"
     ]
    }
   ],
   "source": [
    "for common_token in common_both:\n",
    "    german_next_token = german_bigram_counts[common_token].argmax()\n",
    "    english_next_token = english_bigram_counts[common_token].argmax()\n",
    "    num_occurrences_german = german_bigram_counts[common_token, german_next_token].item()\n",
    "    num_occurrences_english = english_bigram_counts[common_token, english_next_token].item()\n",
    "    if (german_next_token != english_next_token) and (num_occurrences_german>50) and (num_occurrences_english>50):\n",
    "        print(model.to_str_tokens(torch.LongTensor([common_token, german_next_token, english_next_token])), num_occurrences_german, num_occurrences_english)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6600"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(non_letter_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43704\n"
     ]
    }
   ],
   "source": [
    "valid_tokens = [i for i in range(model.cfg.d_vocab) if i not in non_letter_tokens]\n",
    "print(len(valid_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e307efdf49a48f8b39871c6ca2a5dca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1410 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n"
     ]
    }
   ],
   "source": [
    "common_bigrams = []\n",
    "top_german_tokens = get_common_tokens(german_bigram_counts.sum(0), 50)\n",
    "bigram_threshold = 20\n",
    "for first_token in tqdm(top_german_tokens):\n",
    "    german_bigrams = german_bigram_counts[first_token]\n",
    "    english_bigrams = english_bigram_counts[first_token]\n",
    "    new_bigrams = torch.argwhere((german_bigrams > bigram_threshold) & (english_bigrams > bigram_threshold)).flatten()\n",
    "    for second_token in new_bigrams.tolist():\n",
    "        common_bigrams.append((first_token, second_token))\n",
    "print(len(common_bigrams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd1ad7b582de4fe69531cf788fffd855",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "last_trigram_tokens = torch.zeros((len(common_bigrams), model.cfg.d_vocab, 2), dtype=torch.long)\n",
    "for bigram_index, bigram in tqdm(enumerate(common_bigrams)):\n",
    "    for sentence in german_data:\n",
    "        tokens = model.to_tokens(sentence).flatten().cpu()\n",
    "        next_tokens = tokens[1:]\n",
    "        for i in range(len(tokens) - 3):\n",
    "            if (tokens[i] == bigram[0]) and (tokens[i+1] == bigram[1]):\n",
    "                last_trigram_tokens[bigram_index, tokens[i+2], 0] += 1\n",
    "    for sentence in english_data:\n",
    "        tokens = model.to_tokens(sentence).flatten().cpu()\n",
    "        next_tokens = tokens[1:]\n",
    "        for i in range(len(tokens) - 3):\n",
    "            if (tokens[i] == bigram[0]) and (tokens[i+1] == bigram[1]):\n",
    "                last_trigram_tokens[bigram_index, tokens[i+2], 1] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_trigram_tokens[:, non_letter_tokens, :] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' in', ' all', 'en', ' the'] 48 18\n"
     ]
    }
   ],
   "source": [
    "trigram_threshold = 10\n",
    "for trigram in range(len(common_bigrams)):\n",
    "    most_common_german_token = torch.argmax(last_trigram_tokens[trigram, :, 0])\n",
    "    most_common_english_token = torch.argmax(last_trigram_tokens[trigram, :, 1])\n",
    "    german_occurrences = last_trigram_tokens[trigram, most_common_german_token, 0].item()\n",
    "    english_occurrences = last_trigram_tokens[trigram, most_common_english_token, 1].item()\n",
    "    if (most_common_german_token != most_common_english_token) and (german_occurrences>trigram_threshold) and (english_occurrences>trigram_threshold):\n",
    "        print(model.to_str_tokens(torch.LongTensor([common_bigrams[trigram][0], common_bigrams[trigram][1], most_common_german_token, most_common_english_token])), german_occurrences, english_occurrences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<|endoftext|>', 'W', 'ir', ' haben', ' in', ' all']\n",
      "Tokenized answer: ['en']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">        Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">18.37</span><span style=\"font-weight: bold\"> Prob: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26.08</span><span style=\"font-weight: bold\">% Token: |en|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m        Logit: \u001b[0m\u001b[1;36m18.37\u001b[0m\u001b[1m Prob: \u001b[0m\u001b[1;36m26.08\u001b[0m\u001b[1m% Token: |en|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 18.37 Prob: 26.08% Token: |en|\n",
      "Top 1th token. Logit: 17.03 Prob:  6.79% Token: |i|\n",
      "Top 2th token. Logit: 16.44 Prob:  3.77% Token: | the|\n",
      "Top 3th token. Logit: 16.41 Prob:  3.66% Token: |erd|\n",
      "Top 4th token. Logit: 16.39 Prob:  3.61% Token: | der|\n",
      "Top 5th token. Logit: 15.66 Prob:  1.74% Token: | of|\n",
      "Top 6th token. Logit: 15.42 Prob:  1.36% Token: |g|\n",
      "Top 7th token. Logit: 15.17 Prob:  1.06% Token: |o|\n",
      "Top 8th token. Logit: 14.95 Prob:  0.85% Token: | den|\n",
      "Top 9th token. Logit: 14.87 Prob:  0.78% Token: |ig|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">'en'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'en'\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "utils.test_prompt(\"Wir haben in all\", \"en\", model, prepend_space_to_answer=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<|endoftext|>', 'We', ' have', ' all']\n",
      "Tokenized answer: [' the']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">        Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">19.45</span><span style=\"font-weight: bold\"> Prob: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">18.02</span><span style=\"font-weight: bold\">% Token: | the|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m        Logit: \u001b[0m\u001b[1;36m19.45\u001b[0m\u001b[1m Prob: \u001b[0m\u001b[1;36m18.02\u001b[0m\u001b[1m% Token: | the|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 19.45 Prob: 18.02% Token: | the|\n",
      "Top 1th token. Logit: 18.69 Prob:  8.41% Token: | been|\n",
      "Top 2th token. Logit: 18.41 Prob:  6.32% Token: | sorts|\n",
      "Top 3th token. Logit: 18.34 Prob:  5.92% Token: | of|\n",
      "Top 4th token. Logit: 18.04 Prob:  4.38% Token: | seen|\n",
      "Top 5th token. Logit: 17.61 Prob:  2.85% Token: | heard|\n",
      "Top 6th token. Logit: 17.46 Prob:  2.46% Token: | got|\n",
      "Top 7th token. Logit: 17.36 Prob:  2.22% Token: | our|\n",
      "Top 8th token. Logit: 17.32 Prob:  2.14% Token: | a|\n",
      "Top 9th token. Logit: 17.17 Prob:  1.83% Token: | kinds|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' the'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' the'\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "utils.test_prompt(\"We have all\", \"the\", model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deactivate_neurons_hook(value, hook):\n",
    "    value[:, :, NEURON] = 0\n",
    "    return value\n",
    "deactivate_neurons_fwd_hooks=[(f'blocks.{LAYER}.mlp.hook_post', deactivate_neurons_hook)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<|endoftext|>', 'W', 'ir', ' haben', ' in', ' all']\n",
      "Tokenized answer: ['en']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">        Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">15.91</span><span style=\"font-weight: bold\"> Prob:  </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.01</span><span style=\"font-weight: bold\">% Token: |en|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m2\u001b[0m\u001b[1m        Logit: \u001b[0m\u001b[1;36m15.91\u001b[0m\u001b[1m Prob:  \u001b[0m\u001b[1;36m3.01\u001b[0m\u001b[1m% Token: |en|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 16.77 Prob:  7.15% Token: |i|\n",
      "Top 1th token. Logit: 16.77 Prob:  7.14% Token: | the|\n",
      "Top 2th token. Logit: 15.91 Prob:  3.01% Token: |en|\n",
      "Top 3th token. Logit: 15.82 Prob:  2.76% Token: | of|\n",
      "Top 4th token. Logit: 15.60 Prob:  2.22% Token: | but|\n",
      "Top 5th token. Logit: 15.55 Prob:  2.10% Token: |\n",
      "|\n",
      "Top 6th token. Logit: 15.47 Prob:  1.94% Token: | 50|\n",
      "Top 7th token. Logit: 15.05 Prob:  1.28% Token: |erd|\n",
      "Top 8th token. Logit: 15.01 Prob:  1.23% Token: |o|\n",
      "Top 9th token. Logit: 14.96 Prob:  1.16% Token: | g|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">'en'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'en'\u001b[0m, \u001b[1;36m2\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with model.hooks(fwd_hooks=deactivate_neurons_fwd_hooks):\n",
    "    utils.test_prompt(\"Wir haben in all\", \"en\", model, prepend_space_to_answer=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "german_answer = model.to_single_token(\"en\")\n",
    "english_answer = model.to_single_token(\" the\")\n",
    "\n",
    "german_logits = []\n",
    "english_logits = []\n",
    "german_ablated_logits = []\n",
    "english_ablated_logits = []\n",
    "for i in range(NUM_CHECKPOINTS):\n",
    "    model = get_model(i)\n",
    "    logits = model(\"Wir haben in all\", return_type=\"logits\")[0, -1]\n",
    "    german_logits.append(logits[german_answer].item())\n",
    "    english_logits.append(logits[english_answer].item())\n",
    "    with model.hooks(fwd_hooks=deactivate_neurons_fwd_hooks):\n",
    "        logits = model(\"Wir haben in all\", return_type=\"logits\")[0, -1]\n",
    "        german_ablated_logits.append(logits[german_answer].item())\n",
    "        english_ablated_logits.append(logits[english_answer].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>                            <div id=\"e6539a8a-b07e-464d-b912-db349168aebd\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"e6539a8a-b07e-464d-b912-db349168aebd\")) {                    Plotly.newPlot(                        \"e6539a8a-b07e-464d-b912-db349168aebd\",                        [{\"hovertemplate\":\"Type=German\\u003cbr\\u003eCheckpoint=%{x}\\u003cbr\\u003eLogit=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"German\",\"line\":{\"color\":\"#636efa\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"German\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142],\"xaxis\":\"x\",\"y\":[-0.17609600722789764,-0.17609600722789764,-0.17589716613292694,-0.17261561751365662,-0.13066813349723816,0.06241811066865921,1.1081221103668213,2.1031675338745117,4.288607597351074,3.8812618255615234,4.481276988983154,5.688153266906738,11.467586517333984,9.285727500915527,12.22782039642334,12.089486122131348,13.010061264038086,13.331212997436523,12.39062213897705,13.210105895996094,13.65652847290039,13.737577438354492,11.393856048583984,15.568595886230469,14.96314525604248,15.533422470092773,15.44237232208252,15.730753898620605,17.020471572875977,16.579666137695312,16.85425567626953,17.362600326538086,16.786645889282227,15.643082618713379,14.93936824798584,13.543699264526367,16.297733306884766,16.059385299682617,13.899709701538086,15.455411911010742,15.918976783752441,14.41948413848877,15.06544303894043,15.97072696685791,15.060627937316895,16.07341194152832,15.132740020751953,14.179544448852539,17.7728214263916,11.729279518127441,13.870952606201172,15.748942375183105,16.972270965576172,14.785907745361328,13.581761360168457,15.890926361083984,12.314085960388184,12.517271041870117,13.543889045715332,14.165635108947754,13.669905662536621,13.06021785736084,14.061881065368652,15.573131561279297,14.962510108947754,17.727487564086914,17.689441680908203,16.08991813659668,14.864286422729492,16.25519371032715,15.338858604431152,15.802190780639648,15.11798095703125,14.348703384399414,15.825996398925781,14.777636528015137,14.848895072937012,15.272356033325195,14.56591510772705,17.192398071289062,16.715181350708008,15.145357131958008,14.842251777648926,16.30428123474121,15.854042053222656,15.243764877319336,13.323553085327148,14.53606128692627,15.573079109191895,14.220457077026367,16.20458221435547,14.661121368408203,15.896211624145508,13.971603393554688,14.367761611938477,15.7227144241333,14.22768783569336,15.627029418945312,15.898884773254395,15.749570846557617,16.09577751159668,15.275101661682129,16.72171974182129,17.14223861694336,16.643524169921875,15.207356452941895,16.468402862548828,15.032648086547852,15.586734771728516,16.411827087402344,16.617359161376953,16.945058822631836,17.47591209411621,15.660916328430176,15.570659637451172,14.79478645324707,14.723759651184082,15.581416130065918,17.816017150878906,17.353240966796875,17.562341690063477,15.619651794433594,15.96977710723877,15.907221794128418,14.71518325805664,15.075464248657227,15.512523651123047,14.090353012084961,15.47411060333252,15.461862564086914,14.075746536254883,15.830758094787598,17.92070770263672,16.923725128173828,17.77832794189453,17.447107315063477,16.80425262451172,16.372318267822266,18.824960708618164,16.513195037841797,17.28729820251465,16.927112579345703,18.444644927978516],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"Type=English\\u003cbr\\u003eCheckpoint=%{x}\\u003cbr\\u003eLogit=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"English\",\"line\":{\"color\":\"#EF553B\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"English\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142],\"xaxis\":\"x\",\"y\":[0.5707575678825378,0.5707575678825378,0.5723626613616943,0.6024317741394043,0.9734681248664856,2.1917715072631836,4.816174507141113,6.048471450805664,9.180913925170898,9.488296508789062,10.319853782653809,12.510759353637695,11.703801155090332,12.981724739074707,10.924985885620117,12.515929222106934,12.052970886230469,11.8925142288208,11.305676460266113,12.136430740356445,11.586385726928711,11.500919342041016,12.085926055908203,10.650726318359375,9.860406875610352,10.844766616821289,9.92423152923584,9.360459327697754,9.708362579345703,9.994544982910156,9.800299644470215,10.038476943969727,9.303691864013672,10.828590393066406,11.707195281982422,11.659960746765137,10.920267105102539,10.151630401611328,11.59016227722168,11.450363159179688,10.192110061645508,10.719572067260742,9.773785591125488,9.86485481262207,10.34133529663086,10.397791862487793,12.213332176208496,10.679351806640625,10.866207122802734,12.727506637573242,12.014662742614746,11.375956535339355,11.560434341430664,11.836230278015137,11.563995361328125,11.774842262268066,12.661656379699707,12.052255630493164,12.632277488708496,12.998995780944824,11.886858940124512,11.794242858886719,11.279470443725586,11.747373580932617,11.467353820800781,11.522926330566406,12.396764755249023,11.41773509979248,11.615609169006348,11.266353607177734,10.836119651794434,11.99575138092041,11.439009666442871,11.401458740234375,10.71200942993164,10.5733642578125,11.368525505065918,11.1654691696167,11.06751537322998,9.577695846557617,10.138057708740234,12.07150650024414,12.352556228637695,11.758434295654297,11.704349517822266,12.921028137207031,12.740896224975586,13.882551193237305,13.422533988952637,12.84221076965332,12.64645004272461,13.847597122192383,12.39450454711914,13.587259292602539,13.650285720825195,12.715987205505371,13.115017890930176,12.453730583190918,11.99563980102539,13.25558853149414,14.359271049499512,13.34585952758789,13.138864517211914,13.492166519165039,13.047304153442383,14.059749603271484,13.71388053894043,13.934849739074707,14.224386215209961,13.886760711669922,12.955425262451172,13.749088287353516,13.622922897338867,14.959569931030273,13.482109069824219,14.596263885498047,14.296652793884277,13.932312965393066,13.447366714477539,13.583455085754395,14.572944641113281,14.858070373535156,14.331680297851562,14.828617095947266,15.257987976074219,15.693771362304688,15.150877952575684,15.697793006896973,14.69286823272705,15.42609977722168,15.767930030822754,14.776970863342285,15.029215812683105,15.46146297454834,15.790730476379395,15.648966789245605,16.82862663269043,16.494041442871094,15.348098754882812,16.086036682128906,15.684901237487793,16.667606353759766,16.42267417907715],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"Type=German (Ablated)\\u003cbr\\u003eCheckpoint=%{x}\\u003cbr\\u003eLogit=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"German (Ablated)\",\"line\":{\"color\":\"#00cc96\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"German (Ablated)\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142],\"xaxis\":\"x\",\"y\":[-0.17300258576869965,-0.17300258576869965,-0.17280694842338562,-0.16958726942539215,-0.12840408086776733,0.06258606165647507,1.1072261333465576,2.1022820472717285,4.287145614624023,3.884943723678589,4.460921764373779,5.672050952911377,11.522045135498047,9.306735038757324,12.2717924118042,12.117833137512207,12.863978385925293,13.178726196289062,12.212310791015625,13.001518249511719,13.243721008300781,13.411994934082031,11.226743698120117,15.101607322692871,14.391641616821289,14.841590881347656,14.466219902038574,14.53682804107666,15.64335823059082,15.185741424560547,15.23498249053955,15.919408798217773,15.379220962524414,14.387911796569824,14.119624137878418,12.302947998046875,14.54329776763916,13.668237686157227,12.335515975952148,13.644979476928711,13.895681381225586,12.644173622131348,12.926121711730957,13.182228088378906,12.721092224121094,13.573004722595215,11.98377799987793,10.92475414276123,14.215717315673828,9.477818489074707,11.804056167602539,12.819794654846191,13.470602035522461,12.141407012939453,10.528339385986328,12.821798324584961,9.74563980102539,10.150836944580078,10.73841381072998,11.36657428741455,10.832999229431152,10.184021949768066,11.275745391845703,11.908384323120117,11.383052825927734,14.31104850769043,13.550975799560547,13.36739444732666,11.72642707824707,13.045762062072754,12.549264907836914,12.593106269836426,11.689682960510254,11.111806869506836,11.753570556640625,11.582620620727539,12.356302261352539,12.275498390197754,11.358942985534668,13.760791778564453,14.157054901123047,11.847504615783691,11.693516731262207,12.755671501159668,12.227636337280273,12.168498992919922,10.164536476135254,11.394044876098633,12.612016677856445,11.86572265625,13.152242660522461,12.05971908569336,12.710634231567383,11.118390083312988,11.995397567749023,13.142342567443848,11.402451515197754,12.021759033203125,12.615303993225098,12.718178749084473,13.35556411743164,13.108222961425781,14.072195053100586,14.045768737792969,13.525313377380371,12.803945541381836,13.28030776977539,12.093873023986816,12.94128131866455,13.079442977905273,14.367013931274414,14.502793312072754,15.276692390441895,12.656424522399902,13.00744915008545,12.11927318572998,12.131162643432617,13.594219207763672,14.274908065795898,14.237724304199219,15.933621406555176,13.338327407836914,13.336897850036621,12.782976150512695,12.43007755279541,12.572736740112305,12.796407699584961,11.589284896850586,13.183119773864746,12.718198776245117,11.84082317352295,13.43430233001709,14.844158172607422,14.485506057739258,14.716287612915039,13.888482093811035,13.836578369140625,13.129180908203125,15.954861640930176,14.91471004486084,15.233123779296875,14.035134315490723,15.888760566711426],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"Type=English (Ablated)\\u003cbr\\u003eCheckpoint=%{x}\\u003cbr\\u003eLogit=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"English (Ablated)\",\"line\":{\"color\":\"#ab63fa\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"English (Ablated)\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142],\"xaxis\":\"x\",\"y\":[0.5722870826721191,0.5722870826721191,0.573891818523407,0.6039462089538574,0.974795401096344,2.1922550201416016,4.816803455352783,6.048780918121338,9.181288719177246,9.48666000366211,10.32547378540039,12.51365852355957,11.65676498413086,12.923164367675781,10.86159610748291,12.465752601623535,12.049710273742676,11.87348747253418,11.350144386291504,12.188594818115234,11.68039321899414,11.581832885742188,12.144065856933594,11.0441255569458,10.41468620300293,11.303632736206055,10.54481315612793,10.533469200134277,10.920717239379883,11.042878150939941,10.867266654968262,10.901411056518555,10.674886703491211,11.697776794433594,12.320782661437988,12.37687873840332,11.84857177734375,11.656549453735352,12.572652816772461,12.651421546936035,11.328559875488281,11.89487075805664,11.254859924316406,11.855999946594238,11.797475814819336,11.853797912597656,13.42441177368164,12.141212463378906,12.752910614013672,13.644577026367188,12.950772285461426,12.871479034423828,13.317729949951172,13.146241188049316,12.575664520263672,13.489601135253906,13.483314514160156,12.87452507019043,13.896251678466797,14.089442253112793,13.027192115783691,13.109392166137695,13.039542198181152,13.262285232543945,13.170733451843262,13.04321575164795,14.146139144897461,13.125253677368164,13.064796447753906,13.056209564208984,12.392534255981445,13.169038772583008,12.917033195495605,12.875997543334961,12.422408103942871,12.249580383300781,12.518697738647461,12.459890365600586,12.408832550048828,11.46339225769043,12.055339813232422,13.917434692382812,13.705202102661133,13.126947402954102,13.061168670654297,13.620353698730469,13.60565185546875,14.155570983886719,13.94753360748291,13.205754280090332,13.37480354309082,14.044973373413086,13.11798095703125,13.657240867614746,13.952345848083496,13.336395263671875,13.226602554321289,13.04455852508545,12.662677764892578,13.693229675292969,14.585604667663574,13.735194206237793,13.504522323608398,13.877185821533203,13.492705345153809,14.429413795471191,14.24992847442627,14.142616271972656,14.485681533813477,14.109551429748535,13.66098403930664,14.107714653015137,14.38548469543457,15.068267822265625,13.853116989135742,14.65153980255127,14.494306564331055,14.387784957885742,14.47367000579834,14.286806106567383,14.83323860168457,15.446901321411133,15.026910781860352,15.299883842468262,15.700969696044922,16.054079055786133,15.627455711364746,15.868734359741211,15.073970794677734,15.836321830749512,16.193321228027344,15.203965187072754,15.670069694519043,16.148181915283203,16.418834686279297,16.25434112548828,17.109622955322266,16.849424362182617,15.818391799926758,16.193681716918945,16.19570541381836,16.976016998291016,16.695526123046875],\"yaxis\":\"y\",\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Checkpoint\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Logit\"}},\"legend\":{\"title\":{\"text\":\"Type\"},\"tracegroupgap\":0},\"title\":{\"text\":\"Logits for 'Wir haben in all' - 'en' (GER) vs ' the' (ENG)\"}},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('e6539a8a-b07e-464d-b912-db349168aebd');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lines = [german_logits, english_logits, german_ablated_logits, english_ablated_logits] + [[i for i in range(NUM_CHECKPOINTS)]]\n",
    "names = [\"German\", \"English\", \"German (Ablated)\", \"English (Ablated)\", \"Checkpoint\"]\n",
    "title = \"Logits for 'Wir haben in all' - 'en' (GER) vs ' the' (ENG)\"\n",
    "df = pd.DataFrame({name:line for name, line in zip(names, lines)})\n",
    "df = df.melt(id_vars=[\"Checkpoint\"], var_name=\"Type\", value_name=\"Logit\")\n",
    "px.line(df, x=\"Checkpoint\", y=\"Logit\", color=\"Type\", title=title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
