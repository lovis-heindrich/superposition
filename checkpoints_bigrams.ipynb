{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from transformer_lens import HookedTransformer\n",
    "from jaxtyping import Float, Int, Bool\n",
    "from torch import Tensor\n",
    "from tqdm.auto import tqdm\n",
    "import plotly.io as pio\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px \n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from IPython.display import display, HTML\n",
    "from datasets import load_dataset\n",
    "from collections import Counter\n",
    "import pickle\n",
    "import os\n",
    "import haystack_utils\n",
    "from transformer_lens import utils\n",
    "from fancy_einsum import einsum\n",
    "import einops\n",
    "import json\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from datasets import load_dataset\n",
    "import random\n",
    "import math\n",
    "import random\n",
    "import neel.utils as nutils\n",
    "from neel_plotly import *\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.utils import shuffle\n",
    "import probing_utils\n",
    "import pickle\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "import gzip\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotting_utils\n",
    "import re\n",
    "\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "pio.renderers.default = \"notebook_connected+notebook\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "#torch.autograd.set_grad_enabled(False)\n",
    "#torch.set_grad_enabled(False)\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n",
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model EleutherAI/pythia-70m into HookedTransformer\n",
      "data/german_europarl.json: Loaded 2000 examples with 152 to 2000 characters each.\n",
      "data/english_europarl.json: Loaded 2000 examples with 165 to 2000 characters each.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f954f966da1747139eabda4e1df69870",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_model(checkpoint: int) -> HookedTransformer:\n",
    "    model = HookedTransformer.from_pretrained(\"EleutherAI/pythia-70m\",\n",
    "        checkpoint_index=checkpoint,\n",
    "        center_unembed=True,\n",
    "        center_writing_weights=True,\n",
    "        fold_ln=True,\n",
    "        device=device)\n",
    "    return model\n",
    "\n",
    "NUM_CHECKPOINTS = 143\n",
    "LAYER, NEURON = 3, 669\n",
    "model = get_model(142)\n",
    "german_data = haystack_utils.load_json_data(\"data/german_europarl.json\")\n",
    "english_data = haystack_utils.load_json_data(\"data/english_europarl.json\")\n",
    "all_ignore, _ = haystack_utils.get_weird_tokens(model, plot_norms=False)\n",
    "common_tokens = haystack_utils.get_common_tokens(german_data, model, all_ignore, k=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find ambiguous bigrams and trigrams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7ed14a2eed64c06bbc3afb53d803cd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "003e7eb488ca4597b1167530cf4c5aa2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_bigram_token_counts(data, model: HookedTransformer):\n",
    "    counts = torch.zeros((model.cfg.d_vocab, model.cfg.d_vocab))\n",
    "    for sentence in tqdm(data):\n",
    "        tokens = model.to_tokens(sentence).flatten().cpu()\n",
    "        next_tokens = tokens[1:]\n",
    "        for i in range(len(tokens) - 2):\n",
    "            counts[tokens[i], next_tokens[i]] += 1\n",
    "    return counts\n",
    "\n",
    "english_bigram_counts = get_bigram_token_counts(english_data, model)\n",
    "german_bigram_counts = get_bigram_token_counts(german_data, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_punctuation_tokens(model):\n",
    "    punctuation = [\"\\n\", \".\", \",\", \"!\", \"?\", \";\", \":\", \"-\", \"(\", \")\", \"[\", \"]\", \"{\", \"}\", \"<\", \">\", \"/\", \"\\\\\", \"\\\"\", \"'\"]\n",
    "    leading_space_punctuation = [\" \" + char for char in punctuation]\n",
    "    punctuation_tokens = model.to_tokens(punctuation + leading_space_punctuation + [' â€“', \" \", '  ', \"<|endoftext|>\"])[:, 1].flatten()\n",
    "    return punctuation_tokens\n",
    "\n",
    "def get_number_tokens(model:HookedTransformer):\n",
    "    all_tokens = [i for i in range(model.cfg.d_vocab)]\n",
    "    number_tokens = []\n",
    "    for token in all_tokens:\n",
    "        str_token = model.to_single_str_token(token)\n",
    "        try:\n",
    "            float(str_token)\n",
    "            number_tokens.append(token)\n",
    "        except ValueError:\n",
    "            pass\n",
    "    return torch.LongTensor(number_tokens)\n",
    "\n",
    "def get_all_non_letter_tokens(model: HookedTransformer):\n",
    "    all_tokens = [i for i in range(model.cfg.d_vocab)]\n",
    "    letter_tokens = []\n",
    "    for token in all_tokens:\n",
    "        str_token = model.to_single_str_token(token)\n",
    "        if not bool(re.search(r'[a-zA-Z]', str_token)):\n",
    "            letter_tokens.append(token)\n",
    "    return torch.LongTensor(letter_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation_tokens = get_punctuation_tokens(model)\n",
    "number_tokens = get_number_tokens(model)\n",
    "non_letter_tokens = get_all_non_letter_tokens(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_common_tokens(token_counts, threshold=100):\n",
    "    top_tokens = torch.argwhere(token_counts > threshold).flatten()\n",
    "    return top_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "german_bigram_counts[non_letter_tokens] = 0\n",
    "german_bigram_counts[:, non_letter_tokens] = 0\n",
    "english_bigram_counts[non_letter_tokens] = 0\n",
    "english_bigram_counts[:, non_letter_tokens] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50304, 50304])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "german_bigram_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93 [' all', ' EU', ' inform', 'ations', 'idents', 'ia', ' find', 'a', 'b', 'i', 'k', 'l', ' will', 'n', 'o', ' so', 's', 'y', 'z', ' international', ' K', ' Mon', ' her', ' hand', 'ann', ' V', ' best', ' St', ' also', 'PE', ' am', ' get', ' t', ' a', 'in', 're', ' bring', 'er', ' Union', 'en', 'is', 'it', 'ed', 'es', 'an', ' an', 'ing', 'ar', ' in', 'ou', 'as', ' and', 'ro', ' national', 'el', ' T', ' I', 'ol', 'am', 'ation', ' be', ' S', ' for', ' C', ' he', ' M', ' we', ' set', ' P', ' de', 'ise', 'os', ' B', ' H', 'ers', ' D', ' F', ' W', ' R', ' not', ' L', 'ort', ' un', ' G', ' E', 'ies', ' Bar', ' O', ' end', ' me', ' J', 'ast', 'ans']\n"
     ]
    }
   ],
   "source": [
    "top_german_tokens = get_common_tokens(german_bigram_counts.sum(0))\n",
    "top_english_tokens = get_common_tokens(english_bigram_counts.sum(0))\n",
    "common_both = torch.tensor(list(set(top_english_tokens.tolist()).intersection(set(top_german_tokens.tolist()))))\n",
    "print(len(common_both), model.to_str_tokens(torch.LongTensor(common_both)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' all', 'en', ' the'] 270.0 326.0\n",
      "[' EU', ' und', \"'s\"] 51.0 76.0\n",
      "[' inform', 'ieren', ' you'] 58.0 59.0\n",
      "['b', 'ens', 'oda'] 292.0 87.0\n",
      "['l', 'uss', 'ause'] 302.0 214.0\n",
      "[' will', 'kom', ' be'] 102.0 640.0\n",
      "[' so', 'z', ' that'] 196.0 170.0\n",
      "[' Mon', 'aten', 'etary'] 70.0 106.0\n",
      "[' St', 'ell', 'ras'] 116.0 147.0\n",
      "[' am', ' Don', 'ending'] 109.0 130.0\n",
      "[' t', 'ats', 'abled'] 63.0 83.0\n",
      "[' Union', ' und', ' and'] 111.0 149.0\n",
      "[' an', ' die', ' important'] 424.0 129.0\n",
      "[' in', ' der', ' the'] 1261.0 3212.0\n",
      "[' and', 'eren', ' the'] 339.0 1773.0\n",
      "['ro', 'ffen', 'so'] 108.0 77.0\n",
      "[' I', 'hn', ' would'] 528.0 1020.0\n",
      "['ol', 'ge', 'ences'] 89.0 53.0\n",
      "['am', 'ten', ' President'] 110.0 629.0\n",
      "[' be', 'gr', ' a'] 257.0 161.0\n",
      "[' S', 'itz', 'wo'] 901.0 78.0\n",
      "[' for', 'dern', ' the'] 59.0 1641.0\n",
      "[' we', 'il', ' are'] 291.0 645.0\n",
      "[' P', 'unk', 'PE'] 973.0 79.0\n",
      "[' not', 'wend', ' only'] 125.0 150.0\n",
      "['ort', 'en', 'eur'] 72.0 380.0\n",
      "[' end', 'g', ' of'] 94.0 129.0\n",
      "[' me', 'ine', ' to'] 533.0 111.0\n"
     ]
    }
   ],
   "source": [
    "for common_token in common_both:\n",
    "    german_next_token = german_bigram_counts[common_token].argmax()\n",
    "    english_next_token = english_bigram_counts[common_token].argmax()\n",
    "    num_occurrences_german = german_bigram_counts[common_token, german_next_token].item()\n",
    "    num_occurrences_english = english_bigram_counts[common_token, english_next_token].item()\n",
    "    if (german_next_token != english_next_token) and (num_occurrences_german>50) and (num_occurrences_english>50):\n",
    "        print(model.to_str_tokens(torch.LongTensor([common_token, german_next_token, english_next_token])), num_occurrences_german, num_occurrences_english)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6600"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(non_letter_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43704\n"
     ]
    }
   ],
   "source": [
    "valid_tokens = [i for i in range(model.cfg.d_vocab) if i not in non_letter_tokens]\n",
    "print(len(valid_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e307efdf49a48f8b39871c6ca2a5dca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1410 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n"
     ]
    }
   ],
   "source": [
    "common_bigrams = []\n",
    "top_german_tokens = get_common_tokens(german_bigram_counts.sum(0), 50)\n",
    "bigram_threshold = 20\n",
    "for first_token in tqdm(top_german_tokens):\n",
    "    german_bigrams = german_bigram_counts[first_token]\n",
    "    english_bigrams = english_bigram_counts[first_token]\n",
    "    new_bigrams = torch.argwhere((german_bigrams > bigram_threshold) & (english_bigrams > bigram_threshold)).flatten()\n",
    "    for second_token in new_bigrams.tolist():\n",
    "        common_bigrams.append((first_token, second_token))\n",
    "print(len(common_bigrams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd1ad7b582de4fe69531cf788fffd855",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "last_trigram_tokens = torch.zeros((len(common_bigrams), model.cfg.d_vocab, 2), dtype=torch.long)\n",
    "for bigram_index, bigram in tqdm(enumerate(common_bigrams)):\n",
    "    for sentence in german_data:\n",
    "        tokens = model.to_tokens(sentence).flatten().cpu()\n",
    "        next_tokens = tokens[1:]\n",
    "        for i in range(len(tokens) - 3):\n",
    "            if (tokens[i] == bigram[0]) and (tokens[i+1] == bigram[1]):\n",
    "                last_trigram_tokens[bigram_index, tokens[i+2], 0] += 1\n",
    "    for sentence in english_data:\n",
    "        tokens = model.to_tokens(sentence).flatten().cpu()\n",
    "        next_tokens = tokens[1:]\n",
    "        for i in range(len(tokens) - 3):\n",
    "            if (tokens[i] == bigram[0]) and (tokens[i+1] == bigram[1]):\n",
    "                last_trigram_tokens[bigram_index, tokens[i+2], 1] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_trigram_tokens[:, non_letter_tokens, :] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' in', ' all', 'en', ' the'] 48 18\n"
     ]
    }
   ],
   "source": [
    "trigram_threshold = 10\n",
    "for trigram in range(len(common_bigrams)):\n",
    "    most_common_german_token = torch.argmax(last_trigram_tokens[trigram, :, 0])\n",
    "    most_common_english_token = torch.argmax(last_trigram_tokens[trigram, :, 1])\n",
    "    german_occurrences = last_trigram_tokens[trigram, most_common_german_token, 0].item()\n",
    "    english_occurrences = last_trigram_tokens[trigram, most_common_english_token, 1].item()\n",
    "    if (most_common_german_token != most_common_english_token) and (german_occurrences>trigram_threshold) and (english_occurrences>trigram_threshold):\n",
    "        print(model.to_str_tokens(torch.LongTensor([common_bigrams[trigram][0], common_bigrams[trigram][1], most_common_german_token, most_common_english_token])), german_occurrences, english_occurrences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
