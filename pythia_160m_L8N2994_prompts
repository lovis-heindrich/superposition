# %%
import torch
from tqdm.auto import tqdm
from transformer_lens import HookedTransformer
from jaxtyping import Float, Int, Bool
from torch import Tensor
from tqdm.auto import tqdm
import plotly.io as pio
import ipywidgets as widgets
from IPython.display import display, clear_output
import pandas as pd
import numpy as np
import plotly.express as px 
from collections import defaultdict
import matplotlib.pyplot as plt
import re
from IPython.display import display, HTML
from datasets import load_dataset
from collections import Counter
import pickle
import os

import sklearn
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import f1_score

pio.renderers.default = "notebook_connected+notebook"
device = "cuda" if torch.cuda.is_available() else "cpu"
torch.autograd.set_grad_enabled(False)
torch.set_grad_enabled(False)

from haystack_utils import get_mlp_activations
from hook_utils import save_activation
import haystack_utils
import hook_utils
import plotting_utils

%reload_ext autoreload
%autoreload 2

model = HookedTransformer.from_pretrained("EleutherAI/pythia-160m",
    center_unembed=True,
    center_writing_weights=True,
    fold_ln=True,
    device=device)

german_data = haystack_utils.load_json_data("data/german_europarl.json")[:200]
english_data = haystack_utils.load_json_data("data/english_europarl.json")[:200]
all_ignore, _ = haystack_utils.get_weird_tokens(model, plot_norms=False)
common_tokens = haystack_utils.get_common_tokens(german_data, model, all_ignore, k=100)

LAYER, NEURON = 8, 2994
neuron_activations = haystack_utils.get_mlp_activations(german_data, LAYER, model, neurons=torch.LongTensor([NEURON]), mean=False).flatten()

def get_next_token_punctuation_mask(tokens: torch.LongTensor) -> torch.BoolTensor:
    next_token_punctuation_mask = torch.zeros_like(tokens, dtype=torch.bool)
    token_strs = model.to_str_tokens(tokens)
    for i in range(tokens.shape[0] - 1):
        next_token_str = token_strs[i + 1]
        next_is_space = next_token_str[0] in [" ", ",", ".", ":", ";", "!", "?"]
        next_token_punctuation_mask[i] = next_is_space
    return next_token_punctuation_mask
# %%

def interest_measure(original_loss: torch.FloatTensor, ablated_loss: torch.FloatTensor):
    """Per-token measure, mixture of overall loss increase and loss increase from ablating MLP11"""
    loss_diff = (ablated_loss - original_loss) # Loss increase from context neuron
    loss_diff[original_loss > 6] = 0
    loss_diff[original_loss > ablated_loss] = 0
    return loss_diff

def print_prompt(prompt: str, fwd_hooks: list[tuple[str, callable]], max_value=4):
    """Red/blue scale showing the interest measure for each token"""
    tokens = model.to_tokens(prompt)[0]
    str_token_prompt = model.to_str_tokens(tokens)
    with model.hooks(fwd_hooks):
        ablated_loss = model(prompt, return_type='loss', loss_per_token=True).flatten()
    original_loss = model(prompt, return_type='loss', loss_per_token=True).flatten()
    
    pos_wise_diff = interest_measure(original_loss, ablated_loss).flatten().cpu().tolist()

    loss_list = [loss.flatten().cpu().tolist() for loss in [original_loss, ablated_loss]]
    loss_names = ["original_loss", "ablated_loss"]
    haystack_utils.clean_print_strings_as_html(str_token_prompt[1:], pos_wise_diff, max_value=max_value, additional_measures=loss_list, additional_measure_names=loss_names)

def snap_to_closest_peak(value, hook):
    '''Doesn't snap disabled and ambiguous activations'''
    neuron_act = value[:, :, NEURON]
    value[:, :, NEURON][(neuron_act > 0.8) & (neuron_act < 4.1)] = 2.5
    value[:, :, NEURON][(neuron_act > 4.8)] = 6.5
    return value

def snap_to_peak_1(value, hook):
    '''Doesn't snap disabled and ambiguous activations'''
    neuron_act = value[:, :, NEURON]
    value[:, :, NEURON][(neuron_act > 0.8) & (neuron_act < 4.1)] = 2.5
    value[:, :, NEURON][(neuron_act > 4.8)] = 2.5
    return value

def snap_to_peak_2(value, hook):
    '''Doesn't snap disabled and ambiguous activations'''
    neuron_act = value[:, :, NEURON]
    value[:, :, NEURON][(neuron_act > 0.8) & (neuron_act < 4.1)] = 6.5
    value[:, :, NEURON][(neuron_act > 4.8)] = 6.5
    return value

def get_tokenwise_high_loss_diffs(prompt: str, model: HookedTransformer):
    with model.hooks([(f'blocks.{LAYER}.mlp.hook_post', snap_to_closest_peak)]):
        snap_to_closest_peak_loss = model(prompt, return_type='loss', loss_per_token=True).flatten().cpu()
    with model.hooks([(f'blocks.{LAYER}.mlp.hook_post', snap_to_peak_1)]):
        snap_to_peak_1_loss = model(prompt, return_type='loss', loss_per_token=True).flatten().cpu()
    with model.hooks([(f'blocks.{LAYER}.mlp.hook_post', snap_to_peak_2)]):
        snap_to_peak_2_loss = model(prompt, return_type='loss', loss_per_token=True).flatten().cpu()
    original_loss = model(prompt, return_type='loss', loss_per_token=True).flatten().cpu()

    return [snap_to_closest_peak_loss, snap_to_peak_1_loss, snap_to_peak_2_loss, original_loss]

# %%
def get_ablate_at_mask_hooks(mask: torch.BoolTensor, act_value=3.2) -> list[tuple[str, callable]]:
    def ablate_neuron_hook(value, hook):
        value[:, mask, NEURON] = act_value
    hook_name = f'blocks.{LAYER}.mlp.hook_post'
    return [(hook_name, ablate_neuron_hook)]


for prompt in german_data[:50]:
    tokens = model.to_tokens(prompt)[0]
    mask = get_next_token_punctuation_mask(tokens)
    print_prompt(prompt, get_ablate_at_mask_hooks(~mask, 6.5))
# %%
def compute_loss_increase(prompts: list[str], model, snapping_hook, pos=-1):
    loss_diffs = []
    for prompt in tqdm(prompts):
        with model.hooks(snapping_hook):
            ablated_loss = model(prompt, return_type='loss', loss_per_token=True).flatten()[pos].item()
        original_loss = model(prompt, return_type='loss', loss_per_token=True).flatten()[pos].item()
        loss_diffs.append(ablated_loss-original_loss)
    return loss_diffs

def plot_snapping_losses(mid_word_examples, new_word_examples, model, snap_pos_to_peak_1_hook, snap_pos_to_peak_2_hook, compute_loss_increase, target):
    mid_word_peak_1_increase = compute_loss_increase(mid_word_examples, model, snap_pos_to_peak_1_hook)
    mid_word_peak_2_increase = compute_loss_increase(mid_word_examples, model, snap_pos_to_peak_2_hook)
    new_word_peak_1_increase = compute_loss_increase(new_word_examples, model, snap_pos_to_peak_1_hook)
    new_word_peak_2_increase = compute_loss_increase(new_word_examples, model, snap_pos_to_peak_2_hook)

    mid_word_peak_1_mean = np.mean(mid_word_peak_1_increase)
    mid_word_peak_2_mean = np.mean(mid_word_peak_2_increase)
    new_word_peak_1_mean = np.mean(new_word_peak_1_increase)
    new_word_peak_2_mean = np.mean(new_word_peak_2_increase)

    mid_word_peak_1_std = np.std(mid_word_peak_1_increase)
    mid_word_peak_2_std = np.std(mid_word_peak_2_increase)
    new_word_peak_1_std = np.std(new_word_peak_1_increase)
    new_word_peak_2_std = np.std(new_word_peak_2_increase)

    z_value = 1.96

    mid_word_peak_1_ci = (mid_word_peak_1_std / np.sqrt(len(mid_word_peak_1_increase))) * z_value
    mid_word_peak_2_ci = (mid_word_peak_2_std / np.sqrt(len(mid_word_peak_2_increase))) * z_value
    new_word_peak_1_ci = (new_word_peak_1_std / np.sqrt(len(new_word_peak_1_increase))) * z_value
    new_word_peak_2_ci = (new_word_peak_2_std / np.sqrt(len(new_word_peak_2_increase))) * z_value

    data = {
        'Peaks': ['Mid_Word_Peak_1', 'New_Word_Peak_1', 'Mid_Word_Peak_2', 'New_Word_Peak_2'],
        'Mean': [mid_word_peak_1_mean, new_word_peak_1_mean, mid_word_peak_2_mean, new_word_peak_2_mean],
        '95% CI': [mid_word_peak_1_ci, new_word_peak_1_ci, mid_word_peak_2_ci, new_word_peak_2_ci]
    }

    df = pd.DataFrame(data)

    fig = px.bar(df, x='Peaks', y='Mean', error_y='95% CI', title=f"Loss Increase for '{target}'")
    fig.show()

# %%

def snap_pos_to_peak_1(value, hook):
    '''Doesn't snap disabled and ambiguous activations'''
    value[:, -2, NEURON] = 2.5
    return value

def snap_pos_to_peak_2(value, hook):
    '''Doesn't snap disabled and ambiguous activations'''
    value[:, -2, NEURON] = 6.5
    return value

snap_pos_to_peak_1_hook = [(f'blocks.{LAYER}.mlp.hook_post', snap_pos_to_peak_1)]
snap_pos_to_peak_2_hook = [(f'blocks.{LAYER}.mlp.hook_post', snap_pos_to_peak_2)]


def compute_batched_loss_increase(prompts: list[str], model, snapping_hook, pos=-1):
    with model.hooks(snapping_hook):
        ablated_loss = model(prompts, return_type='loss', loss_per_token=True)[:, pos]
    original_loss = model(prompts, return_type='loss', loss_per_token=True)[:, pos]
    return (ablated_loss-original_loss).tolist()


new_word_prompt = " Das Museum bot ihnen einen Rabatt an, weil er Student ist"
mid_word_prompt = " Das Museum bot ihnen einen Rabatt an, weil sie Studenten"

#new_word_prompt = " Zum Wandern gehen sie gerne in der Region im"
#mid_word_prompt = " Zum Wandern gehen sie gerne in den Regionen"

# new_word_prompt = " Guten Wein kann man besonders in der Region s"
# mid_word_prompt = " Guten Wein kann man besonders in den Regionen"


haystack_utils.print_tokenized_word(mid_word_prompt, model), haystack_utils.print_tokenized_word(new_word_prompt, model)
mid_word_prompts = haystack_utils.generate_random_prompts(mid_word_prompt, model, common_tokens, 100, length=20)
new_word_prompts = haystack_utils.generate_random_prompts(new_word_prompt, model, common_tokens, 100, length=20)

plot_snapping_losses(mid_word_prompts, new_word_prompts, model, 
                     snap_pos_to_peak_1_hook, snap_pos_to_peak_2_hook, 
                     compute_batched_loss_increase, target=f'"{mid_word_prompt}" vs "{new_word_prompt}"')
# %%
all_counts = torch.zeros(model.cfg.d_vocab)
next_is_space_counts = torch.zeros(model.cfg.d_vocab)

for prompt in tqdm(german_data):
    tokens = model.to_tokens(prompt, prepend_bos=False).flatten().cpu()
    # 2. Align next and current tokens
    # 3. Apply is_space mask
    is_space = haystack_utils.get_next_token_punctuation_mask(tokens, model)[:-1]
    # 4. Store next_is_space per token
    all_counts.index_add_(0, tokens[:-1], torch.ones_like(tokens[:-1], dtype=torch.float))
    next_is_space_counts.index_add_(0, tokens, is_space.to(torch.float))

print(all_counts.sum(), next_is_space_counts.sum(), all_counts.sum() - next_is_space_counts.sum())
# %%
common_tokens = torch.argwhere(all_counts > 100).flatten()
common_tokens_space_prob = next_is_space_counts[common_tokens] / all_counts[common_tokens]
difficult_tokens = common_tokens[(common_tokens_space_prob < 0.6) & (common_tokens_space_prob > 0.4)]
print(len(difficult_tokens))
print(model.to_str_tokens(difficult_tokens))
# %%

# %%
d_vocab = model.cfg.d_vocab
all_counts = torch.zeros(d_vocab, d_vocab)
next_is_space_counts = torch.zeros(d_vocab, d_vocab)

for prompt in tqdm(german_data):
    tokens = model.to_tokens(prompt, prepend_bos=False).flatten().cpu()
    # 2. Align previous, current, and next tokens
    prev_tokens = tokens[:-2]
    current_tokens = tokens[1:-1]
    next_tokens = tokens[2:]
    # 3. Apply is_space mask
    is_space = haystack_utils.get_next_token_punctuation_mask(tokens[1:], model).to(torch.float)[:-1]
    # 4. Store next_is_space per trigram
    for pt, ct, space in zip(prev_tokens, current_tokens, is_space):
        all_counts[pt, ct] += 1
        if space:
            next_is_space_counts[pt, ct] += 1
# %%
common_tokens_space_prob = next_is_space_counts / (all_counts + 1e-10)
print(common_tokens_space_prob.shape, all_counts.shape)
difficult_tokens = torch.argwhere((all_counts > 50) & (common_tokens_space_prob < 0.6) & (common_tokens_space_prob > 0.4))
print(len(difficult_tokens))
# %%
print(model.to_str_tokens(difficult_tokens.flatten()))

# %%
substr = " Au√üen"
for prompt in german_data[:50]:
    if substr in prompt:
        index = prompt.index(substr)
        print(prompt[index-15:index+15])
# %%
