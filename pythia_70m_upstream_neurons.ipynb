{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformer_lens import HookedTransformer\n",
    "from jaxtyping import Float\n",
    "from torch import Tensor\n",
    "import plotly.io as pio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import trange\n",
    "\n",
    "pio.renderers.default = \"notebook_connected+notebook\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.autograd.set_grad_enabled(False)\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "import haystack_utils\n",
    "import hook_utils\n",
    "import pythia_160m_utils\n",
    "import plotting_utils\n",
    "import probing_utils\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "model = HookedTransformer.from_pretrained(\"EleutherAI/pythia-70m\",\n",
    "    center_unembed=True,\n",
    "    center_writing_weights=True,\n",
    "    fold_ln=True,\n",
    "    device=device)\n",
    "\n",
    "german_data = haystack_utils.load_json_data(\"data/german_europarl.json\")[:200]\n",
    "english_data = haystack_utils.load_json_data(\"data/english_europarl.json\")[:200]\n",
    "\n",
    "LAYER, NEURON = 3, 669"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbatched_dot_product\u001b[39m(x, y):\n\u001b[1;32m      2\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mvmap(torch\u001b[39m.\u001b[39mdot)(x, y)\n\u001b[0;32m----> 5\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mneuron_DLA\u001b[39m(prompt: \u001b[39mstr\u001b[39m, model: HookedTransformer, pos\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39ms_[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mtuple\u001b[39m[Float[Tensor, \u001b[39m\"\u001b[39m\u001b[39mcomponent\u001b[39m\u001b[39m\"\u001b[39m], \u001b[39mlist\u001b[39m[\u001b[39mstr\u001b[39m]]:\n\u001b[1;32m      6\u001b[0m \u001b[39m    \u001b[39m\u001b[39m'''Gets full resid decomposition including all neurons'''\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     tokens \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mto_tokens(prompt)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "def batched_dot_product(x, y):\n",
    "    return torch.vmap(torch.dot)(x, y)\n",
    "    \n",
    "def neuron_DLA(\n",
    "        prompt: str, \n",
    "        model: HookedTransformer, \n",
    "        pos=np.s_[-1:]) -> tuple[Float[Tensor, \"component\"], list[str]]:\n",
    "    '''Gets full resid decomposition including all neurons'''\n",
    "    tokens = model.to_tokens(prompt)\n",
    "    answers = tokens[:, 1:]\n",
    "    tokens = tokens[:, :-1]\n",
    "    \n",
    "    _, cache = model.run_with_cache(tokens)\n",
    "    attrs, labels = cache.get_full_resid_decomposition(-1, expand_neurons=True, apply_ln=True, return_labels=True, pos_slice=pos)\n",
    "    answer_residual_directions = model.tokens_to_residual_directions(answers)\n",
    "    if answer_residual_directions.ndim == 1:\n",
    "        answer_residual_directions = answer_residual_directions.unsqueeze(0)  # [1 d_model]\n",
    "    elif answer_residual_directions.ndim == 3:\n",
    "        answer_residual_directions = answer_residual_directions[0]  # [pos d_model]\n",
    "    answer_residual_directions = answer_residual_directions[pos]  # [pos d_model]\n",
    "    neuron_indices = [i for i in range(len(labels)) if 'N' in labels[i]]\n",
    "    neuron_labels = [labels[i] for i in neuron_indices]\n",
    "    neuron_attrs = attrs[neuron_indices, :].squeeze(1)\n",
    "    results = []\n",
    "    for i in range(neuron_attrs.shape[1]):\n",
    "        results.append(batched_dot_product(neuron_attrs[:, i], answer_residual_directions[[i]].repeat(neuron_attrs.shape[0], 1)))\n",
    "    return torch.stack(results), neuron_labels\n",
    "\n",
    "def get_neuron_mean_acts(dla_layer_neuron_tuples: list[tuple[int, int]], model=model) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    layer_neuron_dict = haystack_utils.get_neurons_by_layer(dla_layer_neuron_tuples)\n",
    "\n",
    "    sorted_dla_layer_neuron_tuples = []\n",
    "    sorted_acts = []\n",
    "\n",
    "    for layer in layer_neuron_dict.keys():\n",
    "        neurons = layer_neuron_dict[layer]\n",
    "        mean_acts = haystack_utils.get_mlp_activations(german_data, layer, model, context_crop_start=0, hook_pre=False, neurons=neurons, disable_tqdm=True)\n",
    "        sorted_dla_layer_neuron_tuples.extend([(layer, neuron) for neuron in neurons])\n",
    "        sorted_acts.extend(mean_acts)\n",
    "        assert len(sorted_dla_layer_neuron_tuples) == len(sorted_acts)\n",
    "\n",
    "    return sorted_dla_layer_neuron_tuples, sorted_acts\n",
    "\n",
    "def get_unspecified_neurons(model: HookedTransformer, neurons: list[tuple[int, int]]):\n",
    "    layer_dict = haystack_utils.get_neurons_by_layer(neurons)\n",
    "    unspecified = []\n",
    "    for layer in range(model.cfg.n_layers):\n",
    "        for neuron in range(model.cfg.d_mlp):\n",
    "            if not neuron in layer_dict[layer]:\n",
    "                unspecified.append((layer, neuron))\n",
    "    return unspecified\n",
    "\n",
    "def get_neuron_loss_increases(prompt: str, positionwise=False, model=model, data=german_data):\n",
    "    original_loss = model([prompt], return_type='loss', loss_per_token=positionwise)\n",
    "    \n",
    "    losses = []\n",
    "    for layer in trange(model.cfg.n_layers):\n",
    "        mean_acts = haystack_utils.get_mlp_activations(data[:200], layer, model, disable_tqdm=True, context_crop_start=0)\n",
    "        for neuron in range(model.cfg.d_mlp):\n",
    "            hook = hook_utils.get_ablate_neuron_hook(layer, neuron, mean_acts[neuron])\n",
    "            with model.hooks([hook]):\n",
    "                ablated_loss = model([prompt], return_type='loss', loss_per_token=positionwise)\n",
    "                losses.append((ablated_loss - original_loss)[0])\n",
    "    return losses\n",
    "\n",
    "def compare_dla_and_ablation(token_index, attrs, losses, num_neurons=20, model=model):\n",
    "    print(\"DLA:\")\n",
    "    values, indices = torch.topk(attrs[token_index], num_neurons, dim=-1)\n",
    "    layer_indices, neuron_indices = np.unravel_index(indices.cpu().numpy(), (model.cfg.n_layers, model.cfg.d_mlp))\n",
    "    dla_layer_neuron_tuples = list(zip(layer_indices.tolist(), neuron_indices.tolist()))\n",
    "    indices_1d_dla = [np.ravel_multi_index(index_2d, (model.cfg.n_layers, model.cfg.d_mlp)) for index_2d in dla_layer_neuron_tuples[:num_neurons]]\n",
    "    \n",
    "    print(dla_layer_neuron_tuples[:num_neurons])\n",
    "    print(attrs[token_index][indices_1d_dla])\n",
    "\n",
    "    print(\"Ablation:\")\n",
    "    loss_increases_by_neuron = torch.tensor([loss[token_index] for loss in losses])\n",
    "    values, indices = torch.topk(loss_increases_by_neuron, num_neurons)\n",
    "    indices = indices.numpy()\n",
    "    layer_indices, neuron_indices = np.unravel_index(indices[:num_neurons], (model.cfg.n_layers, model.cfg.d_mlp))\n",
    "    \n",
    "    print(list(zip(layer_indices.tolist(), neuron_indices.tolist())))\n",
    "    print(attrs[token_index][indices.tolist()[:num_neurons]])\n",
    "\n",
    "def get_hook_inputs_for_token_index(loss_increases_by_neuron, model=model, data=german_data, k=40):\n",
    "    values, indices = torch.topk(loss_increases_by_neuron, k)\n",
    "\n",
    "    layer_indices, neuron_indices = np.unravel_index(indices.cpu().numpy(), (model.cfg.n_layers, model.cfg.d_mlp))\n",
    "    causal_layer_neuron_tuples = list(zip(layer_indices.tolist(), neuron_indices.tolist()))\n",
    "    layer_neuron_dict = haystack_utils.get_neurons_by_layer(causal_layer_neuron_tuples)\n",
    "\n",
    "    sorted_dla_layer_neuron_tuples = []\n",
    "    sorted_acts = []\n",
    "\n",
    "    for layer in layer_neuron_dict.keys():\n",
    "        neurons = layer_neuron_dict[layer]\n",
    "        mean_acts = haystack_utils.get_mlp_activations(data, layer, model, context_crop_start=0, neurons=neurons, disable_tqdm=True)\n",
    "        sorted_dla_layer_neuron_tuples.extend([(layer, neuron) for neuron in neurons])\n",
    "        sorted_acts.extend(mean_acts)\n",
    "        assert len(sorted_dla_layer_neuron_tuples) == len(sorted_acts)\n",
    "\n",
    "    return sorted_dla_layer_neuron_tuples, sorted_acts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First run\n",
    "\n",
    "\n",
    "# Ablate each neuron in turn and look at how it affects the context neuron value (meaned over all prompts)\n",
    "\n",
    "# Check whether it activates for German words in an English context, both single common german chars and a full word\n",
    "# Ablate each neuron in turn and look at how it affects the context neuron value (meaned over a German token position within an English)\n",
    "\n",
    "# Collect a list\n",
    "\n",
    "# Look for head that moves German tokens\n",
    "# Look for head that moves German n-grams\n",
    "# Look for n-gram detector and see if it moves from there\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure context neuron activation for many German tokens that never coalesce into German words\n",
    "# Measure context neuron activation for English words with a single German token mixed in\n",
    "# Measure above but make it semantically clear in the English that a German token is about to appear\n",
    "# Measure above but with a full German word\n",
    "# Measure above but semantically clear in English\n",
    "\n",
    "# Optional (if time permits):\n",
    "# Measure above but with common German unigrams\n",
    "\n",
    "# Need a way to measure at position\n",
    "\n",
    "with model.hooks(hook_utils.get)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
