{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformer_lens import HookedTransformer\n",
    "from jaxtyping import Float\n",
    "from torch import Tensor\n",
    "import plotly.io as pio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import trange\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "pio.renderers.default = \"notebook_connected+notebook\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.autograd.set_grad_enabled(False)\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "import haystack_utils\n",
    "import hook_utils\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HookedTransformer.from_pretrained(\"EleutherAI/pythia-70m\",\n",
    "    center_unembed=True,\n",
    "    center_writing_weights=True,\n",
    "    fold_ln=True,\n",
    "    device=device)\n",
    "\n",
    "german_data = haystack_utils.load_json_data(\"data/german_europarl.json\")[:200]\n",
    "english_data = haystack_utils.load_json_data(\"data/english_europarl.json\")[:200]\n",
    "\n",
    "LAYER, NEURON = 3, 669"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batched_dot_product(x: torch.Tensor, y: torch.Tensor):\n",
    "    return torch.vmap(torch.dot)(x, y)\n",
    "    \n",
    "def neuron_to_context_neuron_DLA(\n",
    "        model: HookedTransformer, \n",
    "        prompt: str | list[str], \n",
    "        pos=np.s_[-1:], \n",
    "        context_neuron=tuple[int, int]\n",
    ") -> tuple[Float[Tensor, \"component\"], list[str]]:\n",
    "    '''Gets full resid decomposition including all neurons. Unbatched.'''\n",
    "    tokens = model.to_tokens(prompt)\n",
    "    _, cache = model.run_with_cache(prompt)\n",
    "    layer, neuron = context_neuron\n",
    "    neuron_attrs, neuron_labels = cache.stack_neuron_results(layer, apply_ln=True, return_labels=True, pos_slice=pos)\n",
    "    neuron_attrs = neuron_attrs.squeeze(1)\n",
    "    \n",
    "    answer_residual_direction = model.W_in[layer, :, neuron].unsqueeze(0)  # [1 d_model]\n",
    "\n",
    "    results = []\n",
    "    for i in range(neuron_attrs.shape[1]):\n",
    "        results.append(batched_dot_product(neuron_attrs[:, i], answer_residual_direction.repeat(neuron_attrs.shape[0], 1)))\n",
    "    return torch.stack(results), neuron_labels\n",
    "\n",
    "def get_neuron_mean_acts(model: HookedTransformer, data: list[str], layer_neuron_dict: dict[int, list[int]]) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    sorted_layer_neuron_tuples = []\n",
    "    sorted_acts = []\n",
    "\n",
    "    for layer, neurons in layer_neuron_dict.items():\n",
    "        mean_acts = haystack_utils.get_mlp_activations(data, layer, model, context_crop_start=0, hook_pre=False, neurons=neurons, disable_tqdm=True)\n",
    "        sorted_layer_neuron_tuples.extend([(layer, neuron) for neuron in neurons])\n",
    "        sorted_acts.extend(mean_acts)\n",
    "        assert len(sorted_layer_neuron_tuples) == len(sorted_acts)\n",
    "\n",
    "    return sorted_layer_neuron_tuples, sorted_acts\n",
    "\n",
    "def get_unspecified_neurons(model: HookedTransformer, layer_neuron_dict: dict[int, list[int]]):\n",
    "    unspecified = []\n",
    "    for layer in range(model.cfg.n_layers):\n",
    "        for neuron in range(model.cfg.d_mlp):\n",
    "            if not neuron in layer_neuron_dict[layer]:\n",
    "                unspecified.append((layer, neuron))\n",
    "    return unspecified\n",
    "\n",
    "def get_neuron_loss_increases(model: HookedTransformer, data: list[str], prompt: str, positionwise: bool=False) -> torch.Tensor:\n",
    "    n_tokens = model.to_tokens(prompt).shape[1] - 1\n",
    "    original_loss = model([prompt], return_type='loss', loss_per_token=positionwise)\n",
    "    \n",
    "    losses = []\n",
    "    for layer in trange(model.cfg.n_layers):\n",
    "        mean_acts = haystack_utils.get_mlp_activations(data[:200], layer, model, disable_tqdm=True, context_crop_start=0)\n",
    "        for neuron in range(model.cfg.d_mlp):\n",
    "            hook = hook_utils.get_ablate_neuron_hook(layer, neuron, mean_acts[neuron])\n",
    "            with model.hooks([hook]):\n",
    "                ablated_loss = model([prompt], return_type='loss', loss_per_token=positionwise)\n",
    "                losses.append((ablated_loss - original_loss)[0])\n",
    "    return torch.stack(losses).reshape(n_tokens, model.cfg.n_layers * model.cfg.d_mlp)\n",
    "\n",
    "def compare_dla_and_ablation(model: HookedTransformer, dla_attrs_by_neuron: torch.Tensor, ablation_losses_by_neuron: torch.Tensor, num_neurons=20):\n",
    "    print(\"DLA:\")\n",
    "    values, indices = torch.topk(dla_attrs_by_neuron, num_neurons, dim=-1)\n",
    "    layer_indices, neuron_indices = np.unravel_index(indices.cpu().numpy(), (model.cfg.n_layers, model.cfg.d_mlp))\n",
    "    print(list(zip(layer_indices.tolist(), neuron_indices.tolist())))\n",
    "    print(dla_attrs_by_neuron[indices.tolist()])\n",
    "\n",
    "    print(\"Ablation:\")\n",
    "    loss_increases_by_neuron = ablation_losses_by_neuron\n",
    "    values, indices = torch.topk(loss_increases_by_neuron, num_neurons)\n",
    "    layer_indices, neuron_indices = np.unravel_index(indices.cpu().numpy()[:num_neurons], (model.cfg.n_layers, model.cfg.d_mlp))\n",
    "    print(list(zip(layer_indices.tolist(), neuron_indices.tolist())))\n",
    "    print(dla_attrs_by_neuron[indices.tolist()])\n",
    "\n",
    "def get_hook_inputs_for_token_index(model: HookedTransformer, data: list[str], loss_increases_by_neuron: torch.Tensor, k=40):\n",
    "    values, indices = torch.topk(loss_increases_by_neuron, k)\n",
    "\n",
    "    layer_indices, neuron_indices = np.unravel_index(indices.cpu().numpy(), (model.cfg.n_layers, model.cfg.d_mlp))\n",
    "    layer_neuron_dict = defaultdict(list)\n",
    "    for layer, neuron in zip(layer_indices, neuron_indices):\n",
    "        layer_neuron_dict[layer].append(neuron)\n",
    "\n",
    "    sorted_dla_layer_neuron_tuples = []\n",
    "    sorted_acts = []\n",
    "    for layer, neurons in layer_neuron_dict.items():\n",
    "        mean_acts = haystack_utils.get_mlp_activations(data, layer, model, context_crop_start=0, neurons=neurons, disable_tqdm=True)\n",
    "        sorted_dla_layer_neuron_tuples.extend([(layer, neuron) for neuron in neurons])\n",
    "        sorted_acts.extend(mean_acts)\n",
    "        assert len(sorted_dla_layer_neuron_tuples) == len(sorted_acts)\n",
    "\n",
    "    return sorted_dla_layer_neuron_tuples, sorted_acts\n",
    "\n",
    "def unravel_top_k(neuron_attrs: torch.Tensor, k: int=10):\n",
    "    values, indices = torch.topk(neuron_attrs, k)\n",
    "    layer_indices, neuron_indices = np.unravel_index(indices.cpu().numpy(), (model.cfg.n_layers, model.cfg.d_mlp))\n",
    "    return list(zip(layer_indices.tolist(), neuron_indices.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({(0, 1259): 79, (2, 306): 74, (2, 983): 73, (1, 347): 72, (1, 13): 67, (2, 1449): 66, (2, 1236): 59, (2, 181): 58, (0, 264): 52, (2, 819): 42, (2, 237): 28, (0, 1186): 20, (2, 1658): 19, (1, 1911): 15, (2, 1166): 15, (2, 950): 14, (0, 637): 10, (2, 743): 10, (0, 1715): 7, (0, 1758): 5, (2, 689): 4, (1, 1765): 3, (2, 1149): 2, (0, 1987): 1, (0, 563): 1, (2, 1888): 1, (0, 336): 1, (0, 1874): 1, (1, 835): 1, (1, 2035): 1, (1, 737): 1, (1, 1034): 1, (1, 1414): 1, (2, 1153): 1, (2, 1087): 1, (1, 1109): 1, (1, 1028): 1, (0, 1318): 1, (1, 509): 1})\n"
     ]
    }
   ],
   "source": [
    "def upstream_for_prompt(prompt):\n",
    "    n_tokens = model.to_tokens(prompt).shape[1]\n",
    "    neuron_attrs_by_token, labels = neuron_to_context_neuron_DLA(model, prompt, np.s_[-n_tokens:], (3, 669))\n",
    "\n",
    "    counter = Counter()\n",
    "    for i in range(n_tokens):\n",
    "        counter.update(unravel_top_k(neuron_attrs_by_token[i], k=10))\n",
    "    return counter\n",
    "\n",
    "test_prompt = \"ü ü ü ü ü ü ü ü ü ü ü ü ü ü ü ü ü ü ü ü ü ü ü ü ü ü ü ü ü ü ü ü ü ü ü ü ü ü ü ü ü ü ü ü ü ü ü ü ü ü ü ü ü ü ü ü ü ü ü ü ü ü ü ü ü ü ü ü ü ü ü ü ü ü ü ü ü ü ü ü\"\n",
    "print(upstream_for_prompt(test_prompt))\n",
    "\n",
    "test_prompt = \"ü ü ü ü ü ü ü ü ü ü ü ü ü ü ü ü ü ü ü ü ü ü ü ü ü ü ü ü ü ü ü ü ü ü ü ü ü ü ü ü ü ü ü ü ü ü ü ü ü ü ü ü ü ü ü ü ü ü ü ü ü ü ü ü ü ü ü ü ü ü ü ü ü ü ü ü ü ü ü ü\"\n",
    "print(upstream_for_prompt(test_prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First run\n",
    "\n",
    "\n",
    "# Ablate each neuron in turn and look at how it affects the context neuron value (meaned over all prompts)\n",
    "\n",
    "# Check whether it activates for German words in an English context, both single common german chars and a full word\n",
    "# Ablate each neuron in turn and look at how it affects the context neuron value (meaned over a German token position within an English)\n",
    "\n",
    "# Collect a list\n",
    "\n",
    "# Look for head that moves German tokens\n",
    "# Look for head that moves German n-grams\n",
    "# Look for n-gram detector and see if it moves from there\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure context neuron activation for many German tokens that never coalesce into German words\n",
    "# Measure context neuron activation for English words with a single German token mixed in\n",
    "# Measure above but make it semantically clear in the English that a German token is about to appear\n",
    "# Measure above but with a full German word\n",
    "# Measure above but semantically clear in English\n",
    "\n",
    "# Optional (if time permits):\n",
    "# Measure above but with common German unigrams\n",
    "\n",
    "# Need a way to measure at position\n",
    "\n",
    "with model.hooks(hook_utils.get)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
