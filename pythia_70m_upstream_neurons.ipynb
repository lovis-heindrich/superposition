{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformer_lens import HookedTransformer\n",
    "from jaxtyping import Float\n",
    "from torch import Tensor\n",
    "import plotly.io as pio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import trange\n",
    "from collections import defaultdict, Counter\n",
    "from torchmetrics.regression import SpearmanCorrCoef\n",
    "import plotly_express as px\n",
    "\n",
    "pio.renderers.default = \"notebook_connected+notebook\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.autograd.set_grad_enabled(False)\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "import haystack_utils\n",
    "import hook_utils\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model EleutherAI/pythia-70m into HookedTransformer\n",
      "data/german_europarl.json: Loaded 2000 examples with 152 to 2000 characters each.\n",
      "data/english_europarl.json: Loaded 2000 examples with 165 to 2000 characters each.\n"
     ]
    }
   ],
   "source": [
    "model = HookedTransformer.from_pretrained(\"EleutherAI/pythia-70m\",\n",
    "    center_unembed=True,\n",
    "    center_writing_weights=True,\n",
    "    fold_ln=True,\n",
    "    device=device)\n",
    "\n",
    "german_data = haystack_utils.load_json_data(\"data/german_europarl.json\")[:200]\n",
    "english_data = haystack_utils.load_json_data(\"data/english_europarl.json\")[:200]\n",
    "\n",
    "LAYER, NEURON = 3, 669"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batched_dot_product(x: torch.Tensor, y: torch.Tensor):\n",
    "    return torch.vmap(torch.dot)(x, y)\n",
    "    \n",
    "def neuron_to_context_neuron_DLA(\n",
    "        model: HookedTransformer, \n",
    "        prompt: str | list[str], \n",
    "        pos=np.s_[-1:], \n",
    "        context_neuron=tuple[int, int]\n",
    ") -> tuple[Float[Tensor, \"component\"], list[str]]:\n",
    "    '''Gets full resid decomposition including all neurons. Unbatched.'''\n",
    "    tokens = model.to_tokens(prompt)\n",
    "    _, cache = model.run_with_cache(prompt)\n",
    "    layer, neuron = context_neuron\n",
    "    neuron_attrs, neuron_labels = cache.stack_neuron_results(layer, apply_ln=True, return_labels=True, pos_slice=pos)\n",
    "    neuron_attrs = neuron_attrs.squeeze(1)\n",
    "    \n",
    "    answer_residual_direction = model.W_in[layer, :, neuron].unsqueeze(0)  # [1 d_model]\n",
    "\n",
    "    results = []\n",
    "    for i in range(neuron_attrs.shape[1]):\n",
    "        results.append(batched_dot_product(neuron_attrs[:, i], answer_residual_direction.repeat(neuron_attrs.shape[0], 1)))\n",
    "    return torch.stack(results), neuron_labels\n",
    "\n",
    "def get_neuron_mean_acts(model: HookedTransformer, data: list[str], layer_neuron_dict: dict[int, list[int]]) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    sorted_layer_neuron_tuples = []\n",
    "    sorted_acts = []\n",
    "\n",
    "    for layer, neurons in layer_neuron_dict.items():\n",
    "        mean_acts = haystack_utils.get_mlp_activations(data, layer, model, context_crop_start=0, hook_pre=False, neurons=neurons, disable_tqdm=True)\n",
    "        sorted_layer_neuron_tuples.extend([(layer, neuron) for neuron in neurons])\n",
    "        sorted_acts.extend(mean_acts)\n",
    "        assert len(sorted_layer_neuron_tuples) == len(sorted_acts)\n",
    "\n",
    "    return sorted_layer_neuron_tuples, sorted_acts\n",
    "\n",
    "def get_unspecified_neurons(model: HookedTransformer, layer_neuron_dict: dict[int, list[int]]):\n",
    "    unspecified = []\n",
    "    for layer in range(model.cfg.n_layers):\n",
    "        for neuron in range(model.cfg.d_mlp):\n",
    "            if not neuron in layer_neuron_dict[layer]:\n",
    "                unspecified.append((layer, neuron))\n",
    "    return unspecified\n",
    "\n",
    "def get_neuron_loss_increases(model: HookedTransformer, data: list[str], prompt: str, positionwise: bool=False) -> torch.Tensor:\n",
    "    n_tokens = model.to_tokens(prompt).shape[1] - 1\n",
    "    original_loss = model([prompt], return_type='loss', loss_per_token=positionwise)\n",
    "    \n",
    "    losses = []\n",
    "    for layer in trange(model.cfg.n_layers):\n",
    "        mean_acts = haystack_utils.get_mlp_activations(data[:200], layer, model, disable_tqdm=True, context_crop_start=0)\n",
    "        for neuron in range(model.cfg.d_mlp):\n",
    "            hook = hook_utils.get_ablate_neuron_hook(layer, neuron, mean_acts[neuron])\n",
    "            with model.hooks([hook]):\n",
    "                ablated_loss = model([prompt], return_type='loss', loss_per_token=positionwise)\n",
    "                losses.append((ablated_loss - original_loss)[0])\n",
    "    return torch.stack(losses).reshape(n_tokens, model.cfg.n_layers * model.cfg.d_mlp)\n",
    "\n",
    "def compare_dla_and_ablation(model: HookedTransformer, dla_attrs_by_neuron: torch.Tensor, ablation_losses_by_neuron: torch.Tensor, num_neurons=20):\n",
    "    print(\"DLA:\")\n",
    "    values, indices = torch.topk(dla_attrs_by_neuron, num_neurons, dim=-1)\n",
    "    layer_indices, neuron_indices = np.unravel_index(indices.cpu().numpy(), (model.cfg.n_layers, model.cfg.d_mlp))\n",
    "    print(list(zip(layer_indices.tolist(), neuron_indices.tolist())))\n",
    "    print(dla_attrs_by_neuron[indices.tolist()])\n",
    "\n",
    "    print(\"Ablation:\")\n",
    "    loss_increases_by_neuron = ablation_losses_by_neuron\n",
    "    values, indices = torch.topk(loss_increases_by_neuron, num_neurons)\n",
    "    layer_indices, neuron_indices = np.unravel_index(indices.cpu().numpy()[:num_neurons], (model.cfg.n_layers, model.cfg.d_mlp))\n",
    "    print(list(zip(layer_indices.tolist(), neuron_indices.tolist())))\n",
    "    print(dla_attrs_by_neuron[indices.tolist()])\n",
    "\n",
    "def get_hook_inputs_for_token_index(model: HookedTransformer, data: list[str], loss_increases_by_neuron: torch.Tensor, k=40):\n",
    "    values, indices = torch.topk(loss_increases_by_neuron, k)\n",
    "\n",
    "    layer_indices, neuron_indices = np.unravel_index(indices.cpu().numpy(), (model.cfg.n_layers, model.cfg.d_mlp))\n",
    "    layer_neuron_dict = defaultdict(list)\n",
    "    for layer, neuron in zip(layer_indices, neuron_indices):\n",
    "        layer_neuron_dict[layer].append(neuron)\n",
    "\n",
    "    sorted_dla_layer_neuron_tuples = []\n",
    "    sorted_acts = []\n",
    "    for layer, neurons in layer_neuron_dict.items():\n",
    "        mean_acts = haystack_utils.get_mlp_activations(data, layer, model, context_crop_start=0, neurons=neurons, disable_tqdm=True)\n",
    "        sorted_dla_layer_neuron_tuples.extend([(layer, neuron) for neuron in neurons])\n",
    "        sorted_acts.extend(mean_acts)\n",
    "        assert len(sorted_dla_layer_neuron_tuples) == len(sorted_acts)\n",
    "\n",
    "    return sorted_dla_layer_neuron_tuples, sorted_acts\n",
    "\n",
    "def unravel_top_k(neuron_attrs: torch.Tensor, k: int=10):\n",
    "    values, indices = torch.topk(neuron_attrs, k)\n",
    "    layer_indices, neuron_indices = np.unravel_index(indices.cpu().numpy(), (model.cfg.n_layers, model.cfg.d_mlp))\n",
    "    return list(zip(layer_indices.tolist(), neuron_indices.tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ä [((1, 1911), 346), ((1, 835), 333), ((2, 1449), 320), ((2, 181), 309), ((2, 1236), 309), ((2, 983), 308), ((2, 1166), 296), ((2, 1149), 289), ((2, 1003), 249), ((2, 1747), 223), ((0, 751), 139), ((0, 191), 83), ((0, 1452), 59), ((2, 230), 39), ((0, 777), 37), ((2, 621), 30), ((0, 596), 27), ((2, 1299), 14), ((0, 146), 9), ((2, 720), 9), ((1, 1034), 8), ((1, 961), 8), ((1, 309), 7), ((1, 1690), 6), ((2, 1489), 6), ((1, 1388), 4), ((2, 145), 4), ((1, 1109), 3), ((1, 1308), 3), ((1, 70), 2), ((2, 467), 2), ((0, 1987), 1), ((0, 563), 1), ((2, 1888), 1), ((0, 336), 1), ((0, 1874), 1), ((1, 1414), 1), ((1, 1032), 1), ((0, 1435), 1), ((2, 1188), 1)]\n",
      " ö [((2, 1449), 335), ((1, 707), 330), ((2, 181), 329), ((2, 1236), 329), ((2, 1931), 326), ((2, 1149), 324), ((2, 1003), 324), ((2, 1166), 322), ((1, 29), 320), ((2, 983), 273), ((2, 863), 73), ((1, 703), 26), ((1, 1966), 23), ((2, 1575), 22), ((0, 736), 20), ((0, 1849), 19), ((0, 645), 19), ((1, 1610), 18), ((1, 1911), 10), ((2, 315), 7), ((2, 819), 6), ((0, 637), 5), ((2, 691), 4), ((2, 230), 4), ((1, 347), 4), ((1, 1118), 3), ((1, 47), 2), ((1, 1028), 2), ((2, 1658), 2), ((0, 1987), 1), ((0, 563), 1), ((2, 1888), 1), ((0, 336), 1), ((0, 1874), 1), ((1, 835), 1), ((1, 1129), 1), ((1, 961), 1), ((2, 689), 1)]\n",
      " ü [((0, 1259), 348), ((2, 983), 343), ((2, 306), 343), ((1, 347), 340), ((2, 1449), 334), ((2, 1236), 328), ((2, 181), 327), ((2, 237), 296), ((2, 1166), 286), ((2, 1149), 277), ((1, 13), 59), ((0, 264), 51), ((2, 819), 40), ((0, 1186), 19), ((2, 950), 18), ((2, 1658), 17), ((1, 1911), 15), ((2, 743), 12), ((0, 637), 10), ((0, 1715), 6), ((0, 1758), 5), ((1, 1765), 2), ((0, 1987), 1), ((0, 563), 1), ((2, 1888), 1), ((0, 336), 1), ((0, 1874), 1), ((1, 835), 1), ((1, 1388), 1), ((1, 1109), 1), ((2, 1287), 1), ((2, 770), 1), ((2, 1191), 1), ((2, 1337), 1), ((2, 689), 1), ((1, 509), 1)]\n",
      " ß [((1, 773), 575), ((2, 467), 361), ((0, 621), 348), ((0, 878), 348), ((0, 736), 348), ((2, 1182), 348), ((1, 85), 348), ((2, 1926), 347), ((2, 1363), 345), ((2, 1485), 338), ((1, 57), 334), ((0, 1006), 332), ((1, 497), 329), ((1, 175), 323), ((2, 660), 309), ((1, 369), 294), ((1, 1273), 258), ((2, 1505), 256), ((2, 228), 181), ((1, 195), 171), ((0, 1887), 92), ((0, 1209), 82), ((1, 1667), 60), ((1, 184), 53), ((2, 1280), 36), ((1, 315), 33), ((2, 1087), 27), ((2, 597), 23), ((1, 864), 16), ((2, 279), 14), ((0, 1279), 8), ((2, 167), 7), ((1, 2040), 4), ((2, 909), 3), ((1, 1308), 2), ((2, 1449), 1), ((2, 181), 1), ((2, 1236), 1), ((2, 1149), 1), ((0, 1987), 1), ((0, 563), 1), ((2, 1888), 1), ((0, 336), 1), ((0, 1874), 1), ((1, 835), 1), ((2, 1287), 1), ((2, 1078), 1), ((2, 1064), 1), ((1, 1028), 1), ((2, 1805), 1), ((1, 671), 1), ((0, 1452), 1)]\n"
     ]
    }
   ],
   "source": [
    "def upstream_for_prompt(prompt):\n",
    "    n_tokens = model.to_tokens(prompt).shape[1]\n",
    "    neuron_attrs_by_token, labels = neuron_to_context_neuron_DLA(model, prompt, np.s_[-n_tokens:], (3, 669))\n",
    "\n",
    "    counter = Counter()\n",
    "    for i in range(n_tokens):\n",
    "        counter.update(unravel_top_k(neuron_attrs_by_token[i], k=10))\n",
    "    return counter\n",
    "\n",
    "german_prompt = \"beraten. H\\u00f6here Investitionen in Forschung und Entwicklung sowie die Erfassung und \\\n",
    "    Verarbeitung von zuverl\\u00e4ssigen Daten w\\u00fcrde zu einer solideren und nachhaltigen Gemeinsamen \\\n",
    "    Fischereipolitik f\\u00fchren.\\nAber obwohl der Satz, den ich von einem Wissenschaftler geh\\u00f6rt \\\n",
    "    habe (\\\"Das Problem ist nicht Geld, sondern Personal\\\") die Lage gut darstellt, werde ich nicht \\\n",
    "    diejenige sein, die sagt, dass die Fischereiforschung gut mit finanziellen Mitteln ausgestattet \\\n",
    "    ist. Ich werde vielmehr sagen, dass wir ein doppeltes Problem haben.\\nAn erster Stelle, Herr \\\n",
    "    Kommissar, die im Siebten Rahmenprogramm f\\u00fcr Meeresforschung festgelegten Betr\\u00e4ge, \\\n",
    "    die ein horizontales Thema h\\u00e4tten sein sollen, scheinen f\\u00fcr den integrierten Ansatz, \\\n",
    "    der bei dieser Angelegenheit gegenw\\u00e4rtig gew\\u00fcnscht wird, unzureichend zu sein.\\nAu\\u00dferdem, \\\n",
    "    Herr Kommissar, haben Wissenschaftler - und ich kann Ihnen versichern, dass ich vor und w\\u00e4hrend \\\n",
    "    der Ausarbeitung dieses Berichts mit vielen gesprochen habe - Probleme bei der Einreichung von Projekten \\\n",
    "    unter dem Siebten Forschungsrahmenprogramm. Diese Probleme sind\"\n",
    "\n",
    "english_prompt = \"given the generally greater adeptness of children at using audio-visual resources, in some \\\n",
    "    areas there are dangers of their obtaining access to unsuitable or harmful material. This is most obvious \\\n",
    "    in the fields of overt sexual material and gratuitous violence.\\nThe principles which have guided this \\\n",
    "    report are to encourage greater public awareness of these issues and to support parental responsibility \\\n",
    "    and to develop co-operation between the content providers, consumer organisations and the \\\n",
    "    respective authorities, both national and European. Self-regulation is considered to be the \\\n",
    "    main instrument, underpinned by legal requirements where necessary.\\nThe report, which \\\n",
    "    analyses the Commission's evaluation report, is primarily concerned with the Internet \\\n",
    "    and with video games, as it was felt important not to anticipate a possible future \\\n",
    "    review of the Television without Frontiers directive. The report calls for user-friendly content filter systems\"\n",
    "\n",
    "# print(\"sample prompt:\", upstream_for_prompt(german_prompt).most_common())\n",
    "n_tokens = model.to_tokens(german_prompt).shape[1]\n",
    "\n",
    "prompts = [] # german_prompt, english_prompt\n",
    "for token in [\" ä\", \" ö\", \" ü\", \" ß\"]:\n",
    "    prompts.append(\"\".join([token for _ in range(n_tokens)]))\n",
    "    print(token, upstream_for_prompt(prompts[-1]).most_common())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the rank correlation within long prompts of different types\n",
    "# Hopefully it's highly correlated\n",
    "# Get the rank correlation between samples or average of prompts of different types\n",
    "spearman = SpearmanCorrCoef()\n",
    "\n",
    "prompt_mean_rhos = torch.zeros(len(prompts))\n",
    "for prompt_n, prompt in enumerate(prompts):\n",
    "    n_tokens = model.to_tokens(prompt).shape[1]\n",
    "    neuron_attrs_by_token, _ = neuron_to_context_neuron_DLA(model, prompt, np.s_[-n_tokens:], (3, 669)) # tokens d_mlp\n",
    "    average_neuron_attrs = neuron_attrs_by_token.mean(dim=0) # d_mlp\n",
    "\n",
    "    rhos = torch.zeros(n_tokens)\n",
    "    for i in range(n_tokens):\n",
    "        rhos[i] = spearman(neuron_attrs_by_token[i], average_neuron_attrs)\n",
    "    prompt_mean_rhos[prompt_n] = rhos.mean()\n",
    "\n",
    "print(prompt_mean_rhos)\n",
    "\n",
    "average_neuron_attrs = []\n",
    "for prompt_n, prompt in enumerate(prompts):\n",
    "    n_tokens = model.to_tokens(prompt).shape[1]\n",
    "    neuron_attrs_by_token, _ = neuron_to_context_neuron_DLA(model, prompt, np.s_[-n_tokens:], (3, 669))\n",
    "    average_neuron_attrs.append(neuron_attrs_by_token.mean(dim=0))\n",
    "    \n",
    "rhos = []\n",
    "for i in range(len(average_neuron_attrs)):\n",
    "    for j in range(i + 1, len(average_neuron_attrs)):\n",
    "        if i == j:\n",
    "            continue\n",
    "        rhos.append(f'{spearman(average_neuron_attrs[i], average_neuron_attrs[j]).item():2f}')\n",
    "\n",
    "print(rhos)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First run\n",
    "\n",
    "\n",
    "# Ablate each neuron in turn and look at how it affects the context neuron value (meaned over all prompts)\n",
    "\n",
    "# Check whether it activates for German words in an English context, both single common german chars and a full word\n",
    "# Ablate each neuron in turn and look at how it affects the context neuron value (meaned over a German token position within an English)\n",
    "\n",
    "# Collect a list\n",
    "\n",
    "# Look for head that moves German tokens\n",
    "# Look for head that moves German n-grams\n",
    "# Look for n-gram detector and see if it moves from there\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure context neuron activation for many German tokens that never coalesce into German words\n",
    "# Measure context neuron activation for English words with a single German token mixed in\n",
    "# Measure above but make it semantically clear in the English that a German token is about to appear\n",
    "# Measure above but with a full German word\n",
    "# Measure above but semantically clear in English\n",
    "\n",
    "# Optional (if time permits):\n",
    "# Measure above but with common German unigrams\n",
    "\n",
    "# Need a way to measure at position\n",
    "\n",
    "with model.hooks(hook_utils.get)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
